<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Spark2.0新特性之Dataset | Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="DataFrame的基本操作实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#employee.json&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Leo&amp;quot;, &amp;quot;age&amp;">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark2.0新特性之Dataset">
<meta property="og:url" content="http://yoursite.com/2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Dataset/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="DataFrame的基本操作实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#employee.json&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Leo&amp;quot;, &amp;quot;age&amp;">
<meta property="og:updated_time" content="2017-04-22T07:23:06.084Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark2.0新特性之Dataset">
<meta name="twitter:description" content="DataFrame的基本操作实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#employee.json&amp;#123;&amp;quot;name&amp;quot;: &amp;quot;Leo&amp;quot;, &amp;quot;age&amp;">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-bigdata/spark从入门到精通_笔记/Spark2.0新特性之Dataset" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Dataset/" class="article-date">
  <time datetime="2017-04-19T02:23:11.944Z" itemprop="datePublished">2017-04-19</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Spark2.0新特性之Dataset
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="DataFrame的基本操作实例"><a href="#DataFrame的基本操作实例" class="headerlink" title="DataFrame的基本操作实例"></a>DataFrame的基本操作实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div></pre></td><td class="code"><pre><div class="line">#employee.json</div><div class="line">&#123;&quot;name&quot;: &quot;Leo&quot;, &quot;age&quot;: 25, &quot;depId&quot;: 1, &quot;gender&quot;: &quot;male&quot;, &quot;salary&quot;: 20000&#125;</div><div class="line">&#123;&quot;name&quot;: &quot;Marry&quot;, &quot;age&quot;: 30, &quot;depId&quot;: 2, &quot;gender&quot;: &quot;female&quot;, &quot;salary&quot;: 25000&#125;</div><div class="line"></div><div class="line">#department.json</div><div class="line">&#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;Technical Department&quot;&#125;</div><div class="line">&#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;Financial Department&quot;&#125;</div><div class="line">&#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;HR Department&quot;&#125;</div><div class="line"></div><div class="line">/*</div><div class="line">需求:</div><div class="line">1.只统计年龄在20岁以上的员工</div><div class="line">2.根据部门和员工性别进行分组,统计出每个部门分性别的平均薪资和年龄</div><div class="line">*/</div><div class="line"></div><div class="line"></div><div class="line">    // 构造SparkSession,基于builder</div><div class="line">    val spark = SparkSession</div><div class="line">      .builder()</div><div class="line">      .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">      .master(&quot;local&quot;)</div><div class="line">      .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">      .getOrCreate()</div><div class="line"></div><div class="line">    //导入spark的隐式转换</div><div class="line">    import spark.implicits._</div><div class="line">    //导入spark sql的functions</div><div class="line">    import org.apache.spark.sql.functions._</div><div class="line"></div><div class="line">    //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">    val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">    val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line">    //进行计算操作</div><div class="line">    employee</div><div class="line">      //首先对employee进行过滤,只统计20岁以上的员工</div><div class="line">      .filter($&quot;age&quot; &gt; 20)</div><div class="line">      //需要跟department数据进行join,注意:untyped join,两个表的字段的连接条件,需要使用三个等号</div><div class="line">      .join(department, $&quot;depId&quot; === $&quot;id&quot;)</div><div class="line">      //根据部门名称和员工性别进行分组</div><div class="line">      .groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;))</div><div class="line">      //执行聚合函数</div><div class="line">      .agg(avg(employee(&quot;salary&quot;)), avg(employee(&quot;age&quot;)))</div><div class="line">     //最后将结构显示出来</div><div class="line">      .show</div><div class="line"></div><div class="line">/*</div><div class="line">+--------------------+------+-----------+--------+</div><div class="line">|                name|gender|avg(salary)|avg(age)|</div><div class="line">+--------------------+------+-----------+--------+</div><div class="line">|       HR Department|female|    21000.0|    21.0|</div><div class="line">|Technical Department|  male|    17500.0|    30.0|</div><div class="line">|Financial Department|female|    26500.0|    30.0|</div><div class="line">|       HR Department|  male|    18000.0|    42.0|</div><div class="line">+--------------------+------+-----------+--------+</div><div class="line"> */</div><div class="line">/*</div><div class="line">总结:</div><div class="line">1.DataFrame==dataset[Row]</div><div class="line">2.DataFrame的类型是Row,所以untyped类型,弱类型</div><div class="line">3.dataset的类型通常是我们自定义的case class ,所以是type类型,强类型</div><div class="line">4.dataset开发,与rdd开发有很多的共同点</div><div class="line">  dataset API也分成transformation和action,transformation是lazy</div><div class="line"> */</div></pre></td></tr></table></figure>
<h1 id="dataset的action操作-collect-count-foreach-reduce"><a href="#dataset的action操作-collect-count-foreach-reduce" class="headerlink" title="dataset的action操作(collect,count,foreach,reduce)"></a>dataset的action操作(collect,count,foreach,reduce)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">// 构造SparkSession,基于builder</div><div class="line">val spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">  .master(&quot;local&quot;)</div><div class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">  .getOrCreate()</div><div class="line"></div><div class="line">//导入spark的隐式转换</div><div class="line">import spark.implicits._</div><div class="line"></div><div class="line">//首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"></div><div class="line">//collect:将存储在集群上的分布式数据集(比如dataset)中的所有数据获取到driver端</div><div class="line">employee.collect.foreach(print)</div><div class="line">//[25,1,male,Leo,20000][30,2,female,Marry,25000][35,1,male,Jack,15000]</div><div class="line"></div><div class="line">//count:对dataset中的记录进行统计个数</div><div class="line">println(employee.count)</div><div class="line"></div><div class="line">//first:获取数据集中的第一条数据</div><div class="line">println(employee.first)</div><div class="line"></div><div class="line">//foreach:遍历数据集中的每一条数据,对数据进行操作,这个跟collect不同,</div><div class="line">//collect是将数据获取到driver端进行操作,而foreach是将计算操作推到集群上去分布式的执行</div><div class="line">//foreach(println(_))这种操作,最终的结果是打印在集群中的各个节点上的</div><div class="line">employee.foreach(println(_))</div><div class="line"></div><div class="line">//reduce:对数据集中的所有的数据进行规约的操作</div><div class="line">employee.map(employee=&gt;1).reduce(_ + _)</div><div class="line"></div><div class="line">//take:从数据集中获取指定条数</div><div class="line">employee.take(3).foreach(println(_))</div></pre></td></tr></table></figure>
<h1 id="基础操作-持久化-临时视图-执行计划-ds-df互转换-写数据"><a href="#基础操作-持久化-临时视图-执行计划-ds-df互转换-写数据" class="headerlink" title="基础操作(持久化,临时视图,执行计划,ds/df互转换,写数据)"></a>基础操作(持久化,临时视图,执行计划,ds/df互转换,写数据)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div></pre></td><td class="code"><pre><div class="line">// 构造SparkSession,基于builder</div><div class="line">val spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">  .master(&quot;local&quot;)</div><div class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">  .getOrCreate()</div><div class="line"></div><div class="line">//导入spark的隐式转换</div><div class="line">import spark.implicits._</div><div class="line"></div><div class="line">//首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"></div><div class="line">//如果要对一个dataset重复计算两次的话,那么建议先对这个dataset进行持久化在进行操作,避免重复计算</div><div class="line">employee.cache</div><div class="line">println(employee.count)</div><div class="line">println(employee.count)</div><div class="line">/*</div><div class="line">17/04/19 15:25:43 INFO DAGScheduler: Job 1 finished: count at TopNBasic.scala:35, took 0.733971 s</div><div class="line">17/04/19 15:25:44 INFO CodeGenerator: Code generated in 18.156697 ms</div><div class="line">7</div><div class="line"></div><div class="line">// ....</div><div class="line"></div><div class="line">17/04/19 15:25:44 INFO DAGScheduler: ResultStage 4 (count at TopNBasic.scala:36) finished in 0.015 s</div><div class="line">17/04/19 15:25:44 INFO DAGScheduler: Job 2 finished: count at TopNBasic.scala:36, took 0.118242 s</div><div class="line">7</div><div class="line"> */</div><div class="line"></div><div class="line">//创建临时视图,主要为了可以直接对数据执行sql语句</div><div class="line">employee.createTempView(&quot;employee&quot;)</div><div class="line">spark.sql(&quot;select * from employee where age&gt;30&quot;).show</div><div class="line">/*</div><div class="line">+---+-----+------+----+------+</div><div class="line">|age|depId|gender|name|salary|</div><div class="line">+---+-----+------+----+------+</div><div class="line">| 35|    1|  male|Jack| 15000|</div><div class="line">| 42|    3|  male| Tom| 18000|</div><div class="line">+---+-----+------+----+------+</div><div class="line"> */</div><div class="line"></div><div class="line">//获取spark sql的执行计划</div><div class="line">spark.sql(&quot;select * from employee where age&gt;30&quot;).explain</div><div class="line">/*</div><div class="line">== Physical Plan ==</div><div class="line">*Filter (isnotnull(age#0L) &amp;&amp; (age#0L &gt; 30))</div><div class="line">+- InMemoryTableScan [age#0L, depId#1L, gender#2, name#3, salary#4L], [isnotnull(age#0L), (age#0L &gt; 30)]</div><div class="line">      +- InMemoryRelation [age#0L, depId#1L, gender#2, name#3, salary#4L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas)</div><div class="line">            +- *FileScan json [age#0L,depId#1L,gender#2,name#3,salary#4L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/C:/Users/Administrator/Desktop/employee.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;age:bigint,depId:bigint,gender:string,name:string,salary:bigint&gt;</div><div class="line">//我们知道带*号的都是自动化生成的(whole-stage-code-generation)</div><div class="line"> */</div><div class="line"></div><div class="line">//查看scheme</div><div class="line">employee.printSchema</div><div class="line">/*</div><div class="line">root</div><div class="line"> |-- age: long (nullable = true)</div><div class="line"> |-- depId: long (nullable = true)</div><div class="line"> |-- gender: string (nullable = true)</div><div class="line"> |-- name: string (nullable = true)</div><div class="line"> |-- salary: long (nullable = true)</div><div class="line"> */</div><div class="line"></div><div class="line">//写数据</div><div class="line">val employeeWithAgeDF = spark.sql(&quot;select * from employee where age&gt;30&quot;)</div><div class="line">employeeWithAgeDF.write.json(&quot;C:\\Users\\Administrator\\Desktop\\employeeWithAge.txt&quot;)</div><div class="line"></div><div class="line">/*在C:\Users\Administrator\Desktop\employeeWithAge.txt目录下</div><div class="line">._SUCCESS.crc</div><div class="line">.part-00000-d683f074-b191-49e4-9725-c92002f25c9f.json.crc</div><div class="line">_SUCCESS</div><div class="line">part-00000-d683f074-b191-49e4-9725-c92002f25c9f.json</div><div class="line"> */</div><div class="line"></div><div class="line">//DataFrame转换成dataset</div><div class="line">//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)</div><div class="line">val employeeDS = employee.as[Employee]</div><div class="line">employeeDS.show()</div><div class="line">employeeDS.printSchema()</div><div class="line">/*</div><div class="line">+---+-----+------+------+------+</div><div class="line">|age|depId|gender|  name|salary|</div><div class="line">+---+-----+------+------+------+</div><div class="line">| 25|    1|  male|   Leo| 20000|</div><div class="line">| 30|    2|female| Marry| 25000|</div><div class="line">| 35|    1|  male|  Jack| 15000|</div><div class="line">| 42|    3|  male|   Tom| 18000|</div><div class="line">| 21|    3|female|Kattie| 21000|</div><div class="line">| 30|    2|female|   Jen| 28000|</div><div class="line">| 19|    2|female|   Jen|  8000|</div><div class="line">+---+-----+------+------+------+</div><div class="line"></div><div class="line">root</div><div class="line"> |-- age: long (nullable = true)</div><div class="line"> |-- depId: long (nullable = true)</div><div class="line"> |-- gender: string (nullable = true)</div><div class="line"> |-- name: string (nullable = true)</div><div class="line"> |-- salary: long (nullable = true)</div><div class="line"> */</div><div class="line"></div><div class="line">//dataset转成DataFrame</div><div class="line">employeeDS.toDF</div></pre></td></tr></table></figure>
<h1 id="type操作-coalesce-reparation"><a href="#type操作-coalesce-reparation" class="headerlink" title="type操作(coalesce,reparation)"></a>type操作(coalesce,reparation)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">// 构造SparkSession,基于builder</div><div class="line">val spark = SparkSession</div><div class="line">  .builder()</div><div class="line">  .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">  .master(&quot;local&quot;)</div><div class="line">  .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">  .getOrCreate()</div><div class="line"></div><div class="line">//导入spark的隐式转换</div><div class="line">import spark.implicits._</div><div class="line"></div><div class="line">//首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">val employeeDS = employee.as[Employee]</div><div class="line"></div><div class="line">println(employeeDS.rdd.partitions.size)</div><div class="line"></div><div class="line">//coalesce和reparation操作,都是用来重新定义分区的</div><div class="line">//区别在于:coalesce只能用于减少分区数据量,而且可以选择不发生shuffle</div><div class="line">//reparation可以增加分区,也可以减少分区,必须会发生shuffle,相当于进行了一次重分区操作</div><div class="line">val employeeDSRepartitioned = employeeDS.repartition(7)</div><div class="line">println(employeeDSRepartitioned.rdd.partitions.size)</div><div class="line"></div><div class="line">val employeeDSCoalesced = employeeDS.coalesce(3)</div><div class="line">println(employeeDSRepartitioned.rdd.partitions.size)</div><div class="line"></div><div class="line">employeeDSCoalesced.show()</div></pre></td></tr></table></figure>
<h1 id="typed操作-distinct和dropDuplicates"><a href="#typed操作-distinct和dropDuplicates" class="headerlink" title="typed操作(distinct和dropDuplicates)"></a>typed操作(distinct和dropDuplicates)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">  // 构造SparkSession,基于builder</div><div class="line">  val spark = SparkSession</div><div class="line">    .builder()</div><div class="line">    .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">    .master(&quot;local&quot;)</div><div class="line">    .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">    .getOrCreate()</div><div class="line"></div><div class="line">  //导入spark的隐式转换</div><div class="line">  import spark.implicits._</div><div class="line"></div><div class="line">//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)</div><div class="line">  //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">  val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">  val employeeDS = employee.as[Employee]</div><div class="line"></div><div class="line">  //distinct和dropDuplicates都是用来去重的</div><div class="line">  //区别在于:</div><div class="line">  //  distinct:是根据每一条数据进行完整内容的对比和去重</div><div class="line">  //  dropDuplicates可以根据指定的字段进行去重</div><div class="line">  employeeDS.distinct.show</div><div class="line">  employeeDS.dropDuplicates(Seq(&quot;name&quot;)).show</div><div class="line">/*</div><div class="line">+---+-----+------+------+------+</div><div class="line">|age|depId|gender|  name|salary|</div><div class="line">+---+-----+------+------+------+</div><div class="line">| 30|    2|female| Marry| 25000|</div><div class="line">| 21|    3|female|Kattie| 21000|</div><div class="line">| 42|    3|  male|   Tom| 18000|</div><div class="line">| 35|    1|  male|  Jack| 15000|</div><div class="line">| 30|    2|female|   Jen| 28000|</div><div class="line">| 19|    2|female|   Jen|  8000|</div><div class="line">| 25|    1|  male|   Leo| 20000|</div><div class="line">+---+-----+------+------+------+</div><div class="line"></div><div class="line"></div><div class="line">+---+-----+------+------+------+</div><div class="line">|age|depId|gender|  name|salary|</div><div class="line">+---+-----+------+------+------+</div><div class="line">| 35|    1|  male|  Jack| 15000|</div><div class="line">| 42|    3|  male|   Tom| 18000|</div><div class="line">| 30|    2|female|   Jen| 28000|</div><div class="line">| 30|    2|female| Marry| 25000|</div><div class="line">| 21|    3|female|Kattie| 21000|</div><div class="line">| 25|    1|  male|   Leo| 20000|</div><div class="line">+---+-----+------+------+------+</div><div class="line">*/</div></pre></td></tr></table></figure>
<h1 id="typed操作-except-filter-interset"><a href="#typed操作-except-filter-interset" class="headerlink" title="typed操作(except,filter,interset)"></a>typed操作(except,filter,interset)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> // 构造SparkSession,基于builder</div><div class="line"> val spark = SparkSession</div><div class="line">   .builder()</div><div class="line">   .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">   .master(&quot;local&quot;)</div><div class="line">   .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">   .getOrCreate()</div><div class="line"></div><div class="line"> //导入spark的隐式转换</div><div class="line"> import spark.implicits._</div><div class="line"></div><div class="line">//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)</div><div class="line"> //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line"> val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"> val employee2 = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee2.json&quot;)</div><div class="line"> val employeeDS = employee.as[Employee]</div><div class="line"> val employeeDS2 = employee2.as[Employee]</div><div class="line"></div><div class="line"> //except:获取当前dataset中有,但是另外一个dataset中没有的元素</div><div class="line"> //filter:根据我们自己的逻辑,如果返回true,那么就保留该元素,否则就过滤掉</div><div class="line"> //intersect:求两个数据集的交集</div><div class="line"> employeeDS.except(employeeDS2).show</div><div class="line"></div><div class="line"> employeeDS.filter(employee =&gt; employee.age &gt; 20)</div><div class="line"></div><div class="line"> employeeDS.intersect(employeeDS2)</div></pre></td></tr></table></figure>
<h1 id="typed-map-flatMap-mapPartitions"><a href="#typed-map-flatMap-mapPartitions" class="headerlink" title="typed(map,flatMap,mapPartitions)"></a>typed(map,flatMap,mapPartitions)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)</div><div class="line">case class Department(id:Long, name:String)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"> // 构造SparkSession,基于builder</div><div class="line"> val spark = SparkSession</div><div class="line">   .builder()</div><div class="line">   .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">   .master(&quot;local&quot;)</div><div class="line">   .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">   .getOrCreate()</div><div class="line"></div><div class="line"> //导入spark的隐式转换</div><div class="line"> import spark.implicits._</div><div class="line"></div><div class="line"> //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line"> val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"> val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line"> val employeeDS = employee.as[Employee]</div><div class="line"> val departmentDS = department.as[Department]</div><div class="line"> //map:将数据集中的每条数据都做一个映射,返回一条新数据</div><div class="line"> //flatMap:数据集中的每条数据都可以返回多条数据,然后进行压平</div><div class="line"> //mapPartitions:一次性对一个partition中的数据进行处理</div><div class="line"></div><div class="line"> employeeDS.map(employee =&gt; (employee.name, employee.age, employee.salary+1000))</div><div class="line"></div><div class="line"> departmentDS.flatMap&#123;</div><div class="line">   department=&gt;</div><div class="line">     Seq(Department(department.id+1, department.name+&quot;_1&quot;), Department(department.id+2, department.name+&quot;_2&quot;))</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> employeeDS.mapPartitions&#123;</div><div class="line">   employeeIter=&gt;</div><div class="line">     val result = mutable.ArrayBuffer[(String,Long)]()</div><div class="line">     while(employeeIter.hasNext)&#123;</div><div class="line">       var emp = employeeIter.next</div><div class="line">       result += ((emp.name, emp.salary+1000))</div><div class="line">     &#125;</div><div class="line">     //需要将集合转成迭代器返回</div><div class="line">     result.toIterator</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<h1 id="typed-joinwith-sort"><a href="#typed-joinwith-sort" class="headerlink" title="typed(joinwith,sort)"></a>typed(joinwith,sort)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)</div><div class="line">case class Department(id:Long, name:String)</div><div class="line"></div><div class="line"></div><div class="line">    // 构造SparkSession,基于builder</div><div class="line">    val spark = SparkSession</div><div class="line">      .builder()</div><div class="line">      .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">      .master(&quot;local&quot;)</div><div class="line">      .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">      .getOrCreate()</div><div class="line"></div><div class="line">    //导入spark的隐式转换</div><div class="line">    import spark.implicits._</div><div class="line"></div><div class="line">    //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">    val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">    val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line">    val employeeDS = employee.as[Employee]</div><div class="line">    val departmentDS = department.as[Department]</div><div class="line"></div><div class="line">    employeeDS.joinWith(departmentDS, $&quot;depId&quot;===$&quot;id&quot;).show</div><div class="line"></div><div class="line">	employeeDS.sort($&quot;salary&quot;.desc).show</div></pre></td></tr></table></figure>
<h1 id="typed-randomSplit-sample"><a href="#typed-randomSplit-sample" class="headerlink" title="typed(randomSplit,sample)"></a>typed(randomSplit,sample)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">randomSplit:将一个数据集随机切分成几份数据集</div><div class="line">sample:按照指定的比例随机抽取出来一些数据</div><div class="line"></div><div class="line"></div><div class="line">    // 构造SparkSession,基于builder</div><div class="line">    val spark = SparkSession</div><div class="line">      .builder()</div><div class="line">      .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">      .master(&quot;local&quot;)</div><div class="line">      .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">      .getOrCreate()</div><div class="line"></div><div class="line">    //导入spark的隐式转换</div><div class="line">    import spark.implicits._</div><div class="line"></div><div class="line">    //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">    val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">    val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line">    val employeeDS = employee.as[Employee]</div><div class="line">    val departmentDS = department.as[Department]</div><div class="line"></div><div class="line">    //切成3分,每份的权重为2,10,20</div><div class="line">    employeeDS.randomSplit(Array(2,10,20))</div><div class="line"></div><div class="line">    //随机抽取数据总量的0.3的比率</div><div class="line">    employeeDS.sample(false, 0.3).show</div></pre></td></tr></table></figure>
<h1 id="untyped操作-select-where-groupBy-agg-col-join"><a href="#untyped操作-select-where-groupBy-agg-col-join" class="headerlink" title="untyped操作(select,where,groupBy,agg,col,join)"></a>untyped操作(select,where,groupBy,agg,col,join)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">untyped操作:实际上就涵盖了普通sql语法的全部了</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">    // 构造SparkSession,基于builder</div><div class="line">    val spark = SparkSession</div><div class="line">      .builder()</div><div class="line">      .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">      .master(&quot;local&quot;)</div><div class="line">      .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">      .getOrCreate()</div><div class="line"></div><div class="line">    //导入spark的隐式转换</div><div class="line">    import spark.implicits._</div><div class="line">    import org.apache.spark.sql.functions._</div><div class="line"></div><div class="line">    //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line">    val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line">    val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line">    employee</div><div class="line">      .where(&quot;age &gt; 20&quot;)</div><div class="line">      .join(department, $&quot;depId&quot; === $&quot;id&quot;)</div><div class="line">      .groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;))</div><div class="line">      //要使用avg函数,需要导入org.apache.spark.sql.functions._</div><div class="line">      .agg(avg(employee(&quot;salary&quot;)))</div><div class="line">      .show</div><div class="line"></div><div class="line">    employee</div><div class="line">      .select($&quot;name&quot;, $&quot;depId&quot;, $&quot;salary&quot;)</div><div class="line">      .where(&quot;age&gt;30&quot;)</div><div class="line">      .show</div></pre></td></tr></table></figure>
<h1 id="聚合函数-avg-sum-max-min-count-countDistinct"><a href="#聚合函数-avg-sum-max-min-count-countDistinct" class="headerlink" title="聚合函数(avg,sum,max,min,count,countDistinct)"></a>聚合函数(avg,sum,max,min,count,countDistinct)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"> // 构造SparkSession,基于builder</div><div class="line"> val spark = SparkSession</div><div class="line">   .builder()</div><div class="line">   .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">   .master(&quot;local&quot;)</div><div class="line">   .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">   .getOrCreate()</div><div class="line"></div><div class="line"> //导入spark的隐式转换</div><div class="line"> import spark.implicits._</div><div class="line"> import org.apache.spark.sql.functions._</div><div class="line"></div><div class="line"> //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line"> val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"> val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line"> employee</div><div class="line">   .join(department, $&quot;depId&quot;===$&quot;id&quot;)</div><div class="line">   .groupBy(department(&quot;name&quot;))</div><div class="line">   //要使用avg函数,需要导入org.apache.spark.sql.functions._</div><div class="line">   .agg(avg(employee(&quot;salary&quot;)), sum(employee(&quot;salary&quot;)), max(employee(&quot;salary&quot;)), min(employee(&quot;salary&quot;)), count(employee(&quot;name&quot;)), countDistinct(employee(&quot;name&quot;)))</div><div class="line">   .show</div><div class="line"></div><div class="line">/*</div><div class="line">+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+</div><div class="line">|                name|       avg(salary)|sum(salary)|max(salary)|min(salary)|count(name)|count(DISTINCT name)|</div><div class="line">+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+</div><div class="line">|Technical Department|           17500.0|      35000|      20000|      15000|          2|                   2|</div><div class="line">|       HR Department|           19500.0|      39000|      21000|      18000|          2|                   2|</div><div class="line">|Financial Department|20333.333333333332|      61000|      28000|       8000|          3|                   2|</div><div class="line">+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+</div><div class="line"> */</div></pre></td></tr></table></figure>
<h1 id="聚合函数-collect-list-collect-set"><a href="#聚合函数-collect-list-collect-set" class="headerlink" title="聚合函数(collect_list, collect_set)"></a>聚合函数(collect_list, collect_set)</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">collect_list:就是将一个分组内,指定字段的值都收集到一起变成一个数组,不去重</div><div class="line">collect_set:同上,唯一的区别是,会去重</div><div class="line"></div><div class="line">常用于行转列</div><div class="line">例如:</div><div class="line">depId=1,employe=leo</div><div class="line">depId=1,employe=jack</div><div class="line">depId=1,employe=[leo,jack]</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"> // 构造SparkSession,基于builder</div><div class="line"> val spark = SparkSession</div><div class="line">   .builder()</div><div class="line">   .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">   .master(&quot;local&quot;)</div><div class="line">   .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">   .getOrCreate()</div><div class="line"></div><div class="line"> //导入spark的隐式转换</div><div class="line"> import spark.implicits._</div><div class="line"> import org.apache.spark.sql.functions._</div><div class="line"></div><div class="line"> //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line"> val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"> val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line"> employee</div><div class="line">   .groupBy(employee(&quot;depId&quot;))</div><div class="line">   .agg(collect_list(employee(&quot;name&quot;)), collect_set(employee(&quot;name&quot;)))</div><div class="line">   .show</div><div class="line"></div><div class="line"> /*</div><div class="line"> +-----+------------------+-----------------+</div><div class="line"> |depId|collect_list(name)|collect_set(name)|</div><div class="line"> +-----+------------------+-----------------+</div><div class="line"> |    1|       [Leo, Jack]|      [Jack, Leo]|</div><div class="line"> |    3|     [Tom, Kattie]|    [Tom, Kattie]|</div><div class="line"> |    2| [Marry, Jen, Jen]|     [Marry, Jen]|</div><div class="line"> +-----+------------------+-----------------+</div><div class="line">  */</div></pre></td></tr></table></figure>
<h1 id="其他常用的函数"><a href="#其他常用的函数" class="headerlink" title="其他常用的函数"></a>其他常用的函数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line">日期函数:current_date, current_timestamp</div><div class="line">数学函数:round</div><div class="line">随机函数:rand</div><div class="line">字符串函数:concat, concat_ws</div><div class="line">自定义函数udf和自定义聚合函数udaf</div><div class="line"></div><div class="line">需要的时候去查看官网的API</div><div class="line">http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$</div><div class="line">*/</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"> // 构造SparkSession,基于builder</div><div class="line"> val spark = SparkSession</div><div class="line">   .builder()</div><div class="line">   .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;)</div><div class="line">   .master(&quot;local&quot;)</div><div class="line">   .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\spark-warehouse&quot;)</div><div class="line">   .getOrCreate()</div><div class="line"></div><div class="line"> //导入spark的隐式转换</div><div class="line"> import spark.implicits._</div><div class="line"> import org.apache.spark.sql.functions._</div><div class="line"></div><div class="line"> //首先将两份数据文件加载进行,形成两个DataFrame</div><div class="line"> val employee = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\employee.json&quot;)</div><div class="line"> val department = spark.read.json(&quot;C:\\Users\\Administrator\\Desktop\\department.json&quot;)</div><div class="line"></div><div class="line"> employee</div><div class="line">   .select(employee(&quot;name&quot;),</div><div class="line">     current_date,</div><div class="line">     current_timestamp,</div><div class="line">     rand,</div><div class="line">     round(employee(&quot;salary&quot;),2),</div><div class="line">     concat(employee(&quot;gender&quot;), employee(&quot;age&quot;)),</div><div class="line">     concat_ws(&quot;|&quot;,employee(&quot;gender&quot;), employee(&quot;age&quot;))</div><div class="line">   ).show</div><div class="line"></div><div class="line">/*</div><div class="line">+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+</div><div class="line">|  name|current_date()| current_timestamp()|rand(9062034925792708500)|round(salary, 2)|concat(gender, age)|concat_ws(|, gender, age)|</div><div class="line">+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+</div><div class="line">|   Leo|    2017-04-19|2017-04-19 17:25:...|       0.8536958083571142|           20000|             male25|                  male|25|</div><div class="line">| Marry|    2017-04-19|2017-04-19 17:25:...|      0.10866516208665833|           25000|           female30|                female|30|</div><div class="line">|  Jack|    2017-04-19|2017-04-19 17:25:...|       0.6128816303412895|           15000|             male35|                  male|35|</div><div class="line">|   Tom|    2017-04-19|2017-04-19 17:25:...|       0.9614274109004534|           18000|             male42|                  male|42|</div><div class="line">|Kattie|    2017-04-19|2017-04-19 17:25:...|      0.10936129046706444|           21000|           female21|                female|21|</div><div class="line">|   Jen|    2017-04-19|2017-04-19 17:25:...|      0.25947595067767937|           28000|           female30|                female|30|</div><div class="line">|   Jen|    2017-04-19|2017-04-19 17:25:...|      0.12866036956519833|            8000|           female19|                female|19|</div><div class="line">+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Dataset/" data-id="cj290sc5g00xlssqqph73grf9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之SparkSession,DataFrame,Dataset/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Spark2.0新特性之SparkSession,DataFrame,Dataset
        
      </div>
    </a>
  
  
    <a href="/2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Structured Streaming/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Spark2.0新特性之Structured Streaming</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>