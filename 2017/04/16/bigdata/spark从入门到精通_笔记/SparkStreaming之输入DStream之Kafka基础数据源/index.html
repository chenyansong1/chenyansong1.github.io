<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="spark," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkStreaming之输入DStream之Kafka基础数据源">
<meta property="og:url" content="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">
<meta property="og:updated_time" content="2017-04-22T07:23:06.282Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkStreaming之输入DStream之Kafka基础数据源">
<meta name="twitter:description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"right","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/"/>





  <title> SparkStreaming之输入DStream之Kafka基础数据源 | Chen's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-right page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一个技术渣的自说自话</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                SparkStreaming之输入DStream之Kafka基础数据源
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="基于Receiver的方式"><a href="#基于Receiver的方式" class="headerlink" title="基于Receiver的方式"></a>基于Receiver的方式</h1><p>这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据</p>
<p>然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用spark streaming 的预写日志机制(Write Ahead Log,WAL),该机制会tongue的将接收到的kafka数据写入分布式文件系统(比如HDFS)山的预写日志中,所以即使底层节点出现了失败,也可以使用预写日志中的数据进行恢复</p>
<p>前提:<br>1.maven添加依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.6.3&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<p>2.使用第三方工具类创建输入DStream<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KafkaUtils.createStream(StreamingContext,[ZK quorum], [consumer group id], [per-topic number of kakfa partitions to consume])</div></pre></td></tr></table></figure></p>
<p>注意事项:<br>1.kafka的topic的partition,与spark中的RDD的partition是没有关系的,所以在KafkaUtils.createStream()中,提高partition的数量,只会增加一个Receiver中读取partition的线程的数量,不会增加spark处理数据的并行度<br>2.可以创建多个kafka输入DStream,使用不同的consumer group和topic,来通过多个receiver并行接收数据<br>3.如果基于容错的文件系统,比如HDFS,启用了预写日志机制,接收到的数据都会被复制一份到预写日志中,因此在KafkaUtils.createStream()中,设置的持久化级别是:StorageLevel.MEMORY_AND_DISK_SER_2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">//创建topic</div><div class="line">bin/kafka-topic.sh --zookeeper zk01:2181,zk02:2181,zk03:2181 --topic WordCount --replication-factor 1 --partitions 1 --create</div><div class="line"></div><div class="line"></div><div class="line">//创建consumer生产者</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092, --topic WordCount</div></pre></td></tr></table></figure>
<p>实例代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序</div><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  topicThreadMap = Map(</div><div class="line">  &quot;WordCount&quot;-&gt;1</div><div class="line">)</div><div class="line"></div><div class="line">// zk是zookeeper的节点地址</div><div class="line">// DefalutConsumerGroup是kafka的groupId</div><div class="line">// topicThreadMap是指定去消费哪个topic</div><div class="line">//Map of (topic_name -&gt; numPartitions) to consume. Each partition is consumed in its own thread</div><div class="line">// topic名字-&gt;分区数量,每个分区将会启动一个Receiver线程去消费(而一个Receiver需要一个cpu core)</div><div class="line">val lines = KafkaUtils.createStream(ssc,zk,&quot;DefalutConsumerGroup&quot;,topicThreadMap)</div><div class="line"></div><div class="line">// 这里需要注意的是lines中是Tuple(index,line)这样的数据,所以_._2就是一行的的数据</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure>
<h1 id="基于Direct的方式"><a href="#基于Direct的方式" class="headerlink" title="基于Direct的方式"></a>基于Direct的方式</h1><p>这种是不基于Receiver的直接方式,是在spark1.3中引入,从而能够确保更加健壮的机制,替代掉使用Receiver来接收数据后,这种方式会周期性(就是我们指定的batch的时间)的查询Kafka,来获取每个topic+partition的最新的offset,从而定义每个batch的offset的范围(而每个batch会形成一个Rdd),当处理数据的job启动时,就会使用kafka的简单consumer API来获取kafka指定offset范围的数据,这就得到了这个Rdd的数据</p>
<p>这种方式有如下的优点:<br>1.简化并行读取,如果要有多个partition,不需要创建多个输入DStream然后对他们进行union操作,spark会创建跟kafka partition一样多的Rdd partition,并且会并行从kafka中读取数据,所在kafka partition和RDD partition之间,有一个一对一的映射关系<br>2.高性能:如果要保证零数据丢失,在语句Receiver的方式中,需要开启WAL机制,这种方式其实效率低下,因为数据实际上被复制了两份,kafka自己本身就有高可靠的机制,会对数据复制一份,而这里又会复制一份到WAL中,而基于direct的方式,不依赖Receiver,不需要开启WAL机制,只要kafka中作了数的复制,那么就可以通过kafka的副本进行恢复<br>3.一次仅且一次的事务机制<br>基于Receiver的方式,是使用kafka的高阶API来在zookeeper中保存消费过的offset,这是消费kafka数据的传统的方式,这种方式配合着WAL机制可以保证数据零丢失的高可靠性,但是却无法保证数据被处理一次且仅一次,可能会处理两次,因为spark和zookeeper之间可能是不同步的</p>
<p>基于direct的方式,使用kafka的简单API,spark streaming自己会负责追踪消费的offset并保存在checkpoint中,spark自己一定是同步的,因此可以保证数据是消费一次且仅消费一次</p>
<p>createDirectStream()方法参数说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">   *</div><div class="line">   * @param ssc StreamingContext object	这里是传入的一个StreamingContext</div><div class="line">   * @param kafkaParams Kafka &lt;a href=&quot;http://kafka.apache.org/documentation.html#configuration&quot;&gt;</div><div class="line">   *   configuration parameters&lt;/a&gt;. Requires &quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot; 必须要指定:&quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot;中的一个</div><div class="line">   *   to be set with Kafka broker(s) (NOT zookeeper servers), specified in</div><div class="line">   *   host1:port1,host2:port2 form.	//指定的格式</div><div class="line">   *   If not starting from a checkpoint, &quot;auto.offset.reset&quot; may be set to &quot;largest&quot; or &quot;smallest&quot;	//如果没有初始化的offset,那么从哪里开始消费(largest从头开始,smallest:从最近开始消费)</div><div class="line">   *   to determine where the stream starts (defaults to &quot;largest&quot;)</div><div class="line">   *   如果开始消费的数据不是从checkpoint中开始的,那么使用&quot;auto.offset.reset&quot; 参数设置成&quot;largest&quot; or &quot;smallest&quot;来决定从Stream流的哪里开始消费数据</div><div class="line">   * @param topics Names of the topics to consume	topic名称</div><div class="line">   * @tparam K type of Kafka message key	消息key的类型</div><div class="line">   * @tparam V type of Kafka message value	消息value的类型</div><div class="line">   * @tparam KD type of Kafka message key decoder	key的编码格式</div><div class="line">   * @tparam VD type of Kafka message value decoder	value的编码格式</div><div class="line">   * @return DStream of (Kafka message key, Kafka message value)</div><div class="line">   */</div><div class="line">  def createDirectStream[</div><div class="line">    K: ClassTag,</div><div class="line">    V: ClassTag,</div><div class="line">    KD &lt;: Decoder[K]: ClassTag,</div><div class="line">    VD &lt;: Decoder[V]: ClassTag] (</div><div class="line">      ssc: StreamingContext,</div><div class="line">      kafkaParams: Map[String, String],</div><div class="line">      topics: Set[String]</div><div class="line">  ): InputDStream[(K, V)]</div><div class="line"></div><div class="line"></div><div class="line">=========================================</div><div class="line">在kafka中对auto.offset.reset参数的解释是:</div><div class="line"></div><div class="line">What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):</div><div class="line">当没有初始化的offset的时候,此时该从哪里读取数据</div><div class="line"></div><div class="line">earliest: automatically reset the offset to the earliest offset	设置offset为最开始的处,即从头开始消费</div><div class="line">latest: automatically reset the offset to the latest offset	从设置offset为最近的offset</div><div class="line">none: throw exception to the consumer if no previous offset is found for the consumer&apos;s group</div><div class="line">anything else: throw exception to the consumer.</div></pre></td></tr></table></figure>
<p>实例代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  kafkaParams = Map(</div><div class="line">  // kafka的broker-list</div><div class="line">  &quot;meta.broker.list&quot;-&gt;&quot;192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092&quot;,</div><div class="line">)</div><div class="line"></div><div class="line">val topics = Set(</div><div class="line">  &quot;WordCount&quot;</div><div class="line">)</div><div class="line"></div><div class="line">val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams,topics)</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/spark/" rel="tag"># spark</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/" rel="next" title="SparkStreaming之输入DStream之基础数据源">
                <i class="fa fa-chevron-left"></i> SparkStreaming之输入DStream之基础数据源
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/" rel="prev" title="SparkStreaming之架构原理">
                SparkStreaming之架构原理 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/header.jpg"
               alt="Mr. Chen" />
          <p class="site-author-name" itemprop="name">Mr. Chen</p>
           
              <p class="site-description motion-element" itemprop="description">一个技术渣的自说自话</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">576</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">30</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">37</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#基于Receiver的方式"><span class="nav-number">1.</span> <span class="nav-text">基于Receiver的方式</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基于Direct的方式"><span class="nav-number">2.</span> <span class="nav-text">基于Direct的方式</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr. Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  

  

  

  

</body>
</html>
