<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>SparkStreaming之输入DStream之Kafka基础数据源 | Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="SparkStreaming之输入DStream之Kafka基础数据源">
<meta property="og:url" content="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">
<meta property="og:updated_time" content="2017-04-22T07:23:06.282Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="SparkStreaming之输入DStream之Kafka基础数据源">
<meta name="twitter:description" content="基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用s">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/" class="article-date">
  <time datetime="2017-04-16T04:47:25.230Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      SparkStreaming之输入DStream之Kafka基础数据源
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="基于Receiver的方式"><a href="#基于Receiver的方式" class="headerlink" title="基于Receiver的方式"></a>基于Receiver的方式</h1><p>这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据</p>
<p>然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用spark streaming 的预写日志机制(Write Ahead Log,WAL),该机制会tongue的将接收到的kafka数据写入分布式文件系统(比如HDFS)山的预写日志中,所以即使底层节点出现了失败,也可以使用预写日志中的数据进行恢复</p>
<p>前提:<br>1.maven添加依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.6.3&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<p>2.使用第三方工具类创建输入DStream<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KafkaUtils.createStream(StreamingContext,[ZK quorum], [consumer group id], [per-topic number of kakfa partitions to consume])</div></pre></td></tr></table></figure></p>
<p>注意事项:<br>1.kafka的topic的partition,与spark中的RDD的partition是没有关系的,所以在KafkaUtils.createStream()中,提高partition的数量,只会增加一个Receiver中读取partition的线程的数量,不会增加spark处理数据的并行度<br>2.可以创建多个kafka输入DStream,使用不同的consumer group和topic,来通过多个receiver并行接收数据<br>3.如果基于容错的文件系统,比如HDFS,启用了预写日志机制,接收到的数据都会被复制一份到预写日志中,因此在KafkaUtils.createStream()中,设置的持久化级别是:StorageLevel.MEMORY_AND_DISK_SER_2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">//创建topic</div><div class="line">bin/kafka-topic.sh --zookeeper zk01:2181,zk02:2181,zk03:2181 --topic WordCount --replication-factor 1 --partitions 1 --create</div><div class="line"></div><div class="line"></div><div class="line">//创建consumer生产者</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092, --topic WordCount</div></pre></td></tr></table></figure>
<p>实例代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序</div><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  topicThreadMap = Map(</div><div class="line">  &quot;WordCount&quot;-&gt;1</div><div class="line">)</div><div class="line"></div><div class="line">// zk是zookeeper的节点地址</div><div class="line">// DefalutConsumerGroup是kafka的groupId</div><div class="line">// topicThreadMap是指定去消费哪个topic</div><div class="line">//Map of (topic_name -&gt; numPartitions) to consume. Each partition is consumed in its own thread</div><div class="line">// topic名字-&gt;分区数量,每个分区将会启动一个Receiver线程去消费(而一个Receiver需要一个cpu core)</div><div class="line">val lines = KafkaUtils.createStream(ssc,zk,&quot;DefalutConsumerGroup&quot;,topicThreadMap)</div><div class="line"></div><div class="line">// 这里需要注意的是lines中是Tuple(index,line)这样的数据,所以_._2就是一行的的数据</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure>
<h1 id="基于Direct的方式"><a href="#基于Direct的方式" class="headerlink" title="基于Direct的方式"></a>基于Direct的方式</h1><p>这种是不基于Receiver的直接方式,是在spark1.3中引入,从而能够确保更加健壮的机制,替代掉使用Receiver来接收数据后,这种方式会周期性(就是我们指定的batch的时间)的查询Kafka,来获取每个topic+partition的最新的offset,从而定义每个batch的offset的范围(而每个batch会形成一个Rdd),当处理数据的job启动时,就会使用kafka的简单consumer API来获取kafka指定offset范围的数据,这就得到了这个Rdd的数据</p>
<p>这种方式有如下的优点:<br>1.简化并行读取,如果要有多个partition,不需要创建多个输入DStream然后对他们进行union操作,spark会创建跟kafka partition一样多的Rdd partition,并且会并行从kafka中读取数据,所在kafka partition和RDD partition之间,有一个一对一的映射关系<br>2.高性能:如果要保证零数据丢失,在语句Receiver的方式中,需要开启WAL机制,这种方式其实效率低下,因为数据实际上被复制了两份,kafka自己本身就有高可靠的机制,会对数据复制一份,而这里又会复制一份到WAL中,而基于direct的方式,不依赖Receiver,不需要开启WAL机制,只要kafka中作了数的复制,那么就可以通过kafka的副本进行恢复<br>3.一次仅且一次的事务机制<br>基于Receiver的方式,是使用kafka的高阶API来在zookeeper中保存消费过的offset,这是消费kafka数据的传统的方式,这种方式配合着WAL机制可以保证数据零丢失的高可靠性,但是却无法保证数据被处理一次且仅一次,可能会处理两次,因为spark和zookeeper之间可能是不同步的</p>
<p>基于direct的方式,使用kafka的简单API,spark streaming自己会负责追踪消费的offset并保存在checkpoint中,spark自己一定是同步的,因此可以保证数据是消费一次且仅消费一次</p>
<p>createDirectStream()方法参数说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">   *</div><div class="line">   * @param ssc StreamingContext object	这里是传入的一个StreamingContext</div><div class="line">   * @param kafkaParams Kafka &lt;a href=&quot;http://kafka.apache.org/documentation.html#configuration&quot;&gt;</div><div class="line">   *   configuration parameters&lt;/a&gt;. Requires &quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot; 必须要指定:&quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot;中的一个</div><div class="line">   *   to be set with Kafka broker(s) (NOT zookeeper servers), specified in</div><div class="line">   *   host1:port1,host2:port2 form.	//指定的格式</div><div class="line">   *   If not starting from a checkpoint, &quot;auto.offset.reset&quot; may be set to &quot;largest&quot; or &quot;smallest&quot;	//如果没有初始化的offset,那么从哪里开始消费(largest从头开始,smallest:从最近开始消费)</div><div class="line">   *   to determine where the stream starts (defaults to &quot;largest&quot;)</div><div class="line">   *   如果开始消费的数据不是从checkpoint中开始的,那么使用&quot;auto.offset.reset&quot; 参数设置成&quot;largest&quot; or &quot;smallest&quot;来决定从Stream流的哪里开始消费数据</div><div class="line">   * @param topics Names of the topics to consume	topic名称</div><div class="line">   * @tparam K type of Kafka message key	消息key的类型</div><div class="line">   * @tparam V type of Kafka message value	消息value的类型</div><div class="line">   * @tparam KD type of Kafka message key decoder	key的编码格式</div><div class="line">   * @tparam VD type of Kafka message value decoder	value的编码格式</div><div class="line">   * @return DStream of (Kafka message key, Kafka message value)</div><div class="line">   */</div><div class="line">  def createDirectStream[</div><div class="line">    K: ClassTag,</div><div class="line">    V: ClassTag,</div><div class="line">    KD &lt;: Decoder[K]: ClassTag,</div><div class="line">    VD &lt;: Decoder[V]: ClassTag] (</div><div class="line">      ssc: StreamingContext,</div><div class="line">      kafkaParams: Map[String, String],</div><div class="line">      topics: Set[String]</div><div class="line">  ): InputDStream[(K, V)]</div><div class="line"></div><div class="line"></div><div class="line">=========================================</div><div class="line">在kafka中对auto.offset.reset参数的解释是:</div><div class="line"></div><div class="line">What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):</div><div class="line">当没有初始化的offset的时候,此时该从哪里读取数据</div><div class="line"></div><div class="line">earliest: automatically reset the offset to the earliest offset	设置offset为最开始的处,即从头开始消费</div><div class="line">latest: automatically reset the offset to the latest offset	从设置offset为最近的offset</div><div class="line">none: throw exception to the consumer if no previous offset is found for the consumer&apos;s group</div><div class="line">anything else: throw exception to the consumer.</div></pre></td></tr></table></figure>
<p>实例代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  kafkaParams = Map(</div><div class="line">  // kafka的broker-list</div><div class="line">  &quot;meta.broker.list&quot;-&gt;&quot;192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092&quot;,</div><div class="line">)</div><div class="line"></div><div class="line">val topics = Set(</div><div class="line">  &quot;WordCount&quot;</div><div class="line">)</div><div class="line"></div><div class="line">val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams,topics)</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/" data-id="cj290sc9c0110ssqqgdva8izn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          SparkStreaming之架构原理
        
      </div>
    </a>
  
  
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">SparkStreaming之输入DStream之基础数据源</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>