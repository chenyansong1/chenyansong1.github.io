<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://yoursite.com">
  <title>第3章 SparkContext的初始化 | Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="SparkContext的初始化是Driver应用程序提交执行的前提,本章内容以local模式为主,并按照代码执行顺序讲解">
<meta name="keywords" content="spark">
<meta property="og:type" content="article">
<meta property="og:title" content="第3章 SparkContext的初始化">
<meta property="og:url" content="http://yoursite.com/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="SparkContext的初始化是Driver应用程序提交执行的前提,本章内容以local模式为主,并按照代码执行顺序讲解">
<meta property="og:image" content="http://yoursite.com/assert/img/bigdata/深入理解spark核心思想与源码分析/2/SparkUI.png">
<meta property="og:image" content="http://yoursite.com/assert/img/bigdata/深入理解spark核心思想与源码分析/2/SparkListener.png">
<meta property="og:image" content="http://yoursite.com/assert/img/bigdata/深入理解spark核心思想与源码分析/2/heartbeat.png">
<meta property="og:updated_time" content="2017-03-27T07:35:23.005Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="第3章 SparkContext的初始化">
<meta name="twitter:description" content="SparkContext的初始化是Driver应用程序提交执行的前提,本章内容以local模式为主,并按照代码执行顺序讲解">
<meta name="twitter:image" content="http://yoursite.com/assert/img/bigdata/深入理解spark核心思想与源码分析/2/SparkUI.png">
  
    <link rel="alternative" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link rel="stylesheet" type="text/css" href="/./main.2d7529.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#43babf,#e8c37e);
    }
  </style>
  

  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?b7ea673c0cc033955a01815f3e3c84d9";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #4d4d4d"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/header.png" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Mr. Chen</a></h1>
		</hgroup>
		
		<p class="header-subtitle">一个技术渣的自说自话</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/archives">归档</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/chenyansong1" title="github"><i class="icon-github"></i></a>
		        
					<a class="weibo" target="_blank" href="http://112.74.38.21:8080/blogweb/" title="weibo"><i class="icon-weibo"></i></a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/chen-he-57" title="zhihu"><i class="icon-zhihu"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #4d4d4d"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/header.png" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Mr. Chen</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>一个技术渣的自说自话<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/chenyansong1" title="github"><i class="icon-github"></i></a>
			        
						<a class="weibo" target="_blank" href="http://112.74.38.21:8080/blogweb/" title="weibo"><i class="icon-weibo"></i></a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/chen-he-57" title="zhihu"><i class="icon-zhihu"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/archives">归档</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      第3章 SparkContext的初始化
    </h1>
  

        
        <a href="/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化/" class="archive-article-date">
  	<time datetime="2017-04-16T04:47:25.359Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-04-16</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>SparkContext的初始化是Driver应用程序提交执行的前提,本章内容以local模式为主,并按照代码执行顺序讲解</p>
<a id="more"></a>
<h1 id="1-SparkContext概述"><a href="#1-SparkContext概述" class="headerlink" title="1.SparkContext概述"></a>1.SparkContext概述</h1><p>Spark Driver用于提交用户应用程序,实际可以看做Spark的客户端,Spark Driver的初始化始终围绕着SparkContext的初始化,SparkContext可以算得上是所有Spark应用程序的发动机引擎,轿车要想跑起来,发送机首先要启动,SparkContext初始化完毕,才能向Spark集群提交任务,在平坦的公路上,发送机只需要以较低的转速,较低的功率就可以游刃有余,而在山区中,可能需要一台能够提供大功率的发动机才能满足你的需求,这些参数都是通过驾驶员操作油门,档位等传送给发送机的,而SparkContext的配置参数则由SparkConf负责,SparkConf就是你的操作面板</p>
<p>SparkConf的构造很简单,主要是通过ConcurrentHashMap来维护各种Spark的配置属性,SparkConf代码结构如下,spark的配置属性都是以”spark.”开头的字符串</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging &#123;</div><div class="line"></div><div class="line">  import SparkConf._</div><div class="line"></div><div class="line">  /** Create a SparkConf that loads defaults from system properties and the classpath */</div><div class="line">  def this() = this(true)</div><div class="line"></div><div class="line">  private val settings = new ConcurrentHashMap[String, String]()</div><div class="line"></div><div class="line">  if (loadDefaults) &#123;</div><div class="line">    //加载任何以spark.开头的系统属性</div><div class="line">    for ((key, value) &lt;- Utils.getSystemProperties if key.startsWith(&quot;spark.&quot;)) &#123;</div><div class="line">      set(key, value)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  //其余代码省略 </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>下面是SparkContext的初始化步骤:<br>1.创建Spark执行环境SparkEnv<br>2.创建RDD清理器metadataCleaner<br>3.创建并初始化SparkUI<br>4.Hadoop相关配置文件及Executor环境变量的设置<br>5.创建任务调度TaskScheduler<br>6.创建和启动DAGScheduler<br>7.TaskScheduler的启动<br>8.初始化块管理器BlockManager(BlockManager是存储系统的主要组件之一,后面介绍)<br>9.启动测量系统MetricsSystem<br>10.创建和启动Executor分配管理器ExecutorAllocationManager<br>11.ContextCleaner的创建与启动<br>12.Spark环境更新<br>13.创建DAGSchedulerSource和BlockManagerSource<br>14.将SparkContext标记为激活</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">class SparkContext(config: SparkConf) extends Logging with ExecutorAllocationClient &#123;</div><div class="line"></div><div class="line">  // The call site where this SparkContext was constructed.</div><div class="line">  private val creationSite: CallSite = Utils.getCallSite()</div><div class="line"></div><div class="line">  // If true, log warnings instead of throwing exceptions when multiple SparkContexts are active</div><div class="line">  private val allowMultipleContexts: Boolean =</div><div class="line">    config.getBoolean(&quot;spark.driver.allowMultipleContexts&quot;, false)</div><div class="line"></div><div class="line">  // In order to prevent multiple SparkContexts from being active at the same time, mark this</div><div class="line">  // context as having started construction.</div><div class="line">  // NOTE: this must be placed at the beginning of the SparkContext constructor.</div><div class="line">  SparkContext.markPartiallyConstructed(this, allowMultipleContexts)</div><div class="line">  </div><div class="line">  //省略代码...</div><div class="line"> &#125;</div></pre></td></tr></table></figure>
<p>上面的代码中:CallSite存储了线程栈中最靠近栈顶的用户类及最靠近栈低的scala或者spark核心类信息<br>SparkContext默认只有一个实例(有属性spark.driver.allowMultipleContexts来控制,用户需要多个SparkContext实例时,可以将其设置为true),方法markPartiallyConstructed用来确保实例的唯一性,并将当前SparkContext标记为正在构建中</p>
<p>接下来对SparkConf进行复制,然后对各种配置信息进行校验,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">_conf = config.clone()</div><div class="line">_conf.validateSettings()</div><div class="line"></div><div class="line">if (!_conf.contains(&quot;spark.master&quot;)) &#123;</div><div class="line">  throw new SparkException(&quot;A master URL must be set in your configuration&quot;)</div><div class="line">&#125;</div><div class="line">if (!_conf.contains(&quot;spark.app.name&quot;)) &#123;</div><div class="line">  throw new SparkException(&quot;An application name must be set in your configuration&quot;)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从上面的代码看到必须制定属性spark.master和spark.app.name,否则会抛出异常,结束初始化过程,spark.master用于设置部署模式,spark.app.name用于指定应用程序名称</p>
<h1 id="2-创建执行环境SparkEnv"><a href="#2-创建执行环境SparkEnv" class="headerlink" title="2.创建执行环境SparkEnv"></a>2.创建执行环境SparkEnv</h1><p>SparkEnv是Spark的执行环境对象,其中包括众多与Executor执行相关的对象,由于在local模式下Driver会创建Executor,local-cluster部署模式或者Standalone部署模式下Worker另起的CoarseGrainedExecutorBackend进行中也会创建Executor,所以SparkEnv存在于Driver或者是CoarseGrainedExecutorBackend进程中,创建SparkEnv主要使用SparkEnv的createDriverEnv,SparkEnv.createDriverEnv方法有三个参数:conf,isLocal,listenerBus<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)</div><div class="line">SparkEnv.set(_env)</div><div class="line"></div><div class="line">---------------------------------</div><div class="line">//下面是createSparkEnv方法的实现</div><div class="line">private[spark] def createSparkEnv(</div><div class="line">    conf: SparkConf,</div><div class="line">    isLocal: Boolean,</div><div class="line">    listenerBus: LiveListenerBus): SparkEnv = &#123;</div><div class="line">  SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master))</div><div class="line">&#125;</div><div class="line"></div><div class="line">//createSparkEnv的参数如下:</div><div class="line">_conf = config.clone()</div><div class="line"></div><div class="line">def isLocal: Boolean = (master == &quot;local&quot; || master.startsWith(&quot;local[&quot;))</div><div class="line"></div><div class="line">private[spark] val listenerBus = new LiveListenerBus</div><div class="line">----------------------------------</div></pre></td></tr></table></figure></p>
<p>上面的代码中的conf是对SparkConf的复制,isLocal标识是否是单机模式,listenerBus采用监听器模式维护各类事件的处理</p>
<p>SparkEnv的方法createDriverEnv最终调用create创建SparkEnv(可以一步一步的点进去看)</p>
<p>SparkEnv的构造步骤如下:<br>1.创建安全管理器SecurityManager<br>2.创建基于Akka的分布式消息系统ActorSystem<br>3.创建Map任务输出更踪器mapOutputTracker<br>4.实例化ShuffleManager<br>5.创建ShuffleMemoryManager<br>6.创建块传输服务BlockTransferService<br>7.创建BlockManagerMaster<br>8.创建块管理器BlockManager<br>9.创建广播管理器BroadcastManager<br>10.创建缓存管理器CacheManger<br>11.创建HTTP文件服务器HttpFileServer<br>12.创建测量系统MetricsSystem<br>13.创建SparkEnv</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">//在create方法中,有下面的代码:</div><div class="line">val envInstance = new SparkEnv(</div><div class="line">  executorId,</div><div class="line">  rpcEnv,</div><div class="line">  actorSystem,</div><div class="line">  serializer,</div><div class="line">  closureSerializer,</div><div class="line">  cacheManager,</div><div class="line">  mapOutputTracker,</div><div class="line">  shuffleManager,</div><div class="line">  broadcastManager,</div><div class="line">  blockTransferService,</div><div class="line">  blockManager,</div><div class="line">  securityManager,</div><div class="line">  sparkFilesDir,</div><div class="line">  metricsSystem,</div><div class="line">  memoryManager,</div><div class="line">  outputCommitCoordinator,</div><div class="line">  conf)</div><div class="line"></div><div class="line"></div><div class="line">//返回实例</div><div class="line">envInstance</div></pre></td></tr></table></figure>
<h2 id="2-1-安全管理器SecurityManager"><a href="#2-1-安全管理器SecurityManager" class="headerlink" title="2.1.安全管理器SecurityManager"></a>2.1.安全管理器SecurityManager</h2><p>SecurityManager主要对权限,账号进行设置,如果使用Hadoop Yarn作为集群管理器,则需要使用证书生成secret key登录,最后给当前系统设置默认的口令认证实例,此实例采用匿名内部类实现<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">private val secretKey = generateSecretKey()</div><div class="line"></div><div class="line">//使用HTTP连接设置口令认证</div><div class="line">if (authOn) &#123;</div><div class="line">  Authenticator.setDefault(</div><div class="line">    new Authenticator() &#123;</div><div class="line">      override def getPasswordAuthentication(): PasswordAuthentication = &#123;</div><div class="line">        var passAuth: PasswordAuthentication = null</div><div class="line">        val userInfo = getRequestingURL().getUserInfo()</div><div class="line">        if (userInfo != null) &#123;</div><div class="line">          val  parts = userInfo.split(&quot;:&quot;, 2)</div><div class="line">          passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray())</div><div class="line">        &#125;</div><div class="line">        return passAuth</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  )</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="2-2-基于Akka的分布式消息系统ActorSystem"><a href="#2-2-基于Akka的分布式消息系统ActorSystem" class="headerlink" title="2.2.基于Akka的分布式消息系统ActorSystem"></a>2.2.基于Akka的分布式消息系统ActorSystem</h2><p>ActorSystem是spark中最基础的设施,spark既使用它发送分布式消息,又用它实现并发编程,消息系统可以实现并发?要解释清楚这个问题,首先应该简单介绍下scala语言的Actor并发编程模型:Scala认为java线程通过共享数据以及通过锁来维护共享数据的一致性是糟糕的做法,容易引起锁的争用,降低并发程序的性能,甚至会引起死锁的问题,在scala中需要自定义类型继承Actor,并且提供act方法,就如同Java里实现Runnable接口,需要实现run方法一样,但是不能直接调用act方法,而是通过发送消息的方式(scala发送消息时异步的)传送数据,如:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">Actor!message</div></pre></td></tr></table></figure></p>
<p>Akka是Actor编程模型的高级类库,雷雨JDK1.5之后越来越丰富的并发工具包,简化了程序员并发编程的难度,ActorSystem便是Akka提供的用于创建分布式消息通信系统的基础类</p>
<p>正是因为Actor轻量级的并发编程,消息发送以及ActorSystem支持分布式消息发送等特点,Spark选择了ActorSystem</p>
<p>SparkEnv中创建ActorSystem时用到了AkkaUtils工具类,AkkaUtils.createActorSystem,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">AkkaUtils.createActorSystem(</div><div class="line">  actorSystemName + &quot;ActorSystem&quot;,</div><div class="line">  hostname,</div><div class="line">  actorSystemPort,</div><div class="line">  conf,</div><div class="line">  securityManager</div><div class="line">)._1</div></pre></td></tr></table></figure></p>
<p>createActorSystem方法如下,他会调用:startServiceOnPort<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def createActorSystem(</div><div class="line">    name: String,</div><div class="line">    host: String,</div><div class="line">    port: Int,</div><div class="line">    conf: SparkConf,</div><div class="line">    securityManager: SecurityManager): (ActorSystem, Int) = &#123;</div><div class="line">  val startService: Int =&gt; (ActorSystem, Int) = &#123; actualPort =&gt;</div><div class="line">    doCreateActorSystem(name, host, actualPort, conf, securityManager)</div><div class="line">  &#125;</div><div class="line">  Utils.startServiceOnPort(port, startService, conf, name)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>startServiceOnPort中调用startService<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def startServiceOnPort[T](</div><div class="line">    startPort: Int,</div><div class="line">    startService: Int =&gt; (T, Int),</div><div class="line">    conf: SparkConf,</div><div class="line">    serviceName: String = &quot;&quot;): (T, Int) = &#123;</div><div class="line">	</div><div class="line">	//....</div><div class="line">	val (service, port) = startService(tryPort)</div><div class="line">	//...</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>//他会调用createActorSystem中的传过来的方法 startService,所以会调用doCreateActorSystem,所以正真启动ActorSystem是由doCreateActorSystem方法完成的</p>
<p>Spark的Driver中Akka的默认访问地址是akka://sparkDriver,Spark的Executor中Akka的默认访问地址是 akka://sparkExecutor,如果不指定ActorSystem的端口,那么所有节点的ActorSystem端口每次启动时随机产生</p>
<h2 id="2-3-map任务输出跟踪器mapOutputTracker"><a href="#2-3-map任务输出跟踪器mapOutputTracker" class="headerlink" title="2.3.map任务输出跟踪器mapOutputTracker"></a>2.3.map任务输出跟踪器mapOutputTracker</h2><p>mapOutputTracker用于跟踪map阶段任务的输出状态,此状态便于reduce阶段任务获取地址及中间输出结果,每个map任务或者reduce任务都会有唯一的标识,分别为mapId何reduceId,每个reduce任务的输入可能是多个map的输出,reduce回到各个map任务的所在节点上拉取Block,这一过程叫做Shuffle,每批Shuffle过程都有唯一的标识ShuffleId</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">val mapOutputTracker = if (isDriver) &#123;</div><div class="line">  new MapOutputTrackerMaster(conf)</div><div class="line">&#125; else &#123;</div><div class="line">  new MapOutputTrackerWorker(conf)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>这里先介绍下MapOutputTrackerMaster,在其内部有下面的代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">protected val mapStatuses = new TimeStampedHashMap[Int, Array[MapStatus]]()</div><div class="line">private val cachedSerializedStatuses = new TimeStampedHashMap[Int, Array[Byte]]()</div></pre></td></tr></table></figure></p>
<p>其中TimeStampedHashMap[Int, Array[MapStatus]]的int是对应ShuffleId,Array存储各个map任务对应的状态信息MapStatus,由于MapStatus维护了map输出Block的地址BlockManagerId,这样reduce任务知道从何处获取map任务的中间输出<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">private[spark] sealed trait MapStatus &#123;</div><div class="line">  /** Location where this task was run. */</div><div class="line">  def location: BlockManagerId</div><div class="line"></div><div class="line">  def getSizeForBlock(reduceId: Int): Long</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>同时MapOutputTrackerMaster还使用cachedSerializedStatuses:TimeStampedHashMap[Int, Array[Byte]]维护序列化后的各个map任务的输出状态,其中int对应的是ShuffleId,Array存储各个序列化MapStatus生成的字节数组</p>
<p>Driver和Executor处理MapOutputTrackerMaster的方式有所不同<br>如果当前应用程序是Driver,则创建MapOutputTrackerMaster,然后创建MapOutputTrackerMasterActor,并且注册到ActorSystem中<br>如果当前应用程序是Executor,则创建MapOutputTrackerWorker,并从ActorSystem中找到MapOutputTrackerMasterActor</p>
<p>无论是Driver还是Executor,最后都是由mapOutputTracker的属性trackerEndpoint持有MapOutputTrackerMasterActor的引用,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">val mapOutputTracker = if (isDriver) &#123;</div><div class="line">  new MapOutputTrackerMaster(conf)</div><div class="line">&#125; else &#123;</div><div class="line">  new MapOutputTrackerWorker(conf)</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Have to assign trackerActor after initialization as MapOutputTrackerActor</div><div class="line">// requires the MapOutputTracker itself</div><div class="line">mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME, new MapOutputTrackerMasterEndpoint(rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))</div><div class="line"></div><div class="line"></div><div class="line">//</div><div class="line">def registerOrLookupEndpoint(</div><div class="line">    name: String, endpointCreator: =&gt; RpcEndpoint):</div><div class="line">  RpcEndpointRef = &#123;</div><div class="line">  if (isDriver) &#123;</div><div class="line">    logInfo(&quot;Registering &quot; + name)</div><div class="line">    rpcEnv.setupEndpoint(name, endpointCreator)</div><div class="line">  &#125; else &#123;</div><div class="line">    RpcUtils.makeDriverRef(name, conf, rpcEnv)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>后面的章节就会知道map任务的状态正是由Executor向持有的MapOutputTrackerMasterActor发送消息,将map任务状态同步到MapOutputTracker的MapStatuses和cachedSerializedStatuses的,Executor究竟是如何找到MapOutputTrackerMasterActor的?registerOrLookupEndpoint方法通过RpcUtils.makeDriverRef找到MapOutputTrackerMasterActor,实际正是利用ActorSystem提供的分布式消息机制实现的</p>
<h2 id="2-4-实例化ShuffleManager"><a href="#2-4-实例化ShuffleManager" class="headerlink" title="2.4.实例化ShuffleManager"></a>2.4.实例化ShuffleManager</h2><p>ShuffleManager负责管理本地及远程的block数据的shuffle操作,ShuffleManager默认为通过反射方法生成的SortShuffleManager的实例,例如可以修改属性spark.shuffle.manager为hash来显示控制使用HashShuffleManager<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">//下面是几种ShuffleManager的短名称和类名的映射</div><div class="line">val shortShuffleMgrNames = Map(</div><div class="line">  &quot;hash&quot; -&gt; &quot;org.apache.spark.shuffle.hash.HashShuffleManager&quot;,</div><div class="line">  &quot;sort&quot; -&gt; &quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;,</div><div class="line">  &quot;tungsten-sort&quot; -&gt; &quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;)</div><div class="line"></div><div class="line">//从配置文件中获取,是否有配置 spark.shuffle.manager</div><div class="line">val shuffleMgrName = conf.get(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)</div><div class="line"></div><div class="line">//得到ShuffleManager的类名</div><div class="line">val shuffleMgrClass = shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase, shuffleMgrName)</div><div class="line"></div><div class="line">//根据类名反射</div><div class="line">val shuffleManager = instantiateClass[ShuffleManager](shuffleMgrClass)</div><div class="line"></div><div class="line">//instantiateClass的方法内容</div><div class="line">def instantiateClass[T](className: String): T = &#123;</div><div class="line">  val cls = Utils.classForName(className)</div><div class="line">  try &#123;</div><div class="line">    cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE)</div><div class="line">      .newInstance(conf, new java.lang.Boolean(isDriver))</div><div class="line">      .asInstanceOf[T]</div><div class="line">  &#125;</div><div class="line">  //....</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>SortShuffleManager通过持有的IndexShuffleBlockResolver间接BlockManager中的DiskBlockManager将map结果写入本地,并根据shuffleId,mapId写入索引文件,也能通过MapOutputTrackerMaster中维护的MapStatuses从本地或者其他远程节点读取文件,有人会问,为什么需要shuffle?spark作为并行计算框架,同一个作业会被划分为多个任务在多个节点上并行执行,reduce的输入可能存在于多个节点上,因此需要通过”洗牌”将所有的reduce的输入汇总起来,这个过程就是shuffle</p>
<h2 id="2-5-shuffle线程内存管理器ShuffleMemoryManager"><a href="#2-5-shuffle线程内存管理器ShuffleMemoryManager" class="headerlink" title="2.5.shuffle线程内存管理器ShuffleMemoryManager"></a>2.5.shuffle线程内存管理器ShuffleMemoryManager</h2><p>在我阅读spark1.6的时候,没有看到作者指定的ShuffleMemoryManager类,在spark1.6的源码中是这样的:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">//使用传统内存管理器</div><div class="line">val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)</div><div class="line">val memoryManager: MemoryManager =</div><div class="line">  if (useLegacyMemoryManager) &#123;</div><div class="line">    new StaticMemoryManager(conf, numUsableCores) //静态内存管理器</div><div class="line">  &#125; else &#123;</div><div class="line">    UnifiedMemoryManager(conf, numUsableCores)	//统一内存管理器</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>如果配置了传统的内存管理器,代码实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">private[spark] class StaticMemoryManager(</div><div class="line">    conf: SparkConf,</div><div class="line">    maxOnHeapExecutionMemory: Long,</div><div class="line">    override val maxStorageMemory: Long,</div><div class="line">    numCores: Int)</div><div class="line">  extends MemoryManager(</div><div class="line">    conf,</div><div class="line">    numCores,</div><div class="line">    maxStorageMemory,</div><div class="line">    maxOnHeapExecutionMemory) &#123;</div><div class="line"></div><div class="line">  def this(conf: SparkConf, numCores: Int) &#123;</div><div class="line">    this(</div><div class="line">      conf,</div><div class="line">      StaticMemoryManager.getMaxExecutionMemory(conf),</div><div class="line">      StaticMemoryManager.getMaxStorageMemory(conf),</div><div class="line">      numCores)</div><div class="line">  &#125;</div><div class="line">//....</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">其中两个重要的方法实现如下:</div><div class="line"> /**</div><div class="line"> * Return the total amount of memory available for the storage region, in bytes.</div><div class="line"> */</div><div class="line">private def getMaxStorageMemory(conf: SparkConf): Long = &#123;</div><div class="line">  val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</div><div class="line">  val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6)</div><div class="line">  val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9)</div><div class="line">  (systemMaxMemory * memoryFraction * safetyFraction).toLong</div><div class="line">&#125;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Return the total amount of memory available for the execution region, in bytes.</div><div class="line"> */</div><div class="line">private def getMaxExecutionMemory(conf: SparkConf): Long = &#123;</div><div class="line">  val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</div><div class="line">  val memoryFraction = conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2)</div><div class="line">  val safetyFraction = conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8)</div><div class="line">  (systemMaxMemory * memoryFraction * safetyFraction).toLong</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>没有配置传统内存管理器,使用的是统一内存管理器的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = &#123;</div><div class="line">  val maxMemory = getMaxMemory(conf)</div><div class="line">  new UnifiedMemoryManager(</div><div class="line">    conf,</div><div class="line">    maxMemory = maxMemory,</div><div class="line">    storageRegionSize =</div><div class="line">      (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong,</div><div class="line">    numCores = numCores)</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//而getMaxMemory的方法实现如下:</div><div class="line">/**</div><div class="line"> * Return the total amount of memory shared between execution and storage, in bytes.</div><div class="line"> */</div><div class="line">private def getMaxMemory(conf: SparkConf): Long = &#123;</div><div class="line">  val systemMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory)</div><div class="line">	//保留内存</div><div class="line">  val reservedMemory = conf.getLong(&quot;spark.testing.reservedMemory&quot;, if (conf.contains(&quot;spark.testing&quot;)) 0 else RESERVED_SYSTEM_MEMORY_BYTES)</div><div class="line">  val minSystemMemory = reservedMemory * 1.5</div><div class="line"></div><div class="line">  if (systemMemory &lt; minSystemMemory) &#123;</div><div class="line">    throw new IllegalArgumentException(s&quot;System memory $systemMemory must &quot; +</div><div class="line">      s&quot;be at least $minSystemMemory. Please use a larger heap size.&quot;)</div><div class="line">  &#125;</div><div class="line">  val usableMemory = systemMemory - reservedMemory</div><div class="line">	//内存百分比</div><div class="line">  val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.75)</div><div class="line">  (usableMemory * memoryFraction).toLong</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="2-6-块传输服务BlockTransferService"><a href="#2-6-块传输服务BlockTransferService" class="headerlink" title="2.6.块传输服务BlockTransferService"></a>2.6.块传输服务BlockTransferService</h2><p>BlockTransferService使用的是NettyBlockTransferService,他使用Netty提供的异步事件驱动的网络应用框架,提供web服务及客户端,获取远程节点上Block的集合</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val blockTransferService = new NettyBlockTransferService(conf, securityManager, numUsableCores)</div></pre></td></tr></table></figure>
<p>NettyBlockTransferService的具体实现在第4章有详细介绍</p>
<h2 id="2-7-BlockManagerMaster介绍"><a href="#2-7-BlockManagerMaster介绍" class="headerlink" title="2.7.BlockManagerMaster介绍"></a>2.7.BlockManagerMaster介绍</h2><p>BlockManagerMaster负责对Block的管理和协调,具体操作依赖于BlockManagerMasterEndpoint,Driver和Executor处理BlockManagerMaster的方式不同<br>如果当前应用程序是Driver,则创建BlockManagerMasterEndpoint,并且注册到ActorSystem中,如果当前应用程序是Executor,则从ActorSystem中找到BlockManagerMasterEndpoint.</p>
<p>无论是Driver还是Executor,最后BlockManagerMaster的属性driverEndpoint将持有对BlockManagerMasterEndpoint的引用,BlockManagerMaster的创建代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">val blockManagerMaster = new BlockManagerMaster(</div><div class="line">							registerOrLookupEndpoint(BlockManagerMaster.DRIVER_ENDPOINT_NAME, new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)),   </div><div class="line">							conf, </div><div class="line">							isDriver)</div></pre></td></tr></table></figure>
<p>registerOrLookupEndpoint在2.3节有介绍,不再详述</p>
<h2 id="2-8-创建块管理器BlockManager"><a href="#2-8-创建块管理器BlockManager" class="headerlink" title="2.8.创建块管理器BlockManager"></a>2.8.创建块管理器BlockManager</h2><p>BlockManager负责对Block的管理,只有在BlockManager的初始化initialize被调用后,他才是有效的,具体实现见第4章</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">// NB: blockManager is not valid until initialize() is called later.</div><div class="line">val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster,</div><div class="line">  serializer, conf, memoryManager, mapOutputTracker, shuffleManager,  blockTransferService, securityManager, numUsableCores)</div></pre></td></tr></table></figure>
<h2 id="2-9-创建广播管理器BroadcastManager"><a href="#2-9-创建广播管理器BroadcastManager" class="headerlink" title="2.9.创建广播管理器BroadcastManager"></a>2.9.创建广播管理器BroadcastManager</h2><p>BroadcastManager用于将配置信息和序列化后的RDD,Job以及ShuffleDependency等信息在本地存储,如果为了容灾,也会复制到其他节点上,实现代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val broadcastManager = new BroadcastManager(isDriver, conf, securityManager)</div></pre></td></tr></table></figure>
<p>BroadcastManager必须在其初始化方法initialize被调用后才能生效,initialize方法实际利用反射生成广播工厂实例broadcastFactory(可以配置属性:spark.broadcast.factory</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">// Called by SparkContext or Executor before using Broadcast</div><div class="line">private def initialize() &#123;</div><div class="line">  synchronized &#123;</div><div class="line">    if (!initialized) &#123;</div><div class="line">      val broadcastFactoryClass =</div><div class="line">        conf.get(&quot;spark.broadcast.factory&quot;, &quot;org.apache.spark.broadcast.TorrentBroadcastFactory&quot;)</div><div class="line"></div><div class="line">      broadcastFactory =</div><div class="line">        Utils.classForName(broadcastFactoryClass).newInstance.asInstanceOf[BroadcastFactory]</div><div class="line"></div><div class="line">      // Initialize appropriate BroadcastFactory and BroadcastObject</div><div class="line">      broadcastFactory.initialize(isDriver, conf, securityManager)</div><div class="line"></div><div class="line">      initialized = true</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>BroadcastManager的newBroadcast实际代理了工厂的newBroadcast方法来生成广播对象,代理unbroadcast来生成非广播对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean): Broadcast[T] = &#123;</div><div class="line">  broadcastFactory.newBroadcast[T](value_, isLocal, nextBroadcastId.getAndIncrement())</div><div class="line">&#125;</div><div class="line"></div><div class="line">def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean) &#123;</div><div class="line">  broadcastFactory.unbroadcast(id, removeFromDriver, blocking)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="2-10-创建缓存管理器CacheManager"><a href="#2-10-创建缓存管理器CacheManager" class="headerlink" title="2.10.创建缓存管理器CacheManager"></a>2.10.创建缓存管理器CacheManager</h2><p>CacheManager用于缓存RDD某个分区计算后的中间结果,缓存计算结果发生在迭代计算的时候,在6.1节会讲到,而CacheManager将在4.10节详细描述,创建CacheManager的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">val cacheManager = new CacheManager(blockManager)</div></pre></td></tr></table></figure></p>
<h2 id="2-11-HTTP文件服务器httpFileServer"><a href="#2-11-HTTP文件服务器httpFileServer" class="headerlink" title="2.11.HTTP文件服务器httpFileServer"></a>2.11.HTTP文件服务器httpFileServer</h2><p>在spark1.6中是下面的代码实现</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">// Set the sparkFiles directory, used when downloading dependencies.  </div><div class="line">// In local mode, this is a temporary directory; </div><div class="line">// in distributed mode, this is the executor&apos;s current working directory.</div><div class="line">val sparkFilesDir: String = if (isDriver) &#123;</div><div class="line">  Utils.createTempDir(Utils.getLocalDir(conf), &quot;userFiles&quot;).getAbsolutePath</div><div class="line">&#125; else &#123;</div><div class="line">  &quot;.&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>HttpFileServer的初始化过程代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">def initialize() &#123;</div><div class="line">  baseDir = Utils.createTempDir(Utils.getLocalDir(conf), &quot;httpd&quot;)</div><div class="line">  fileDir = new File(baseDir, &quot;files&quot;)</div><div class="line">  jarDir = new File(baseDir, &quot;jars&quot;)</div><div class="line">  fileDir.mkdir()</div><div class="line">  jarDir.mkdir()</div><div class="line">  logInfo(&quot;HTTP File server directory is &quot; + baseDir)</div><div class="line">  httpServer = new HttpServer(conf, baseDir, securityManager, requestedPort, &quot;HTTP file server&quot;)</div><div class="line">  httpServer.start()</div><div class="line">  serverUri = httpServer.uri</div><div class="line">  logDebug(&quot;HTTP file server started at: &quot; + serverUri)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>包括下面的步骤:<br>1.使用Utils工具类创建文件服务器的根目录及临时目录(临时目录在运行环境关闭时会删除)<br>2.创建存放jar包及其他文件的文件目录<br>3.创建并启动Http服务</p>
<p>httpServer的构造和start方法的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def start() &#123;</div><div class="line">  if (server != null) &#123;</div><div class="line">    throw new ServerStateException(&quot;Server is already started&quot;)</div><div class="line">  &#125; else &#123;</div><div class="line">    logInfo(&quot;Starting HTTP Server&quot;)</div><div class="line">    val (actualServer, actualPort) =</div><div class="line">      Utils.startServiceOnPort[Server](requestedPort, doStart, conf, serverName)</div><div class="line">    server = actualServer</div><div class="line">    port = actualPort</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>用到了 Utils.startServiceOnPort方法,因此会回调doStart方法,在doStart方法总内嵌了Jetty所提供的HTTP服务</p>
<h2 id="2-12-创建测量系统MetricsSystem"><a href="#2-12-创建测量系统MetricsSystem" class="headerlink" title="2.12.创建测量系统MetricsSystem"></a>2.12.创建测量系统MetricsSystem</h2><p>MetricsSystem是spark的测量系统,创建代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">val metricsSystem = if (isDriver) &#123;</div><div class="line">  // Don&apos;t start metrics system right now for Driver.</div><div class="line">  // We need to wait for the task scheduler to give us an app ID.</div><div class="line">  // Then we can start the metrics system.</div><div class="line">  MetricsSystem.createMetricsSystem(&quot;driver&quot;, conf, securityManager)</div><div class="line">&#125; else &#123;</div><div class="line">  // We need to set the executor ID before the MetricsSystem is created because sources and</div><div class="line">  // sinks specified in the metrics configuration file will want to incorporate this executor&apos;s</div><div class="line">  // ID into the metrics they report.</div><div class="line">  conf.set(&quot;spark.executor.id&quot;, executorId)</div><div class="line">  val ms = MetricsSystem.createMetricsSystem(&quot;executor&quot;, conf, securityManager)</div><div class="line">  ms.start()</div><div class="line">  ms</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>上面调用的createMetricsSystem方法实际创建了MetricsSystem,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def createMetricsSystem(</div><div class="line">    instance: String, conf: SparkConf, securityMgr: SecurityManager): MetricsSystem = &#123;</div><div class="line">  new MetricsSystem(instance, conf, securityMgr)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>构造MetricsSystem的过程最重要的是调用了MetricsConfig.initialize()方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">def initialize() &#123;</div><div class="line">  // Add default properties in case there&apos;s no properties file</div><div class="line">  setDefaultProperties(properties)</div><div class="line"></div><div class="line">  loadPropertiesFromFile(conf.getOption(&quot;spark.metrics.conf&quot;))</div><div class="line"></div><div class="line">  // Also look for the properties in provided Spark configuration</div><div class="line">  val prefix = &quot;spark.metrics.conf.&quot;</div><div class="line">  conf.getAll.foreach &#123;</div><div class="line">    case (k, v) if k.startsWith(prefix) =&gt;</div><div class="line">      properties.setProperty(k.substring(prefix.length()), v)</div><div class="line">    case _ =&gt;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  propertyCategories = subProperties(properties, INSTANCE_REGEX)</div><div class="line">  if (propertyCategories.contains(DEFAULT_PREFIX)) &#123;</div><div class="line">    val defaultProperty = propertyCategories(DEFAULT_PREFIX).asScala</div><div class="line">    for((inst, prop) &lt;- propertyCategories if (inst != DEFAULT_PREFIX);</div><div class="line">        (k, v) &lt;- defaultProperty if (prop.get(k) == null)) &#123;</div><div class="line">      prop.put(k, v)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从以上实现可以看出,MetricsConfig的initialize方法主要负责加载metrics.properties文件中的属性配置,并对属性进行初始化转换,变成Map</p>
<h2 id="2-13-创建SparkEnv"><a href="#2-13-创建SparkEnv" class="headerlink" title="2.13.创建SparkEnv"></a>2.13.创建SparkEnv</h2><p>当所有的基础组件准备好后,最终使用下面的代码创建执行环境SparkEnv<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">val envInstance = new SparkEnv(</div><div class="line">  executorId,</div><div class="line">  rpcEnv,</div><div class="line">  actorSystem,</div><div class="line">  serializer,</div><div class="line">  closureSerializer,</div><div class="line">  cacheManager,</div><div class="line">  mapOutputTracker,</div><div class="line">  shuffleManager,</div><div class="line">  broadcastManager,</div><div class="line">  blockTransferService,</div><div class="line">  blockManager,</div><div class="line">  securityManager,</div><div class="line">  sparkFilesDir,</div><div class="line">  metricsSystem,</div><div class="line">  memoryManager,</div><div class="line">  outputCommitCoordinator,</div><div class="line">  conf)</div></pre></td></tr></table></figure></p>
<h1 id="3-创建metadtaCleaner"><a href="#3-创建metadtaCleaner" class="headerlink" title="3.创建metadtaCleaner"></a>3.创建metadtaCleaner</h1><p>我们回到SparkContext类中,如下代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">// Create the Spark execution environment (cache, map output tracker, etc)</div><div class="line">_env = createSparkEnv(_conf, isLocal, listenerBus)</div><div class="line">SparkEnv.set(_env)</div><div class="line"></div><div class="line">_metadataCleaner = new MetadataCleaner(MetadataCleanerType.SPARK_CONTEXT, this.cleanup, _conf)</div></pre></td></tr></table></figure></p>
<p>我们发现在创建完SparkEnv之后,接下来是创建MetadataCleaner</p>
<p>sparkContext为了保持对所有持久化的RDD的跟踪,使用类型是TimeStampedWeakValueHashMap的persistentRDDs缓存,metadataCleaner的功能是清除过期的持久化RDD,创建metadataCleaner的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_metadataCleaner = new MetadataCleaner(MetadataCleanerType.SPARK_CONTEXT, this.cleanup, _conf)</div></pre></td></tr></table></figure></p>
<p>注意上面的代码中会传给MetadataCleaner构造器一个函数cleanup,cleanup的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">/** Called by MetadataCleaner to clean up the persistentRdds map periodically */</div><div class="line">private[spark] def cleanup(cleanupTime: Long) &#123;</div><div class="line">  persistentRdds.clearOldValues(cleanupTime)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>我们来看下MetadataCleaner构造器的代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/**</div><div class="line"> * Runs a timer task to periodically clean up metadata (e.g. old files or hashtable entries)</div><div class="line"> */</div><div class="line">private[spark] class MetadataCleaner(</div><div class="line">    cleanerType: MetadataCleanerType.MetadataCleanerType,</div><div class="line">    cleanupFunc: (Long) =&gt; Unit,//cleanup方法会传递过来</div><div class="line">    conf: SparkConf)</div><div class="line">  extends Logging</div><div class="line">&#123;</div><div class="line">  val name = cleanerType.toString</div><div class="line"></div><div class="line">  private val delaySeconds = MetadataCleaner.getDelaySeconds(conf, cleanerType)</div><div class="line">  private val periodSeconds = math.max(10, delaySeconds / 10)</div><div class="line">  private val timer = new Timer(name + &quot; cleanup timer&quot;, true)</div><div class="line"></div><div class="line">  //将cleanup方法封装成一个task,然后去定时执行</div><div class="line">  private val task = new TimerTask &#123;</div><div class="line">    override def run() &#123;</div><div class="line">      try &#123;</div><div class="line">        cleanupFunc(System.currentTimeMillis() - (delaySeconds * 1000))</div><div class="line">        logInfo(&quot;Ran metadata cleaner for &quot; + name)</div><div class="line">      &#125; catch &#123;</div><div class="line">        case e: Exception =&gt; logError(&quot;Error running cleanup task for &quot; + name, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  if (delaySeconds &gt; 0) &#123;</div><div class="line">    logDebug(</div><div class="line">      &quot;Starting metadata cleaner for &quot; + name + &quot; with delay of &quot; + delaySeconds + &quot; seconds &quot; +</div><div class="line">      &quot;and period of &quot; + periodSeconds + &quot; secs&quot;)</div><div class="line"></div><div class="line">	/启动定时任务</div><div class="line">    timer.schedule(task, delaySeconds * 1000, periodSeconds * 1000)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def cancel() &#123;</div><div class="line">    timer.cancel()</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>从上面的代码可以看出MetadataCleaner实质上是一个用TimerTask实现的定时器,不断的调用cleanupFunc这样的函数,在构造metadataCleaner时的函数参数是cleanup,用于清理persistentRdds的过期内容</p>
<h1 id="4-SparkUI详解"><a href="#4-SparkUI详解" class="headerlink" title="4.SparkUI详解"></a>4.SparkUI详解</h1><p>任何系统都需要提供监控系统,用浏览器能访问具有样式及布局并提供丰富监控数据的页面无疑是一种简单,高效的方式.</p>
<p>在大型分布式系统中,采用事件监听机制是最常见的,为何要使用事件监听机制,加入SparkUI采用scala的函数调用方式,那么随着整个集群规模的增加,对函数的调用会越来越多,最终会受到Driver所在JVM的线程数量限制而影响监控数据的更新,甚至出现监控数据无法及时显示给用户的情况,由于函数调用多数情况下是同步调用,这就导致现场被阻塞,在分布式环境中,还可能因为网络问题,导致线程被长时间占用,将函数调用更换为发送事件,事件的处理是异步的,当前线程可以继续执行后续逻辑,线程池中的线程还可以被重用,这样整个系统的并发度会大大增加,发送的事件会存入缓存,由定时调度器取出后,分配给监听此事件的监听器对监控数据进行更新</p>
<p>SparkUI就是这样的服务,他的架构如下图:<br><img src="/assert/img/bigdata/深入理解spark核心思想与源码分析/2/SparkUI.png" alt=""></p>
<p>我们首先简单介绍图中的各个组件:</p>
<ul>
<li>DAGScheduler:主要的产生各类SparkListenerEvent的源头,他将各种SparkListenEvent发送到listenBus的事件队列中</li>
<li>listenBus通过定时器将SparkListenerEvent事件匹配到具体的SparkListener,改变SparkListener中的统计监控数据,最终由SparkUI的界面展示</li>
<li>图中还可以看到Spark里面定义了很多监听器SparkListener的时间,包括JobProgressListener,EnvironmentListener,StorageListener,ExecutorsListenter,他们的继承体系如下图:</li>
</ul>
<p><img src="/assert/img/bigdata/深入理解spark核心思想与源码分析/2/SparkListener.png" alt=""></p>
<h2 id="4-1-listenerBus详解"><a href="#4-1-listenerBus详解" class="headerlink" title="4.1.listenerBus详解"></a>4.1.listenerBus详解</h2><p>listenerBus的类型是LiveListenerBus,LiveListenerBus实现了监听器模型,通过监听事件触发对各种监听器状态信息的修改,达到UI界面的数据刷新效果,LiveListenerBus由以下部分组成:</p>
<ul>
<li>事件阻塞队列:类型为LinkedBlockingQueue[SparkListenerEvent],固定大小为10000</li>
<li>监听器数组:类型为CopyOnWriteArrayList,存放各类监听器SparkListener</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">private[spark] class LiveListenerBus</div><div class="line">  extends AsynchronousListenerBus[SparkListener, SparkListenerEvent](&quot;SparkListenerBus&quot;)</div><div class="line">  with SparkListenerBus &#123;</div><div class="line"></div><div class="line">//...</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">private[spark] abstract class AsynchronousListenerBus[L &lt;: AnyRef, E](name: String)</div><div class="line">  extends ListenerBus[L, E] &#123;</div><div class="line"></div><div class="line">  self =&gt;</div><div class="line"></div><div class="line">  private var sparkContext: SparkContext = null</div><div class="line"></div><div class="line">  private val EVENT_QUEUE_CAPACITY = 10000</div><div class="line">  //事件阻塞队列</div><div class="line">  private val eventQueue = new LinkedBlockingQueue[E](EVENT_QUEUE_CAPACITY)</div><div class="line"></div><div class="line">//...</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//</div><div class="line">private[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging &#123;</div><div class="line"></div><div class="line">  //监听器数组</div><div class="line">  private[spark] val listeners = new CopyOnWriteArrayList[L]</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Add a listener to listen events. This method is thread-safe and can be called in any thread.</div><div class="line">   */</div><div class="line">  final def addListener(listener: L) &#123;</div><div class="line">    listeners.add(listener)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">//....</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<ul>
<li>事件匹配监听器的线程:此Thread不断拉取LinkedBlockingQueue中的事件,遍历监听器,调用监听器的方法,任何事件都会在LinkedBlockingQueue中存在一段时间,然后Thread处理了此事件后,会将其清除,因此使用listenerBus这个名字再合适不过了,到站就下车,listenerBus的实现如下:<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div></pre></td><td class="code"><pre><div class="line">private val eventLock = new Semaphore(0)</div><div class="line"></div><div class="line">private val listenerThread = new Thread(name) &#123;</div><div class="line">  setDaemon(true)</div><div class="line">  override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) &#123;</div><div class="line">    AsynchronousListenerBus.withinListenerThread.withValue(true) &#123;</div><div class="line">      while (true) &#123;</div><div class="line">        eventLock.acquire()</div><div class="line">        self.synchronized &#123;</div><div class="line">          processingEvent = true</div><div class="line">        &#125;</div><div class="line">        try &#123;</div><div class="line">          val event = eventQueue.poll</div><div class="line">          if (event == null) &#123;</div><div class="line">            // Get out of the while loop and shutdown the daemon thread</div><div class="line">            if (!stopped.get) &#123;</div><div class="line">              throw new IllegalStateException(&quot;Polling `null` from eventQueue means&quot; +</div><div class="line">                &quot; the listener bus has been stopped. So `stopped` must be true&quot;)</div><div class="line">            &#125;</div><div class="line">            return</div><div class="line">          &#125;</div><div class="line">          postToAll(event)</div><div class="line">        &#125; finally &#123;</div><div class="line">          self.synchronized &#123;</div><div class="line">            processingEvent = false</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">/**</div><div class="line"> * Start sending events to attached listeners.</div><div class="line"> *</div><div class="line"> * This first sends out all buffered events posted before this listener bus has started, then</div><div class="line"> * listens for any additional events asynchronously while the listener bus is still running.</div><div class="line"> * This should only be called once.</div><div class="line"> *</div><div class="line"> * @param sc Used to stop the SparkContext in case the listener thread dies.</div><div class="line"> */</div><div class="line">def start(sc: SparkContext) &#123;</div><div class="line">  if (started.compareAndSet(false, true)) &#123;</div><div class="line">    sparkContext = sc</div><div class="line">    listenerThread.start()</div><div class="line">  &#125; else &#123;</div><div class="line">    throw new IllegalStateException(s&quot;$name already started!&quot;)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">def post(event: E) &#123;</div><div class="line">  if (stopped.get) &#123;</div><div class="line">    // Drop further events to make `listenerThread` exit ASAP</div><div class="line">    logError(s&quot;$name has already stopped! Dropping event $event&quot;)</div><div class="line">    return</div><div class="line">  &#125;</div><div class="line">  val eventAdded = eventQueue.offer(event)</div><div class="line">  if (eventAdded) &#123;</div><div class="line">    eventLock.release()</div><div class="line">  &#125; else &#123;</div><div class="line">    onDropEvent(event)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">def listenerThreadIsAlive: Boolean = listenerThread.isAlive</div><div class="line"></div><div class="line"> </div><div class="line">def stop() &#123;</div><div class="line">  if (!started.get()) &#123;</div><div class="line">    throw new IllegalStateException(s&quot;Attempted to stop $name that has not yet started!&quot;)</div><div class="line">  &#125;</div><div class="line">  if (stopped.compareAndSet(false, true)) &#123;</div><div class="line">    // Call eventLock.release() so that listenerThread will poll `null` from `eventQueue` and know</div><div class="line">    // `stop` is called.</div><div class="line">    eventLock.release()</div><div class="line">    listenerThread.join()</div><div class="line">  &#125; else &#123;</div><div class="line">    // Keep quiet</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
</li>
</ul>
<p>LiveListenerBus中调用的postToAll方法实际定义在父类SparkListenerBus中,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div></pre></td><td class="code"><pre><div class="line">//在类ListenerBus中:</div><div class="line"></div><div class="line">  final def postToAll(event: E): Unit = &#123;</div><div class="line">	//这里是遍历所有的listeners</div><div class="line">    val iter = listeners.iterator</div><div class="line">    while (iter.hasNext) &#123;</div><div class="line">      val listener = iter.next()</div><div class="line">      try &#123;</div><div class="line">        onPostEvent(listener, event)</div><div class="line">      &#125; catch &#123;</div><div class="line">        case NonFatal(e) =&gt;</div><div class="line">          logError(s&quot;Listener $&#123;Utils.getFormattedClassName(listener)&#125; threw an exception&quot;, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">//而onPostEvent的实现是在SparkListenerBus实现的,如下</div><div class="line"></div><div class="line">  override def onPostEvent(listener: SparkListener, event: SparkListenerEvent): Unit = &#123;</div><div class="line">    event match &#123;</div><div class="line">      case stageSubmitted: SparkListenerStageSubmitted =&gt;</div><div class="line">        listener.onStageSubmitted(stageSubmitted)</div><div class="line">      case stageCompleted: SparkListenerStageCompleted =&gt;</div><div class="line">        listener.onStageCompleted(stageCompleted)</div><div class="line">      case jobStart: SparkListenerJobStart =&gt;</div><div class="line">        listener.onJobStart(jobStart)</div><div class="line">      case jobEnd: SparkListenerJobEnd =&gt;</div><div class="line">        listener.onJobEnd(jobEnd)</div><div class="line">      case taskStart: SparkListenerTaskStart =&gt;</div><div class="line">        listener.onTaskStart(taskStart)</div><div class="line">      case taskGettingResult: SparkListenerTaskGettingResult =&gt;</div><div class="line">        listener.onTaskGettingResult(taskGettingResult)</div><div class="line">      case taskEnd: SparkListenerTaskEnd =&gt;</div><div class="line">        listener.onTaskEnd(taskEnd)</div><div class="line">      case environmentUpdate: SparkListenerEnvironmentUpdate =&gt;</div><div class="line">        listener.onEnvironmentUpdate(environmentUpdate)</div><div class="line">      case blockManagerAdded: SparkListenerBlockManagerAdded =&gt;</div><div class="line">        listener.onBlockManagerAdded(blockManagerAdded)</div><div class="line">      case blockManagerRemoved: SparkListenerBlockManagerRemoved =&gt;</div><div class="line">        listener.onBlockManagerRemoved(blockManagerRemoved)</div><div class="line">      case unpersistRDD: SparkListenerUnpersistRDD =&gt;</div><div class="line">        listener.onUnpersistRDD(unpersistRDD)</div><div class="line">      case applicationStart: SparkListenerApplicationStart =&gt;</div><div class="line">        listener.onApplicationStart(applicationStart)</div><div class="line">      case applicationEnd: SparkListenerApplicationEnd =&gt;</div><div class="line">        listener.onApplicationEnd(applicationEnd)</div><div class="line">      case metricsUpdate: SparkListenerExecutorMetricsUpdate =&gt;</div><div class="line">        listener.onExecutorMetricsUpdate(metricsUpdate)</div><div class="line">      case executorAdded: SparkListenerExecutorAdded =&gt;</div><div class="line">        listener.onExecutorAdded(executorAdded)</div><div class="line">      case executorRemoved: SparkListenerExecutorRemoved =&gt;</div><div class="line">        listener.onExecutorRemoved(executorRemoved)</div><div class="line">      case blockUpdated: SparkListenerBlockUpdated =&gt;</div><div class="line">        listener.onBlockUpdated(blockUpdated)</div><div class="line">      case logStart: SparkListenerLogStart =&gt; // ignore event log metadata</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>其实上的过程就是将对应的事件发送到对应的Listener进行处理</p>
<h2 id="4-2-构造JobProgressListener"><a href="#4-2-构造JobProgressListener" class="headerlink" title="4.2.构造JobProgressListener"></a>4.2.构造JobProgressListener</h2><p>我们以JobProgressListener为例来讲解SparkListener,<strong>JobProgressListener是SparkContext中一个重要的组成部分(在SparkContext代码中可以看到)</strong>,通过监听listenBus中的事件更新任务进度,SparkStatusTracker和SparkUI实际上也是通过JobProgressListener来实现任务状态跟踪的,在SparkContext中创建JobProgressListener的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"> // &quot;_jobProgressListener&quot; should be set up before creating SparkEnv because when creating</div><div class="line"> // &quot;SparkEnv&quot;, some messages will be posted to &quot;listenerBus&quot; and we should not miss them.</div><div class="line"> _jobProgressListener = new JobProgressListener(_conf)</div><div class="line"> listenerBus.addListener(jobProgressListener)</div><div class="line"></div><div class="line">_statusTracker = new SparkStatusTracker(this)</div></pre></td></tr></table></figure></p>
<p>JobProgressListener的作用是通过HashMap,ListBuffer等数据结构存储JobId及对应的JobUIData信息,并按照激活,完成,失败等job状态统计,对stageId,stageInfo等信息按照激活,完成,忽略,失败等Stage状态统计,并且存储StageId与jobId的一对多关系,这些统计信息最终会被JobPage和StagePage等页面访问和渲染,JobProgressListener的数据结构如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">class JobProgressListener(conf: SparkConf) extends SparkListener with Logging &#123;</div><div class="line"></div><div class="line">  // Define a handful of type aliases so that data structures&apos; types can serve as documentation.</div><div class="line">  // These type aliases are public because they&apos;re used in the types of public fields:</div><div class="line"></div><div class="line">  type JobId = Int</div><div class="line">  type JobGroupId = String</div><div class="line">  type StageId = Int</div><div class="line">  type StageAttemptId = Int</div><div class="line">  type PoolName = String</div><div class="line">  type ExecutorId = String</div><div class="line"></div><div class="line">  // Application:</div><div class="line">  @volatile var startTime = -1L</div><div class="line">  @volatile var endTime = -1L</div><div class="line"></div><div class="line">  // Jobs:</div><div class="line">  val activeJobs = new HashMap[JobId, JobUIData]</div><div class="line">  val completedJobs = ListBuffer[JobUIData]()</div><div class="line">  val failedJobs = ListBuffer[JobUIData]()</div><div class="line">  val jobIdToData = new HashMap[JobId, JobUIData]</div><div class="line">  val jobGroupToJobIds = new HashMap[JobGroupId, HashSet[JobId]]</div><div class="line"></div><div class="line">  // Stages:</div><div class="line">  val pendingStages = new HashMap[StageId, StageInfo]</div><div class="line">  val activeStages = new HashMap[StageId, StageInfo]</div><div class="line">  val completedStages = ListBuffer[StageInfo]()</div><div class="line">  val skippedStages = ListBuffer[StageInfo]()</div><div class="line">  val failedStages = ListBuffer[StageInfo]()</div><div class="line">  val stageIdToData = new HashMap[(StageId, StageAttemptId), StageUIData]</div><div class="line">  val stageIdToInfo = new HashMap[StageId, StageInfo]</div><div class="line">  val stageIdToActiveJobIds = new HashMap[StageId, HashSet[JobId]]</div><div class="line">  val poolToActiveStages = HashMap[PoolName, HashMap[StageId, StageInfo]]()</div><div class="line">  // Total of completed and failed stages that have ever been run.  These may be greater than</div><div class="line">  // `completedStages.size` and `failedStages.size` if we have run more stages or jobs than</div><div class="line">  // JobProgressListener&apos;s retention limits.</div><div class="line">  var numCompletedStages = 0</div><div class="line">  var numFailedStages = 0</div><div class="line">  var numCompletedJobs = 0</div><div class="line">  var numFailedJobs = 0</div><div class="line">//...</div></pre></td></tr></table></figure></p>
<p>JobProgressListener实现了onJobStart,onJobEnd,onStageCompleted,onStageSubmitted,onTashStart,onTaskEnd等方法,这些方法正是在listenBus的驱动下,改变JobProgressListener中的各种Job,Stage相关的数据</p>
<h2 id="4-3-SparkUI的创建与初始化"><a href="#4-3-SparkUI的创建与初始化" class="headerlink" title="4.3.SparkUI的创建与初始化"></a>4.3.SparkUI的创建与初始化</h2><p>在SparkContext中接下来的是SparkUI的创建</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">_ui =</div><div class="line">  if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123;</div><div class="line">    Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener,</div><div class="line">      _env.securityManager, appName, startTime = startTime))</div><div class="line">  &#125; else &#123;</div><div class="line">    // For tests, do not enable the UI</div><div class="line">    None</div><div class="line">  &#125;</div><div class="line"></div><div class="line">// Bind the UI before starting the task scheduler to communicate</div><div class="line">// the bound port to the cluster manager properly</div><div class="line">_ui.foreach(_.bind())</div></pre></td></tr></table></figure>
<p>可以看到如果不需要提供SparkUI服务,可以将属性spark.ui.enabled修改为false,其中createLiveUI实际是调用create方法,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def createLiveUI(</div><div class="line">    sc: SparkContext,</div><div class="line">    conf: SparkConf,</div><div class="line">    listenerBus: SparkListenerBus,</div><div class="line">    jobProgressListener: JobProgressListener,</div><div class="line">    securityManager: SecurityManager,</div><div class="line">    appName: String,</div><div class="line">    startTime: Long): SparkUI = &#123;</div><div class="line">  create(Some(sc), conf, listenerBus, securityManager, appName,</div><div class="line">    jobProgressListener = Some(jobProgressListener), startTime = startTime)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>而create方法的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> private def create(</div><div class="line">     sc: Option[SparkContext],</div><div class="line">     conf: SparkConf,</div><div class="line">     listenerBus: SparkListenerBus,</div><div class="line">     securityManager: SecurityManager,</div><div class="line">     appName: String,</div><div class="line">     basePath: String = &quot;&quot;,</div><div class="line">     jobProgressListener: Option[JobProgressListener] = None,</div><div class="line">     startTime: Long): SparkUI = &#123;</div><div class="line"></div><div class="line">   val _jobProgressListener: JobProgressListener = jobProgressListener.getOrElse &#123;</div><div class="line">     val listener = new JobProgressListener(conf)</div><div class="line">     listenerBus.addListener(listener)</div><div class="line">     listener</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   val environmentListener = new EnvironmentListener</div><div class="line">   val storageStatusListener = new StorageStatusListener</div><div class="line">   val executorsListener = new ExecutorsListener(storageStatusListener)</div><div class="line">   val storageListener = new StorageListener(storageStatusListener)</div><div class="line">   val operationGraphListener = new RDDOperationGraphListener(conf)</div><div class="line"></div><div class="line">   listenerBus.addListener(environmentListener)</div><div class="line">   listenerBus.addListener(storageStatusListener)</div><div class="line">   listenerBus.addListener(executorsListener)</div><div class="line">   listenerBus.addListener(storageListener)</div><div class="line">   listenerBus.addListener(operationGraphListener)</div><div class="line"></div><div class="line">创建SparkUI</div><div class="line">   new SparkUI(sc, conf, securityManager, environmentListener, storageStatusListener,</div><div class="line">     executorsListener, _jobProgressListener, storageListener, operationGraphListener,</div><div class="line">     appName, basePath, startTime)</div><div class="line"> &#125;</div></pre></td></tr></table></figure></p>
<p>在上述的代码中可以看到,在create方法里除了JobProgressListener是外部传入的之外,又增加了一些SparkListener,例如,用于对JVM参数,Spark属性,java系统属性,classpath等进行监控的EnvironmentListener;用于维护Executor的存储状态的StorageStatusListener;用于准备将Executor的信息展示在ExecutorsTab的ExecutorsListener;用于准备将Executor相关存储信息展示在BlockManagerUI的StorageListener等,</p>
<p>最后创建SparkUI,SparkUI服务默认是可以被杀掉的,通过修改属性spark.ui.killEnabled为false,可以保证不被杀死,initialize方法会组织前端页面各个Tab和Page的展示及布局:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">private[spark] class SparkUI private (</div><div class="line">    val sc: Option[SparkContext],</div><div class="line">    val conf: SparkConf,</div><div class="line">    securityManager: SecurityManager,</div><div class="line">    val environmentListener: EnvironmentListener,</div><div class="line">    val storageStatusListener: StorageStatusListener,</div><div class="line">    val executorsListener: ExecutorsListener,</div><div class="line">    val jobProgressListener: JobProgressListener,</div><div class="line">    val storageListener: StorageListener,</div><div class="line">    val operationGraphListener: RDDOperationGraphListener,</div><div class="line">    var appName: String,</div><div class="line">    val basePath: String,</div><div class="line">    val startTime: Long)</div><div class="line">  extends WebUI(securityManager, SparkUI.getUIPort(conf), conf, basePath, &quot;SparkUI&quot;)</div><div class="line">  with Logging</div><div class="line">  with UIRoot &#123;</div><div class="line"></div><div class="line">  val killEnabled = sc.map(_.conf.getBoolean(&quot;spark.ui.killEnabled&quot;, true)).getOrElse(false)</div><div class="line"></div><div class="line"></div><div class="line">  val stagesTab = new StagesTab(this)</div><div class="line"></div><div class="line">  var appId: String = _</div><div class="line"></div><div class="line">  /** Initialize all components of the server. */</div><div class="line">  def initialize() &#123;</div><div class="line">    attachTab(new JobsTab(this))</div><div class="line">    attachTab(stagesTab)</div><div class="line">    attachTab(new StorageTab(this))</div><div class="line">    attachTab(new EnvironmentTab(this))</div><div class="line">    attachTab(new ExecutorsTab(this))</div><div class="line">    attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;))</div><div class="line">    attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath))</div><div class="line">    attachHandler(ApiRootResource.getServletHandler(this))</div><div class="line">    // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs</div><div class="line">    attachHandler(createRedirectHandler(</div><div class="line">      &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest,</div><div class="line">      httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;)))</div><div class="line">  &#125;</div><div class="line">  </div><div class="line">  //初始化</div><div class="line">  initialize()</div><div class="line"></div><div class="line">///....</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="4-4-Spark-UI的页面布局与展示"><a href="#4-4-Spark-UI的页面布局与展示" class="headerlink" title="4.4.Spark UI的页面布局与展示"></a>4.4.Spark UI的页面布局与展示</h2><p>SparkUI究竟是如何实现页面布局及展示的?JobsTab展示所有的Job的进度,状态信息,这里我们以他为例来说明,JobsTab会复用SparkUI的killEnabled,SparkContext,jobProgressListener,包括AllJobsPage和JobPage两个页面,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123;</div><div class="line">  val sc = parent.sc</div><div class="line">  val killEnabled = parent.killEnabled</div><div class="line">  val jobProgresslistener = parent.jobProgressListener</div><div class="line">  val executorListener = parent.executorsListener</div><div class="line">  val operationGraphListener = parent.operationGraphListener</div><div class="line"></div><div class="line">  def isFairScheduler: Boolean =</div><div class="line">    jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR)</div><div class="line"></div><div class="line">  attachPage(new AllJobsPage(this))</div><div class="line">  attachPage(new JobPage(this))</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p> AllJobsPage由render方法渲染,利用JobProgressListener中的统计监控数据生成:激活,完成,失败等job的状态摘要信息,并调用jobsTable方法生成表格等html元素,最终使用UIUtils的headerSparkPage封装好css,js,header及页面布局等,代码如下:</p>
<p>下面是AllJobsPage.render方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">def render(request: HttpServletRequest): Seq[Node] = &#123;</div><div class="line">  val listener = parent.jobProgresslistener</div><div class="line">  listener.synchronized &#123;</div><div class="line">    val startTime = listener.startTime</div><div class="line">    val endTime = listener.endTime</div><div class="line">    val activeJobs = listener.activeJobs.values.toSeq</div><div class="line">    val completedJobs = listener.completedJobs.reverse.toSeq</div><div class="line">    val failedJobs = listener.failedJobs.reverse.toSeq</div><div class="line"></div><div class="line">    val activeJobsTable =</div><div class="line">      jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse)</div><div class="line">    val completedJobsTable =</div><div class="line">      jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</div><div class="line">    val failedJobsTable =</div><div class="line">      jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse)</div><div class="line"></div><div class="line">    val shouldShowActiveJobs = activeJobs.nonEmpty</div><div class="line">    val shouldShowCompletedJobs = completedJobs.nonEmpty</div><div class="line">    val shouldShowFailedJobs = failedJobs.nonEmpty</div><div class="line"></div><div class="line">    val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123;</div><div class="line">      s&quot;$&#123;completedJobs.size&#125;&quot;</div><div class="line">    &#125; else &#123;</div><div class="line">      s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    val summary: NodeSeq =</div><div class="line">      &lt;div&gt;</div><div class="line">        &lt;ul class=&quot;unstyled&quot;&gt;</div><div class="line">          &lt;li&gt;</div><div class="line">            &lt;strong&gt;Total Uptime:&lt;/strong&gt;</div><div class="line">            &#123;</div><div class="line">              if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123;</div><div class="line">                UIUtils.formatDuration(System.currentTimeMillis() - startTime)</div><div class="line">              &#125; else if (endTime &gt; 0) &#123;</div><div class="line">                UIUtils.formatDuration(endTime - startTime)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &lt;/li&gt;</div><div class="line">          &lt;li&gt;</div><div class="line">            &lt;strong&gt;Scheduling Mode: &lt;/strong&gt;</div><div class="line">            &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125;</div><div class="line">          &lt;/li&gt;</div><div class="line">          &#123;</div><div class="line">            if (shouldShowActiveJobs) &#123;</div><div class="line">              &lt;li&gt;</div><div class="line">                &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt;</div><div class="line">                &#123;activeJobs.size&#125;</div><div class="line">              &lt;/li&gt;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">          &#123;</div><div class="line">            if (shouldShowCompletedJobs) &#123;</div><div class="line">              &lt;li id=&quot;completed-summary&quot;&gt;</div><div class="line">                &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt;</div><div class="line">                &#123;completedJobNumStr&#125;</div><div class="line">              &lt;/li&gt;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">          &#123;</div><div class="line">            if (shouldShowFailedJobs) &#123;</div><div class="line">              &lt;li&gt;</div><div class="line">                &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt;</div><div class="line">                &#123;listener.numFailedJobs&#125;</div><div class="line">              &lt;/li&gt;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &lt;/ul&gt;</div><div class="line">      &lt;/div&gt;</div><div class="line"></div><div class="line">    var content = summary</div><div class="line">    val executorListener = parent.executorListener</div><div class="line">    content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs,</div><div class="line">        executorListener.executorIdToData, startTime)</div><div class="line"></div><div class="line">    if (shouldShowActiveJobs) &#123;</div><div class="line">      content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++</div><div class="line">        activeJobsTable</div><div class="line">    &#125;</div><div class="line">    if (shouldShowCompletedJobs) &#123;</div><div class="line">      content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++</div><div class="line">        completedJobsTable</div><div class="line">    &#125;</div><div class="line">    if (shouldShowFailedJobs) &#123;</div><div class="line">      content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++</div><div class="line">        failedJobsTable</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; +</div><div class="line">      &quot; Click on a job to see information about the stages of tasks inside it.&quot;</div><div class="line"></div><div class="line">    UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText))</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>jobsTable用来生成表格数据,jobsTable(job),将传过来的job信息生成对应的表格,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123;</div><div class="line">   val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined)</div><div class="line"></div><div class="line">   val columns: Seq[Node] = &#123;</div><div class="line">     &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt;</div><div class="line">     &lt;th&gt;Description&lt;/th&gt;</div><div class="line">     &lt;th&gt;Submitted&lt;/th&gt;</div><div class="line">     &lt;th&gt;Duration&lt;/th&gt;</div><div class="line">     &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt;</div><div class="line">     &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">//表格中每行数据又是通过makeRow方法渲染的</div><div class="line">   def makeRow(job: JobUIData): Seq[Node] = &#123;</div><div class="line">     val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job)</div><div class="line">     val duration: Option[Long] = &#123;</div><div class="line">       job.submissionTime.map &#123; start =&gt;</div><div class="line">         val end = job.completionTime.getOrElse(System.currentTimeMillis())</div><div class="line">         end - start</div><div class="line">       &#125;</div><div class="line">     &#125;</div><div class="line">     val formattedDuration = duration.map(d =&gt; UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;)</div><div class="line">     val formattedSubmissionTime = job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;)</div><div class="line">     val basePathUri = UIUtils.prependBaseUri(parent.basePath)</div><div class="line">     val jobDescription = UIUtils.makeDescription(lastStageDescription, basePathUri)</div><div class="line"></div><div class="line">     val detailUrl = &quot;%s/jobs/job?id=%s&quot;.format(basePathUri, job.jobId)</div><div class="line">     &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt;</div><div class="line">       &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt;</div><div class="line">         &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125;</div><div class="line">       &lt;/td&gt;</div><div class="line">       &lt;td&gt;</div><div class="line">         &#123;jobDescription&#125;</div><div class="line">         &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt;</div><div class="line">       &lt;/td&gt;</div><div class="line">       &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt;</div><div class="line">         &#123;formattedSubmissionTime&#125;</div><div class="line">       &lt;/td&gt;</div><div class="line">       &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt;</div><div class="line">       &lt;td class=&quot;stage-progress-cell&quot;&gt;</div><div class="line">         &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125;</div><div class="line">         &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125;</div><div class="line">         &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125;</div><div class="line">       &lt;/td&gt;</div><div class="line">       &lt;td class=&quot;progress-cell&quot;&gt;</div><div class="line">         &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks,</div><div class="line">          failed = job.numFailedTasks, skipped = job.numSkippedTasks,</div><div class="line">          total = job.numTasks - job.numSkippedTasks)&#125;</div><div class="line">       &lt;/td&gt;</div><div class="line">     &lt;/tr&gt;</div><div class="line">   &#125;</div><div class="line"></div><div class="line">   &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt;</div><div class="line">     &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt;</div><div class="line">     &lt;tbody&gt;</div><div class="line">       &#123;jobs.map(makeRow)&#125;</div><div class="line">     &lt;/tbody&gt;</div><div class="line">   &lt;/table&gt;</div><div class="line"> &#125;</div></pre></td></tr></table></figure></p>
<h2 id="4-5-SparkUI的启动"><a href="#4-5-SparkUI的启动" class="headerlink" title="4.5.SparkUI的启动"></a>4.5.SparkUI的启动</h2><p>SparkUI创建好后,需要调用父类WebUI的bind方法,绑定服务和端口,bind方法中主要的代码实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name))</div></pre></td></tr></table></figure></p>
<p>最终启动了Jetty提供的服务,默认端口是4040</p>
<h1 id="5-Hadoop相关配置及Executor环境变量"><a href="#5-Hadoop相关配置及Executor环境变量" class="headerlink" title="5.Hadoop相关配置及Executor环境变量"></a>5.Hadoop相关配置及Executor环境变量</h1><h2 id="5-1-Hadoop相关配置信息"><a href="#5-1-Hadoop相关配置信息" class="headerlink" title="5.1.Hadoop相关配置信息"></a>5.1.Hadoop相关配置信息</h2><p>默认情况下,Spark使用HDFS作为分布式文件系统,所以需要获取Hadoop相关配置信息的,在SparkContext的相关代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)</div></pre></td></tr></table></figure></p>
<p>获取的配置信息包括:</p>
<ul>
<li>将Amazon S3文件系统的AccessKeyId和SecretAccessKey加载到Hadoop的Configuration</li>
<li>将SparkConf中所有以spark.hadoop.开头的属性都复制到Hadoop的Configuration</li>
<li>将SparkConf的属性spark.buff.size复制为Hadoop的Configuration的配置io.file.buffer.size</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line">def newConfiguration(conf: SparkConf): Configuration = &#123;</div><div class="line">  val hadoopConf = new Configuration()</div><div class="line"></div><div class="line">  // Note: this null check is around more than just access to the &quot;conf&quot; object to maintain</div><div class="line">  // the behavior of the old implementation of this code, for backwards compatibility.</div><div class="line">  if (conf != null) &#123;</div><div class="line">    // Explicitly check for S3 environment variables</div><div class="line">    if (System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;) != null &amp;&amp;</div><div class="line">        System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;) != null) &#123;</div><div class="line">      val keyId = System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;)</div><div class="line">      val accessKey = System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;)</div><div class="line"></div><div class="line">      hadoopConf.set(&quot;fs.s3.awsAccessKeyId&quot;, keyId)</div><div class="line">      hadoopConf.set(&quot;fs.s3n.awsAccessKeyId&quot;, keyId)</div><div class="line">      hadoopConf.set(&quot;fs.s3a.access.key&quot;, keyId)</div><div class="line">      hadoopConf.set(&quot;fs.s3.awsSecretAccessKey&quot;, accessKey)</div><div class="line">      hadoopConf.set(&quot;fs.s3n.awsSecretAccessKey&quot;, accessKey)</div><div class="line">      hadoopConf.set(&quot;fs.s3a.secret.key&quot;, accessKey)</div><div class="line">    &#125;</div><div class="line"></div><div class="line"></div><div class="line">    // Copy any &quot;spark.hadoop.foo=bar&quot; system properties into conf as &quot;foo=bar&quot;</div><div class="line">    conf.getAll.foreach &#123; case (key, value) =&gt;</div><div class="line">      if (key.startsWith(&quot;spark.hadoop.&quot;)) &#123;</div><div class="line">        hadoopConf.set(key.substring(&quot;spark.hadoop.&quot;.length), value)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">	</div><div class="line">    val bufferSize = conf.get(&quot;spark.buffer.size&quot;, &quot;65536&quot;)</div><div class="line">    hadoopConf.set(&quot;io.file.buffer.size&quot;, bufferSize)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  hadoopConf</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注意:如果指定了SPARK_YARN_MODE属性,则会使用YarnSparkHadoopUtil,否则默认为SparkHadoopUtil</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">object SparkHadoopUtil &#123;</div><div class="line"></div><div class="line">//2种模式</div><div class="line">  private lazy val hadoop = new SparkHadoopUtil</div><div class="line">  private lazy val yarn = try &#123;</div><div class="line">    Utils.classForName(&quot;org.apache.spark.deploy.yarn.YarnSparkHadoopUtil&quot;)</div><div class="line">      .newInstance()</div><div class="line">      .asInstanceOf[SparkHadoopUtil]</div><div class="line">  &#125; catch &#123;</div><div class="line">    case e: Exception =&gt; throw new SparkException(&quot;Unable to load YARN support&quot;, e)</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">  val SPARK_YARN_CREDS_TEMP_EXTENSION = &quot;.tmp&quot;</div><div class="line"></div><div class="line">  val SPARK_YARN_CREDS_COUNTER_DELIM = &quot;-&quot;</div><div class="line"></div><div class="line">//SPARK_YARN_MODE选择对应的Utils</div><div class="line">  def get: SparkHadoopUtil = &#123;</div><div class="line">    // Check each time to support changing to/from YARN</div><div class="line">    val yarnMode = java.lang.Boolean.valueOf(</div><div class="line">        System.getProperty(&quot;SPARK_YARN_MODE&quot;, System.getenv(&quot;SPARK_YARN_MODE&quot;)))</div><div class="line">    if (yarnMode) &#123;</div><div class="line">      yarn  //</div><div class="line">    &#125; else &#123;</div><div class="line">      hadoop</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="5-2-Executor环境变量"><a href="#5-2-Executor环境变量" class="headerlink" title="5.2.Executor环境变量"></a>5.2.Executor环境变量</h2><p>对Executor的环境变量的处理,如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">_executorMemory = _conf.getOption(&quot;spark.executor.memory&quot;)</div><div class="line">  .orElse(Option(System.getenv(&quot;SPARK_EXECUTOR_MEMORY&quot;)))</div><div class="line">  .orElse(Option(System.getenv(&quot;SPARK_MEM&quot;))</div><div class="line">  .map(warnSparkMem))</div><div class="line">  .map(Utils.memoryStringToMb)</div><div class="line">  .getOrElse(1024)</div><div class="line"></div><div class="line">// Convert java options to env vars as a work around</div><div class="line">// since we can&apos;t set env vars directly in sbt.</div><div class="line">for &#123; (envKey, propKey) &lt;- Seq((&quot;SPARK_TESTING&quot;, &quot;spark.testing&quot;))</div><div class="line">  value &lt;- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))&#125; &#123;</div><div class="line">  executorEnvs(envKey) = value</div><div class="line">&#125;</div><div class="line">Option(System.getenv(&quot;SPARK_PREPEND_CLASSES&quot;)).foreach &#123; v =&gt;</div><div class="line">  executorEnvs(&quot;SPARK_PREPEND_CLASSES&quot;) = v</div><div class="line">&#125;</div><div class="line">// The Mesos scheduler backend relies on this environment variable to set executor memory.</div><div class="line">// TODO: Set this only in the Mesos scheduler.</div><div class="line">executorEnvs(&quot;SPARK_EXECUTOR_MEMORY&quot;) = executorMemory + &quot;m&quot;</div><div class="line">executorEnvs ++= _conf.getExecutorEnv</div><div class="line">executorEnvs(&quot;SPARK_USER&quot;) = sparkUser</div></pre></td></tr></table></figure>
<p>executorEnvs包含的环境变量将在7.2.2节中介绍的注册应用的过程中发送给Master,Master给Worker发送调度后,Worker最终使用executorEnvs提供的信息启动Executor,可以通过配置spark.executor.memory指定Executor占用的内存大小,也可以配置系统变量SPARK_EXECUTOR_MEMORY或者SPARK_MEM对其大小进行设置</p>
<h1 id="6-创建任务调度器TaskScheduler"><a href="#6-创建任务调度器TaskScheduler" class="headerlink" title="6.创建任务调度器TaskScheduler"></a>6.创建任务调度器TaskScheduler</h1><p>TaskScheduler也是SparkContext的重要组成部分,负责任务的提交,并且请求集群管理器对任务调度,TaskScheduler也可以看做任务调度的客户端,创建TaskScheduler的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val (sched, ts) = SparkContext.createTaskScheduler(this, master)</div><div class="line">_schedulerBackend = sched</div><div class="line">_taskScheduler = ts</div></pre></td></tr></table></figure></p>
<p>createTaskScheduler方法会根据master的配置匹配部署模式,创建TaskSchedulerImpl,并生成不同的SchedulerBackend,本章为了使读者更容易理解Spark的初始化流程,故以local模式为例,其余模式将在第7章详解,master匹配local模式的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">private def createTaskScheduler(</div><div class="line">    sc: SparkContext,</div><div class="line">    master: String): (SchedulerBackend, TaskScheduler) = &#123;</div><div class="line">  import SparkMasterRegex._</div><div class="line"></div><div class="line">  // When running locally, don&apos;t try to re-execute tasks on failure.</div><div class="line">  val MAX_LOCAL_TASK_FAILURES = 1</div><div class="line"></div><div class="line">  master match &#123;</div><div class="line">    case &quot;local&quot; =&gt;</div><div class="line">      val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)</div><div class="line">      val backend = new LocalBackend(sc.getConf, scheduler, 1)</div><div class="line">      scheduler.initialize(backend)</div><div class="line">      (backend, scheduler)</div><div class="line">//...</div></pre></td></tr></table></figure></p>
<h2 id="6-1-创建TaskSchedulerImpl"><a href="#6-1-创建TaskSchedulerImpl" class="headerlink" title="6.1.创建TaskSchedulerImpl"></a>6.1.创建TaskSchedulerImpl</h2><p>TaskSchedulerImpl的构造过程如下:<br>1.从SparkConf中读取配置信息,包括每个任务分配的CPU数,调度模式(调度模式有fair和fifo两种,默认为fifo,可以修改属性spark.scheduler.mode来改变)等<br>2.创建TaskSchedulerGetter,他的作用是通过线程池(Executor.newFixedThreadPool创建的,默认4个线程,线程名字以task-result-getter开头,线程工厂默认是Executors.defaultThreadFactory)对Worker上的Executor发送的Task的执行结果进行处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">// Listener object to pass upcalls into</div><div class="line">var dagScheduler: DAGScheduler = null</div><div class="line"></div><div class="line">var backend: SchedulerBackend = null</div><div class="line"></div><div class="line">val mapOutputTracker = SparkEnv.get.mapOutputTracker</div><div class="line"></div><div class="line">var schedulableBuilder: SchedulableBuilder = null</div><div class="line">var rootPool: Pool = null</div><div class="line">// default scheduler is FIFO</div><div class="line">private val schedulingModeConf = conf.get(&quot;spark.scheduler.mode&quot;, &quot;FIFO&quot;)</div><div class="line">val schedulingMode: SchedulingMode = try &#123;</div><div class="line">  SchedulingMode.withName(schedulingModeConf.toUpperCase)</div><div class="line">&#125; catch &#123;</div><div class="line">  case e: java.util.NoSuchElementException =&gt;</div><div class="line">    throw new SparkException(s&quot;Unrecognized spark.scheduler.mode: $schedulingModeConf&quot;)</div><div class="line">&#125;</div><div class="line"></div><div class="line">// This is a var so that we can reset it for testing purposes.</div><div class="line">private[spark] var taskResultGetter = new TaskResultGetter(sc.env, this)</div></pre></td></tr></table></figure>
<p>TaskSchedulerImpl的调度模式有fair和fifo两种,任务的最终调度实际都是落实到接口SchedulerBackend的具体实现上的,为方便分析,我们先来看看local模式中SchedulerBackend的实现LocalBackend,LocalBackend依赖localEndpoint与ActorSystem进行消息通信,LocalBackend的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div></pre></td><td class="code"><pre><div class="line">private[spark] class LocalBackend(</div><div class="line">    conf: SparkConf,</div><div class="line">    scheduler: TaskSchedulerImpl,</div><div class="line">    val totalCores: Int)</div><div class="line">  extends SchedulerBackend with ExecutorBackend with Logging &#123;</div><div class="line"></div><div class="line">  private val appId = &quot;local-&quot; + System.currentTimeMillis</div><div class="line">  private var localEndpoint: RpcEndpointRef = null</div><div class="line">  </div><div class="line">  //....  </div><div class="line"></div><div class="line">  override def start() &#123;</div><div class="line">    val rpcEnv = SparkEnv.get.rpcEnv</div><div class="line">    val executorEndpoint = new LocalEndpoint(rpcEnv, userClassPath, scheduler, this, totalCores)</div><div class="line">    localEndpoint = rpcEnv.setupEndpoint(&quot;LocalBackendEndpoint&quot;, executorEndpoint)</div><div class="line">    listenerBus.post(SparkListenerExecutorAdded(</div><div class="line">      System.currentTimeMillis,</div><div class="line">      executorEndpoint.localExecutorId,</div><div class="line">      new ExecutorInfo(executorEndpoint.localExecutorHostname, totalCores, Map.empty)))</div><div class="line">    launcherBackend.setAppId(appId)</div><div class="line">    launcherBackend.setState(SparkAppHandle.State.RUNNING)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def stop() &#123;</div><div class="line">    stop(SparkAppHandle.State.FINISHED)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def reviveOffers() &#123;</div><div class="line">    localEndpoint.send(ReviveOffers)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def defaultParallelism(): Int =</div><div class="line">    scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores)</div><div class="line"></div><div class="line">  override def killTask(taskId: Long, executorId: String, interruptThread: Boolean) &#123;</div><div class="line">    localEndpoint.send(KillTask(taskId, interruptThread))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def statusUpdate(taskId: Long, state: TaskState, serializedData: ByteBuffer) &#123;</div><div class="line">    localEndpoint.send(StatusUpdate(taskId, state, serializedData))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def applicationId(): String = appId</div><div class="line"></div><div class="line">  private def stop(finalState: SparkAppHandle.State): Unit = &#123;</div><div class="line">    localEndpoint.ask(StopExecutor)</div><div class="line">    try &#123;</div><div class="line">      launcherBackend.setState(finalState)</div><div class="line">    &#125; finally &#123;</div><div class="line">      launcherBackend.close()</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="6-2-TaskSchedulerImpl的初始化"><a href="#6-2-TaskSchedulerImpl的初始化" class="headerlink" title="6.2.TaskSchedulerImpl的初始化"></a>6.2.TaskSchedulerImpl的初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">  private def createTaskScheduler(</div><div class="line">      sc: SparkContext,</div><div class="line">      master: String): (SchedulerBackend, TaskScheduler) = &#123;</div><div class="line">    import SparkMasterRegex._</div><div class="line"></div><div class="line">    // When running locally, don&apos;t try to re-execute tasks on failure.</div><div class="line">    val MAX_LOCAL_TASK_FAILURES = 1</div><div class="line"></div><div class="line">    master match &#123;</div><div class="line">      case &quot;local&quot; =&gt;</div><div class="line">        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)</div><div class="line">        val backend = new LocalBackend(sc.getConf, scheduler, 1)</div><div class="line">        scheduler.initialize(backend)</div><div class="line">        (backend, scheduler)</div><div class="line"></div><div class="line">//....</div></pre></td></tr></table></figure>
<p>创建完TaskSchedulerImpl和LocalBackend后对TaskSchedulerImpl调用方法initialize进行初始化,以默认的fifo调度为例,TaskSchedulerImpl的初始化过程如下:<br>1.使TaskSchedulerImpl持有LocalBackend的引用<br>2.创建Pool,Pool中缓存了调度队列,调度算法及TaskSetManager集合等信息<br>3.创建FIFOSchedulableBuilder,FIFOSchedulableBuilder用来操作Pool中的调度队列</p>
<p>initialize方法的实现如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">def initialize(backend: SchedulerBackend) &#123;</div><div class="line">  this.backend = backend</div><div class="line">  // temporarily set rootPool name to empty</div><div class="line">  rootPool = new Pool(&quot;&quot;, schedulingMode, 0, 0)</div><div class="line">  schedulableBuilder = &#123;</div><div class="line">    schedulingMode match &#123;</div><div class="line">      case SchedulingMode.FIFO =&gt;</div><div class="line">        new FIFOSchedulableBuilder(rootPool)</div><div class="line">      case SchedulingMode.FAIR =&gt;</div><div class="line">        new FairSchedulableBuilder(rootPool, conf)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  schedulableBuilder.buildPools()</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="7-创建和启动DAGScheduler"><a href="#7-创建和启动DAGScheduler" class="headerlink" title="7.创建和启动DAGScheduler"></a>7.创建和启动DAGScheduler</h1><p>DAGScheduler主要在任务正式交给TaskSchedulerImpl提交之前做一些准备工作,包括:创建Job,将DAG中的RDD划分到不同的stage,提交stage等等,创建DAGScheduler的代码在SparkContext中如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">_dagScheduler = new DAGScheduler(this)</div></pre></td></tr></table></figure></p>
<p>DAGScheduler的数据结构主要维护jobId和stageId的关系,Stage,ActiveJob,以及缓存的RDD的Partitions的位置信息,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">private[scheduler] val nextJobId = new AtomicInteger(0)</div><div class="line">private[scheduler] def numTotalJobs: Int = nextJobId.get()</div><div class="line">private val nextStageId = new AtomicInteger(0)</div><div class="line"></div><div class="line">private[scheduler] val jobIdToStageIds = new HashMap[Int, HashSet[Int]]</div><div class="line">private[scheduler] val stageIdToStage = new HashMap[Int, Stage]</div><div class="line">private[scheduler] val shuffleToMapStage = new HashMap[Int, ShuffleMapStage]</div><div class="line">private[scheduler] val jobIdToActiveJob = new HashMap[Int, ActiveJob]</div><div class="line"></div><div class="line">// Stages we need to run whose parents aren&apos;t done</div><div class="line">private[scheduler] val waitingStages = new HashSet[Stage]</div><div class="line"></div><div class="line">// Stages we are running right now</div><div class="line">private[scheduler] val runningStages = new HashSet[Stage]</div><div class="line"></div><div class="line">// Stages that must be resubmitted due to fetch failures</div><div class="line">private[scheduler] val failedStages = new HashSet[Stage]</div><div class="line"></div><div class="line">private[scheduler] val activeJobs = new HashSet[ActiveJob]</div><div class="line"></div><div class="line">/**</div><div class="line"> * Contains the locations that each RDD&apos;s partitions are cached on.  This map&apos;s keys are RDD ids</div><div class="line"> * and its values are arrays indexed by partition numbers. Each array value is the set of</div><div class="line"> * locations where that RDD partition is cached.</div><div class="line"> *</div><div class="line"> * All accesses to this map should be guarded by synchronizing on it (see SPARK-4454).</div><div class="line"> */</div><div class="line">private val cacheLocs = new HashMap[Int, IndexedSeq[Seq[TaskLocation]]]</div><div class="line"></div><div class="line">// For tracking failed nodes, we use the MapOutputTracker&apos;s epoch number, which is sent with</div><div class="line">// every task. When we detect a node failing, we note the current epoch number and failed</div><div class="line">// executor, increment it for new tasks, and use this to ignore stray ShuffleMapTask results.</div><div class="line">//</div><div class="line">// TODO: Garbage collect information about failure epochs when we know there are no more</div><div class="line">//       stray messages to detect.</div><div class="line">private val failedEpoch = new HashMap[String, Long]</div><div class="line"></div><div class="line">private [scheduler] val outputCommitCoordinator = env.outputCommitCoordinator</div><div class="line"></div><div class="line">// A closure serializer that we reuse.</div><div class="line">// This is only safe because DAGScheduler runs in a single thread.</div><div class="line">private val closureSerializer = SparkEnv.get.closureSerializer.newInstance()</div><div class="line"></div><div class="line">private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this)</div></pre></td></tr></table></figure></p>
<p>上面的代码中有一个DAGSchedulerEventProcessLoop,他继承了EventLoop,EventLoop的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">private[spark] abstract class EventLoop[E](name: String) extends Logging &#123;</div><div class="line"></div><div class="line">  private val eventQueue: BlockingQueue[E] = new LinkedBlockingDeque[E]()</div><div class="line"></div><div class="line">  private val stopped = new AtomicBoolean(false)</div><div class="line"></div><div class="line">  private val eventThread = new Thread(name) &#123;</div><div class="line">    setDaemon(true)</div><div class="line"></div><div class="line">    override def run(): Unit = &#123;</div><div class="line">      try &#123;</div><div class="line">        while (!stopped.get) &#123;</div><div class="line">          val event = eventQueue.take()</div><div class="line">          try &#123;</div><div class="line">            onReceive(event)</div><div class="line">          &#125; catch &#123;</div><div class="line">            case NonFatal(e) =&gt; &#123;</div><div class="line">              try &#123;</div><div class="line">                onError(e)</div><div class="line">              &#125; catch &#123;</div><div class="line">                case NonFatal(e) =&gt; logError(&quot;Unexpected error in &quot; + name, e)</div><div class="line">              &#125;</div><div class="line">            &#125;</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125; catch &#123;</div><div class="line">        case ie: InterruptedException =&gt; // exit even if eventQueue is not empty</div><div class="line">        case NonFatal(e) =&gt; logError(&quot;Unexpected error in &quot; + name, e)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def start(): Unit = &#123;</div><div class="line">    if (stopped.get) &#123;</div><div class="line">      throw new IllegalStateException(name + &quot; has already been stopped&quot;)</div><div class="line">    &#125;</div><div class="line">    // Call onStart before starting the event thread to make sure it happens before onReceive</div><div class="line">    onStart()</div><div class="line">    eventThread.start()</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def stop(): Unit = &#123;</div><div class="line">    if (stopped.compareAndSet(false, true)) &#123;</div><div class="line">      eventThread.interrupt()</div><div class="line">      var onStopCalled = false</div><div class="line">      try &#123;</div><div class="line">        eventThread.join()</div><div class="line">        // Call onStop after the event thread exits to make sure onReceive happens before onStop</div><div class="line">        onStopCalled = true</div><div class="line">        onStop()</div><div class="line">      &#125; catch &#123;</div><div class="line">        case ie: InterruptedException =&gt;</div><div class="line">          Thread.currentThread().interrupt()</div><div class="line">          if (!onStopCalled) &#123;</div><div class="line">            // ie is thrown from `eventThread.join()`. Otherwise, we should not call `onStop` since</div><div class="line">            // it&apos;s already called.</div><div class="line">            onStop()</div><div class="line">          &#125;</div><div class="line">      &#125;</div><div class="line">    &#125; else &#123;</div><div class="line">      // Keep quiet to allow calling `stop` multiple times.</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  /**</div><div class="line">   * Put the event into the event queue. The event thread will process it later.</div><div class="line">   */</div><div class="line">  def post(event: E): Unit = &#123;</div><div class="line">    eventQueue.put(event)</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>他会有一个线程从队列中循环取Event,然后调用onReceive(event)去处理事件,onReceive在DAGSchedulerEventProcessLoop类中实现,最终是match去匹配处理不同的事件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div></pre></td><td class="code"><pre><div class="line">override def onReceive(event: DAGSchedulerEvent): Unit = &#123;</div><div class="line">  val timerContext = timer.time()</div><div class="line">  try &#123;</div><div class="line">    doOnReceive(event)</div><div class="line">  &#125; finally &#123;</div><div class="line">    timerContext.stop()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123;</div><div class="line">  case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</div><div class="line">    dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</div><div class="line"></div><div class="line">  case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt;</div><div class="line">    dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</div><div class="line"></div><div class="line">  case StageCancelled(stageId) =&gt;</div><div class="line">    dagScheduler.handleStageCancellation(stageId)</div><div class="line"></div><div class="line">  case JobCancelled(jobId) =&gt;</div><div class="line">    dagScheduler.handleJobCancellation(jobId)</div><div class="line"></div><div class="line">  case JobGroupCancelled(groupId) =&gt;</div><div class="line">    dagScheduler.handleJobGroupCancelled(groupId)</div><div class="line"></div><div class="line">  case AllJobsCancelled =&gt;</div><div class="line">    dagScheduler.doCancelAllJobs()</div><div class="line"></div><div class="line">  case ExecutorAdded(execId, host) =&gt;</div><div class="line">    dagScheduler.handleExecutorAdded(execId, host)</div><div class="line"></div><div class="line">  case ExecutorLost(execId) =&gt;</div><div class="line">    dagScheduler.handleExecutorLost(execId, fetchFailed = false)</div><div class="line"></div><div class="line">  case BeginEvent(task, taskInfo) =&gt;</div><div class="line">    dagScheduler.handleBeginEvent(task, taskInfo)</div><div class="line"></div><div class="line">  case GettingResultEvent(taskInfo) =&gt;</div><div class="line">    dagScheduler.handleGetTaskResult(taskInfo)</div><div class="line"></div><div class="line">  case completion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =&gt;</div><div class="line">    dagScheduler.handleTaskCompletion(completion)</div><div class="line"></div><div class="line">  case TaskSetFailed(taskSet, reason, exception) =&gt;</div><div class="line">    dagScheduler.handleTaskSetFailed(taskSet, reason, exception)</div><div class="line"></div><div class="line">  case ResubmitFailedStages =&gt;</div><div class="line">    dagScheduler.resubmitFailedStages()</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="8-TaskScheduler的启动"><a href="#8-TaskScheduler的启动" class="headerlink" title="8.TaskScheduler的启动"></a>8.TaskScheduler的启动</h1><p>在3.6节介绍了任务调度器TaskScheduler的创建,要想TaskScheduler发挥作用,必须要启动它,如下代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&apos;s constructor</div><div class="line">_taskScheduler.start()</div></pre></td></tr></table></figure></p>
<p>TaskScheduler在启动的时候,实际调用了Backend的start方法,以TaskSchedulerImpl为例<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">override def start() &#123;</div><div class="line">  backend.start()</div><div class="line"></div><div class="line">  if (!isLocal &amp;&amp; conf.getBoolean(&quot;spark.speculation&quot;, false)) &#123;</div><div class="line">    logInfo(&quot;Starting speculative execution thread&quot;)</div><div class="line">    speculationScheduler.scheduleAtFixedRate(new Runnable &#123;</div><div class="line">      override def run(): Unit = Utils.tryOrStopSparkContext(sc) &#123;</div><div class="line">        checkSpeculatableTasks()</div><div class="line">      &#125;</div><div class="line">    &#125;, SPECULATION_INTERVAL_MS, SPECULATION_INTERVAL_MS, TimeUnit.MILLISECONDS)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>以LocalBackend为例,启动LocalBackend时向rpcEnv注册了LocalEndpoint,下面是LocalBackend的start方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">override def start() &#123;</div><div class="line">  val rpcEnv = SparkEnv.get.rpcEnv</div><div class="line">  val executorEndpoint = new LocalEndpoint(rpcEnv, userClassPath, scheduler, this, totalCores)</div><div class="line">  localEndpoint = rpcEnv.setupEndpoint(&quot;LocalBackendEndpoint&quot;, executorEndpoint)</div><div class="line">  listenerBus.post(SparkListenerExecutorAdded(</div><div class="line">    System.currentTimeMillis,</div><div class="line">    executorEndpoint.localExecutorId,</div><div class="line">    new ExecutorInfo(executorEndpoint.localExecutorHostname, totalCores, Map.empty)))</div><div class="line">  launcherBackend.setAppId(appId)</div><div class="line">  launcherBackend.setState(SparkAppHandle.State.RUNNING)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="8-1-创建LocalEndpoint"><a href="#8-1-创建LocalEndpoint" class="headerlink" title="8.1.创建LocalEndpoint"></a>8.1.创建LocalEndpoint</h2><p>LocalEndpoint的创建过程主要是构建本地的Executor,如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">private[spark] class LocalEndpoint(</div><div class="line">    override val rpcEnv: RpcEnv,</div><div class="line">    userClassPath: Seq[URL],</div><div class="line">    scheduler: TaskSchedulerImpl,</div><div class="line">    executorBackend: LocalBackend,</div><div class="line">    private val totalCores: Int)</div><div class="line">  extends ThreadSafeRpcEndpoint with Logging &#123;</div><div class="line"></div><div class="line">  private var freeCores = totalCores</div><div class="line"></div><div class="line">  val localExecutorId = SparkContext.DRIVER_IDENTIFIER</div><div class="line">  val localExecutorHostname = &quot;localhost&quot;</div><div class="line"></div><div class="line">  private val executor = new Executor(</div><div class="line">    localExecutorId, localExecutorHostname, SparkEnv.get, userClassPath, isLocal = true)</div><div class="line"></div><div class="line">  override def receive: PartialFunction[Any, Unit] = &#123;</div><div class="line">    case ReviveOffers =&gt;</div><div class="line">      reviveOffers()</div><div class="line"></div><div class="line">    case StatusUpdate(taskId, state, serializedData) =&gt;</div><div class="line">      scheduler.statusUpdate(taskId, state, serializedData)</div><div class="line">      if (TaskState.isFinished(state)) &#123;</div><div class="line">        freeCores += scheduler.CPUS_PER_TASK</div><div class="line">        reviveOffers()</div><div class="line">      &#125;</div><div class="line"></div><div class="line">    case KillTask(taskId, interruptThread) =&gt;</div><div class="line">      executor.killTask(taskId, interruptThread)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">//...</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>Executor的构建代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">// Start worker thread pool</div><div class="line">private val threadPool = ThreadUtils.newDaemonCachedThreadPool(&quot;Executor task launch worker&quot;)</div><div class="line">private val executorSource = new ExecutorSource(threadPool, executorId)</div><div class="line"></div><div class="line">if (!isLocal) &#123;</div><div class="line">  env.metricsSystem.registerSource(executorSource)</div><div class="line">  env.blockManager.initialize(conf.getAppId)</div><div class="line">&#125;</div><div class="line"></div><div class="line">// Whether to load classes in user jars before those in Spark jars</div><div class="line">private val userClassPathFirst = conf.getBoolean(&quot;spark.executor.userClassPathFirst&quot;, false)</div><div class="line"></div><div class="line">// Create our ClassLoader</div><div class="line">// do this after SparkEnv creation so can access the SecurityManager</div><div class="line">private val urlClassLoader = createClassLoader()</div><div class="line">private val replClassLoader = addReplClassLoaderIfNeeded(urlClassLoader)</div><div class="line"></div><div class="line">// Set the classloader for serializer</div><div class="line">env.serializer.setDefaultClassLoader(replClassLoader)</div><div class="line"></div><div class="line">// Akka&apos;s message frame size. If task result is bigger than this, we use the block manager</div><div class="line">// to send the result back.</div><div class="line">private val akkaFrameSize = AkkaUtils.maxFrameSizeBytes(conf)</div><div class="line"></div><div class="line">// Limit of bytes for total size of results (default is 1GB)</div><div class="line">private val maxResultSize = Utils.getMaxResultSize(conf)</div><div class="line"></div><div class="line">// Maintains the list of running tasks.</div><div class="line">private val runningTasks = new ConcurrentHashMap[Long, TaskRunner]</div></pre></td></tr></table></figure></p>
<p>Executor的构建,主要包括以下步骤:<br>1.创建并注册ExecutorSource,ExecutorSource是做什么的呢?在8.2节会有介绍<br>2.获取SparkEnv,如果是非local模式,Worker上的CoarseGrainedExecutorBackend向Driver的CoarseGrainedExecutorBackend注册Executor时,则需要新建SparkEnv<br>3.创建并注册ExecutorActor,Executor负责接收发送给Executor的消息(在spark1.6中没有找到)<br>4.urlClassLoader的创建,为什么需要创建这个urlClassLoader?在非local模式中,Driver或者Worker上都会有多个Executor,每个Executor都设置自身的urlClassLoader,用于加载任务上传的jar包中的类,有效的对任务的类加载环境进行隔离<br>5.创建Executor执行的Task的线程池,此线程池用于执行任务<br>6.启动Executor的心跳线程,此线程用于向Driver发送心跳</p>
<p>此外还包括Akka发送消息的帧大小,结果总大小的字节限制,正在运行的task的列表,设置serializer的默认ClassLoader为创建的ClassLoader等</p>
<h2 id="8-2-ExecutorSource的创建与注册"><a href="#8-2-ExecutorSource的创建与注册" class="headerlink" title="8.2.ExecutorSource的创建与注册"></a>8.2.ExecutorSource的创建与注册</h2><p>ExecutorSource用于测量系统,通过metriRegistry的register方法注册计量,这些计量信息包括threadpool.activeTasks,threadpool.completeTasks,threadpool.currentPool_size,threadpool.maxPool_size等,详见代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">class ExecutorSource(threadPool: ThreadPoolExecutor, executorId: String) extends Source &#123;</div><div class="line"></div><div class="line">  private def fileStats(scheme: String) : Option[FileSystem.Statistics] =</div><div class="line">    FileSystem.getAllStatistics.asScala.find(s =&gt; s.getScheme.equals(scheme))</div><div class="line"></div><div class="line">  private def registerFileSystemStat[T](</div><div class="line">        scheme: String, name: String, f: FileSystem.Statistics =&gt; T, defaultValue: T) = &#123;</div><div class="line">    metricRegistry.register(MetricRegistry.name(&quot;filesystem&quot;, scheme, name), new Gauge[T] &#123;</div><div class="line">      override def getValue: T = fileStats(scheme).map(f).getOrElse(defaultValue)</div><div class="line">    &#125;)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override val metricRegistry = new MetricRegistry()</div><div class="line"></div><div class="line">  override val sourceName = &quot;executor&quot;</div><div class="line"></div><div class="line">  // Gauge for executor thread pool&apos;s actively executing task counts</div><div class="line">  metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;activeTasks&quot;), new Gauge[Int] &#123;</div><div class="line">    override def getValue: Int = threadPool.getActiveCount()</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  // Gauge for executor thread pool&apos;s approximate total number of tasks that have been completed</div><div class="line">  metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;completeTasks&quot;), new Gauge[Long] &#123;</div><div class="line">    override def getValue: Long = threadPool.getCompletedTaskCount()</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  // Gauge for executor thread pool&apos;s current number of threads</div><div class="line">  metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;currentPool_size&quot;), new Gauge[Int] &#123;</div><div class="line">    override def getValue: Int = threadPool.getPoolSize()</div><div class="line">  &#125;)</div><div class="line"></div><div class="line">  // Gauge got executor thread pool&apos;s largest number of threads that have ever simultaneously</div><div class="line">  // been in th pool</div><div class="line">  metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;maxPool_size&quot;), new Gauge[Int] &#123;</div><div class="line">    override def getValue: Int = threadPool.getMaximumPoolSize()</div><div class="line">  &#125;)</div></pre></td></tr></table></figure></p>
<p>创建完ExecutorSource后,调用MetricsSystem的registerSource方法将ExecutorSource注册到MetricsSystem<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">if (!isLocal) &#123;</div><div class="line">  env.metricsSystem.registerSource(executorSource)</div><div class="line">  env.blockManager.initialize(conf.getAppId)</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">def registerSource(source: Source) &#123;</div><div class="line">  sources += source</div><div class="line">  try &#123;</div><div class="line">    val regName = buildRegistryName(source)</div><div class="line">    registry.register(regName, source.metricRegistry)</div><div class="line">  &#125; catch &#123;</div><div class="line">    case e: IllegalArgumentException =&gt; logInfo(&quot;Metrics already registered&quot;, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h2 id="8-4-spark自身ClassLoader的创建"><a href="#8-4-spark自身ClassLoader的创建" class="headerlink" title="8.4.spark自身ClassLoader的创建"></a>8.4.spark自身ClassLoader的创建</h2><p>获取要创建的ClassLoader的父加载器currentLoader,然后根据currentJars生成URL数组,spark.files.userClassPathFirst属性指定加载类时是否先从用户的classpath下加载,最后创建MutableURLClassLoader或者ChildExecutorURLClassLoader</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">private def createClassLoader(): MutableURLClassLoader = &#123;</div><div class="line">  // Bootstrap the list of jars with the user class path.</div><div class="line">  val now = System.currentTimeMillis()</div><div class="line">  userClassPath.foreach &#123; url =&gt;</div><div class="line">    currentJars(url.getPath().split(&quot;/&quot;).last) = now</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  val currentLoader = Utils.getContextOrSparkClassLoader</div><div class="line"></div><div class="line">  // For each of the jars in the jarSet, add them to the class loader.</div><div class="line">  // We assume each of the files has already been fetched.</div><div class="line">  val urls = userClassPath.toArray ++ currentJars.keySet.map &#123; uri =&gt;</div><div class="line">    new File(uri.split(&quot;/&quot;).last).toURI.toURL</div><div class="line">  &#125;</div><div class="line">  if (userClassPathFirst) &#123;</div><div class="line">    new ChildFirstURLClassLoader(urls, currentLoader)</div><div class="line">  &#125; else &#123;</div><div class="line">    new MutableURLClassLoader(urls, currentLoader)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="8-5-启动Executor的心跳线程"><a href="#8-5-启动Executor的心跳线程" class="headerlink" title="8.5.启动Executor的心跳线程"></a>8.5.启动Executor的心跳线程</h2><p>Executor的心跳由startDriverHeadrtbeater启动,在Executor类中的代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">// Executor for the heartbeat task.</div><div class="line">private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;driver-heartbeater&quot;)</div><div class="line"></div><div class="line">// must be initialized before running startDriverHeartbeat()</div><div class="line">private val heartbeatReceiverRef =</div><div class="line">  RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)</div><div class="line"></div><div class="line">startDriverHeartbeater()</div><div class="line"></div><div class="line"></div><div class="line">private def startDriverHeartbeater(): Unit = &#123;</div><div class="line">  val intervalMs = conf.getTimeAsMs(&quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;)</div><div class="line"></div><div class="line">  // Wait a random interval so the heartbeats don&apos;t end up in sync</div><div class="line">  val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf[Int]</div><div class="line"></div><div class="line">  val heartbeatTask = new Runnable() &#123;</div><div class="line">    override def run(): Unit = Utils.logUncaughtExceptions(reportHeartBeat())</div><div class="line">  &#125;</div><div class="line">  heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>Executor心跳线程的间隔由属性spark.executor.heartbeatInterval配置,默认是10s,此外还有一个延迟时间,最终是调用run方法中的reportHeartBeat方法,该方法代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/** Reports heartbeat and metrics for active tasks to the driver. */</div><div class="line">private def reportHeartBeat(): Unit = &#123;</div><div class="line">  // list of (task id, metrics) to send back to the driver</div><div class="line">  val tasksMetrics = new ArrayBuffer[(Long, TaskMetrics)]()</div><div class="line">  val curGCTime = computeTotalGcTime()</div><div class="line"></div><div class="line">  for (taskRunner &lt;- runningTasks.values().asScala) &#123;</div><div class="line">    if (taskRunner.task != null) &#123;</div><div class="line">      taskRunner.task.metrics.foreach &#123; metrics =&gt;</div><div class="line">        metrics.updateShuffleReadMetrics()</div><div class="line">        metrics.updateInputMetrics()</div><div class="line">        metrics.setJvmGCTime(curGCTime - taskRunner.startGCTime)</div><div class="line">        metrics.updateAccumulators()</div><div class="line"></div><div class="line">        if (isLocal) &#123;</div><div class="line">          // JobProgressListener will hold an reference of it during</div><div class="line">          // onExecutorMetricsUpdate(), then JobProgressListener can not see</div><div class="line">          // the changes of metrics any more, so make a deep copy of it</div><div class="line">          val copiedMetrics = Utils.deserialize[TaskMetrics](Utils.serialize(metrics))</div><div class="line">          tasksMetrics += ((taskRunner.taskId, copiedMetrics))</div><div class="line">        &#125; else &#123;</div><div class="line">          // It will be copied by serialization</div><div class="line">          tasksMetrics += ((taskRunner.taskId, metrics))</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  val message = Heartbeat(executorId, tasksMetrics.toArray, env.blockManager.blockManagerId)</div><div class="line">  try &#123;</div><div class="line">    val response = heartbeatReceiverRef.askWithRetry[HeartbeatResponse](</div><div class="line">        message, RpcTimeout(conf, &quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;))</div><div class="line">    if (response.reregisterBlockManager) &#123;</div><div class="line">      logInfo(&quot;Told to re-register on heartbeat&quot;)</div><div class="line">      env.blockManager.reregister()</div><div class="line">    &#125;</div><div class="line">  &#125; catch &#123;</div><div class="line">    case NonFatal(e) =&gt; logWarning(&quot;Issue communicating with driver in heartbeater&quot;, e)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这个心跳的作用有两个<br>1.更新正在处理的任务的测量信息<br>2.通知BlockManagerMaster,此Executor上的BlockManager依然活着</p>
<p>下面是对心跳线程的实现详细分析<br>初始化TaskSchedulerImpl后会创建心跳接收器HeartbeatReceiver,HeartbeatReceiver接收所有分配给当前DriverApplication的Executor的心跳,并将Task,Task计量信息,心跳等交给TaskSchedulerImpl和DAGScheduler作进一步处理,创建心跳接收器的代码在SparkContext中如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// We need to register &quot;HeartbeatReceiver&quot; before &quot;createTaskScheduler&quot; because Executor will</div><div class="line">// retrieve &quot;HeartbeatReceiver&quot; in the constructor. (SPARK-6640)</div><div class="line">_heartbeatReceiver = env.rpcEnv.setupEndpoint(</div><div class="line">  HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))</div></pre></td></tr></table></figure></p>
<p>HeartbeatReceiver在接收到心跳消息后,会调用TaskScheduler的executorHeartbeatReceived方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">override def executorHeartbeatReceived(</div><div class="line">    execId: String,</div><div class="line">    taskMetrics: Array[(Long, TaskMetrics)], // taskId -&gt; TaskMetrics</div><div class="line">    blockManagerId: BlockManagerId): Boolean = &#123;</div><div class="line"></div><div class="line">  val metricsWithStageIds: Array[(Long, Int, Int, TaskMetrics)] = synchronized &#123;</div><div class="line">    taskMetrics.flatMap &#123; case (id, metrics) =&gt;</div><div class="line">      taskIdToTaskSetManager.get(id).map &#123; taskSetMgr =&gt;</div><div class="line">        (id, taskSetMgr.stageId, taskSetMgr.taskSet.stageAttemptId, metrics)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  dagScheduler.executorHeartbeatReceived(execId, metricsWithStageIds, blockManagerId)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>这段程序通过遍历taskMetrices,依据taskIdToTaskSetId和activeTaskSets找到TaskSetManager,然后将taskId,TaskSetManager.stageId,TaskSetManager.taskSet.attempt,TaskMetrices封装到Array[(Long, Int, Int, TaskMetrics)]的数组中,最后调用dagScheduler的executorHeartbeatReceived方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def executorHeartbeatReceived(</div><div class="line">    execId: String,</div><div class="line">    taskMetrics: Array[(Long, Int, Int, TaskMetrics)], // (taskId, stageId, stateAttempt, metrics)</div><div class="line">    blockManagerId: BlockManagerId): Boolean = &#123;</div><div class="line">  listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, taskMetrics))</div><div class="line">  blockManagerMaster.driverEndpoint.askWithRetry[Boolean](</div><div class="line">    BlockManagerHeartbeat(blockManagerId), new RpcTimeout(600 seconds, &quot;BlockManagerHeartbeat&quot;))</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>dagScheduler将executorId,metricsWithStageIds封装为SparkListenerExecutorMetricsUpdate事件,并post到listenerBus中,此事件用于更新stage的各种测量数据,最后给BlockManagerMaster持有的driverEndpoint发送BlockManagerHeartbeat,在local模式下Executor的心跳通信过程如下图:</p>
<p><img src="/assert/img/bigdata/深入理解spark核心思想与源码分析/2/heartbeat.png" alt=""></p>
<p>在非local模式下,Executor发送心跳的过程是一样的,主要的区别是Executor进程与Driver不再同一个进程,甚至不再同一个节点上</p>
<h1 id="9-启动测量系统MetricsSystem"><a href="#9-启动测量系统MetricsSystem" class="headerlink" title="9.启动测量系统MetricsSystem"></a>9.启动测量系统MetricsSystem</h1><p>MetricsSystem使用codahale提供的第三方测量仓库Metrics,MetricsSystem中有三个概念:<br>1.Instance:指定了谁在使用测量系统<br>2.Source:指定了从哪里收集测量数据<br>3.Sink:指定了往哪里输出测量数据</p>
<p>Spark按照Instance的不同,区分为Master,Worker,Application,Driver和Executor</p>
<p>Spark目前提供的Sink有ConsoleSink,CsvSink,jmxSink,MetricsServlet,GraphiteSink等</p>
<p>Spark中使用MetriceServlet作为模式的Sink</p>
<p>MetricsSystem的启动代码在SparkContext中如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">def metricsSystem: MetricsSystem = if (_env != null) _env.metricsSystem else null</div><div class="line"></div><div class="line">metricsSystem.start()</div></pre></td></tr></table></figure></p>
<p>MetricsSystem的启动过程包括以下的步骤:<br>1.注册Source<br>2.注册Sinks<br>3.给Sinks增加Jetty的ServletContextHandler</p>
<p>MetricsSystem启动完毕后,会遍历与Sinks有关的ServletContextHandler,并调用attachHandle将他们绑定到SparkUI上</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">// Attach the driver metrics servlet handler to the web ui after the metrics system is started.</div><div class="line">metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler)))</div></pre></td></tr></table></figure>
<p>start方法的实现如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def start() &#123;</div><div class="line">  require(!running, &quot;Attempting to start a MetricsSystem that is already running&quot;)</div><div class="line">  running = true</div><div class="line">  registerSources()</div><div class="line">  registerSinks()</div><div class="line">  sinks.foreach(_.start)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h2 id="9-1-注册Sources"><a href="#9-1-注册Sources" class="headerlink" title="9.1.注册Sources"></a>9.1.注册Sources</h2><p>registerSources方法用于注册Sources,告诉测量系统从哪里收集测量数据,代码实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">private def registerSources() &#123;</div><div class="line">  val instConfig = metricsConfig.getInstance(instance)</div><div class="line">  val sourceConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SOURCE_REGEX)</div><div class="line"></div><div class="line">  // Register all the sources related to instance</div><div class="line">  sourceConfigs.foreach &#123; kv =&gt;</div><div class="line">    val classPath = kv._2.getProperty(&quot;class&quot;)</div><div class="line">    try &#123;</div><div class="line">      val source = Utils.classForName(classPath).newInstance()</div><div class="line">      registerSource(source.asInstanceOf[Source])</div><div class="line">    &#125; catch &#123;</div><div class="line">      case e: Exception =&gt; logError(&quot;Source class &quot; + classPath + &quot; cannot be instantiated&quot;, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>注册Sources的过程分为以下步骤<br>1.从metricsConfig获取Driver的Properties,默认为创建MetricsSystem的过程中解析的<br>2.用正则匹配Driver的Properties中以source.开头的属性,然后将属性中的Source反射得到的实例加入ArrayBuffer[Source]<br>3.将每个source的metricRegistry注册到ConcurrentMap<string,metric>metrics</string,metric></p>
<h2 id="9-2-注册Sinks"><a href="#9-2-注册Sinks" class="headerlink" title="9.2.注册Sinks"></a>9.2.注册Sinks</h2><p>registerSinks方法用于注册Sinks,即告诉测量系统MetricsSystem往哪里输出测量数据,他的实现代码如下:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">private def registerSinks() &#123;</div><div class="line">  val instConfig = metricsConfig.getInstance(instance)</div><div class="line">  val sinkConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SINK_REGEX)</div><div class="line"></div><div class="line">  sinkConfigs.foreach &#123; kv =&gt;</div><div class="line">    val classPath = kv._2.getProperty(&quot;class&quot;)</div><div class="line">    if (null != classPath) &#123;</div><div class="line">      try &#123;</div><div class="line">        val sink = Utils.classForName(classPath)</div><div class="line">          .getConstructor(classOf[Properties], classOf[MetricRegistry], classOf[SecurityManager])</div><div class="line">          .newInstance(kv._2, registry, securityMgr)</div><div class="line">        if (kv._1 == &quot;servlet&quot;) &#123;</div><div class="line">          metricsServlet = Some(sink.asInstanceOf[MetricsServlet])</div><div class="line">        &#125; else &#123;</div><div class="line">          sinks += sink.asInstanceOf[Sink]</div><div class="line">        &#125;</div><div class="line">      &#125; catch &#123;</div><div class="line">        case e: Exception =&gt; &#123;</div><div class="line">          logError(&quot;Sink class &quot; + classPath + &quot; cannot be instantiated&quot;)</div><div class="line">          throw e</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>注册Sinks的步骤如下:<br>1.从Driver的Properties中用正则匹配以sink.开头的属性,如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&#123;sink.servlet.class=org.apache.spark.metrics.sink.MetricsServlet，sink.servlet.path=/metrics/json&#125;</div></pre></td></tr></table></figure></p>
<p>将其转换为Map（servlet-&gt;{class=org.apache.spark.metrics.sink.MetricsServlet，path=/metrics/json}）<br>2.将子属性class对应的类metricsServlet反射得到MetricsServlet实例,如果属性的key是servlet,将其设置为metricsServlet,如果是sink,则加入到ArrayBuffer[Sink]中</p>
<h2 id="9-3-给Sinks增加Jetty的ServletContextHandler"><a href="#9-3-给Sinks增加Jetty的ServletContextHandler" class="headerlink" title="9.3.给Sinks增加Jetty的ServletContextHandler"></a>9.3.给Sinks增加Jetty的ServletContextHandler</h2><p>为了能够在SparkUI(网页)访问到测量数据,所以需要给Sinks增加Jetty的ServletContextHandler,这里主要用到MetricsSystem的getServletHandlers方法,实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line"> * Get any UI handlers used by this metrics system; can only be called after start().</div><div class="line"> */</div><div class="line">def getServletHandlers: Array[ServletContextHandler] = &#123;</div><div class="line">  require(running, &quot;Can only call getServletHandlers on a running MetricsSystem&quot;)</div><div class="line">  metricsServlet.map(_.getHandlers(conf)).getOrElse(Array())</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>可以看到调用了metricsServlet的getHandlers,其实现如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">def getHandlers(conf: SparkConf): Array[ServletContextHandler] = &#123;</div><div class="line">  Array[ServletContextHandler](</div><div class="line">    createServletHandler(servletPath,</div><div class="line">      new ServletParams(request =&gt; getMetricsSnapshot(request), &quot;text/json&quot;), securityMgr, conf)</div><div class="line">  )</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>最终生成处理/metrics/json请求的ServletContextHandler,而请求的真正处理由getMetricsSnapshot方法,利用fastjson解析,生成的ServletContextHandler通过SparkUI的attachHandler方法,也被绑定到SparkUI,最红我们可以使用以下这些地址来访问测量数据:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">http://localhost:4040/metrics/applications/json</div><div class="line">http://localhost:4040/metrics/json</div><div class="line">http://localhost:4040/metrics/master/json</div></pre></td></tr></table></figure></p>
<h1 id="10-创建和启动ExecutorAllocationManager"><a href="#10-创建和启动ExecutorAllocationManager" class="headerlink" title="10.创建和启动ExecutorAllocationManager"></a>10.创建和启动ExecutorAllocationManager</h1><p>ExecutorAllocationManager用于对已分配的Executor进行管理,创建和启动ExecutorAllocationManager的代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">_executorAllocationManager =</div><div class="line">  if (dynamicAllocationEnabled) &#123;</div><div class="line">    Some(new ExecutorAllocationManager(this, listenerBus, _conf))</div><div class="line">  &#125; else &#123;</div><div class="line">    None</div><div class="line">  &#125;</div><div class="line"></div><div class="line">_executorAllocationManager.foreach(_.start())</div></pre></td></tr></table></figure></p>
<p>默认情况下不会创建ExecutorAllocationManager,可以修改属性spark.dynamicAllocation.enabled为true来创建,ExecutorAllocationManager可以设置动态分配最小Executor数量,动态分配最大Executor数量,每个Executor可以运行的Task数量等配置信息,并对配置信息进行校验,start方法将ExecutorAllocationListener加入listenerBus中,ExecutorAllocationListener通过监听listenBus里的事件,动态添加,删除Executor,并且通过Thread不断添加Executor,遍历Executor,将超时的Executor杀掉并移除,ExecutorAllocationListener的实现与其他SparkListener类似,ExecutorAllocationManager的关键代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">// Clock used to schedule when executors should be added and removed</div><div class="line">private var clock: Clock = new SystemClock()</div><div class="line"></div><div class="line">// Listener for Spark events that impact the allocation policy</div><div class="line">private val listener = new ExecutorAllocationListener</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">/**</div><div class="line"> * Register for scheduler callbacks to decide when to add and remove executors, and start</div><div class="line"> * the scheduling task.</div><div class="line"> */</div><div class="line">def start(): Unit = &#123;</div><div class="line">  listenerBus.addListener(listener)</div><div class="line"></div><div class="line">  val scheduleTask = new Runnable() &#123;</div><div class="line">    override def run(): Unit = &#123;</div><div class="line">      try &#123;</div><div class="line">        schedule()</div><div class="line">      &#125; catch &#123;</div><div class="line">        case ct: ControlThrowable =&gt;</div><div class="line">          throw ct</div><div class="line">        case t: Throwable =&gt;</div><div class="line">          logWarning(s&quot;Uncaught exception in thread $&#123;Thread.currentThread().getName&#125;&quot;, t)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  executor.scheduleAtFixedRate(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="11-ContextCleaner的创建与启动"><a href="#11-ContextCleaner的创建与启动" class="headerlink" title="11.ContextCleaner的创建与启动"></a>11.ContextCleaner的创建与启动</h1><p>ContextCleaner用于清理那些超出应用范围的RDD,ShuffleDependency和Broadcast对象,由于配置属性spark.cleaner.referenceTracking默认是true,所以会构造并请ContextCleaner,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">_cleaner =</div><div class="line">  if (_conf.getBoolean(&quot;spark.cleaner.referenceTracking&quot;, true)) &#123;</div><div class="line">    Some(new ContextCleaner(this))</div><div class="line">  &#125; else &#123;</div><div class="line">    None</div><div class="line">  &#125;</div><div class="line"></div><div class="line">_cleaner.foreach(_.start())</div></pre></td></tr></table></figure></p>
<p>ContextCleaner的组成如下:<br>1.referenceQueue:缓存顶级的AnyRef引用<br>2.referenceBuffer:缓存AnyRef的虚引用<br>3.listeners:缓存清理工作的监听器数组<br>4.cleaningThread:用于具体清理工作的线程</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">private[spark] class ContextCleaner(sc: SparkContext) extends Logging &#123;</div><div class="line"></div><div class="line">  private val referenceBuffer = new ArrayBuffer[CleanupTaskWeakReference]</div><div class="line">    with SynchronizedBuffer[CleanupTaskWeakReference]</div><div class="line"></div><div class="line">  private val referenceQueue = new ReferenceQueue[AnyRef]</div><div class="line"></div><div class="line">  private val listeners = new ArrayBuffer[CleanerListener]</div><div class="line">    with SynchronizedBuffer[CleanerListener]</div><div class="line"></div><div class="line">  private val cleaningThread = new Thread() &#123; override def run() &#123; keepCleaning() &#125;&#125;</div><div class="line"></div><div class="line">///....</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>ContextCleaner的工作原理和listenerBus一样,也采用监听器模式,由线程来处理,此线程实际只是调用keepCleaning方法,该方法的实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/** Keep cleaning RDD, shuffle, and broadcast state. */</div><div class="line">private def keepCleaning(): Unit = Utils.tryOrStopSparkContext(sc) &#123;</div><div class="line">  while (!stopped) &#123;</div><div class="line">    try &#123;</div><div class="line">      val reference = Option(referenceQueue.remove(ContextCleaner.REF_QUEUE_POLL_TIMEOUT))</div><div class="line">        .map(_.asInstanceOf[CleanupTaskWeakReference])</div><div class="line">      // Synchronize here to avoid being interrupted on stop()</div><div class="line">      synchronized &#123;</div><div class="line">        reference.map(_.task).foreach &#123; task =&gt;</div><div class="line">          logDebug(&quot;Got cleaning task &quot; + task)</div><div class="line">          referenceBuffer -= reference.get</div><div class="line">          task match &#123;</div><div class="line">            case CleanRDD(rddId) =&gt;</div><div class="line">              doCleanupRDD(rddId, blocking = blockOnCleanupTasks)</div><div class="line">            case CleanShuffle(shuffleId) =&gt;</div><div class="line">              doCleanupShuffle(shuffleId, blocking = blockOnShuffleCleanupTasks)</div><div class="line">            case CleanBroadcast(broadcastId) =&gt;</div><div class="line">              doCleanupBroadcast(broadcastId, blocking = blockOnCleanupTasks)</div><div class="line">            case CleanAccum(accId) =&gt;</div><div class="line">              doCleanupAccum(accId, blocking = blockOnCleanupTasks)</div><div class="line">            case CleanCheckpoint(rddId) =&gt;</div><div class="line">              doCleanCheckpoint(rddId)</div><div class="line">          &#125;</div><div class="line">        &#125;</div><div class="line">      &#125;</div><div class="line">    &#125; catch &#123;</div><div class="line">      case ie: InterruptedException if stopped =&gt; // ignore</div><div class="line">      case e: Exception =&gt; logError(&quot;Error in cleaning thread&quot;, e)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="12-Spark环境更新"><a href="#12-Spark环境更新" class="headerlink" title="12.Spark环境更新"></a>12.Spark环境更新</h1><p>在SparkContext的初始化过程中,可能对其环境造成影响,所以需要更新环境,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">postEnvironmentUpdate()</div><div class="line">postApplicationStart()</div></pre></td></tr></table></figure></p>
<p>SparkContext初始化过程中,如果设置了spark.jars属性,spark.jars指定的jar包将有addJar方法假如HttpFileServer的jarDir变量指定的路径下,spark.files指定的文件将由addFile方法假如HttpFileServer的fileDir变量指定的路径下,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">/** Post the environment update event once the task scheduler is ready */</div><div class="line">private def postEnvironmentUpdate() &#123;</div><div class="line">  if (taskScheduler != null) &#123;</div><div class="line">    val schedulingMode = getSchedulingMode.toString</div><div class="line">    val addedJarPaths = addedJars.keys.toSeq</div><div class="line">    val addedFilePaths = addedFiles.keys.toSeq</div><div class="line">    val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths,</div><div class="line">      addedFilePaths)</div><div class="line">    val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)</div><div class="line">    listenerBus.post(environmentUpdate)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>httpFileServer的addFile和addJar方法,如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">def addFile(file: File) : String = &#123;</div><div class="line">  addFileToDir(file, fileDir)</div><div class="line">  serverUri + &quot;/files/&quot; + Utils.encodeFileNameToURIRawPath(file.getName)</div><div class="line">&#125;</div><div class="line"></div><div class="line">def addJar(file: File) : String = &#123;</div><div class="line">  addFileToDir(file, jarDir)</div><div class="line">  serverUri + &quot;/jars/&quot; + Utils.encodeFileNameToURIRawPath(file.getName)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>postEnvironmentUpdate的实现见代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">private def postEnvironmentUpdate() &#123;</div><div class="line">  if (taskScheduler != null) &#123;</div><div class="line">    val schedulingMode = getSchedulingMode.toString</div><div class="line">    val addedJarPaths = addedJars.keys.toSeq</div><div class="line">    val addedFilePaths = addedFiles.keys.toSeq</div><div class="line">    val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths,</div><div class="line">      addedFilePaths)</div><div class="line">    val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)</div><div class="line">    listenerBus.post(environmentUpdate)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>其处理步骤如下:<br>1.通过调用SparkEnv的方法environmentDetails最终影响环境的JVM参数,/Spark属性,系统属性,classpath等<br>2.生成时间SparkListenerEnvironmentUpdate,并post到listenerBus,此事件被EnvironmentListener监听,最终影响EnvironmentPage页面中的额输出内容</p>
<p>environmentDetails的代码实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">  def environmentDetails(</div><div class="line">      conf: SparkConf,</div><div class="line">      schedulingMode: String,</div><div class="line">      addedJars: Seq[String],</div><div class="line">      addedFiles: Seq[String]): Map[String, Seq[(String, String)]] = &#123;</div><div class="line"></div><div class="line">    import Properties._</div><div class="line">    val jvmInformation = Seq(</div><div class="line">      (&quot;Java Version&quot;, s&quot;$javaVersion ($javaVendor)&quot;),</div><div class="line">      (&quot;Java Home&quot;, javaHome),</div><div class="line">      (&quot;Scala Version&quot;, versionString)</div><div class="line">    ).sorted</div><div class="line"></div><div class="line">    // Spark properties</div><div class="line">    // This includes the scheduling mode whether or not it is configured (used by SparkUI)</div><div class="line">    val schedulerMode =</div><div class="line">      if (!conf.contains(&quot;spark.scheduler.mode&quot;)) &#123;</div><div class="line">        Seq((&quot;spark.scheduler.mode&quot;, schedulingMode))</div><div class="line">      &#125; else &#123;</div><div class="line">        Seq[(String, String)]()</div><div class="line">      &#125;</div><div class="line">    val sparkProperties = (conf.getAll ++ schedulerMode).sorted</div><div class="line"></div><div class="line">    // System properties that are not java classpaths</div><div class="line">    val systemProperties = Utils.getSystemProperties.toSeq</div><div class="line">    val otherProperties = systemProperties.filter &#123; case (k, _) =&gt;</div><div class="line">      k != &quot;java.class.path&quot; &amp;&amp; !k.startsWith(&quot;spark.&quot;)</div><div class="line">    &#125;.sorted</div><div class="line"></div><div class="line">    // Class paths including all added jars and files</div><div class="line">    val classPathEntries = javaClassPath</div><div class="line">      .split(File.pathSeparator)</div><div class="line">      .filterNot(_.isEmpty)</div><div class="line">      .map((_, &quot;System Classpath&quot;))</div><div class="line">    val addedJarsAndFiles = (addedJars ++ addedFiles).map((_, &quot;Added By User&quot;))</div><div class="line">    val classPaths = (addedJarsAndFiles ++ classPathEntries).sorted</div><div class="line"></div><div class="line">    Map[String, Seq[(String, String)]](</div><div class="line">      &quot;JVM Information&quot; -&gt; jvmInformation,</div><div class="line">      &quot;Spark Properties&quot; -&gt; sparkProperties,</div><div class="line">      &quot;System Properties&quot; -&gt; otherProperties,</div><div class="line">      &quot;Classpath Entries&quot; -&gt; classPaths)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p> postApplicationStart()方法很简单,只是向listenerBus发送了SparkListenerApplicationStart事件,代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">private def postApplicationStart() &#123;</div><div class="line"></div><div class="line">  listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),</div><div class="line">    startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="13-创建DAGSchedulerSource和BlockManagerSource"><a href="#13-创建DAGSchedulerSource和BlockManagerSource" class="headerlink" title="13.创建DAGSchedulerSource和BlockManagerSource"></a>13.创建DAGSchedulerSource和BlockManagerSource</h1><p>在创建DAGSchedulerSource,BlockManagerSource之前首先会调用TaskScheduler的postStartHook方法,其目的是为了等待Backend就绪,如下代码:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">// Post init</div><div class="line">_taskScheduler.postStartHook()</div><div class="line">_env.metricsSystem.registerSource(_dagScheduler.metricsSource)</div><div class="line">_env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))</div><div class="line"></div><div class="line">_executorAllocationManager.foreach &#123; e =&gt;</div><div class="line">  _env.metricsSystem.registerSource(e.executorAllocationManagerSource)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>postStartHook方法如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">override def postStartHook() &#123;</div><div class="line">  waitBackendReady()</div><div class="line">&#125;</div><div class="line"></div><div class="line">private def waitBackendReady(): Unit = &#123;</div><div class="line">  if (backend.isReady) &#123;</div><div class="line">    return</div><div class="line">  &#125;</div><div class="line">  while (!backend.isReady) &#123;</div><div class="line">    synchronized &#123;</div><div class="line">      this.wait(100)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<h1 id="14-将SparkContext标记为激活"><a href="#14-将SparkContext标记为激活" class="headerlink" title="14.将SparkContext标记为激活"></a>14.将SparkContext标记为激活</h1><p>SparkContext初始化的最后将当前SparkContext的状态从contextBeingConstructed(正在构建中)改为activeContext(已激活),代码如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// In order to prevent multiple SparkContexts from being active at the same time, mark this</div><div class="line">// context as having finished construction.</div><div class="line">// NOTE: this must be placed at the end of the SparkContext constructor.</div><div class="line">SparkContext.setActiveContext(this, allowMultipleContexts)</div></pre></td></tr></table></figure></p>
<p>setActiveContext的方法实现如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/**</div><div class="line"> * Called at the end of the SparkContext constructor to ensure that no other SparkContext has</div><div class="line"> * raced with this constructor and started.</div><div class="line"> */</div><div class="line">private[spark] def setActiveContext(</div><div class="line">    sc: SparkContext,</div><div class="line">    allowMultipleContexts: Boolean): Unit = &#123;</div><div class="line">  SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized &#123;</div><div class="line">    assertNoOtherContextIsRunning(sc, allowMultipleContexts)</div><div class="line">    contextBeingConstructed = None</div><div class="line">    activeContext.set(sc)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

      

      
    </div>
    <div class="article-info article-info-index">
      
      <!--
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color1">spark</a>
        		</li>
      		
		</ul>
	</div>
-->
      
	<div class="article-category tagcloud">
		<i class="icon-book icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="/categories/spark/" class="article-tag-list-link color1">spark</a>
        		</li>
      		
		</ul>
	</div>


      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="http://s.jiathis.com/qrcode.php?url=http://yoursite.com/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第2章 spark设计理念与基本架构/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          第2章 spark设计理念与基本架构
        
      </div>
    </a>
  
  
    <a href="/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第4章 存储系统/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">第4章 存储系统</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>






  
  <div class="duoshuo">
	<!-- 多说评论框 start -->
	<div class="ds-thread" data-thread-key="bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化" data-title="第3章 SparkContext的初始化" data-url="http://yoursite.com/2017/04/16/bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化/"></div>
	<!-- 多说评论框 end -->
	<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
	<script type="text/javascript">
	var duoshuoQuery = {short_name:"true"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
	<!-- 多说公共JS代码 end -->
</div>

  




          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Mr. Chen
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true,
		showTags: true
	}
</script>

<script>!function(t){function n(r){if(e[r])return e[r].exports;var o=e[r]={exports:{},id:r,loaded:!1};return t[r].call(o.exports,o,o.exports,n),o.loaded=!0,o.exports}var e={};return n.m=t,n.c=e,n.p="./",n(0)}([function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}function o(t,n){var e=/\/|index.html/g;return t.replace(e,"")===n.replace(e,"")}function i(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,e=0,r=t.length;e<r;e++){var i=t[e];o(n,i.getAttribute("href"))&&(0,d.default)(i,"active")}}function u(t){for(var n=t.offsetLeft,e=t.offsetParent;null!==e;)n+=e.offsetLeft,e=e.offsetParent;return n}function f(t){for(var n=t.offsetTop,e=t.offsetParent;null!==e;)n+=e.offsetTop,e=e.offsetParent;return n}function c(t,n,e,r,o){var i=u(t),c=f(t)-n;if(c-e<=o){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,h.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(e||c)+"px",a.style.left=i+"px",a.style.zIndex=r||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");c(t,document.body.scrollTop,-63,2,0),c(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}function l(){x.default.versions.mobile&&window.screen.width<800&&(i(),s())}var p=e(71),d=r(p),v=e(72),y=(r(v),e(84)),h=r(y),b=e(69),x=r(b),m=e(75),g=r(m),w=e(70);l(),(0,w.addLoadEvent)(function(){g.default.init()}),t.exports={}},function(t,n){var e=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=e)},function(t,n){var e={}.hasOwnProperty;t.exports=function(t,n){return e.call(t,n)}},function(t,n,e){var r=e(49),o=e(15);t.exports=function(t){return r(o(t))}},function(t,n,e){t.exports=!e(8)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,e){var r=e(6),o=e(12);t.exports=e(4)?function(t,n,e){return r.f(t,n,o(1,e))}:function(t,n,e){return t[n]=e,t}},function(t,n,e){var r=e(10),o=e(30),i=e(24),u=Object.defineProperty;n.f=e(4)?Object.defineProperty:function(t,n,e){if(r(t),n=i(n,!0),r(e),o)try{return u(t,n,e)}catch(t){}if("get"in e||"set"in e)throw TypeError("Accessors not supported!");return"value"in e&&(t[n]=e.value),t}},function(t,n,e){var r=e(22)("wks"),o=e(13),i=e(1).Symbol,u="function"==typeof i,f=t.exports=function(t){return r[t]||(r[t]=u&&i[t]||(u?i:o)("Symbol."+t))};f.store=r},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,e){var r=e(35),o=e(16);t.exports=Object.keys||function(t){return r(t,o)}},function(t,n,e){var r=e(11);t.exports=function(t){if(!r(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var e=0,r=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++e+r).toString(36))}},function(t,n){var e=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=e)},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,e){var r=e(6).f,o=e(2),i=e(7)("toStringTag");t.exports=function(t,n,e){t&&!o(t=e?t:t.prototype,i)&&r(t,i,{configurable:!0,value:n})}},function(t,n,e){var r=e(22)("keys"),o=e(13);t.exports=function(t){return r[t]||(r[t]=o(t))}},function(t,n,e){var r=e(1),o="__core-js_shared__",i=r[o]||(r[o]={});t.exports=function(t){return i[t]||(i[t]={})}},function(t,n){var e=Math.ceil,r=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?r:e)(t)}},function(t,n,e){var r=e(11);t.exports=function(t,n){if(!r(t))return t;var e,o;if(n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;if("function"==typeof(e=t.valueOf)&&!r(o=e.call(t)))return o;if(!n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;throw TypeError("Can't convert object to primitive value")}},function(t,n,e){var r=e(1),o=e(14),i=e(18),u=e(26),f=e(6).f;t.exports=function(t){var n=o.Symbol||(o.Symbol=i?{}:r.Symbol||{});"_"==t.charAt(0)||t in n||f(n,t,{value:u.f(t)})}},function(t,n,e){n.f=e(7)},function(t,n,e){var r=e(1),o=e(14),i=e(46),u=e(5),f="prototype",c=function(t,n,e){var a,s,l,p=t&c.F,d=t&c.G,v=t&c.S,y=t&c.P,h=t&c.B,b=t&c.W,x=d?o:o[n]||(o[n]={}),m=x[f],g=d?r:v?r[n]:(r[n]||{})[f];d&&(e=n);for(a in e)s=!p&&g&&void 0!==g[a],s&&a in x||(l=s?g[a]:e[a],x[a]=d&&"function"!=typeof g[a]?e[a]:h&&s?i(l,r):b&&g[a]==l?function(t){var n=function(n,e,r){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,e)}return new t(n,e,r)}return t.apply(this,arguments)};return n[f]=t[f],n}(l):y&&"function"==typeof l?i(Function.call,l):l,y&&((x.virtual||(x.virtual={}))[a]=l,t&c.R&&m&&!m[a]&&u(m,a,l)))};c.F=1,c.G=2,c.S=4,c.P=8,c.B=16,c.W=32,c.U=64,c.R=128,t.exports=c},function(t,n){var e={}.toString;t.exports=function(t){return e.call(t).slice(8,-1)}},function(t,n,e){var r=e(11),o=e(1).document,i=r(o)&&r(o.createElement);t.exports=function(t){return i?o.createElement(t):{}}},function(t,n,e){t.exports=!e(4)&&!e(8)(function(){return 7!=Object.defineProperty(e(29)("div"),"a",{get:function(){return 7}}).a})},function(t,n,e){"use strict";var r=e(18),o=e(27),i=e(36),u=e(5),f=e(2),c=e(17),a=e(51),s=e(20),l=e(58),p=e(7)("iterator"),d=!([].keys&&"next"in[].keys()),v="@@iterator",y="keys",h="values",b=function(){return this};t.exports=function(t,n,e,x,m,g,w){a(e,n,x);var O,S,_,j=function(t){if(!d&&t in A)return A[t];switch(t){case y:return function(){return new e(this,t)};case h:return function(){return new e(this,t)}}return function(){return new e(this,t)}},P=n+" Iterator",E=m==h,M=!1,A=t.prototype,T=A[p]||A[v]||m&&A[m],L=T||j(m),N=m?E?j("entries"):L:void 0,C="Array"==n?A.entries||T:T;if(C&&(_=l(C.call(new t)),_!==Object.prototype&&(s(_,P,!0),r||f(_,p)||u(_,p,b))),E&&T&&T.name!==h&&(M=!0,L=function(){return T.call(this)}),r&&!w||!d&&!M&&A[p]||u(A,p,L),c[n]=L,c[P]=b,m)if(O={values:E?L:j(h),keys:g?L:j(y),entries:N},w)for(S in O)S in A||i(A,S,O[S]);else o(o.P+o.F*(d||M),n,O);return O}},function(t,n,e){var r=e(10),o=e(55),i=e(16),u=e(21)("IE_PROTO"),f=function(){},c="prototype",a=function(){var t,n=e(29)("iframe"),r=i.length,o="<",u=">";for(n.style.display="none",e(48).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write(o+"script"+u+"document.F=Object"+o+"/script"+u),t.close(),a=t.F;r--;)delete a[c][i[r]];return a()};t.exports=Object.create||function(t,n){var e;return null!==t?(f[c]=r(t),e=new f,f[c]=null,e[u]=t):e=a(),void 0===n?e:o(e,n)}},function(t,n,e){var r=e(35),o=e(16).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return r(t,o)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,e){var r=e(2),o=e(3),i=e(45)(!1),u=e(21)("IE_PROTO");t.exports=function(t,n){var e,f=o(t),c=0,a=[];for(e in f)e!=u&&r(f,e)&&a.push(e);for(;n.length>c;)r(f,e=n[c++])&&(~i(a,e)||a.push(e));return a}},function(t,n,e){t.exports=e(5)},function(t,n,e){var r=e(15);t.exports=function(t){return Object(r(t))}},function(t,n,e){t.exports={default:e(41),__esModule:!0}},function(t,n,e){t.exports={default:e(42),__esModule:!0}},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var o=e(39),i=r(o),u=e(38),f=r(u),c="function"==typeof f.default&&"symbol"==typeof i.default?function(t){return typeof t}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":typeof t};n.default="function"==typeof f.default&&"symbol"===c(i.default)?function(t){return"undefined"==typeof t?"undefined":c(t)}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":"undefined"==typeof t?"undefined":c(t)}},function(t,n,e){e(65),e(63),e(66),e(67),t.exports=e(14).Symbol},function(t,n,e){e(64),e(68),t.exports=e(26).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,e){var r=e(3),o=e(61),i=e(60);t.exports=function(t){return function(n,e,u){var f,c=r(n),a=o(c.length),s=i(u,a);if(t&&e!=e){for(;a>s;)if(f=c[s++],f!=f)return!0}else for(;a>s;s++)if((t||s in c)&&c[s]===e)return t||s||0;return!t&&-1}}},function(t,n,e){var r=e(43);t.exports=function(t,n,e){if(r(t),void 0===n)return t;switch(e){case 1:return function(e){return t.call(n,e)};case 2:return function(e,r){return t.call(n,e,r)};case 3:return function(e,r,o){return t.call(n,e,r,o)}}return function(){return t.apply(n,arguments)}}},function(t,n,e){var r=e(9),o=e(34),i=e(19);t.exports=function(t){var n=r(t),e=o.f;if(e)for(var u,f=e(t),c=i.f,a=0;f.length>a;)c.call(t,u=f[a++])&&n.push(u);return n}},function(t,n,e){t.exports=e(1).document&&document.documentElement},function(t,n,e){var r=e(28);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==r(t)?t.split(""):Object(t)}},function(t,n,e){var r=e(28);t.exports=Array.isArray||function(t){return"Array"==r(t)}},function(t,n,e){"use strict";var r=e(32),o=e(12),i=e(20),u={};e(5)(u,e(7)("iterator"),function(){return this}),t.exports=function(t,n,e){t.prototype=r(u,{next:o(1,e)}),i(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,e){var r=e(9),o=e(3);t.exports=function(t,n){for(var e,i=o(t),u=r(i),f=u.length,c=0;f>c;)if(i[e=u[c++]]===n)return e}},function(t,n,e){var r=e(13)("meta"),o=e(11),i=e(2),u=e(6).f,f=0,c=Object.isExtensible||function(){return!0},a=!e(8)(function(){return c(Object.preventExtensions({}))}),s=function(t){u(t,r,{value:{i:"O"+ ++f,w:{}}})},l=function(t,n){if(!o(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!i(t,r)){if(!c(t))return"F";if(!n)return"E";s(t)}return t[r].i},p=function(t,n){if(!i(t,r)){if(!c(t))return!0;if(!n)return!1;s(t)}return t[r].w},d=function(t){return a&&v.NEED&&c(t)&&!i(t,r)&&s(t),t},v=t.exports={KEY:r,NEED:!1,fastKey:l,getWeak:p,onFreeze:d}},function(t,n,e){var r=e(6),o=e(10),i=e(9);t.exports=e(4)?Object.defineProperties:function(t,n){o(t);for(var e,u=i(n),f=u.length,c=0;f>c;)r.f(t,e=u[c++],n[e]);return t}},function(t,n,e){var r=e(19),o=e(12),i=e(3),u=e(24),f=e(2),c=e(30),a=Object.getOwnPropertyDescriptor;n.f=e(4)?a:function(t,n){if(t=i(t),n=u(n,!0),c)try{return a(t,n)}catch(t){}if(f(t,n))return o(!r.f.call(t,n),t[n])}},function(t,n,e){var r=e(3),o=e(33).f,i={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],f=function(t){try{return o(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==i.call(t)?f(t):o(r(t))}},function(t,n,e){var r=e(2),o=e(37),i=e(21)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=o(t),r(t,i)?t[i]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,e){var r=e(23),o=e(15);t.exports=function(t){return function(n,e){var i,u,f=String(o(n)),c=r(e),a=f.length;return c<0||c>=a?t?"":void 0:(i=f.charCodeAt(c),i<55296||i>56319||c+1===a||(u=f.charCodeAt(c+1))<56320||u>57343?t?f.charAt(c):i:t?f.slice(c,c+2):(i-55296<<10)+(u-56320)+65536)}}},function(t,n,e){var r=e(23),o=Math.max,i=Math.min;t.exports=function(t,n){return t=r(t),t<0?o(t+n,0):i(t,n)}},function(t,n,e){var r=e(23),o=Math.min;t.exports=function(t){return t>0?o(r(t),9007199254740991):0}},function(t,n,e){"use strict";var r=e(44),o=e(52),i=e(17),u=e(3);t.exports=e(31)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,e=this._i++;return!t||e>=t.length?(this._t=void 0,o(1)):"keys"==n?o(0,e):"values"==n?o(0,t[e]):o(0,[e,t[e]])},"values"),i.Arguments=i.Array,r("keys"),r("values"),r("entries")},function(t,n){},function(t,n,e){"use strict";var r=e(59)(!0);e(31)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,e=this._i;return e>=n.length?{value:void 0,done:!0}:(t=r(n,e),this._i+=t.length,{value:t,done:!1})})},function(t,n,e){"use strict";var r=e(1),o=e(2),i=e(4),u=e(27),f=e(36),c=e(54).KEY,a=e(8),s=e(22),l=e(20),p=e(13),d=e(7),v=e(26),y=e(25),h=e(53),b=e(47),x=e(50),m=e(10),g=e(3),w=e(24),O=e(12),S=e(32),_=e(57),j=e(56),P=e(6),E=e(9),M=j.f,A=P.f,T=_.f,L=r.Symbol,N=r.JSON,C=N&&N.stringify,k="prototype",F=d("_hidden"),q=d("toPrimitive"),I={}.propertyIsEnumerable,B=s("symbol-registry"),D=s("symbols"),W=s("op-symbols"),H=Object[k],K="function"==typeof L,R=r.QObject,J=!R||!R[k]||!R[k].findChild,U=i&&a(function(){return 7!=S(A({},"a",{get:function(){return A(this,"a",{value:7}).a}})).a})?function(t,n,e){var r=M(H,n);r&&delete H[n],A(t,n,e),r&&t!==H&&A(H,n,r)}:A,G=function(t){var n=D[t]=S(L[k]);return n._k=t,n},$=K&&"symbol"==typeof L.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof L},z=function(t,n,e){return t===H&&z(W,n,e),m(t),n=w(n,!0),m(e),o(D,n)?(e.enumerable?(o(t,F)&&t[F][n]&&(t[F][n]=!1),e=S(e,{enumerable:O(0,!1)})):(o(t,F)||A(t,F,O(1,{})),t[F][n]=!0),U(t,n,e)):A(t,n,e)},Y=function(t,n){m(t);for(var e,r=b(n=g(n)),o=0,i=r.length;i>o;)z(t,e=r[o++],n[e]);return t},Q=function(t,n){return void 0===n?S(t):Y(S(t),n)},X=function(t){var n=I.call(this,t=w(t,!0));return!(this===H&&o(D,t)&&!o(W,t))&&(!(n||!o(this,t)||!o(D,t)||o(this,F)&&this[F][t])||n)},V=function(t,n){if(t=g(t),n=w(n,!0),t!==H||!o(D,n)||o(W,n)){var e=M(t,n);return!e||!o(D,n)||o(t,F)&&t[F][n]||(e.enumerable=!0),e}},Z=function(t){for(var n,e=T(g(t)),r=[],i=0;e.length>i;)o(D,n=e[i++])||n==F||n==c||r.push(n);return r},tt=function(t){for(var n,e=t===H,r=T(e?W:g(t)),i=[],u=0;r.length>u;)!o(D,n=r[u++])||e&&!o(H,n)||i.push(D[n]);return i};K||(L=function(){if(this instanceof L)throw TypeError("Symbol is not a constructor!");var t=p(arguments.length>0?arguments[0]:void 0),n=function(e){this===H&&n.call(W,e),o(this,F)&&o(this[F],t)&&(this[F][t]=!1),U(this,t,O(1,e))};return i&&J&&U(H,t,{configurable:!0,set:n}),G(t)},f(L[k],"toString",function(){return this._k}),j.f=V,P.f=z,e(33).f=_.f=Z,e(19).f=X,e(34).f=tt,i&&!e(18)&&f(H,"propertyIsEnumerable",X,!0),v.f=function(t){return G(d(t))}),u(u.G+u.W+u.F*!K,{Symbol:L});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),et=0;nt.length>et;)d(nt[et++]);for(var nt=E(d.store),et=0;nt.length>et;)y(nt[et++]);u(u.S+u.F*!K,"Symbol",{for:function(t){return o(B,t+="")?B[t]:B[t]=L(t)},keyFor:function(t){if($(t))return h(B,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){J=!0},useSimple:function(){J=!1}}),u(u.S+u.F*!K,"Object",{create:Q,defineProperty:z,defineProperties:Y,getOwnPropertyDescriptor:V,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),N&&u(u.S+u.F*(!K||a(function(){var t=L();return"[null]"!=C([t])||"{}"!=C({a:t})||"{}"!=C(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!$(t)){for(var n,e,r=[t],o=1;arguments.length>o;)r.push(arguments[o++]);return n=r[1],"function"==typeof n&&(e=n),!e&&x(n)||(n=function(t,n){if(e&&(n=e.call(this,t,n)),!$(n))return n}),r[1]=n,C.apply(N,r)}}}),L[k][q]||e(5)(L[k],q,L[k].valueOf),l(L,"Symbol"),l(Math,"Math",!0),l(r.JSON,"JSON",!0)},function(t,n,e){e(25)("asyncIterator")},function(t,n,e){e(25)("observable")},function(t,n,e){e(62);for(var r=e(1),o=e(5),i=e(17),u=e(7)("toStringTag"),f=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],c=0;c<5;c++){var a=f[c],s=r[a],l=s&&s.prototype;l&&!l[u]&&o(l,u,a),i[a]=i.Array}},function(t,n){"use strict";var e={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&t.indexOf("KHTML")==-1,mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:t.indexOf("Safari")==-1,weixin:t.indexOf("MicroMessenger")==-1}}()};t.exports=e},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}var o=e(40),i=r(o),u=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):o[t]||t}function n(t){return l[t]}var e=/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,r=/['<> "&]/g,o={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},f=/\u00a0/g,c=/<br\s*\/?>/gi,a=/\r?\n/g,s=/\s/g,l={};for(var p in o)l[o[p]]=p;return o["&apos;"]="'",l["'"]="&#39;",{encode:function(t){return t?(""+t).replace(r,n).replace(a,"<br/>").replace(s,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(c,"\n").replace(e,t).replace(f," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e+=2)n.push(String.fromCharCode("0x"+t.slice(e,e+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,e=t.length;e>n;n++)t[n]=u.encodeObject(t[n]);else if("object"==("undefined"==typeof t?"undefined":(0,i.default)(t)))for(var r in t)t[r]=u.encodeObject(t[r]);else if("string"==typeof t)return u.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=u},function(t,n){function e(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=e},function(t,n){function e(t,n){if(t.classList)t.classList.remove(n);else{var e=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(e," ")}}t.exports=e},,,function(t,n){"use strict";function e(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){var n=document.querySelectorAll(".article-entry a:not(.article-more-a)");n.forEach(function(t){t.setAttribute("target","_blank")})}var e=document.querySelector("#js-aboutme");e&&0!==e.length&&(e.innerHTML=e.innerText)}t.exports={init:e}},,,,,,,,,function(t,n){function e(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var e=t.nextSibling;return e?t.parentNode.insertBefore(n,e):t.parentNode.appendChild(n)}t.exports=e}])</script><script src="/./main.2d7529.js"></script><script>!function(){var e=function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)};e("/slider.885efe.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 50%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 50%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">echarts</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">IDEA</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">scala</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">scala函数式编程</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">markdown</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">mysql</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">scala编程</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">数据仓库</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">Linux基础命令</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Linux重要配置文件</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">nginx</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NFS</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">shell</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">inotify</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">rsync</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">linux</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">flume</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">azkaban</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">hadoop</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">mapreduce</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">hbase</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">kafka</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">hive</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">logstash</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">spark</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">sqoop</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">storm</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">zookeeper</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">java</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">netty</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">socket</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">NIO</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color4">rpc</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">memcached</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">mongodb</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color1">redis</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            2、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: true
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">一个在生活中挣扎的小人物</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>