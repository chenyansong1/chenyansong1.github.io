<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/51/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark/RDD-API之combineByKey" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark/RDD-API之combineByKey/" class="article-date">
  <time datetime="2017-04-16T04:47:25.129Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark/RDD-API之combineByKey/">RDD-API之combineByKey</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-combineByKey函数的运行机制"><a href="#1-combineByKey函数的运行机制" class="headerlink" title="1.combineByKey函数的运行机制"></a>1.combineByKey函数的运行机制</h1><p>&emsp;RDD提供了很多针对元素类型为(K,V)的API，这些API封装在PairRDDFunctions类中，通过Scala隐式转换使用。这些API实现上是借助于combineByKey实现的。combineByKey函数本身也是RDD开放给Spark开发人员使用的API之一<br> 首先看一下combineByKey的方法说明：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div></pre></td><td class="code"><pre><div class="line">/**</div><div class="line">   * Generic function to combine the elements for each key using a custom set of aggregation</div><div class="line">   * functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a &quot;combined type&quot; C</div><div class="line">   * Note that V and C can be different -- for example, one might group an RDD of type</div><div class="line">   * (Int, Int) into an RDD of type (Int, Seq[Int]). Users provide three functions:</div><div class="line">   *</div><div class="line">   * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list)</div><div class="line">   * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list)</div><div class="line">   * - `mergeCombiners`, to combine two C&apos;s into a single one.</div><div class="line">   *</div><div class="line">   * In addition, users can control the partitioning of the output RDD, and whether to perform</div><div class="line">   * map-side aggregation (if a mapper can produce multiple items with the same key).</div><div class="line">   */</div><div class="line">  def combineByKey[C](createCombiner: V =&gt; C,</div><div class="line">      mergeValue: (C, V) =&gt; C,</div><div class="line">      mergeCombiners: (C, C) =&gt; C,</div><div class="line">      partitioner: Partitioner,</div><div class="line">      mapSideCombine: Boolean = true,</div><div class="line">      serializer: Serializer = null): RDD[(K, C)] = &#123;</div><div class="line">    require(mergeCombiners != null, &quot;mergeCombiners must be defined&quot;) // required as of Spark 0.9.0</div><div class="line">    if (keyClass.isArray) &#123;</div><div class="line">      if (mapSideCombine) &#123;</div><div class="line">        throw new SparkException(&quot;Cannot use map-side combining with array keys.&quot;)</div><div class="line">      &#125;</div><div class="line">      if (partitioner.isInstanceOf[HashPartitioner]) &#123;</div><div class="line">        throw new SparkException(&quot;Default partitioner cannot partition array keys.&quot;)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line">    val aggregator = new Aggregator[K, V, C](</div><div class="line">      self.context.clean(createCombiner),</div><div class="line">      self.context.clean(mergeValue),</div><div class="line">      self.context.clean(mergeCombiners))</div><div class="line">    if (self.partitioner == Some(partitioner)) &#123;</div><div class="line">      self.mapPartitions(iter =&gt; &#123;</div><div class="line">        val context = TaskContext.get()</div><div class="line">        new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context))</div><div class="line">      &#125;, preservesPartitioning = true)</div><div class="line">    &#125; else &#123;</div><div class="line">      new ShuffledRDD[K, V, C](self, partitioner)</div><div class="line">        .setSerializer(serializer)</div><div class="line">        .setAggregator(aggregator)</div><div class="line">        .setMapSideCombine(mapSideCombine)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">/* 主要看:</div><div class="line">      createCombiner: V =&gt; C,</div><div class="line">      mergeValue: (C, V) =&gt; C,</div><div class="line">      mergeCombiners: (C, C) =&gt; C,</div><div class="line"></div><div class="line">combineByKey的功能是对RDD中的数据集按照Key进行聚合(想象下Hadoop MapReduce的Combiner，用于Map端做Reduce)。聚合的逻辑是通过自定义函数提供给combineByKey。</div><div class="line">从上面的源代码中可以看到，combineByKey是把(K,V)类型的RDD转换为(K,C)类型的RDD，C和V可以不一样。</div><div class="line">combineByKey函数需要三个重要的函数作为参数</div><div class="line"></div><div class="line">createCombiner：在遍历RDD的数据集合过程中，对于遍历到的(k,v)，如果combineByKey第一次遇到值为k的Key（类型K），那么将对这个(k,v)调用combineCombiner函数，它的作用是将v转换为c(类型是C，聚合对象的类型，c作为局和对象的初始值)</div><div class="line"></div><div class="line">mergeValue：在遍历RDD的数据集合过程中，对于遍历到的(k,v)，如果combineByKey不是第一次(或者第二次，第三次...)遇到值为k的Key（类型K），那么将对这个(k,v)调用mergeValue函数，它的作用是将v累加到聚合对象（类型C）中，mergeValue的类型是(C,V)=&gt;C,参数中的C遍历到此处的聚合对象，然后对v进行聚合得到新的聚合对象值</div><div class="line"></div><div class="line">mergeCombiners：因为combineByKey是在分布式环境下执行，RDD的每个分区单独进行combineByKey操作，最后需要对各个分区的结果进行最后的聚合，它的函数类型是(C,C)=&gt;C，每个参数是分区聚合得到的聚合对象。</div><div class="line"> </div><div class="line"></div><div class="line">*/</div></pre></td></tr></table></figure>
<p> combineByKey的流程是：</p>
<p>假设<strong>一组具有相同 K 的 <k, v=""> records 正在一个个流向 combineByKey()</k,></strong>，createCombiner 将第一个 record 的value 初始化为 c （比如，c = value），然后从第二个 record 开始，来一个 record 就使用 mergeValue(c,record.value) 来更新 c，比如想要对这些 records 的所有 values 做 sum，那么使用 c = c + record.value。等到records 全部被 mergeValue()，得到结果 c。假设还有一组 records（key 与前面那组的 key 均相同）一个个到来，combineByKey() 使用前面的方法不断计算得到 c’。现在如果要求这两组 records 总的 combineByKey() 后的结果，那么可以使用 final c = mergeCombiners(c, c’) 来计算。</p>
<h1 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"> </div><div class="line">combineByKey : 和reduceByKey是相同的效果</div><div class="line">###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算</div><div class="line">###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)--&gt;(hello(1,1),good(1))--&gt;x就相当于hello的第一个1, good中的1</div><div class="line">val rdd1 = sc.textFile(&quot;hdfs://master:9000/wordcount/input/&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1))</div><div class="line">val rdd2 = rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)</div><div class="line">rdd1.collect</div><div class="line">rdd2.collect</div><div class="line"> </div><div class="line">###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10</div><div class="line">val rdd3 = rdd1.combineByKey(x =&gt; x + 10, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n) //x =&gt; x + 10 只是将分区的第一个元素作为x,然后加10 ,将x+10作为局部计算的初始值</div><div class="line">rdd3.collect</div><div class="line"> </div><div class="line"> </div><div class="line">val rdd4 = sc.parallelize(List(&quot;dog&quot;,&quot;cat&quot;,&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;turkey&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;), 3)</div><div class="line">val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)</div><div class="line">val rdd6 = rdd5.zip(rdd4) //List((1,dog),(1,cat),(2,gnu),(2,salmon),(2,rabbit),(1,turkey),(2,wolf),(2,bear),(2,bee))</div><div class="line">val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) =&gt; x :+ y, (m: List[String], n: List[String]) =&gt; m ++ n)        //(1,list(&quot;dog&quot;,&quot;cat&quot;,&quot;turkey&quot;) ) ,  (2, list(&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;)) )</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark/RDD-API之combineByKey/" data-id="cj290sc3200vdssqq4k7xhmft" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark/RDD-API之aggregateByKey" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark/RDD-API之aggregateByKey/" class="article-date">
  <time datetime="2017-04-16T04:47:25.128Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark/RDD-API之aggregateByKey/">RDD-API之aggregateByKey.md</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-aggregateByKey的原理"><a href="#1-aggregateByKey的原理" class="headerlink" title="1. aggregateByKey的原理"></a>1. aggregateByKey的原理</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">  /**</div><div class="line">   * Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;.</div><div class="line">   * This function can return a different result type, U, than the type of the values in this RDD,</div><div class="line">   * V. Thus, we need one operation for merging a V into a U and one operation for merging two U&apos;s,</div><div class="line">   * as in scala.TraversableOnce. The former operation is used for merging values within a</div><div class="line">   * partition, and the latter is used for merging values between partitions. To avoid memory</div><div class="line">   * allocation, both of these functions are allowed to modify and return their first argument</div><div class="line">   * instead of creating a new U.</div><div class="line">   */</div><div class="line">  def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =&gt; U,</div><div class="line">      combOp: (U, U) =&gt; U): RDD[(K, U)] = &#123;</div><div class="line">    // Serialize the zero value to a byte array so that we can get a new clone of it on each key</div><div class="line">    val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue)</div><div class="line">    val zeroArray = new Array[Byte](zeroBuffer.limit)</div><div class="line">    zeroBuffer.get(zeroArray)</div><div class="line"> </div><div class="line">    lazy val cachedSerializer = SparkEnv.get.serializer.newInstance()</div><div class="line">    val createZero = () =&gt; cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray))</div><div class="line"> </div><div class="line">    combineByKey[U]((v: V) =&gt; seqOp(createZero(), v), seqOp, combOp, partitioner)</div><div class="line">  &#125;</div><div class="line"></div><div class="line"></div><div class="line">/*</div><div class="line">从aggregateByKey的源代码中，可以看出</div><div class="line">a.aggregateByKey把类型为(K,V)的RDD转换为类型为(K,U)的RDD，V和U的类型可以不一样，这一点跟combineByKey是一样的，即返回的二元组的值类型可以不一样</div><div class="line"></div><div class="line">b.aggregateByKey内部是通过调用combineByKey实现的，combineByKey的createCombiner函数逻辑由zeroValue这个变量实现，zeroValue作为聚合的初始值，通常对于加法聚合则为0，乘法聚合则为1，集合操作则为空集合</div><div class="line">c.seqOp在combineByKey中的功能是mergeValues，(U,V)=&gt;U</div><div class="line">d.combOp在combineByKey中的功能是mergeCombiners</div><div class="line"></div><div class="line">*/</div></pre></td></tr></table></figure>
<h1 id="2-aggregateByKey举例"><a href="#2-aggregateByKey举例" class="headerlink" title="2.aggregateByKey举例"></a>2.aggregateByKey举例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">#求均值</div><div class="line"></div><div class="line">val rdd = sc.textFile(&quot;气象数据&quot;)  </div><div class="line">val rdd2 = rdd.map(x=&gt;x.split(&quot; &quot;)).map(x =&gt; (x(0).substring(&quot;从年月日中提取年月&quot;),x(1).toInt))  </div><div class="line">val zeroValue = (0,0) </div><div class="line">val seqOp= (u:(Int, Int), v:Int) =&gt; &#123;  </div><div class="line"> (u._1 + v, u._2 + 1)  </div><div class="line">&#125;  </div><div class="line">  </div><div class="line">val compOp= (c1:(Int,Int),c2:(Int,Int))=&gt;&#123;  </div><div class="line">  (u1._1 + u2._1, u1._2 + u2._2)  </div><div class="line">&#125;  </div><div class="line">  </div><div class="line">  </div><div class="line">val vdd3 = vdd2.aggregateByKey(  </div><div class="line">zeroValue ,  </div><div class="line">seqOp,  </div><div class="line">compOp</div><div class="line">)  </div><div class="line">  </div><div class="line">rdd3.foreach(x=&gt;println(x._1 + &quot;: average tempreture is &quot; + x._2._1/x._2._2) </div><div class="line"></div><div class="line">/*</div><div class="line">从求均值的实现来看，aggregate通过提供零值的方式，避免了combineByKey中的createCombiner步骤(createCombiner本质工作就是遇到第一个key时进行初始化操作，这个初始化不是提供零值，而是对第一个(k,v)进行转换得到c的初始值））</div><div class="line">*/</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark/RDD-API之aggregateByKey/" data-id="cj290sc3000vassqqozy5ig7n" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark/RDD-API(转)" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark/RDD-API(转)/" class="article-date">
  <time datetime="2017-04-16T04:47:25.127Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark/RDD-API(转)/">RDD-API(转)</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">package spark.examples.rddapi  </div><div class="line">  </div><div class="line">import org.apache.spark.&#123;SparkConf, SparkContext&#125;  </div><div class="line">  </div><div class="line">//测试RDD的aggregate方法  </div><div class="line">object AggregateTest &#123;  </div><div class="line">  def main(args: Array[String]) &#123;  </div><div class="line">    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AggregateTest_00&quot;)  </div><div class="line">    val sc = new SparkContext(conf);  </div><div class="line">    val z1 = sc.parallelize(List(1, 3, 5, 7, 7, 5, 3, 3, 79), 2)  </div><div class="line">    /** </div><div class="line">     * Aggregate the elements of each partition, and then the results for all the partitions, using </div><div class="line">     * given combine functions and a neutral &quot;zero value&quot;. This function can return a different result </div><div class="line">     * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U </div><div class="line">     * and one operation for merging two U&apos;s, as in scala.TraversableOnce. Both of these functions are </div><div class="line">     * allowed to modify and return their first argument instead of creating a new U to avoid memory </div><div class="line">     * allocation. </div><div class="line">     */  </div><div class="line">  </div><div class="line">    // def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U  </div><div class="line">    //T是RDD中的元素类型，U是aggregate方法自定义的泛型参数，aggregate返回U(而不一定是T)  </div><div class="line">    //两个分区取最大值，然后相加  </div><div class="line">    //math.max(_, _)表示针对每个partition实施的操作, _ + _表示combiner  </div><div class="line">  </div><div class="line">    val r1 = z1.aggregate(0)(math.max(_, _), _ + _)  </div><div class="line">    println(r1) //86  </div><div class="line">  </div><div class="line">    //RDD元素类型字符串，aggregate的返回类型同样为String  </div><div class="line">    val z2 = sc.parallelize(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;), 2)  </div><div class="line">    val r2 = z2.aggregate(&quot;xx&quot;)(_ + _, _ + _)  </div><div class="line">    println(r2) //连接操作，结果xxxxabcxxdef，每个分区计算时，加上xx，最后两个分区计算时，继续把xx加上  </div><div class="line">  </div><div class="line">    //_ + _的道理也是(x,y) =&gt; x + y  </div><div class="line">    //(x,y)=&gt;math.max是做两两比较吗？  </div><div class="line">    val z3 = sc.parallelize(List(&quot;12&quot;, &quot;23&quot;, &quot;345&quot;, &quot;4567&quot;), 2)  </div><div class="line">    val r3 = z3.aggregate(&quot;&quot;)((x, y) =&gt; math.max(x.length, y.length).toString, (x, y) =&gt; x + y)  </div><div class="line">    println(r3)   ///结果24，表示两个分区的字符串长度最长的长度转成String后，做拼接  </div><div class="line">  </div><div class="line">    //结果11</div><div class="line">    val r4 = sc.parallelize(List(&quot;12&quot;, &quot;23&quot;, &quot;345&quot;, &quot;4567&quot;), 2).aggregate(&quot;&quot;)((x, y) =&gt; math.min(x.length, y.length).toString, (x, y) =&gt; x + y)  </div><div class="line">    println(r4)  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="cartesian"><a href="#cartesian" class="headerlink" title="cartesian"></a>cartesian</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">package spark.examples.rddapi  </div><div class="line">  </div><div class="line">import org.apache.spark.rdd.&#123;CartesianRDD, RDD&#125;  </div><div class="line">import org.apache.spark.&#123;SparkContext, SparkConf&#125;  </div><div class="line">  </div><div class="line">  </div><div class="line">object CartesianTest_01 &#123;  </div><div class="line">  def main(args: Array[String]) &#123;  </div><div class="line">    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AggregateTest_00&quot;)  </div><div class="line">    val sc = new SparkContext(conf);  </div><div class="line">    val z1 = sc.parallelize(List(2, 3, 4, 5, 6), 2)  </div><div class="line">    val z2 = sc.parallelize(List(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;), 3)  </div><div class="line">  </div><div class="line">    /** </div><div class="line">     * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of </div><div class="line">     * elements (a, b) where a is in `this` and b is in `other`. </div><div class="line">     */  </div><div class="line">  </div><div class="line">    //def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = new CartesianRDD(sc, this, other)  </div><div class="line">    //z1 和 z2集合的元素类型可以不同，并且cartesian是个转换算子，  </div><div class="line">    //调用z.collect触发作业  </div><div class="line">    val z = z1.cartesian(z2)  </div><div class="line">    println(&quot;Number of partitions: &quot; + z.partitions.length) //6  </div><div class="line">    var count = 0  </div><div class="line">  </div><div class="line">    z.collect().foreach(x  =&gt; &#123;println(x._1 + &quot;,&quot; + x._2); count = count + 1&#125;) //  </div><div class="line">  </div><div class="line">   println(&quot;count =&quot; + count) //50</div></pre></td></tr></table></figure>
<h1 id="Repartition"><a href="#Repartition" class="headerlink" title="Repartition"></a>Repartition</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">package spark.examples.rddapi  </div><div class="line">  </div><div class="line">import org.apache.spark.&#123;SparkContext, SparkConf&#125;  </div><div class="line">  </div><div class="line">object RepartitionTest_04 &#123;  </div><div class="line">  def main(args: Array[String]) &#123;  </div><div class="line">    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;RepartitionTest_04&quot;)  </div><div class="line">    val sc = new SparkContext(conf);  </div><div class="line">    val z1 = sc.parallelize(List(3, 9, 18, 22, 11, 9, 8), 3)  </div><div class="line">    //z1.coalesce(5, true)的效果一样，开启shuffle  </div><div class="line">    /** </div><div class="line">     * Return a new RDD that has exactly numPartitions partitions. </div><div class="line">     * </div><div class="line">     * Can increase or decrease the level of parallelism in this RDD. Internally, this uses </div><div class="line">     * a shuffle to redistribute data. </div><div class="line">     * </div><div class="line">     * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, </div><div class="line">     * which can avoid performing a shuffle. </div><div class="line">     */  </div><div class="line">    val r1 = z1.repartition(5)  </div><div class="line">     r1.collect().foreach(println)  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line">package spark.examples.rddapi  </div><div class="line">  </div><div class="line">import org.apache.spark.&#123;SparkContext, SparkConf&#125;  </div><div class="line">  </div><div class="line">//coalesce：合并  </div><div class="line">object CoalesceTest_03 &#123;  </div><div class="line">  def main(args: Array[String]) &#123;  </div><div class="line">    val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;CoalesceTest_03&quot;)  </div><div class="line">    val sc = new SparkContext(conf);  </div><div class="line">    val z = sc.parallelize(List(3, 9, 18, 22, 11, 9, 8), 3)  </div><div class="line">  </div><div class="line">    /** </div><div class="line">     * Return a new RDD that is reduced into `numPartitions` partitions. </div><div class="line">     * </div><div class="line">     * This results in a narrow dependency, e.g. if you go from 1000 partitions </div><div class="line">     * to 100 partitions, there will not be a shuffle, instead each of the 100 </div><div class="line">     * new partitions will claim 10 of the current partitions. </div><div class="line">     * </div><div class="line">     * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1, </div><div class="line">     * this may result in your computation taking place on fewer nodes than </div><div class="line">     * you like (e.g. one node in the case of numPartitions = 1). To avoid this, </div><div class="line">     * you can pass shuffle = true. This will add a shuffle step, but means the </div><div class="line">     * current upstream partitions will be executed in parallel (per whatever </div><div class="line">     * the current partitioning is). </div><div class="line">     * </div><div class="line">     * Note: With shuffle = true, you can actually coalesce to a larger number </div><div class="line">     * of partitions. This is useful if you have a small number of partitions, </div><div class="line">     * say 100, potentially with a few partitions being abnormally large. Calling </div><div class="line">     * coalesce(1000, shuffle = true) will result in 1000 partitions with the </div><div class="line">     * data distributed using a hash partitioner. </div><div class="line">     */  </div><div class="line">    //shuffle默认为false  </div><div class="line">    //将分区数由3变成2，大变小使用narrow dependency  </div><div class="line">    val zz = z.coalesce(2, false)  </div><div class="line">    println(&quot;Partitions length: &quot; + zz.partitions.length) //2  </div><div class="line">    println(zz.collect()) //结果是[I@100498c？  </div><div class="line">    zz.collect().foreach(println)  </div><div class="line">  </div><div class="line">    //将分区数由3变成6，少变多必须使用shuffle=true  </div><div class="line">    //在单机上没有发现有问题  </div><div class="line">    //在cluster环境下，为了保证新的分区分布到不同的节点，应该使用shuffle为true  </div><div class="line">    //也就是说，少变多也可以使用shuffle为false，但是达不到分区数据进行重新分布的目的  </div><div class="line">    val z2 = z.coalesce(6, false)  </div><div class="line">    z2.collect().foreach(println)  </div><div class="line">  </div><div class="line">    //分区扩大，同时设置shuffle为true  </div><div class="line">    val z3 = z.coalesce(6, true)  </div><div class="line">    z3.collect().foreach(println)  </div><div class="line">  </div><div class="line">  &#125;  </div><div class="line">&#125;</div></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark/RDD-API(转)/" data-id="cj290sc2v00v5ssqqe9akhy6c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark/IP查找并插入数据到mysql" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark/IP查找并插入数据到mysql/" class="article-date">
  <time datetime="2017-04-16T04:47:25.126Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark/IP查找并插入数据到mysql/">IP查找并插入数据到mysql</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-需求"><a href="#1-需求" class="headerlink" title="1.需求"></a>1.需求</h1><p>&emsp;根据网关日志,查询用户的地址, 并在此基础上统计所有的地址的用户数量</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark/ip_address/1.png" alt=""></p>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/spark/IP查找并插入数据到mysql/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark/IP查找并插入数据到mysql/" data-id="cj290sc2s00v2ssqqqblpmeln" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark/checkpoint" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark/checkpoint/" class="article-date">
  <time datetime="2017-04-16T04:47:25.125Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark/checkpoint/">checkpoint</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-为什么要有checkpoint"><a href="#1-为什么要有checkpoint" class="headerlink" title="1.为什么要有checkpoint?"></a>1.为什么要有checkpoint?</h1><p>因为在spark进行计算的时候,会有很多的中间结果,但是一旦中间某一步失败,那么又要重新从头开始计算,但是如果我们将中间的某一个计算的结果checkpoint下来,那么下次计算的时候,直接从checkpoint的点拿数据,那么将会大大提高计算的速度</p>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/spark/checkpoint/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark/checkpoint/" data-id="cj290sc3p00vyssqqtyyf4xx8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/logstash/logstash安装及启动" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/logstash/logstash安装及启动/" class="article-date">
  <time datetime="2017-04-16T04:47:25.122Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/logstash/">logstash</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/logstash/logstash安装及启动/">logstash安装及启动</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/logstash/1.png" alt=""></p>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/logstash/logstash安装及启动/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/logstash/logstash安装及启动/" data-id="cj290sc2p00uzssqquefzlkbs" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/logstash/">logstash</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/logstash/logstash几种配置示例" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/logstash/logstash几种配置示例/" class="article-date">
  <time datetime="2017-04-16T04:47:25.121Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/logstash/">logstash</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/logstash/logstash几种配置示例/">logstash几种配置示例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面是logstash的几种配置示例,更多的配置看<a href="https://www.elastic.co/guide/en/logstash/current/config-examples.html">官网</a></p>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/logstash/logstash几种配置示例/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/logstash/logstash几种配置示例/" data-id="cj290sc2i00urssqqn3eo5ipm" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/logstash/">logstash</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/kafka/kafka集群安装及常用shell命令" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/kafka/kafka集群安装及常用shell命令/" class="article-date">
  <time datetime="2017-04-16T04:47:25.119Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/kafka/">kafka</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/kafka/kafka集群安装及常用shell命令/">kafka集群安装及常用shell命令</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-集群安装"><a href="#1-集群安装" class="headerlink" title="1.集群安装"></a>1.集群安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">#解压，创建软链接</div><div class="line">cd /home/hadoop/app/</div><div class="line">tar -zxvf kafka_2.11-0.9.0.1.tgz</div><div class="line">ln -s kafka_2.11-0.9.0.1 kafka</div><div class="line">rm -rf kafka_2.11-0.9.0.1.tgz            #删除安装文件，节省空间</div><div class="line"></div><div class="line"></div><div class="line">#修改配置文件</div><div class="line">cd kafka/config/</div><div class="line">vim server.properties</div><div class="line"></div><div class="line">broker.id=1            #每个机器上不同，如（broker.id=2， broker.id=3）</div><div class="line">log.dirs=/export/servers/log/kafka                 #是kafka的日志文件目录，存放的是消息数据，以主题命名的分区</div><div class="line">zookeeper.connect=zk03:2181,zk02:2181,zk01:2181</div><div class="line"></div><div class="line">#################################################</div><div class="line"></div><div class="line">#分发到其他机器</div><div class="line">mkdir /export/servers/log/kafka -p</div><div class="line">scp -rp ./kafka hdp-node-02:/home/hadoop/app/</div><div class="line">scp -rp ./kafka hdp-node-03:/home/hadoop/app/</div><div class="line"></div><div class="line"></div><div class="line">#启动</div><div class="line">#依次在各节点上启动kafka</div><div class="line">bin/kafka-server-start.sh  config/server.properties</div></pre></td></tr></table></figure>
<h1 id="2-Kafka常用操作命令"><a href="#2-Kafka常用操作命令" class="headerlink" title="2.Kafka常用操作命令"></a>2.Kafka常用操作命令</h1><ol>
<li>查看当前服务器中的所有topic<br>bin/kafka-topics.sh –list –zookeeper  zk01:2181</li>
<li>创建topic<br>./kafka-topics.sh –create –zookeeper zk01:2181 –replication-factor 1 –partitions 3 –topic first</li>
<li>删除topic<br>sh bin/kafka-topics.sh –delete –zookeeper zk01:2181 –topic test<br>需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。</li>
<li>通过shell命令发送消息<br>kafka-console-producer.sh –broker-list kafka01:9092 –topic itheima</li>
<li>通过shell消费消息<br>sh bin/kafka-console-consumer.sh –zookeeper zk01:2181 –from-beginning –topic test1<br>写–from-beginning会显示历史消息，如果只想显示最新的可以不写</li>
<li>查看消费位置<br>sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper zk01:2181 –group testGroup</li>
<li>查看某个Topic的详情<br>sh kafka-topics.sh –topic test –describe –zookeeper zk01:2181</li>
<li>停止服务<br>./kafka-server-stop.sh</li>
</ol>
<h1 id="3-报错和解决"><a href="#3-报错和解决" class="headerlink" title="3.报错和解决"></a>3.报错和解决</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[2015-06-16 11:24:13,015] ERROR Failed to send requests for topics mykafka with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler) </div><div class="line">[2015-06-16 11:24:13,015] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread) </div><div class="line">kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries. </div><div class="line">    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90) </div><div class="line">    at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) </div><div class="line">    at scala.collection.immutable.Stream.foreach(Stream.scala:594) </div><div class="line">    at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) </div><div class="line">    at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45)</div></pre></td></tr></table></figure>
<p>这个错误是config/server.properties的host.name写的不对，可能是前面的“#”没有去掉或是写的主机名称，改成服务器ip地址就可以了，如果改成localhost单机模式不会有问题，但分布式的时候会报下面错误。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">[2015-06-16 14:20:59,519] WARN Fetching topic metadata with correlation id 9 for topics [Set(mykafka)] from broker [id:0,host:192.168.10.114,port:9092] failed (kafka.client.ClientUtils$) </div><div class="line">java.nio.channels.ClosedChannelException </div><div class="line">    at kafka.network.BlockingChannel.send(BlockingChannel.scala:100) </div><div class="line">    at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73) </div><div class="line">    at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:72) </div><div class="line">    at kafka.producer.SyncProducer.send(SyncProducer.scala:113) </div><div class="line">    at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:58) </div><div class="line">    at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82) </div><div class="line">    at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67) </div><div class="line">    at kafka.utils.Utils$.swallow(Utils.scala:172) </div><div class="line">    at kafka.utils.Logging$class.swallowError(Logging.scala:106) </div><div class="line">    at kafka.utils.Utils$.swallowError(Utils.scala:45) </div><div class="line">    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67) </div><div class="line">    at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) </div><div class="line">    at scala.collection.immutable.Stream.foreach(Stream.scala:594) </div><div class="line">    at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) </div><div class="line">    at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45) </div><div class="line">[2015-06-16 14:20:59,520] ERROR fetching topic metadata for topics [Set(mykafka)] from broker [ArrayBuffer(id:0,host:192.168.10.114,port:9092)] failed (kafka.utils.Utils$) </div><div class="line">kafka.common.KafkaException: fetching topic metadata for topics [Set(mykafka)] from broker [ArrayBuffer(id:0,host:192.168.10.114,port:9092)] failed </div><div class="line">    at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:72) </div><div class="line">    at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82) </div><div class="line">    at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67) </div><div class="line">    at kafka.utils.Utils$.swallow(Utils.scala:172) </div><div class="line">    at kafka.utils.Logging$class.swallowError(Logging.scala:106) </div><div class="line">    at kafka.utils.Utils$.swallowError(Utils.scala:45) </div><div class="line">    at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67) </div><div class="line">    at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) </div><div class="line">    at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) </div><div class="line">    at scala.collection.immutable.Stream.foreach(Stream.scala:594) </div><div class="line">    at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) </div><div class="line">    at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45) </div><div class="line">Caused by: java.nio.channels.ClosedChannelException </div><div class="line">    at kafka.network.BlockingChannel.send(BlockingChannel.scala:100) </div><div class="line">    at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73) </div><div class="line">    at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:72) </div><div class="line">    at kafka.producer.SyncProducer.send(SyncProducer.scala:113) </div><div class="line">    at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:58) </div><div class="line">    ... 12 more</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/kafka/kafka集群安装及常用shell命令/" data-id="cj290sc2d00umssqqoal8opj8" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/kafka/Kafka配置文件说明图示" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/kafka/Kafka配置文件说明图示/" class="article-date">
  <time datetime="2017-04-16T04:47:25.117Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/kafka/">kafka</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/kafka/Kafka配置文件说明图示/">Kafka配置文件说明图示</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/kafka/conf/conf.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/kafka/Kafka配置文件说明图示/" data-id="cj290sc2300ubssqq6ygvbrep" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/kafka/kafka简介以及原理详解" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/kafka/kafka简介以及原理详解/" class="article-date">
  <time datetime="2017-04-16T04:47:25.116Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/kafka/">kafka</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/kafka/kafka简介以及原理详解/">kafka简介</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么"></a>Kafka是什么</h1><p>在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。如: KAFKA + STORM +REDIS</p>
<ul>
<li>Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。</li>
<li>Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。</li>
<li>Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。</li>
<li>无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性</li>
</ul>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/kafka/kafka简介以及原理详解/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/kafka/kafka简介以及原理详解/" data-id="cj290sc2l00uussqqfe6key09" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/kafka/">kafka</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/50/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/49/">49</a><a class="page-number" href="/page/50/">50</a><span class="page-number current">51</span><a class="page-number" href="/page/52/">52</a><a class="page-number" href="/page/53/">53</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/52/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>