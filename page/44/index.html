<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/44/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream和Receiver" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream和Receiver/" class="article-date">
  <time datetime="2017-04-16T04:47:25.234Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream和Receiver/">SparkStreaming之输入DStream和Receiver</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>输入DStream代表了来自数据源的输入数据流,在之前的WordCount例子中,lines就是一个输入DStream(SocketInputDStream),代表了从socket服务接收到的数据流,除了文件数据流之外,所有的输入DStream都会绑定一个Receiver对象,该对象是一个关键的组件,用来从数据源接收数据,并将其存储在spark的内存中,以供后续处理</p>
<p>spark Streaming提供了两种内置的数据源的支持<br>1.基础数据源:StreamingContext API(如:StreamingContext.socketTextStream()方法)中直接提供了对这些数据源的支持,比如:文件,socket,Akka Actor等,<br>2.高级数据源:诸如Kafka,flume,Kinesis,Twitter等书卷,通过第三方工具类提供支持,这些数据源的使用,需要引用其依赖<br>3.自定义数据源:我们可以自己定义数据源,来决定如何接受和存储数据</p>
<p>输入DStream和Receiver详解</p>
<p>要注意的是,如果你想要在实时计算应用中并行接收多条数据流,可以创建多个输入DStream,这样就会创建多个Receiver,从而并行的接收多个数据流,但是要注意的是,一个spark Streaming Application的Executor是一个长时间运行的任务,因此,他会独占分配给spark streaming Application的cpu core,从而只要spark streaming运行起来以后,这个节点上的cpu core,就没法给其他应用使用了</p>
<p>使用本地模式,运行程序时,绝对不能使用local或者是local[1],因为那样的话,只会给执行输入DStream的executor分配一个线程,而spark streaming底层的原理是,至少要有两条线程,一个线程用来分配给Receiver接收数据,一条线程用来处理接收到的数据,因此必须使用local[n],n&gt;=2的模式</p>
<p>如果不设置Master,也就是直接将spark streaming应用提交到集群上运行,那么首先,必须要求集群节点上,有&gt;1个cpu core,其次给spark streaming的每个executor分配的core,必须&gt;1,这样,才能保证分配到executor上运行的的输入DStream,两条线程并行,一条运行Receiver,接收数据,一条处理数据,否则的话,只会接收数据,不会处理数据</p>
<p>因此,在实际工作中,都要给每个executor的cpu core设置超过1个即可</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/Receiver.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream和Receiver/" data-id="cj290sc9l0119ssqqzgtzbscb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/" class="article-date">
  <time datetime="2017-04-16T04:47:25.232Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/">SparkStreaming之输入DStream之基础数据源</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>基础数据源<br>1.socket:StreamingContext.socketTextStream()</p>
<p>2.HDFS文件<br>StreamingContext.fileStream()<br>基于HDFS的文件实时计算,其实就是监控一个HDFS目录,只要其中有新文件出现,就实时处理,相当于处理实时的文件流<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">def fileStream[</div><div class="line">  K: ClassTag,</div><div class="line">  V: ClassTag,</div><div class="line">  F &lt;: NewInputFormat[K, V]: ClassTag</div><div class="line">] (directory: String): InputDStream[(K, V)]</div></pre></td></tr></table></figure></p>
<p>spark Streaming会监视指定的HDFS目录,并且处理出现在目录中的文件,要注意的是:<br>1.所有放入HDFS目录中的文件,都必须有相同的格式,<br>2.必须使用移动或者重命名的方式<br>3.将文件移入目录,一旦处理之后,文件的内容即使改变了,也不会再处理了<br>4.基于HDFS文件的数据源是没有Receiver的,因此不会占用一个cpu core</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序</div><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 针对HDFS目录创建DStream</div><div class="line">val lines = ssc.textFileStream(&quot;hdfs:spark1:9000/wordcount_dir&quot;)</div><div class="line">/* 其实在textFileStream底层是调用了fileStream</div><div class="line">  def textFileStream(directory: String): DStream[String] = withNamedScope(&quot;text file stream&quot;) &#123;</div><div class="line">    fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString)</div><div class="line">  &#125;</div><div class="line"> */</div><div class="line"></div><div class="line">// 执行WordCount逻辑</div><div class="line">val words = lines.flatMap(_.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/" data-id="cj290sc9i0116ssqqj7ocsaid" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/" class="article-date">
  <time datetime="2017-04-16T04:47:25.230Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/">SparkStreaming之输入DStream之Kafka基础数据源</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="基于Receiver的方式"><a href="#基于Receiver的方式" class="headerlink" title="基于Receiver的方式"></a>基于Receiver的方式</h1><p>这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据</p>
<p>然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用spark streaming 的预写日志机制(Write Ahead Log,WAL),该机制会tongue的将接收到的kafka数据写入分布式文件系统(比如HDFS)山的预写日志中,所以即使底层节点出现了失败,也可以使用预写日志中的数据进行恢复</p>
<p>前提:<br>1.maven添加依赖<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&lt;dependency&gt;</div><div class="line">    &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</div><div class="line">    &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt;</div><div class="line">    &lt;version&gt;1.6.3&lt;/version&gt;</div><div class="line">&lt;/dependency&gt;</div></pre></td></tr></table></figure></p>
<p>2.使用第三方工具类创建输入DStream<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">KafkaUtils.createStream(StreamingContext,[ZK quorum], [consumer group id], [per-topic number of kakfa partitions to consume])</div></pre></td></tr></table></figure></p>
<p>注意事项:<br>1.kafka的topic的partition,与spark中的RDD的partition是没有关系的,所以在KafkaUtils.createStream()中,提高partition的数量,只会增加一个Receiver中读取partition的线程的数量,不会增加spark处理数据的并行度<br>2.可以创建多个kafka输入DStream,使用不同的consumer group和topic,来通过多个receiver并行接收数据<br>3.如果基于容错的文件系统,比如HDFS,启用了预写日志机制,接收到的数据都会被复制一份到预写日志中,因此在KafkaUtils.createStream()中,设置的持久化级别是:StorageLevel.MEMORY_AND_DISK_SER_2</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">//创建topic</div><div class="line">bin/kafka-topic.sh --zookeeper zk01:2181,zk02:2181,zk03:2181 --topic WordCount --replication-factor 1 --partitions 1 --create</div><div class="line"></div><div class="line"></div><div class="line">//创建consumer生产者</div><div class="line">bin/kafka-console-producer.sh --broker-list 192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092, --topic WordCount</div></pre></td></tr></table></figure>
<p>实例代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序</div><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  topicThreadMap = Map(</div><div class="line">  &quot;WordCount&quot;-&gt;1</div><div class="line">)</div><div class="line"></div><div class="line">// zk是zookeeper的节点地址</div><div class="line">// DefalutConsumerGroup是kafka的groupId</div><div class="line">// topicThreadMap是指定去消费哪个topic</div><div class="line">//Map of (topic_name -&gt; numPartitions) to consume. Each partition is consumed in its own thread</div><div class="line">// topic名字-&gt;分区数量,每个分区将会启动一个Receiver线程去消费(而一个Receiver需要一个cpu core)</div><div class="line">val lines = KafkaUtils.createStream(ssc,zk,&quot;DefalutConsumerGroup&quot;,topicThreadMap)</div><div class="line"></div><div class="line">// 这里需要注意的是lines中是Tuple(index,line)这样的数据,所以_._2就是一行的的数据</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure>
<h1 id="基于Direct的方式"><a href="#基于Direct的方式" class="headerlink" title="基于Direct的方式"></a>基于Direct的方式</h1><p>这种是不基于Receiver的直接方式,是在spark1.3中引入,从而能够确保更加健壮的机制,替代掉使用Receiver来接收数据后,这种方式会周期性(就是我们指定的batch的时间)的查询Kafka,来获取每个topic+partition的最新的offset,从而定义每个batch的offset的范围(而每个batch会形成一个Rdd),当处理数据的job启动时,就会使用kafka的简单consumer API来获取kafka指定offset范围的数据,这就得到了这个Rdd的数据</p>
<p>这种方式有如下的优点:<br>1.简化并行读取,如果要有多个partition,不需要创建多个输入DStream然后对他们进行union操作,spark会创建跟kafka partition一样多的Rdd partition,并且会并行从kafka中读取数据,所在kafka partition和RDD partition之间,有一个一对一的映射关系<br>2.高性能:如果要保证零数据丢失,在语句Receiver的方式中,需要开启WAL机制,这种方式其实效率低下,因为数据实际上被复制了两份,kafka自己本身就有高可靠的机制,会对数据复制一份,而这里又会复制一份到WAL中,而基于direct的方式,不依赖Receiver,不需要开启WAL机制,只要kafka中作了数的复制,那么就可以通过kafka的副本进行恢复<br>3.一次仅且一次的事务机制<br>基于Receiver的方式,是使用kafka的高阶API来在zookeeper中保存消费过的offset,这是消费kafka数据的传统的方式,这种方式配合着WAL机制可以保证数据零丢失的高可靠性,但是却无法保证数据被处理一次且仅一次,可能会处理两次,因为spark和zookeeper之间可能是不同步的</p>
<p>基于direct的方式,使用kafka的简单API,spark streaming自己会负责追踪消费的offset并保存在checkpoint中,spark自己一定是同步的,因此可以保证数据是消费一次且仅消费一次</p>
<p>createDirectStream()方法参数说明</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">   *</div><div class="line">   * @param ssc StreamingContext object	这里是传入的一个StreamingContext</div><div class="line">   * @param kafkaParams Kafka &lt;a href=&quot;http://kafka.apache.org/documentation.html#configuration&quot;&gt;</div><div class="line">   *   configuration parameters&lt;/a&gt;. Requires &quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot; 必须要指定:&quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot;中的一个</div><div class="line">   *   to be set with Kafka broker(s) (NOT zookeeper servers), specified in</div><div class="line">   *   host1:port1,host2:port2 form.	//指定的格式</div><div class="line">   *   If not starting from a checkpoint, &quot;auto.offset.reset&quot; may be set to &quot;largest&quot; or &quot;smallest&quot;	//如果没有初始化的offset,那么从哪里开始消费(largest从头开始,smallest:从最近开始消费)</div><div class="line">   *   to determine where the stream starts (defaults to &quot;largest&quot;)</div><div class="line">   *   如果开始消费的数据不是从checkpoint中开始的,那么使用&quot;auto.offset.reset&quot; 参数设置成&quot;largest&quot; or &quot;smallest&quot;来决定从Stream流的哪里开始消费数据</div><div class="line">   * @param topics Names of the topics to consume	topic名称</div><div class="line">   * @tparam K type of Kafka message key	消息key的类型</div><div class="line">   * @tparam V type of Kafka message value	消息value的类型</div><div class="line">   * @tparam KD type of Kafka message key decoder	key的编码格式</div><div class="line">   * @tparam VD type of Kafka message value decoder	value的编码格式</div><div class="line">   * @return DStream of (Kafka message key, Kafka message value)</div><div class="line">   */</div><div class="line">  def createDirectStream[</div><div class="line">    K: ClassTag,</div><div class="line">    V: ClassTag,</div><div class="line">    KD &lt;: Decoder[K]: ClassTag,</div><div class="line">    VD &lt;: Decoder[V]: ClassTag] (</div><div class="line">      ssc: StreamingContext,</div><div class="line">      kafkaParams: Map[String, String],</div><div class="line">      topics: Set[String]</div><div class="line">  ): InputDStream[(K, V)]</div><div class="line"></div><div class="line"></div><div class="line">=========================================</div><div class="line">在kafka中对auto.offset.reset参数的解释是:</div><div class="line"></div><div class="line">What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):</div><div class="line">当没有初始化的offset的时候,此时该从哪里读取数据</div><div class="line"></div><div class="line">earliest: automatically reset the offset to the earliest offset	设置offset为最开始的处,即从头开始消费</div><div class="line">latest: automatically reset the offset to the latest offset	从设置offset为最近的offset</div><div class="line">none: throw exception to the consumer if no previous offset is found for the consumer&apos;s group</div><div class="line">anything else: throw exception to the consumer.</div></pre></td></tr></table></figure>
<p>实例代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line"></div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 创建针对Kafka的输入流</div><div class="line">val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;</div><div class="line">val  kafkaParams = Map(</div><div class="line">  // kafka的broker-list</div><div class="line">  &quot;meta.broker.list&quot;-&gt;&quot;192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092&quot;,</div><div class="line">)</div><div class="line"></div><div class="line">val topics = Set(</div><div class="line">  &quot;WordCount&quot;</div><div class="line">)</div><div class="line"></div><div class="line">val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams,topics)</div><div class="line">val words = lines.flatMap(_._2.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordcount = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 打印测试</div><div class="line">wordcount.print</div><div class="line"></div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line">ssc.stop()</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/" data-id="cj290sc9c0110ssqqgdva8izn" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/" class="article-date">
  <time datetime="2017-04-16T04:47:25.228Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/">SparkStreaming之架构原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark_streaming架构原理.png" alt=""></p>
<h1 id="StreamingContext的初始化与Receiver的启动原理"><a href="#StreamingContext的初始化与Receiver的启动原理" class="headerlink" title="StreamingContext的初始化与Receiver的启动原理"></a>StreamingContext的初始化与Receiver的启动原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/StreamingContext的初始化与Receiver的启动原理.png" alt=""></p>
<h1 id="SparkStreaming之数据接收原理和源码分析"><a href="#SparkStreaming之数据接收原理和源码分析" class="headerlink" title="SparkStreaming之数据接收原理和源码分析"></a>SparkStreaming之数据接收原理和源码分析</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/SparkStreaming之数据接收原理.png" alt=""></p>
<h1 id="数据处理原理剖析-block与batch关系"><a href="#数据处理原理剖析-block与batch关系" class="headerlink" title="数据处理原理剖析(block与batch关系)"></a>数据处理原理剖析(block与batch关系)</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/数据处理原理剖析.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/" data-id="cj290sc92010ussqqtuglh75b" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之性能调优" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之性能调优/" class="article-date">
  <time datetime="2017-04-16T04:47:25.227Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之性能调优/">SparkStreaming之性能调优</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="数据接收的并行度调优"><a href="#数据接收的并行度调优" class="headerlink" title="数据接收的并行度调优"></a>数据接收的并行度调优</h1><p>通过网络接收数据时(比如kafka,flume),会将数据反序列化,并存储在spark的内存中,如果数据接收称为系统的瓶颈,那么可以考虑并行化的数据接收,每一个输入DStream都会在某个Worker的Executor上,启动一个Receiver,该Receiver接收一个数据流,因此可以通过创建多个输入DStream,并且配置他们接收数据源不同的分区数据,达到多个数据流的效果,比如说,一个接收两个kafka topic的输入DStream,可以被拆分为两个DStream,每个分别接收一个topic的数据,这样就会创建两个Receiver,从而并行的接收数据,进而提升吞吐量,读个DStream可以使用union算子进行聚合,从而形成一个DStream,然后后续的Transformation算在操作都针对一个聚合后的DStream即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">int numStreams = 5</div><div class="line">List&lt;DStream&gt; kafkaStreams = new ArrayList&lt;DStream&gt;(numStreams)</div><div class="line"></div><div class="line">for(int i=0;i&lt;DStream; i++)&#123;</div><div class="line">	kafkaStreams.add(KafkaUtils.createStream(...))</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">unionedDStream = streamingContext.union(kafkaStreams.get(0),kafkaStreams.get(2)...)</div><div class="line"></div><div class="line"></div><div class="line">unionedDStream.print()</div></pre></td></tr></table></figure>
<p>数据接收并行度调优,除了创建更多输入DStream和Receiver以外,还可以考虑调节block interval参数,”spark.streaming.blockInterval”可以设置block interval(默认是200ms),对于大多数Receiver来说,在将接收到的数据保存到Spark的BlockManager之前,都会将数据切分为一个一个的block,而每个batch中的block数量,则决定了该batch对应的RDD的partition的数量,以及针对该RDD执行Transformation操作时,创建的task的数量,每个batch对应的task数量是可以大约估计的:<br>batch interval / (block interval)</p>
<p>例如说:batch interval为2s,block interval为200ms,会创建10个task,如果你认为每个batch的task的数量太少,即低于每台机器的cpu core数量,那么就说明batch的task数量是不够的,因为所有的cpu资源无法完全被利用起来,要为batch增加block的数量,那么就减小block interval,而然,推荐的block interval最小值是50ms,如果低于这个数值,那么大量task的启动时间,可能会变成一个性能开销点</p>
<p>除了上述说的两个提升设局接收并行度的方式,还有一种方法,技术显示的对输入数据流进行重分区,使用<br>inputStream.reparation(num of partitions)即可,这样就可以将接收到的batch,分布到指定的数量的机器上,然后再进行进一步的操作</p>
<h1 id="任务启动调优"><a href="#任务启动调优" class="headerlink" title="任务启动调优"></a>任务启动调优</h1><p>如果每秒钟启动的task过多,比如每秒启动50个,那么发送这些task到Worker节点上的Executor的性能开销会比较大,而且此时基本就很难达到毫秒级的延迟了,使用下面的操作可以减少这方面的性能开销;<br>1.Task序列化:使用Kryo序列化类库来序列化task,可以减小task的大小,从而减少发送这些task到各个Worker节点上的Executor的时间<br>2.执行模式:在Strandalone模式下,运行spark,可以达到更少的task启动时间</p>
<h1 id="数据处理的并行度调优"><a href="#数据处理的并行度调优" class="headerlink" title="数据处理的并行度调优"></a>数据处理的并行度调优</h1><p>如果在计算的任何stage中使用并行task的数量没有足够多,那么集群资源时无法被充分利用的,举例说:对于分布式的reduce操作,比如reduceByKey和reduceByKeyAndWindow,默认的并行task的数量是由”spark.default.parallelism”参数决定的,你可以在reduceByKey等操作中,传入第二个参数,手动指定该参数的并行度,也可以调节全局的”spark.default.parallelism”参数</p>
<h1 id="数据序列化的调优"><a href="#数据序列化的调优" class="headerlink" title="数据序列化的调优"></a>数据序列化的调优</h1><p>数据序列化造成的系统开销可以由序列化的优化来减小,在流式计算的场景下,有两种类型的数据需要序列化:<br>1.输入数据,默认情况下,接收到的输入数据,是存储在Executor的内存中的,使用的持久化级别是StorageLevel.MEMORY_AND_DISK_SER_2,这意味着,数据被序列化为字节从而减少GC开销,并且会复制以进行Executor失败的容错,因此数据首先会存储在内存中,然后在内存不足时会溢写到磁盘上,从而为流式计算来保存所有需要的数据,这里的序列化有明显的性能开销—Receiver必须反序列化从网络接收到的数据,然后再使用spark的序列化格式序列化数据</p>
<p>3.流式计算操作生成的持久化RDD,流式计算操作生成的持久化RDD可能会持久化到内存中,例如:窗口操作默认就会将数据持久化在内存章,因为这些数据后面可能会在多个窗口中被使用,并被处理多次,然而,不像spark core的默认持久化级别,StorageLevel.MEMORY_ONLY,流式计算操作生成的RDD的默认持久化级别是:StorageLevel.MEMORY_ONLY_SER,默认就会减小GC开销</p>
<p>在上述的场景中,使用Kryo序列化类库可以减小cpu和内存的性能开销,使用Kryo时,一定要考虑注册自定义的类,并且禁用对应引用的tracking(spark.Kryo.referenceTracking)</p>
<p>在写特殊的场景下,比如需要为流式应用保持的数据总量并不是很多,也许可以将数据以非序列化的方式进行持久化,从而减少序列化和反序列化的cpu开销,而且又不会有太昂贵的GC开销,那么你可以考虑通过显示的设置持久化级别,来禁止持久化时对数据进行序列化,这样就减少用于序列化和反序列化的cpu性能开销,并且不用承担太多的gc开销</p>
<h1 id="batch-interval"><a href="#batch-interval" class="headerlink" title="batch interval"></a>batch interval</h1><p>如果想让一个运行在集群上的spark streaming应用程序可以稳定,他就必须尽可能快的处理接收到的数据,换句话说,batch应该在生成之后,就尽可能的处理掉,对于一个应用来说,可以通过观察spark UI上的batch的处理时间来定,batch处理时间必须小于batch interval时间,不然上一个batch还没有处理成功,那么下一个batch就来了,这样会造成数据堆积</p>
<p>基于流式计算的本质,batch interval对于,在固定集群资源条件下,应用能保持的数据接收速率,会有巨大的影响,例如:在WordCount例子中,对于一个特定的数据接收速率,应用业务可以保证每2秒打印一次单词计数,而不是每500ms,因为batch interval 需要被设置的让与其的数据接收速率可以在生产环境中保持住</p>
<p>为你的应用计算正确的batch大小的比较好的方法,是在一个很保守的batch interval ,比如5-10s,以很慢的数据接收速率进行测试,要检查应用是否跟得上这个数据速率,可以检查每个batch的处理时间的延迟,如果处理时间与batch interval基本吻合,那么应用就是稳定的,否则,如果batch调度的延迟持续增加,那么就意味着无法跟得上这个速率,也就是不稳定的,因此,你要想有一个稳定的配置,可以尝试提升数据处理的速度,或者增加batch interval,记住,由于临时性的数据增长导致的暂时的延迟,可以合理的,只要延迟情况可以在短时间内恢复即可</p>
<h1 id="内存调优"><a href="#内存调优" class="headerlink" title="内存调优"></a>内存调优</h1><p>Transformation操作会决定你的内存的使用:<br>spark streaming应用需要的集群内UC你资源,是由使用的Transformation操作类型决定的,举例来说,如果想要使用一个窗口长度为10分钟的window操作,那么集群就必须有足够的内存来保存10分钟的数据,如果想要使用updateStateByKey来维护许多key的state,那么你的内存资源就必须足够大,返货来说,如果想要做一个简单的map-filter-sotre操作,那么需要使用的内存就很少</p>
<p>通常来说,通过Receiver接收到的数据,会使用StorageLevel.MEMORY_AND_DISK_SER_2持久化级别来进行存储,因此无法保存在内存中的数据会溢写到磁盘上,而溢写到磁盘上,是会降低应用的性能的,因此,通常是建议为应用提供他需要的足够的内存资源,建议在一个小规模的场景下测试内存的使用量,并进行评估</p>
<p>内存调优的另外一个方面是垃圾回收,对于流式应用来说,如果要获得低延迟的,肯定不想要有因为JVM垃圾回收导致的长时间延迟,有很多参数可以帮助降低内存使用和GC开销:<br>1.DStream的持久化级别:<br>输入数据和某些操作产生的中间RDD,默认持久化时都会序列化为字节,与非序列化的方式相比,这会降低内存和GC开销,使用Kryo序列化机制可以进一步减少内存使用和GC开销,进一步降低内存使用率,可以对数据进行压缩,由”spark.rdd.compress”参数控制(默认false)</p>
<p>2.清理旧数据:<br>默认情况下,所有输入数据和通过DStream Transformation操作生成的持久化的RDD,会自动被清理,spark streaming会决定何时清理这些数据,取决于Transformation操作类型,例如:你在使用窗口长度为10分钟的window操作,spark会保持10分钟以内的数据,时间过了以后会清理旧数据,但是在某些特殊场景下,比如spark sql和spark streaming整合使用时,在异步开启的线程中,使用spark streaming针对batch RDD进行执行查询,那么就㤇让spark 保持更长时间的数据,知道sparksql查询结束,可以使用:streamingContext.remember()方法来实现</p>
<p>3.CMS垃圾回收:<br>使用并行的mark-sweep垃圾回收机制,被推荐使用,用来保持GC开销,虽然并行的GC会降低吞吐量,但是还是建议使用它,来减少batch的处理时间(降低处理过程中的gc开销),如果要使用,那么要在driver端和Executor端都开启,在spark-submit中使用–driver-java-options设置,使用spark.executor.extra.javaOptions参数设置<br>XX:+UseConMarkSweepGC</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之性能调优/" data-id="cj290sc8w010ossqqqxdvez6j" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之实时wordcount程序开发" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之实时wordcount程序开发/" class="article-date">
  <time datetime="2017-04-16T04:47:25.225Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之实时wordcount程序开发/">SparkStreaming之实时wordcount程序开发</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line">// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序</div><div class="line">val conf = new SparkConf()</div><div class="line">  .setAppName(&quot;Streaming&quot;)</div><div class="line">  .setMaster(&quot;local[2]&quot;)</div><div class="line">// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)</div><div class="line">val ssc = new StreamingContext(conf,Seconds(1))</div><div class="line"></div><div class="line">// 首先,创建输入DStream,代表了一个从数据源(kafka,socket)来的持续不断的实时数据流</div><div class="line">// 这里创建的数据源是socket:参数:监听的主机和端口</div><div class="line">val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)</div><div class="line">// 返回的是一个DStream,表示每隔一秒会有一个RDD,其中封装了这一秒发送过来的数据</div><div class="line">// RDD的元素类型为String,即一行一行的文本</div><div class="line"></div><div class="line">// 开始对接收到的数据,对DStream执行算子操作</div><div class="line">// 在底层实际上会对DStream中的一个一个的RDD,执行我们应用在DStream上的算子</div><div class="line">// 产生的新的RDD会作为新DStream中的RDD</div><div class="line">val words = lines.flatMap(_.split(&quot; &quot;))</div><div class="line">val pairs = words.map((_,1))</div><div class="line">val wordCounts = pairs.reduceByKey(_ + _)</div><div class="line"></div><div class="line">// 可以看到spark streaming开发程序和spark core很像</div><div class="line">// 因为DStream是对Rdd的封装,那么DStream操作,就是对Rdd的操作</div><div class="line"></div><div class="line">// 休眠,打印(测试用)</div><div class="line">Thread.sleep(50000)</div><div class="line">wordCounts.print</div><div class="line"></div><div class="line">ssc.start()</div><div class="line">ssc.awaitTermination()</div><div class="line"></div><div class="line">/*总结:</div><div class="line">1.每秒钟发送到指定socket端口中的数据,都会被lines DStream接收到</div><div class="line">2.lines DStream会把每秒的数据,也就是一行一行的文本,诸如&quot;hello world&quot;, 封装成一个RDD</div><div class="line">3.然后对每秒钟中对应的RDD执行后续的一系列的算子操作</div><div class="line">4.最终就得到了每秒钟发送过来的单词统计</div><div class="line">5.可以将最后计算出的wordcount中的一个一个的RDD,写入外部的缓存,或者持久化DB</div><div class="line"> */</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之实时wordcount程序开发/" data-id="cj290sc8s010lssqq4wlk3wdw" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理/" class="article-date">
  <time datetime="2017-04-16T04:47:25.223Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理/">SparkStreaming之基本工作原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>接收实时输入数据流,然后将数据拆分成多个batch,比如每收集1s的数据封装为一个batch,然后将每个batch交给spark的计算引擎进行处理,最后会产生出一个结果数据流,其中的数据,也是由一个一个的batch所组成的</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/sparkstreaming_yuanli.png" alt=""></p>
<p>DStream</p>
<p>Spark Streaming提供了一种高级的抽象,叫做Dstream(Discretized Stream 离散流),他代表了一个持续不断的数据流,DStream可以通过输入数据源来创建,比如:Kafka,Flume,Kinesis,也可以通过对其他DStream应用高阶函数来创建,比如:map,reduce,join,window</p>
<p>DStream的内部,其实是一系列持续不断产生的RDD,DStream中的每个RDD都包含了一个时间段内的数据</p>
<p>对DStream应用的算子,比如map,其实在底层会被翻译为对DStream中每个RDD的操作,比如对一个DStream执行一个map操作,会产生一个新的DStream,但是,在底层,其实其原理为,对输入DStream中每个时间段的RDD,都应用一遍map操作,然后生成新的RDD,即作为新的DStream中的那个时间段的一个RDD,底层的RDD的Transformation操作,其实,还是由spark core的计算引擎来实现的,spark Streaming对spark core进行了一层封装,隐藏了细节,然后对开发人员提供了方便易用的高层次的API</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/DStream2.png" alt=""></p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理/" data-id="cj290sc8o010issqqho68hk35" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之介绍" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之介绍/" class="article-date">
  <time datetime="2017-04-16T04:47:25.221Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之介绍/">SparkStreaming之介绍</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>SparkStreaming其实就是一种spark提供的一种实时计算框架,他的底层组件或者概念,其实还是最核心的RDD,只不过,针对实时计算的特点,在RDD之上,进行了一层封装,叫做Dstream,就像spark sql针对数据查询应用提供了一种基于RDD之上的全新的概念叫DataFrame一样</p>
<p>spark Streaming是spark core API的一种扩展,他可以用于进行大规模,高吞吐,容错的实时数据流的处理,他支持从多种数据源中消费数据,比如:kafka,flume,Twitter,ZeroMQ,或者TCP Socket,并且能够使用类似高阶函数的复杂算法来进行数据处理,比如:map,reduce,join,和window,处理后的数据可以被保存到文件系统,数据库等存储中</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/sparkstreaming_jianjie.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之介绍/" data-id="cj290sc8l010fssqq89ngl0ld" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之与缓存与持久化机制" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与缓存与持久化机制/" class="article-date">
  <time datetime="2017-04-16T04:47:25.219Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与缓存与持久化机制/">SparkStreaming之与缓存与持久化机制</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>与RDD类似,spark streaming也可以让开发人员手动控制,将数据流中的数据持久化到内存中,对DStream调用persist()方法,就可以让spark streaming自动将该数据流中的所有产生的RDD,都持久化到内存中</p>
<p>如果要对一个DStream多次执行操作,那么对DStream持久化是非常有用的,因为多次操作,可以共享使用内存中的一份缓存数据,对于基于窗口的操作,比如reduceByKeyAndWindow,以及基于状态的操作,比如updateStateByKey,默认就隐式开启了持久化的机制,即spark streaming默认就会将上述操作产生的数据,缓存到内存中,不需要开发人员手动调用persist()方法</p>
<p>对于通过网络接收数据的输入流,比如:socket,kafka,flume等,默认的持久化级别是将数据复制一份,以便于容错</p>
<p>与RDD不同的是,默认的持久化级别,统一都是要序列化的</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与缓存与持久化机制/" data-id="cj290sc8i010cssqqusj16i19" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkStreaming之与Storm的对比分析" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与Storm的对比分析/" class="article-date">
  <time datetime="2017-04-16T04:47:25.217Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与Storm的对比分析/">SparkStreaming之与Storm的对比分析</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>sparkStreaming与storm的对比</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/sparkStreaming与storm的对比.png" alt=""></p>
<p>sparkStreaming与storm的优劣分析</p>
<p>事实上,spark streaming绝对谈不上比Storm优秀,这两个框架在实时计算领域中,都很优秀,只是擅长的细分场景并不相同</p>
<p>spark streaming仅仅在吞吐量上比storm要优秀,而吞吐量这一点,也是历来挺spark streaming的人着重强调的,但是问题是,并不是所有的实时计算场景下,都那么注重吞吐量,因此,通过吞吐量说spark streaming强于storm并不能说服人</p>
<p>事实上,storm在实时延迟度上,比spark streaming就好多了,前者是纯实时的,但是后者是准实时的,而且,storm的事务机制,健壮性,容错性,动态调整并行度等特性,都要比spark streaming更加优秀</p>
<p>spark streaming有一点是storm绝对比不上的,就是:它位于spark生态技术栈中,因此,spark streaming可以和spark core,spark sql无缝整合,也就意味着,我们可以对实时处理出来的中间数据,立即在程序中无缝进行延时批处理,交互式查询等操作,这个特点大大增强了spark streaming的优势和功能</p>
<p>spark streaming与storm的应用场景<br>对于storm来说:<br>1.建议那种需要纯实时,不能忍受1秒以上延时的场景下使用,比如实时金融系统,要求纯实时进行金融交易和分析<br>2.此外,如果对于实时计算的功能中,要求可靠的事务机制,即数据的处理完全精准,一条也不能少,一条也不能多,那么可以考虑storm<br>3.如果还需要针对高峰低峰时间段,动态调整实时计算程序的并行度,以最大限度利用集群资源(通常是小型公司,集群资源紧张的情况),也可以考虑storm<br>4.如果一个大数据应用系统,他就是纯粹的实时计算,不需要在中间执行sql交互式查询,复杂的Transformation算子等,那么用storm是比较好的选择</p>
<p>对于spark streaming来说:<br>1.如果对上述适用于storm的三点,一条都不满足的实时场景,即:不要求纯实时,不要求强大可靠的事务机制,不要求动态调整并行度,那么可以考虑使用spark streaming<br>2.考虑使用spark streaming最主要的一个因素,应该是针对整个项目进行宏观的考虑,可能还会牵扯到高延迟批处理,交互式查询等功能,那么就应该首先spark生态,用spark core开发离线批处理,用spark sql开发交互式查询,用spark streaming开发实时计算,三者可以无缝整合,给系统提供非常高的可扩展性</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与Storm的对比分析/" data-id="cj290sc8f0109ssqqvh05fqyv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/43/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/42/">42</a><a class="page-number" href="/page/43/">43</a><span class="page-number current">44</span><a class="page-number" href="/page/45/">45</a><a class="page-number" href="/page/46/">46</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/45/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>