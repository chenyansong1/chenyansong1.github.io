<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/41/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark从入门到精通_笔记/spark的常见的rdd" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/" class="article-date">
  <time datetime="2017-04-16T04:47:25.281Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/">spark的常见的rdd</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h1><ul>
<li>使用程序中的集合创建rdd<br>主要用于进行测试,可以在实际部署到集群运行之前,自己使用集合构造册数数据,来测试后面的spark应用的流程</li>
<li>使用本地文件创建rdd</li>
<li>使用HDFS文件创建rdd<br>主要可以针对HDFS上存储的大数据,进行离线批处理操作</li>
</ul>
<p>使用程序中的集合创建rdd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">object MakeRDD &#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line"></div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    test(sc)</div><div class="line">    sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def test(sc:SparkContext)=&#123;</div><div class="line">    val numberRdd = sc.parallelize(Seq(1,2,3,8,22))</div><div class="line">    val reducedRdd = numberRdd.reduce(_+_)</div><div class="line">    println(reducedRdd)</div><div class="line"></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //构建SparkContext</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line"></div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用本地文件创建rdd和HDFS文件创建rdd<br>1.如果是针对本地文件的话,如果是在Windows上本地测试,Windows上有一份文件即可;如果是在spark集群上针对linux本地文件,那么需要将文件拷贝到所有worker节点上<br>2.spark的textFile方法支持针对目录,压缩文件以及通配符进行rdd的创建<br>3.spark默认会为HDFS的每一个block创建一个partition,但是也可以通过textFile()的第二个参数手动设置分区数量,只能比block数量多,不能比block数量少</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">object MakeRDD &#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line"></div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    test(sc)</div><div class="line">    sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def test(sc:SparkContext)=&#123;</div><div class="line">    val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">    val rddreuslt = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)</div><div class="line">    rddreuslt.foreach(println)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //在实际的生成中,我们是封装函数来进行逻辑的组织</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line"></div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//如果是使用HDFS文件创建rdd,只要把textFile的文件路径修改为HDFS文件路径即可</div><div class="line">val rdd = sc.textFile(&quot;hdfs://spark1:9000/xx.txt&quot;)</div><div class="line"></div><div class="line">//如果是在集群中运行的时候,那么需要将.setMaster(&quot;local&quot;)去掉</div></pre></td></tr></table></figure>
<h1 id="Transformation和action"><a href="#Transformation和action" class="headerlink" title="Transformation和action"></a>Transformation和action</h1><p>spark支持两种rdd操作:Transformation和action,Transformation操作会针对已有的rdd创建一个新的rdd,而action则主要是对rdd进行最后的操作,比如遍历,reduce,保存到文件等,并可以返回结果给Driver程序</p>
<p>例如map就是一种Transformation操作,他用于将已有的rdd的每个元素传入一个自定义的函数,并获取一个新的元素,然后将所有的新元素组成一个新的rdd;而reduce就是一种action操作,它用于对rdd中的所有元素进行聚合操作,并获取一个最终的结果,然后返回给Driver程序</p>
<p>Transformation的特点就是lazy特性,lazy特性的指的是:如果一个spark应用中只定义了Transformation操作,那么即使你执行该应用,这些操作也不会执行,也就是说,Transformation是不会触发spark程序的执行的,他只是记录了对rdd的所作的操作,但是不会自发的执行,只有当Transformation之后,接着执行一个action操作,那么所有的Transformation才会被执行,spark通过这种lazy特性,来进行底层的spark应用执行的优化,避免产生过多的中间结果</p>
<p>action操作执行,会触发一个spark job的运行,从而触发这个action之前所有的Transformation的执行,这是action的特性</p>
<p>综上:Transformation会产生rdd,而action会产生结果而不是rdd</p>
<p>下图是程序提交的流程上来反映spark的Transformation的lazy特性</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/Transformation_action.png" alt=""></p>
<p>案例:统计文件中每行出现的次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">var reducedByKeyRdd = rdd.map((_,1)).reduceByKey(_+_)</div><div class="line">reducedByKeyRdd.foreach(println)</div></pre></td></tr></table></figure></p>
<p>常用的Transformation介绍</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">map</td>
<td style="text-align:left">将rdd中的每个元素传入自定义函数,获取一个新的元素,然后用新的元素组成的新的rdd</td>
</tr>
<tr>
<td style="text-align:left">filter</td>
<td style="text-align:left">对RDD中的每一个元素进行判断,如果返回true就保留</td>
</tr>
<tr>
<td style="text-align:left">flatMap</td>
<td style="text-align:left">与map类似,大那是对每个元素都可以返回一个或多个新元素,然后对所有的元素flat</td>
</tr>
<tr>
<td style="text-align:left">groupByKey</td>
<td style="text-align:left">根据key进行分组,每个key对应一个Iterator<value></value></td>
</tr>
<tr>
<td style="text-align:left">reduceByKey</td>
<td style="text-align:left">对每个key对应的value进行reduce操作</td>
</tr>
<tr>
<td style="text-align:left">sortByKey</td>
<td style="text-align:left">对每个key对应的value进行排序操作</td>
</tr>
<tr>
<td style="text-align:left">join</td>
<td style="text-align:left">对两个包含<key,value>对的rdd进行join操作,每个keyjoin上的pair,都会传入自定义函数进行处理</key,value></td>
</tr>
<tr>
<td style="text-align:left">cogroup</td>
<td style="text-align:left">同join,但是是每个key对应的Iterable<value>都会传入自定义的函数进行处理</value></td>
</tr>
</tbody>
</table>
<p>常用的action操作</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">reduce</td>
<td style="text-align:left">将rdd中的所有元素进行聚合操作,第一个和第二个元素聚合产生的值,和第三个元素聚合,产生的值和第四个元素聚合,一次类推</td>
</tr>
<tr>
<td style="text-align:left">collect</td>
<td style="text-align:left">将rdd中的所有元素获取到本地客户端</td>
</tr>
<tr>
<td style="text-align:left">count</td>
<td style="text-align:left">获取rdd元素总数</td>
</tr>
<tr>
<td style="text-align:left">take(n)</td>
<td style="text-align:left">后rdd前n个元素</td>
</tr>
<tr>
<td style="text-align:left">saveAsTextFile</td>
<td style="text-align:left">将rdd元素保存到文件中,对每个元素调用toString方法</td>
</tr>
<tr>
<td style="text-align:left">countByKey</td>
<td style="text-align:left">对每个key对应的值进行count计数</td>
</tr>
<tr>
<td style="text-align:left">foreach</td>
<td style="text-align:left">遍历rdd中的每一个元素</td>
</tr>
</tbody>
</table>
<h1 id="Transformation实例"><a href="#Transformation实例" class="headerlink" title="Transformation实例"></a>Transformation实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line">#map:将集合中每个元素乘以2</div><div class="line">val numberRdd = sc.parallelize(1 to 7)</div><div class="line">val resultRdd = numberRdd.map(_*2)</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">2</div><div class="line">4</div><div class="line">6</div><div class="line">8</div><div class="line">10</div><div class="line">12</div><div class="line">14</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#filter:过滤出集合中的偶数</div><div class="line">val numberRdd = sc.parallelize(1 to 7)</div><div class="line">val resultRdd = numberRdd.filter(_%2==0)</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">2</div><div class="line">4</div><div class="line">6</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#flatMap:将行拆分为单词</div><div class="line">val linesRdd = sc.parallelize(Seq(&quot;zhangsna 88&quot;,&quot;lisi 99&quot;))</div><div class="line">val resultRdd = linesRdd.flatMap(_.split(&quot; &quot;))</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">zhangsna</div><div class="line">88</div><div class="line">lisi</div><div class="line">99</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#groupByKey:将每个班级的成绩进行分组</div><div class="line"></div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))</div><div class="line">val resultRdd = linesRdd.groupByKey()//返回: RDD[(K, Iterable[V])]</div><div class="line">resultRdd.foreach&#123;</div><div class="line">  score=&gt;&#123;</div><div class="line">    print(score._1+&quot; :&quot;)</div><div class="line"></div><div class="line">	//println(score._2.toList)</div><div class="line">    score._2.foreach(sco=&gt;print(sco.toString + &quot; &quot;))</div><div class="line">    println</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">cls1 :80 82</div><div class="line">cls2 :88 98</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#reduceByKey:统计每个班级的总分</div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))</div><div class="line">val resultRdd = linesRdd.reduceByKey(_+_)</div><div class="line">resultRdd.foreach(println)</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">(cls1,162)</div><div class="line">(cls2,186)</div><div class="line"> */</div><div class="line"></div><div class="line">#sortByKey:将学生分数进行排序</div><div class="line"></div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;zhangsna&quot;,80),(&quot;lisi&quot;,88),(&quot;wangwu&quot;,82),(&quot;zhaoliu&quot;,98)))</div><div class="line"></div><div class="line">//要将key放在tuple2的第一个位置,这就是第一个map的作用,而最后一个map的作用就是调整打印的顺序</div><div class="line">val resultRdd = linesRdd.map(t=&gt;(t._2,t._1)).sortByKey(false).map(t=&gt;(t._2,t._1))</div><div class="line">resultRdd.foreach(println)</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">(zhaoliu,98)</div><div class="line">(lisi,88)</div><div class="line">(wangwu,82)</div><div class="line">(zhangsna,80)</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#join:打印每个学生的成绩</div><div class="line"></div><div class="line">val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(3,82)))</div><div class="line">val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(3,&quot;wangwu&quot;)))</div><div class="line"></div><div class="line">val joinedRdd = scoreRdd.join(studRdd)</div><div class="line">/*</div><div class="line">def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</div><div class="line">返回的是:(K, (V, W))</div><div class="line"> */</div><div class="line">joinedRdd.foreach&#123;</div><div class="line">  t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取</div><div class="line">    val snu = t._1</div><div class="line">    val (score,name) = t._2</div><div class="line">    println(name + &quot;:&quot; + score + &quot;:&quot; + snu)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">  zhangsna:80:1</div><div class="line">  wangwu:82:3</div><div class="line">  lsii:88:2</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#cogroup</div><div class="line"></div><div class="line">    val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(1,80),(2,88),(3,82)))</div><div class="line">    val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(1,&quot;zhangsna2&quot;),(3,&quot;wangwu&quot;)))</div><div class="line"></div><div class="line">    val joinedRdd = scoreRdd.cogroup(studRdd)</div><div class="line">    /*</div><div class="line">    def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]</div><div class="line">    返回的是:(K, (Iterable[V], Iterable[W]))</div><div class="line">     */</div><div class="line">    joinedRdd.foreach&#123;</div><div class="line">      t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取</div><div class="line">        val snu = t._1</div><div class="line">        val (score,name) = t._2</div><div class="line">        println(snu + &quot;: &quot; + score.toList.toString + &quot;  &quot; + name.toList.toString)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /*打印结果:</div><div class="line">      1: List(80, 80)  List(zhangsna, zhangsna2)</div><div class="line">      3: List(82)  List(wangwu)</div><div class="line">      2: List(88, 88)  List(lsii)</div><div class="line">     */</div></pre></td></tr></table></figure>
<h1 id="action实例"><a href="#action实例" class="headerlink" title="action实例"></a>action实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#reduce操作</div><div class="line"> val scoreRdd = sc.parallelize(Seq(1,2,3,4,5))</div><div class="line"> val result = scoreRdd.reduce(_+_)</div><div class="line"> println(result)</div><div class="line"></div><div class="line"> /*打印结果:</div><div class="line"> 15</div><div class="line">  */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#collect</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">//使用collect操作将分布在远程的数据拉取到本地,对大数据量不要这么做,测试可以,因为可能造成本地内存溢出,还可能因为将远程的数据拉倒本地,走网络的话,性能会很差</div><div class="line">val result = scoreRdd.collect</div><div class="line">result.foreach(println)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#count</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">val result = scoreRdd.count</div><div class="line">println(result)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#take</div><div class="line"></div><div class="line"> val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line"> //从远程获取指定数量的数据,返回:Array[T]</div><div class="line"> val result = scoreRdd.take(3)</div><div class="line"> result.foreach(println)</div><div class="line"> </div><div class="line"> /*结果打印:</div><div class="line"> 1</div><div class="line"> 2</div><div class="line"> 3</div><div class="line">  */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#saveAsTextFile</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">//从远程获取指定数量的数据,返回:Array[T]</div><div class="line">val result = scoreRdd.saveAsTextFile(&quot;C:\\Users\\Administrator\\Desktop\\hadoop\\result&quot;)</div><div class="line"></div><div class="line">/*结果:</div><div class="line">在C:\\Users\\Administrator\\Desktop\\hadoop\\result目录下,有下面的文件:</div><div class="line"></div><div class="line">._SUCCESS.crc</div><div class="line">.part-00000.crc</div><div class="line">.part-00001.crc</div><div class="line">.part-00002.crc</div><div class="line">_SUCCESS</div><div class="line">part-00000</div><div class="line">part-00001</div><div class="line">part-00002</div><div class="line"></div><div class="line">因为在parallelize的指定的分区为3,所以会生成3个part,其中的在</div><div class="line">part-00000文件中存在的数据:1</div><div class="line">part-00000文件中存在的数据:2\n3</div><div class="line">part-00000文件中存在的数据:4\n5\n6</div><div class="line"></div><div class="line">*/</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#countByKey</div><div class="line"> val scoreRdd = sc.parallelize(Seq((&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls3&quot;,&quot;zhangsan3&quot;),(&quot;cls2&quot;,&quot;zhangsan2&quot;)),3)</div><div class="line"></div><div class="line"> val result = scoreRdd.countByKey() //返回:Map[K, Long]</div><div class="line"> for((k,v)&lt;-result)&#123;</div><div class="line">   println(k+&quot;:&quot;+v.toString)</div><div class="line"> &#125;</div><div class="line"> /*结果打印:</div><div class="line"> cls2:1</div><div class="line"> cls3:1</div><div class="line"> cls1:3</div><div class="line">  */</div></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/" data-id="cj290scig018dssqqlguolyak" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark的基本工作原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/" class="article-date">
  <time datetime="2017-04-16T04:47:25.279Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/">spark的基本工作原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>下面的这张图表示的是spark的基本的工作原理图(简图)<br><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark的基本工作原理.png" alt=""></p>
<p>下面是对RDD的概念解释<br>1.RDD在抽象上来说是一种元素的集合,包含了数据,他是被分区的,分为多个分区,每个分区分布在集群中的不同节点上,从而让RDD中的数据可以被并行操作(分布式数据集)<br>2.RDD的创建:通过HDFS文件或hive表创建;通过应用程序的集合来创建<br>3.RDD的数据默认情况下存放在内存中,但是在内存资源不足时,spark会自动将RDD数据写入磁盘(弹性)<br>4.RDD最重要的特性是:提供了容错性,可以自动从节点失败中恢复过来,即:如果某个节点山的RDD partition因为节点故障,导致数据丢了,那么RDD会自动通过自己的数据来源重新计算该partition,这一切对使用者是透明的<br><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/rdd的概念理解.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/" data-id="cj290scic018assqqy3r2lz9m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark的topN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的topN/" class="article-date">
  <time datetime="2017-04-16T04:47:25.277Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的topN/">spark的topN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>取最大的前3个数字:其实就是在sortByKey之后取take(3)<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">val lineRdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">val reducedRdd = lineRdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)</div><div class="line">val countWords = reducedRdd.map(count=&gt;(count._2,count._1))</div><div class="line">val sortedRdd = countWords.sortByKey(false)</div><div class="line">val result = sortedRdd.map(sort=&gt;(sort._2,sort._1))</div><div class="line">val top3Number = result.take(3)</div><div class="line"></div><div class="line">top3Number.foreach(println)</div><div class="line"></div><div class="line">/*执行结果:</div><div class="line">(spark,19)</div><div class="line">(hadoop,13)</div><div class="line">(88,6)</div><div class="line"> */</div></pre></td></tr></table></figure></p>
<p>获取分组之后的组内的topN,实现步骤如下:<br>1.对rdd进行groupByKey,返回的是(K, Iterable[V])<br>2.在遍历1的结果,然后在每组中使用sortWith(排序的规则),对组内数据进行排序,取组内的topN,并返回组内的数据<br>3.对组与组之间进行排序,sortBy可以指定某列进行排序<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">val lines=rdd.map&#123; line =&gt; (line.split(&quot; &quot;)(0),line.split(&quot; &quot;)(1).toInt) &#125;</div><div class="line"></div><div class="line">//分组</div><div class="line">val groups=lines.groupByKey() //返回:RDD[(K, Iterable[V])]</div><div class="line">//组内进行排序</div><div class="line">val groupsSort=groups.map(tu=&gt;&#123;</div><div class="line">  val key=tu._1</div><div class="line">  val values=tu._2</div><div class="line">  val sortValues=values.toList.sortWith(_&gt;_).take(4)//取top 4</div><div class="line">  (key,sortValues)</div><div class="line">&#125;)</div><div class="line"></div><div class="line">//组与组之间进行排序</div><div class="line">groupsSort.sortBy(tu=&gt;tu._1, false, 1).collect.foreach(value=&gt;&#123;</div><div class="line">  print(value._1)</div><div class="line">  value._2.foreach(v=&gt;print(&quot;\t&quot;+v))</div><div class="line">  println()</div><div class="line">&#125;)</div><div class="line"></div><div class="line">/*</div><div class="line">打印结果:</div><div class="line">spark	100	99	94	88</div><div class="line">hadoop	88	56	35	33</div><div class="line"> */</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的topN/" data-id="cj290sci80187ssqqps675c2t" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark架构原理" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark架构原理/" class="article-date">
  <time datetime="2017-04-16T04:47:25.276Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark架构原理/">spark架构原理图解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark架构原理.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark架构原理/" data-id="cj290sci50184ssqq3sjdzhn1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化四之对多次使用的RDD进行持久化或checkpoint" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化四之对多次使用的RDD进行持久化或checkpoint/" class="article-date">
  <time datetime="2017-04-16T04:47:25.274Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化四之对多次使用的RDD进行持久化或checkpoint/">spark性能优化四之对多次使用的RDD进行持久化或checkpoint</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如果程序中,对某一个RDD,基于他进行了多次Transformation或者action操作,那么就非常有必要对其进行持久化操作,以避免对一个RDD反复进行计算</p>
<p>此外,如果要保证在RDD的持久化数据可能丢失的情况下,还要保证高性能,那么可以对RDD进行checkpoint操作</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/checkpoint_cache.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化四之对多次使用的RDD进行持久化或checkpoint/" data-id="cj290schx017yssqqlk8gosj2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化十之reduceByKey和groupByKey性能对比" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十之reduceByKey和groupByKey性能对比/" class="article-date">
  <time datetime="2017-04-16T04:47:25.273Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十之reduceByKey和groupByKey性能对比/">spark性能优化十之reduceByKey和groupByKey性能对比</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>reduceByKey和groupByKey</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">val counts = pairs.reduceByKey(_+_)</div><div class="line">val counts = pairs.groupByKey().map(wordCounts=&gt;(wordCounts._1,wordCounts._2.sum))</div></pre></td></tr></table></figure>
<p>如果能用reduceByKey,那就用reduceByKey,因为他会<strong>在map端,先进行本地combine</strong>,可以大大减少要传输到reduce端的数据量,减小网络传输的开销</p>
<p>只有在reduceByKey处理不了时,才用groupByKey().map()来替代</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十之reduceByKey和groupByKey性能对比/" data-id="cj290scha017gssqq8dz8rlj2" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化十一之shuffle性能优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十一之shuffle性能优化/" class="article-date">
  <time datetime="2017-04-16T04:47:25.271Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十一之shuffle性能优化/">spark性能优化十一之shuffle性能优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">new SparkConf().set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;)</div><div class="line"></div><div class="line">spark.shuffle.consolidateFiles:是否开启shuffle block file的合并,默认是false</div><div class="line">spark.reducer.maxSizeFlight: reduce task的拉取缓存,默认48M</div><div class="line">spark.shuffle.file.buffer: map task的写磁盘缓存,默认32k</div><div class="line">spark.shuffle.io.maxRetries:拉取失败的最大重试次数,默认3次</div><div class="line">spark.shuffle.io.retryWait:拉取失败的重试间隔,默认5s</div><div class="line">spark.shuffle.memoryFraction:用于reduce端聚合的内存比例,默认0.2,超过比例机会溢出到磁盘上</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十一之shuffle性能优化/" data-id="cj290scgz0177ssqqi7gydcme" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化六之java虚拟机垃圾回收调优" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化六之java虚拟机垃圾回收调优/" class="article-date">
  <time datetime="2017-04-16T04:47:25.270Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化六之java虚拟机垃圾回收调优/">spark性能优化六之java虚拟机垃圾回收调优</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>java虚拟机垃圾回收调优的背景</p>
<p>如果在持久化RDD的时候,持久化了大量的数据,那么java虚拟机的垃圾回收就可能成为一个性能瓶颈,因为java虚拟机会定期进行垃圾回收,此时就会追踪所有的java对象,并且在垃圾回收时,找到那些已经不再使用的对象,然后清理旧的对象,来给新的对象腾出内存空间</p>
<p>垃圾回收的性能开销,是跟内存中的对象的数量,成正比的,所以,对于垃圾回收的性能问题,首先要做的就是,使用高性能的数据结构,比如array和String;其次就是在持久化rdd的时候,使用序列化级别,而且用Kyo序列化类库,这样每个partition就只是一个对象—– 一个字节数组</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/GC对spark性能影响的原理.png" alt=""></p>
<p>监测垃圾回收</p>
<p>我们可以对垃圾回收进行监测,包括多久进行一次垃圾回收,以及每次垃圾回收耗费的时间,只要在spark-submit脚本中,增加一个配置即可:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--conf &quot;spark.executor.extraJavaOptions=-verbose:gc-X;+PrintGCDetails-XX;+PrintGCTimeStamps&quot;</div></pre></td></tr></table></figure></p>
<p>但是要记住,这里虽然会打印出java虚拟机的垃圾回收的相关信息,但是是输出到了worker上的日志中,而不是driver的日志中</p>
<p>另一种方式:其实也完全可以通过SparKUI(4040端口)来观察每个stage的垃圾回收情况</p>
<p>优化Executor内存比例<br><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/GC对spark性能影响的原理2.png" alt=""></p>
<p>对于垃圾回收来说,最重要的就是调节RDD缓存占用的内存空间,与算子执行时创建的对象占用的内存空间的比例,默认情况下,spark使用每个Executor60%的内存空间来缓存RDD,那么在task执行期间创建的对象,只有40%的内存空间来存放</p>
<p>在这种情况下,很有可能因为你的内存空间的不足,task创建的对象过大,那么一旦发现40%内存空间不够用了,就会触发java虚拟机的垃圾回收操作,因为在极端情况下,垃圾回收操作可能会被频繁触发</p>
<p>在上述情况下,如果发现垃圾回收频繁发生,那么就需要对那个比例进行调优,使用:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">SparkConf().set(&quot;spark.storage.memoryFaction&quot;, &quot;0.5&quot;)</div></pre></td></tr></table></figure></p>
<p>可以将RDD缓存占用空间的比例降低,从而给更多的空间让task创建的对象进行使用</p>
<p>因此,对于RDD持久化,完全可以使用Kryo序列化,加上降低其Executor内存占比的方式来减少其内存消耗,给task提供更多的内存,从而避免task的执行频繁触发GC</p>
<p>高级垃圾回收调优</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/full_gc.png" alt=""></p>
<p>java堆空间被划分成了两块空间,一个是年轻代,一个是老年代,年轻代存放的是段时间存活的对象,老年代存放的是长时间存活的对象,年轻代又被划分为三块空间:Eden,Survivor1,Survivor2</p>
<p>首先,Eden区域和Survivor1区域用于存放对象,Survivor2区域备用,创建的对象,首先放入Eden区域和Survivor1区域,如果Eden区域满了,那么就会触发一次minor gc,进行年轻代的垃圾回收,Eden和Survivor1区域中存活的对象,会被移动到Survivor2区域中,然后Survivor1和Survivor2的角色调换,Survivor1变成了备用</p>
<p>如果一个对象,在年轻代中,撑过了多次垃圾回收,都没有被回收掉,那么会被认为是长时间存活的,因此会被移入老年代中,此外,如果将Eden和Survivor1中的存活对象,尝试放入Survivor2中时,发现Survivor2放满了,那么会直接放入老年代中,此时就出现了,短时间存活的对象进入老年代的问题</p>
<p>如果老年代的空间满了,那么就会触发full gc ,进行老年代的垃圾回收操作</p>
<p>spark中,垃圾回收调优的目标就是:只有真正长时间存活的对象,才能进入老年代,短时间存活的对象,只能待在年轻代中,不能因为某个Survivor区域空间不够,在minor gc时,就进入老年代,从而造成短时间存活的对象长期待在老年代中占据了空间,而且full gc时要回收大量的短时间存活的对象,导致full gc速度缓慢</p>
<p>如果发现,在task执行期间,大量full gc发生了,那么说明,年轻代的Survivor区域,给的空间不够大,此时可以执行一些操作来优化垃圾回收行为:<br>1.包括降低spark.storage.memoryFaction的比例,给年轻代更多的空间,来存放短时间存活的对象<br>2.给Eden区域分配更大的空间,使用-xmn即可(在spark.executor.extraJavaOptions中配置,见上面),通常建议给Eden区域,预计大小的4/3<br>3.如果使用的是HDFS文件,那么很好估计Eden区域大小,如果每个Executor有4个task,然后每个HDFS压缩块解压后大小是3倍,此外每个HDFS块的大小为128M,那么Eden区域的预计大小就是:4<em>3</em>128M,然后通过-Xmn参数,将Eden区域大小设置为4<em>3</em>128*4/3</p>
<p>其实,根据经验来看,对于垃圾回收的调优,尽量就是调节Executor内存的比例就可以了,因为jvm的调优是非常负责和敏感的,除非是真的到了万不得已的地步,自己本身对jvm相关的技术很了解,那么此时进行Eden区域的调优是可以的</p>
<p>一些高级的参数:<br>-XX:SurvivorRatio=4 如果值为4,那么就是一个Survivor跟Eden的比例是1:4,也就是说每个Survivor占据的年轻代的比例是1/6,所以你其实也可以尝试调大Survivor区域的大小</p>
<p>-XX:NewRatio=4 调节新生代和老年代的比例</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化六之java虚拟机垃圾回收调优/" data-id="cj290scgw0174ssqqwxti83ar" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化八之广播共享数据" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化八之广播共享数据/" class="article-date">
  <time datetime="2017-04-16T04:47:25.269Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化八之广播共享数据/">spark性能优化八之广播共享数据</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/broadcast.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化八之广播共享数据/" data-id="cj290scgs0171ssqqpxg801o0" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark性能优化五之使用序列化的持久化级别" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化五之使用序列化的持久化级别/" class="article-date">
  <time datetime="2017-04-16T04:47:25.267Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化五之使用序列化的持久化级别/">spark性能优化五之使用序列化的持久化级别</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>除了对多次使用的RDD进行持久化操作之外,还可以进一步优化其性能,因为很有可能,RDD的数据是持久化到内存,或者磁盘中的,那么此时,如果内存大小不是特别充足,完全可以使用序列化的持久化级别,比如:MEMORY_ONLY_SER,MEMORY_AND_DISK_SER等,使用RDD.persist(StorageLevel.MEMORY_ONLY_SER)这样的语法即可</p>
<p>这样的话,将数据序列化之后,再持久化,可以大大减小对内存的消耗,此外,数据量小了之后,如果需要写入磁盘,那么磁盘IO性能消耗也比较小</p>
<p>对RDD持久话序列化后,RDD的每个partition的数据,都是序列化为一个巨大的字节数组,这样,对于内存的消耗就小的多了,但是唯一的缺点是:获取RDD数据时,需要对其进行反序列化,会增大性能开销</p>
<p>因为,对于序列化的持久化级别,还可以进一步优化,也就是说,使用Kryo序列化类库,这样可以获得更快的序列化速度,并且占用更小的内存空间,但是要记住,如果RDD的元素(RDD<t>的泛型类型)是自定义的话,那么在Kryo中提前注册自定义类型</t></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化五之使用序列化的持久化级别/" data-id="cj290scgp016yssqqnrva505w" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/40/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/39/">39</a><a class="page-number" href="/page/40/">40</a><span class="page-number current">41</span><a class="page-number" href="/page/42/">42</a><a class="page-number" href="/page/43/">43</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/42/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>