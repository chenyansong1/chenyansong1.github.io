<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/47/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL之RDD与DataFrame的转换" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之RDD与DataFrame的转换/" class="article-date">
  <time datetime="2017-04-16T04:47:25.185Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之RDD与DataFrame的转换/">SparkSQL之RDD与DataFrame的转换</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>为什么要将RDD转换为DataFrame,因为这样的话,我们就可以直接针对HDFS等任何可以构建为RDD的数据,使用spark sql进行sql查询了,这个功能是无比强大的,想象一下,针对HDFS中的数据,直接就可以使用sql进行查询</p>
<p>spark sql支持两种方式来将RDD转换为DataFrame<br>1.使用反射来推断包含了特定数据类型的RDD的元数据,这种基于反射的方式,代码比较简洁,当你已经知道你的RDD的元数据时,这是一种不错的方式<br>2.通过编程接口来创建DataFrame,你可以在程序运行时动态构建一份元数据,然后将其应用到已经存在的RDD上,这种方式的代码比较冗长,但是如果在编写程序时,还不知道RDD的元数据,只有在程序运行时,才能动态得知元数据,那么只能通过这种动态构建元数据的方式</p>
<p><strong>使用反射的方式推断元数据</strong><br>由于scala具有隐式转换的特点,所以spark sql的scala接口,是支持自动将包含了case class的RDD转换为DataFrame的,case class就定义了元数据,spark sql会通过反射读取传递给case class的参数的名称,然后将其作为列名,spark sql是支持将包含了嵌套数据结构的case class作为元数据的,比如包含了Array等</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">//要求Bean是可序列化的</div><div class="line">case class Student(id:Int, name:String, age:Int) //extends Serializable</div><div class="line"></div><div class="line">def RDD2DataFrameByReflection(): Unit =&#123;</div><div class="line">  val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">  val sc = new SparkContext(sparkConf)</div><div class="line">  val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">  // 在scala中使用反射方式,进行RDD到DataFrame的转换,需要手动导入一个隐式转换</div><div class="line">  import sqlContext.implicits._</div><div class="line"></div><div class="line">  // 对一行数据解析成Student对象返回</div><div class="line">  def parseStudent(str: String): Student =&#123;</div><div class="line">    val fields = str.split(&quot;,&quot;)</div><div class="line">    assert(fields.size == 3)</div><div class="line">    Student(fields(0).trim.toInt, fields(1).toString, fields(2).trim.toInt)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // 因为前面已经导入了隐式的转换,所以这里可以将rdd转成DF</div><div class="line">  val studentDF = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\student.txt&quot;)</div><div class="line">      .map(parseStudent)</div><div class="line">      .toDF()</div><div class="line"></div><div class="line">  // 将studentDF注册到成一张临时表</div><div class="line">  studentDF.registerTempTable(&quot;students&quot;)</div><div class="line"></div><div class="line">  // 操作这张临时表,返回的是一个DF</div><div class="line">  val teenagerDF = sqlContext.sql(&quot;select * from students where age &lt;= 18&quot;)</div><div class="line">  // 将DF转回rdd</div><div class="line">  val teenagerRdd = teenagerDF.rdd</div><div class="line">  //teenagerRdd.foreach(println)</div><div class="line">	/*因为打印的是数组,那么在row中取的时候,是去数组元素</div><div class="line">	  [1,leo,17]</div><div class="line">	  [2,marry,17]</div><div class="line">	  [3,jack,18]</div><div class="line">  	*/</div><div class="line"></div><div class="line">  teenagerRdd.foreach&#123;row=&gt;println(Student(row(0).toString.toInt, row(1).toString, row(2).toString.toInt))&#125;</div><div class="line"></div><div class="line">  /*</div><div class="line">  打印结果:</div><div class="line">  Student(1,leo,17)</div><div class="line">  Student(2,marry,17)</div><div class="line">  Student(3,jack,18)</div><div class="line">   */</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>上面的操作可以概括为下面的步骤:</p>
<p>1.将一个RDD通过反射的方式转换成为一个DataFrame(需要隐式转换);<br>2.将DataFrame注册成为一张表;<br>3.从表中查询数据,返回一个新的DataFrame;<br>4.将新的DataFrame转成rdd;<br>5.将RDD写会存储</p>
<p><strong>通过编程接口来创建DataFrame</strong></p>
<p>在编写程序时,还不知道RDD的元数据,只有在程序运行时,才能动态得知元数据,那么只能通过这种动态构建元数据的方式</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line"> val sc = new SparkContext(sparkConf)</div><div class="line"> val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line"> // 第一步:构造出元素为Row的普通RDD</div><div class="line"> def parseStudent(str: String): Row =&#123;</div><div class="line">   val fields = str.split(&quot;,&quot;)</div><div class="line">   assert(fields.size == 3)</div><div class="line">   Row(fields(0).trim.toInt, fields(1).toString, fields(2).trim.toInt)</div><div class="line"> &#125;</div><div class="line"></div><div class="line"> val studentRdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\student.txt&quot;)</div><div class="line">     .map(parseStudent)</div><div class="line"></div><div class="line"> // 第二步:编程方式动态的构造元数据</div><div class="line"> val structType = StructType(Array(</div><div class="line">   StructField(&quot;id&quot;, IntegerType, true),</div><div class="line">   StructField(&quot;name&quot;, StringType, true),</div><div class="line">   StructField(&quot;age&quot;, IntegerType, true)</div><div class="line"> ))</div><div class="line"></div><div class="line"> // 第三步:进行RDD到DataFrame的转换</div><div class="line"> val studentDF = sqlContext.createDataFrame(studentRdd, structType)</div><div class="line"></div><div class="line"> // 使用DF</div><div class="line"> studentDF.registerTempTable(&quot;students&quot;)</div><div class="line"></div><div class="line"> //使用这张表</div><div class="line"> val teenagerDF = sqlContext.sql(&quot;select * from students where age &lt;=18&quot;)</div><div class="line"></div><div class="line"> // 将DF转回RDD</div><div class="line"> teenagerDF.rdd.foreach(println)</div><div class="line"></div><div class="line"> /*</div><div class="line"> 打印结果:</div><div class="line"> [1,leo,17]</div><div class="line"> [2,marry,17]</div><div class="line"> [3,jack,18]</div><div class="line">  */</div></pre></td></tr></table></figure>
<p>以上的操作主要是围绕着下面的方法进行的<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">createDataFrame(RDD[Row], StructType) :DataFrame = &#123; /* compiled code */ &#125;</div></pre></td></tr></table></figure></p>
<p>1.首先将构造普通的RDD[Row]<br>2.构造StructType,定义元数据<br>3.调用createDataFrame方法返回DataFrame<br>4.将返回的DataFrame注册成为一张表<br>5.对表进行操作,返回新的DataFrame<br>6.将新的DataFrame转回RDD</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之RDD与DataFrame的转换/" data-id="cj290sc6h00yissqqwfywo2x4" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL之Hive On Spark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之Hive On Spark/" class="article-date">
  <time datetime="2017-04-16T04:47:25.183Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之Hive On Spark/">SparkSQL之Hive On Spark</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>hive是目前大数据领域,事实上的sql标准,底层默认是基于MapReduce实现的,但是由于MapReduce速度实在比较慢,因此,陆续出现了新的sql查询引擎,包括spark sql,hive on Taz,hive on spark等</p>
<p>spark sql与hive on spark是不一样的,spark sql是spark自己研发出来的针对各种数据源,包括hive,json,Parquet,jdbc,rdd等都可以执行查询的,一套基于spark计算引擎的查询引擎,因此它是spark的一个项目,只不过提供了针对hive执行查询的功能而已,适合在一些使用spark技术栈的大数据应用类系统中使用</p>
<p>而hive on spark,是hive的一个项目,它是指,不通过MapReduce作为唯一的查询引擎,而是将spark作为底层的查询引擎,hive on spark,只适用于hive,在可预见的未来,很有可能hive的默认的底层引擎就从MapReduce切换为spark了,适合于原有的hive数据仓库以及数据统计分析替换为spark引擎</p>
<p>hive on spark 环境搭建</p>
<p>1.安装hive<br>参见:”hive安装”一文</p>
<p>2.使用</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/hive_on_spark_1.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之Hive On Spark/" data-id="cj290sc6e00yfssqqzx8bh0wv" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark1.4.x新特性和spark1.5.x的新特性" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark1.4.x新特性和spark1.5.x的新特性/" class="article-date">
  <time datetime="2017-04-16T04:47:25.182Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark1.4.x新特性和spark1.5.x的新特性/">spark1.4.x新特性和spark1.5.x的新特性</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="spark1-4-x新特性"><a href="#spark1-4-x新特性" class="headerlink" title="spark1.4.x新特性"></a>spark1.4.x新特性</h1><p>1.spark core</p>
<p>1.1.提供REST API供外界开发者获取spark内部的各种信息(jobs/stages/tasks/storage info),基于这些API,可以搭建自己的spark监控系统<br>1.2.shuffle阶段,默认将map端写入磁盘的数据进行序列化,优化IO性能<br>1.3.钨丝计划(Project Tungsten),提供了UNSafeShuffleManager,使用缓存友好的排序算法,降低了shuffle的内存使用,提高了排序性能</p>
<p>2.spark streaming<br>2.1.提供了新的spark streaming的UI,能够更好,更清晰的监控spark streaming应用程序的运行状况<br>2.2.支持kafka0.8.2版本</p>
<p>3.spark sql and DataFrame<br>3.1.支持ORCFile<br>3.2.提供了一些window function(窗口函数)<br>3.3.优化了join的性能</p>
<h1 id="spark1-5-x新特性"><a href="#spark1-5-x新特性" class="headerlink" title="spark1.5.x新特性"></a>spark1.5.x新特性</h1><p>1.DataFrame底层执行的性能优化(钨丝计划的第一阶段)<br>1.1.spark自己来管理内存,而不再依靠JVM管理内存,这样就可以避免JVM GC的性能开销,并且能够控制OOM的问题<br>1.2.java对象直接使用内部的二进制格式化存储和计算,省去了序列化和反序列的性能开销,而且更加节省内存开销<br>1.3.完善了shuffle阶段的UnsafeShuffleManager,增加了不少新功能,优化shuffle性能<br>1.4.默认使用code-gen,使用cache-aware算法,加强了join,aggregation,shuffle,sorting的性能,增强了window function的性能,性能比1.4.x版本提高了数倍</p>
<p>2.DataFrame<br>2.1.实现了新的聚合函数接口,AggregateFunction2,并且提供了7个新的内置聚合函数<br>2.2.实现了100多个新的expression function,例如unix_timestamp等,增强了对NaN的处理<br>2.3.支持连接不通版本的hive metastore<br>2.4.支持Parquet1.7</p>
<p>3.Spark Streaming,更完善的Python支持,非试验的Kafka Direct API等等</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark1.4.x新特性和spark1.5.x的新特性/" data-id="cj290scc0013dssqqu8rriimj" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/" class="article-date">
  <time datetime="2017-04-16T04:47:25.180Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/">spark core之新闻网站关键指标离线统计</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h1><p>新闻网站<br>1、版块<br>2、新闻页面<br>3、新用户注册<br>4、用户跳出</p>
<p>案例需求分析</p>
<p>每天每个页面的PV：PV是Page View，是指一个页面被所有用户访问次数的总和，页面被访问一次就被记录1次PV<br>每天每个页面的UV：UV是User View，是指一个页面被多少个用户访问了，一个用户访问一次是1次UV，一个用户访问多次还是1次UV<br>新用户注册比率：当天注册用户数 / 当天未注册用户数<br>用户跳出率：IP只浏览了一个页面就离开网站的次数/网站总访问数（PV）<br>版块热度排行榜：根据每个版块每天被访问的次数，做出一个排行榜</p>
<h1 id="网站日志格式"><a href="#网站日志格式" class="headerlink" title="网站日志格式"></a>网站日志格式</h1><p>date timestamp userid pageid section action </p>
<p>日志字段说明<br>date: 日期，yyyy-MM-dd格式<br>timestamp: 时间戳<br>userid: 用户id<br>pageid: 页面id<br>section: 新闻版块<br>action: 用户行为，两类，点击页面和注册</p>
<p>模拟数据生成程序<br>模式数据演示</p>
<h1 id="创建hive表和数据导入"><a href="#创建hive表和数据导入" class="headerlink" title="创建hive表和数据导入"></a>创建hive表和数据导入</h1><p>在hive中创建访问日志表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">create table news_access (</div><div class="line">  date string,</div><div class="line">  timestamp bigint,</div><div class="line">  userid bigint,</div><div class="line">  pageid bigint,</div><div class="line">  section string,</div><div class="line">  action string)</div></pre></td></tr></table></figure></p>
<p>将模拟数据导入hive表中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">load data local inpath &apos;/usr/local/test/news_access.log&apos; into table news_access;</div></pre></td></tr></table></figure></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div></pre></td><td class="code"><pre><div class="line">package cn.spark.study.sql.upgrade.news;</div><div class="line"></div><div class="line">import java.math.BigDecimal;</div><div class="line">import java.text.SimpleDateFormat;</div><div class="line">import java.util.Calendar;</div><div class="line">import java.util.Date;</div><div class="line"></div><div class="line">import org.apache.spark.SparkConf;</div><div class="line">import org.apache.spark.api.java.JavaSparkContext;</div><div class="line">import org.apache.spark.sql.DataFrame;</div><div class="line">import org.apache.spark.sql.hive.HiveContext;</div><div class="line"></div><div class="line">/**</div><div class="line"> * 新闻网站关键指标离线统计Spark作业</div><div class="line"> * @author Administrator</div><div class="line"> *</div><div class="line"> */</div><div class="line">public class NewsOfflineStatSpark &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) &#123;</div><div class="line">		// 一般来说，在小公司中，可能就是将我们的spark作业使用linux的crontab进行调度</div><div class="line">			// 将作业jar放在一台安装了spark客户端的机器上，并编写了对应的spark-submit shell脚本</div><div class="line">			// 在crontab中可以配置，比如说每天凌晨3点执行一次spark-submit shell脚本，提交一次spark作业</div><div class="line">			// 一般来说，离线的spark作业，每次运行，都是去计算昨天的数据</div><div class="line">		// 大公司总，可能是使用较为复杂的开源大数据作业调度平台，比如常用的有azkaban、oozie等</div><div class="line">			// 但是，最大的那几个互联网公司，比如说BAT、美团、京东，作业调度平台，都是自己开发的</div><div class="line">			// 我们就会将开发好的Spark作业，以及对应的spark-submit shell脚本，配置在调度平台上，几点运行</div><div class="line">			// 同理，每次运行，都是计算昨天的数据</div><div class="line">		</div><div class="line">		// 一般来说，每次spark作业计算出来的结果，实际上，大部分情况下，都会写入mysql等存储</div><div class="line">		// 这样的话，我们可以基于mysql，用java web技术开发一套系统平台，来使用图表的方式展示每次spark计算</div><div class="line">		// 出来的关键指标</div><div class="line">		// 比如用折线图，可以反映最近一周的每天的用户跳出率的变化</div><div class="line">		</div><div class="line">		// 也可以通过页面，给用户提供一个查询表单，可以查询指定的页面的最近一周的pv变化</div><div class="line">		// date pageid pv</div><div class="line">		// 插入mysql中，后面用户就可以查询指定日期段内的某个page对应的所有pv，然后用折线图来反映变化曲线</div><div class="line">		</div><div class="line">		// 拿到昨天的日期，去hive表中，针对昨天的数据执行SQL语句</div><div class="line">		String yesterday = getYesterday();</div><div class="line">		</div><div class="line">		// 创建SparkConf以及Spark上下文</div><div class="line">		SparkConf conf = new SparkConf()</div><div class="line">				.setAppName(&quot;NewsOfflineStatSpark&quot;)    </div><div class="line">				.setMaster(&quot;local&quot;);  </div><div class="line">		JavaSparkContext sc = new JavaSparkContext(conf);</div><div class="line">		HiveContext hiveContext = new HiveContext(sc.sc());</div><div class="line">	</div><div class="line">		// 开发第一个关键指标：页面pv统计以及排序</div><div class="line">		calculateDailyPagePv(hiveContext, yesterday);  </div><div class="line">		// 开发第二个关键指标：页面uv统计以及排序</div><div class="line">		calculateDailyPageUv(hiveContext, yesterday);</div><div class="line">		// 开发第三个关键指标：新用户注册比率统计</div><div class="line">		calculateDailyNewUserRegisterRate(hiveContext, yesterday);</div><div class="line">		// 开发第四个关键指标：用户跳出率统计</div><div class="line">		calculateDailyUserJumpRate(hiveContext, yesterday);</div><div class="line">		// 开发第五个关键指标：版块热度排行榜</div><div class="line">		calculateDailySectionPvSort(hiveContext, yesterday);</div><div class="line">		</div><div class="line">		// 关闭Spark上下文</div><div class="line">		sc.close();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	/**</div><div class="line">	 * 获取昨天的字符串类型的日期</div><div class="line">	 * @return 日期</div><div class="line">	 */</div><div class="line">	private static String getYesterday() &#123;</div><div class="line">		Calendar cal = Calendar.getInstance();</div><div class="line">		cal.setTime(new Date());</div><div class="line">		cal.add(Calendar.DAY_OF_YEAR, -1);  </div><div class="line">		</div><div class="line">		Date yesterday = cal.getTime();</div><div class="line">		</div><div class="line">		SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</div><div class="line">		return sdf.format(yesterday);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天每个页面的pv以及排序</div><div class="line">	 *   排序的好处：排序后，插入mysql，java web系统要查询每天pv top10的页面，直接查询mysql表limit 10就可以</div><div class="line">	 *   如果我们这里不排序，那么java web系统就要做排序，反而会影响java web系统的性能，以及用户响应时间</div><div class="line">	 */</div><div class="line">	private static void calculateDailyPagePv(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;pageid,&quot;</div><div class="line">					+ &quot;pv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;pageid,&quot;</div><div class="line">						+ &quot;count(*) pv &quot;</div><div class="line">					+ &quot;FROM news_access &quot;</div><div class="line">					+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">					+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot; </div><div class="line">					+ &quot;GROUP BY date,pageid &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY pv DESC &quot;;  </div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">	</div><div class="line">		// 在这里，我们也可以转换成一个RDD，然后对RDD执行一个foreach算子</div><div class="line">		// 在foreach算子中，将数据写入mysql中</div><div class="line">		</div><div class="line">		df.show();  </div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天每个页面的uv以及排序</div><div class="line">	 *   Spark SQL的count(distinct)语句，有bug，默认会产生严重的数据倾斜</div><div class="line">	 *   只会用一个task，来做去重和汇总计数，性能很差</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyPageUv(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;pageid,&quot;</div><div class="line">					+ &quot;uv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;pageid,&quot;</div><div class="line">						+ &quot;count(*) uv &quot;</div><div class="line">					+ &quot;FROM ( &quot;</div><div class="line">						+ &quot;SELECT &quot;</div><div class="line">							+ &quot;date,&quot;</div><div class="line">							+ &quot;pageid,&quot;</div><div class="line">							+ &quot;userid &quot;</div><div class="line">						+ &quot;FROM news_access &quot;</div><div class="line">						+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">						+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot;</div><div class="line">						+ &quot;GROUP BY date,pageid,userid &quot;</div><div class="line">					+ &quot;) t2 &quot;</div><div class="line">					+ &quot;GROUP BY date,pageid &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY uv DESC &quot;;</div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">		</div><div class="line">		df.show();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的新用户注册比例</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyNewUserRegisterRate(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		// 昨天所有访问行为中，userid为null，新用户的访问总数</div><div class="line">		String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NULL&quot;;</div><div class="line">		// 昨天的总注册用户数</div><div class="line">		String sql2 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;register&apos; AND date=&apos;&quot; + date + &quot;&apos; &quot;;</div><div class="line">	</div><div class="line">		// 执行两条SQL，获取结果</div><div class="line">		Object result1 = hiveContext.sql(sql1).collect()[0].get(0);</div><div class="line">		long number1 = 0L;</div><div class="line">		if(result1 != null) &#123;</div><div class="line">			number1 = Long.valueOf(String.valueOf(result1));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		Object result2 = hiveContext.sql(sql2).collect()[0].get(0);</div><div class="line">		long number2 = 0L;</div><div class="line">		if(result2 != null) &#123;</div><div class="line">			number2 = Long.valueOf(String.valueOf(result2));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 计算结果</div><div class="line">		System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;);  </div><div class="line">		System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;);  </div><div class="line">		double rate = (double)number2 / (double)number1;</div><div class="line">		System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;);  </div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的用户跳出率</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyUserJumpRate(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		// 计算已注册用户的昨天的总的访问pv</div><div class="line">		String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL &quot;;</div><div class="line">		</div><div class="line">		// 已注册用户的昨天跳出的总数</div><div class="line">		String sql2 = &quot;SELECT count(*) FROM ( SELECT count(*) cnt FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL GROUP BY userid HAVING cnt=1 ) t &quot;;</div><div class="line">		</div><div class="line">		// 执行两条SQL，获取结果</div><div class="line">		Object result1 = hiveContext.sql(sql1).collect()[0].get(0);</div><div class="line">		long number1 = 0L;</div><div class="line">		if(result1 != null) &#123;</div><div class="line">			number1 = Long.valueOf(String.valueOf(result1));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		Object result2 = hiveContext.sql(sql2).collect()[0].get(0);</div><div class="line">		long number2 = 0L;</div><div class="line">		if(result2 != null) &#123;</div><div class="line">			number2 = Long.valueOf(String.valueOf(result2));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 计算结果</div><div class="line">		System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;);  </div><div class="line">		System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;);  </div><div class="line">		double rate = (double)number2 / (double)number1;</div><div class="line">		System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的版块热度排行榜</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailySectionPvSort(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;section,&quot;</div><div class="line">					+ &quot;pv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;section,&quot;</div><div class="line">						+ &quot;count(*) pv &quot;</div><div class="line">					+ &quot;FROM news_access &quot;</div><div class="line">					+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">					+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot;</div><div class="line">					+ &quot;GROUP BY date,section &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY pv DESC &quot;;</div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">		  </div><div class="line">		df.show();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 格式化小数</div><div class="line">	 * @param str 字符串</div><div class="line">	 * @param scale 四舍五入的位数</div><div class="line">	 * @return 格式化小数</div><div class="line">	 */</div><div class="line">	private static double formatDouble(double num, int scale) &#123;</div><div class="line">		BigDecimal bd = new BigDecimal(num);  </div><div class="line">		return bd.setScale(scale, BigDecimal.ROUND_HALF_UP).doubleValue();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/" data-id="cj290scbx013assqqb9lx12ie" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/" class="article-date">
  <time datetime="2017-04-16T04:47:25.179Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/">spark core之单独启动master和worker脚本</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="单独启动master和worker脚本"><a href="#单独启动master和worker脚本" class="headerlink" title="单独启动master和worker脚本"></a>单独启动master和worker脚本</h1><p>sbin/start-all.sh脚本可以直接启动集群中的master进程和worker进程</p>
<p>这里讲的是单独启动master和worker进程</p>
<p>因为worker进程启动之后,会向master进程去注册,所以需要先启动master进程</p>
<p>为什么有的时候要单独启动master和worker进程?<br>因为可以通过命令行参数为进程配置一些独特的参数,如监听的端口号,web ui 的端口号,使用的cpu和内存等,比如:你可能向单独给某个节点配置不同的cpu和内存资源的使用限制,那么就可以使用脚本单独启动worker进程的时候,通过命令行参数来设置</p>
<p>手动启动master进程<br>需要在某个部署了spark安装包的节点上,使用sbin/start-master.sh启动,master启动之后,启动日志就会打印一行spark://HOST:PORT出来,这就是master的url地址,worker进程就会通过这个地址来连接到master进程,并且进行注册</p>
<p>另外，除了worker进程要使用这个URL以外，我们自己在编写spark代码时，也可以给SparkContext的setMaster()方法，传入这个URL地址<br>然后我们的spark作业，就会使用standalone模式连接master，并提交作业</p>
<p>此外，还可以通过<a href="http://MASTER_HOST:8080" target="_blank" rel="external">http://MASTER_HOST:8080</a> URL来访问master集群的监控web ui，那个web ui上，也会显示master的URL地址</p>
<p>手动启动worker进程<br>在部署了spark安装包的前提下,在你希望作为worker node的节点上,使用sbin/start-slave.sh <master-spark-url>在当前节点上启动,启动worker的时候需要指定master的url</master-spark-url></p>
<p>启动worker进程之后,再访问:http:MASTER_HOST:8080,在集群web ui上,就可以看到新启动的节点,包括该节点的cpu和内存资源</p>
<p>此外,以下参数是可以在手动启动master和worker的时候指定的:<br>-h host, –host host     在哪台机器上启动,默认都是本机<br>-p port, –port port 在机器上启动后,使用哪个端口对外提供服务,master默认是7077,worker默认是随机的<br>–webui-port port  web ui端口,master默认的是8080,worker默认的是8081<br>-c cores, –cores cores 仅限于worker,总共能让spark Application使用多少个cpu core,默认是当前机器上的所有的cpu core<br>-m mem, –memory mem  仅限于worker,总共能让spark Application使用多少内存,是100M或者1G这样的格式<br>-d dir, –worker-dir dir    仅限于worker,工作目录,默认是spark home/work目录<br>–properties-file file master和worker加载配置文件的地址,默认是spark_home/conf/spark-defaults.conf</p>
<p>咱们举个例子，比如说小公司里面，物理集群可能就一套，同一台机器上面，可能要部署Storm的supervisor进程，可能还要同时部署Spark的worker进程机器，cpu和内存，既要供storm使用，还要供spark使用<br>这个时候，可能你就需要限制一下worker节点能够使用的cpu和内存的数量</p>
<p>小公司里面，搭建spark集群的机器可能还不太一样，有的机器比如说是有5个g内存，有的机器才1个g内存那你对于1个g内存的机器，是不是得限制一下内存使用量，比如说500m</p>
<p>实例:<br>1、启动master: 日志和web ui，观察master url</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/单独启动master.png" alt=""></p>
<p>2、启动worker: 观察web ui，是否有新加入的worker节点，以及对应的信息</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/单独启动worker.png" alt=""></p>
<p>3、单独关闭master和worker,此时的顺序得反过来,先关闭worker,再去关闭master</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/stop_worker_master.png" alt=""></p>
<p>4、再次单独启动master和worker，给worker限定，就使用500m内存，跟之前看到的worker信息比对一下内存最大使用量</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/worker_allocate_memory.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/" data-id="cj290scbt0137ssqquu5kyf0h" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之主要的几个术语" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/" class="article-date">
  <time datetime="2017-04-16T04:47:25.177Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/">spark core之主要的几个术语</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <table>
<thead>
<tr>
<th style="text-align:center">术语</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Application</td>
<td style="text-align:left">spark应用程序,就是用户基于spark API开发的程序,一定是通过一个有main方法的类执行的</td>
</tr>
<tr>
<td style="text-align:center">Application jar</td>
<td style="text-align:left">这个就是把写好的spark工程，打包成一个jar包，其中包括了所有的第三方jar依赖包，比如java中，就用maven+assembly插件打包最方便</td>
</tr>
<tr>
<td style="text-align:center">Driver</td>
<td style="text-align:left">在使用spark-submit提交应用的时候,会指定一个主类,这个类有一个main方法,driver进程就是在运行程序中的main方法的进程,这就是driver</td>
</tr>
<tr>
<td style="text-align:center">cluster manager</td>
<td style="text-align:left">集群管理器,就是为每个spark application，在集群中调度和分配资源的组件，比如Spark Standalone、YARN、Mesos等</td>
</tr>
<tr>
<td style="text-align:center">deploy mode</td>
<td style="text-align:left">部署模式，无论是基于哪种集群管理器(Standalone、YARN、Mesos)，spark作业部署或者运行模式，都分为两种，client和cluster，client模式下driver运行在提交spark作业的机器上,即执行应用的main类(主要用于测试)；cluster模式下，driver运行在spark集群中的某一个节点上</td>
</tr>
<tr>
<td style="text-align:center">Worker Node</td>
<td style="text-align:left">集群中的工作节点，能够运行executor进程，运行作业代码的节点</td>
</tr>
<tr>
<td style="text-align:center">Executor</td>
<td style="text-align:left">集群管理器为application分配的进程，运行在worker节点上，负责执行作业的任务，并将数据保存在内存或磁盘中，每个application都有自己的executor</td>
</tr>
<tr>
<td style="text-align:center">Job</td>
<td style="text-align:left">每个spark application，根据你执行了多少次action操作，就会有多少个job</td>
</tr>
<tr>
<td style="text-align:center">Stage</td>
<td style="text-align:left">根据是否有shuffle,每个job都会划分为多个stage（阶段），每个stage都会有对应的一批task，分配到executor上去执行</td>
</tr>
<tr>
<td style="text-align:center">Task</td>
<td style="text-align:left">driver发送到executor上执行的计算单元，每个task负责在一个阶段（stage），处理一小片数据，计算出对应的结果</td>
</tr>
</tbody>
</table>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/" data-id="cj290scbq0134ssqqlmwx7mx1" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之yarn模式" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/" class="article-date">
  <time datetime="2017-04-16T04:47:25.175Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/">spark core之yarn模式</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="yarn-client模式原理"><a href="#yarn-client模式原理" class="headerlink" title="yarn-client模式原理"></a>yarn-client模式原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/yarn-client模式原理.png" alt=""></p>
<h1 id="yarn-cluster模式原理"><a href="#yarn-cluster模式原理" class="headerlink" title="yarn-cluster模式原理"></a>yarn-cluster模式原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/yarn-cluster模式原理.png" alt=""></p>
<h1 id="yarn-client模式提交spark作业"><a href="#yarn-client模式提交spark作业" class="headerlink" title="yarn-client模式提交spark作业"></a>yarn-client模式提交spark作业</h1><h2 id="yarn运行spark作业的前提"><a href="#yarn运行spark作业的前提" class="headerlink" title="yarn运行spark作业的前提"></a>yarn运行spark作业的前提</h2><p>如果想要让spark作业可以运行在yarn上面,那么首先就必须在spark-env.sh文件中,配置HADOOP_CONF_DIR或者YARN_CONF_DIR属性,值为hadoop的配置文件的目录,即:HADOOP_HOME/etc/hadoop,其中包含了hadoop和yarn所有的配置文件,比如:hdfs-site.xml,yarn-site.xml等,spark需要这些配置来读写HDFS,以及连接到yarn ResourceManager上,这个目录中包含的配置文件都会被分发到yarn集群中去的</p>
<p>vim spark/conf/spark-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</div><div class="line"></div><div class="line">/*在/usr/local/hadoop/etc/hadoop目录下有:</div><div class="line">yarn-site.xml(其中可以找到ResourceManager所在的机器)</div><div class="line">还有一些其他的配置文件</div><div class="line">*/</div></pre></td></tr></table></figure></p>
<p>跟spark standalone模式不同,通常不需要使用–master指定master URL<br>因为spark会从hadoop的配置文件中去读ResourceManager的配置,这样就知道了ResourceManager所在的机器(master),所以不需要我们指定,但是我们需要指定deploy mode,如下示例:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">/export/servers/spark/bin/spark-submit \</div><div class="line">--class cn.spark.study.core.WordCount \</div><div class="line">--master yarn-cluster</div><div class="line">#--master yarn-client</div><div class="line">--num-executors 1 \</div><div class="line">--driver-memory 100m \</div><div class="line">--executor-memory 100m \</div><div class="line">--executor-cores 1 \</div><div class="line">--queue hadoop队列</div><div class="line">/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</div></pre></td></tr></table></figure></p>
<p>–queue,在不同的部门,或者是不同的大数据项目,共用一个yarn集群,运行spark作业,推荐一定要用–queue,指定不同的hadoop队列,做项目或者部门之间的队列隔离</p>
<p>与Standalone模式类似,yarn-client模式通常建议在测试的时候使用,方便你直接在提交作业的机器上查看日志,但是作业实际部署到生产环境中进行运行的时候,还是使用yarn-cluster模式</p>
<p>yarn模式下需要观察的点:<br>1.日志<br>命令行日志<br>web ui日志</p>
<p>2.web ui的地址不再是spark://192.168.0.108:8080这种URL了,因为那是Standalone模式下的监控web ui,在yarn模式下,要看yarn的web ui: <a href="http://192.168.0.108:8088/" target="_blank" rel="external">http://192.168.0.108:8088/</a> 这是yarn的URL地址</p>
<p>3.进程<br>driver是什么进程</p>
<p>AppLicationMaster是什么进程</p>
<p>executor进程</p>
<h1 id="yarn模式下的日志查看"><a href="#yarn模式下的日志查看" class="headerlink" title="yarn模式下的日志查看"></a>yarn模式下的日志查看</h1><p>在yarn模式下,spark作业运行相关的executor和ApplicationMaster都是运行在yarn的container中的,一个作业运行完了以后,yarn有两种方式来处理spark作业打印出的日志</p>
<p>1.聚合日志方式(推荐,比较常用)<br>这种格式将散落在集群中各个机器上的日志,最后都给聚合起来,让我们可以统一查看,如果yarn的日志聚合的选项打开了,即:yarn.log-aggregation-enable(yarn-site.xml文件中配置), container的日志会拷贝到HDFS上去,并从机器中删除</p>
<p>然后我们使用yarn logs -applicationId <app id=""> 命令来查看日志(app Id在yarn的web ui上看:resourceManager_host:8088)</app></p>
<p>yarn logs命令,会打印出application对应的所有container的日志出来,当然,因为日志是在HDFS上的,我们自然可以通过HDFS的命令行来直接从HDFS中查看日志,日志在HDFS中的目录,可以通过查看yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix属性来获知</p>
<p>2.web ui<br>日志也可以通过spark web ui来查看executor的输出日志<br>但是此时需要启动History Server,需要让spark history server和mapreduce history server运行着;并且在yarn-site.xml文件中,配置yarn.log.server.url属性<br>spark history server web ui中的log url,会将你重新定向到mapreduce history server上去查看日志</p>
<p>3.分散查看(通常不推荐)<br>如果没有打开聚合日志选项,那么日志默认就是散落在各个机器上的本次磁盘目录中的,在YARN_APP_LOGS_DIR目录下,根据hadoop版本的不同,通常在/tmp/logs目录下,或者在$HADOOP_HOME/logs/userlogs目录下,如果你要查看某个container的日志,那么就得登录到那台机器上去,然后到指定的目录下如,找到那个日志文件,然后才能查看</p>
<h1 id="yarn模式相关的参数"><a href="#yarn模式相关的参数" class="headerlink" title="yarn模式相关的参数"></a>yarn模式相关的参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">yarn模式运行spark作业所有属性详解</div><div class="line"></div><div class="line">属性名称											默认值							含义</div><div class="line">spark.yarn.am.memory								512m							client模式下，YARN Application Master使用的内存总量</div><div class="line">spark.yarn.am.cores									1								client模式下，Application Master使用的cpu数量</div><div class="line">spark.driver.cores									1								cluster模式下，driver使用的cpu core数量，driver与Application Master运行在一个进程中，所以也控制了Application Master的cpu数量</div><div class="line">spark.yarn.am.waitTime								100s							cluster模式下，Application Master要等待SparkContext初始化的时长; client模式下，application master等待driver来连接它的时长</div><div class="line">spark.yarn.submit.file.replication					hdfs副本数						作业写到hdfs上的文件的副本数量，比如工程jar，依赖jar，配置文件等，最小一定是1</div><div class="line">spark.yarn.preserve.staging.files					false							如果设置为true，那么在作业运行完之后，会避免工程jar等文件被删除掉</div><div class="line">spark.yarn.scheduler.heartbeat.interval-ms			3000							application master向resourcemanager发送心跳的间隔，单位ms</div><div class="line">spark.yarn.scheduler.initial-allocation.interval	200ms							application master在有pending住的container分配需求时，立即向resourcemanager发送心跳的间隔</div><div class="line">spark.yarn.max.executor.failures					executor数量*2，最小3			整个作业判定为失败之前，executor最大的失败次数</div><div class="line">spark.yarn.historyServer.address					无								spark history server的地址</div><div class="line">spark.yarn.dist.archives							无								每个executor都要获取并放入工作目录的archive</div><div class="line">spark.yarn.dist.files								无								每个executor都要放入的工作目录的文件</div><div class="line">spark.executor.instances							2								默认的executor数量</div><div class="line">spark.yarn.executor.memoryOverhead					executor内存10%					每个executor的堆外内存大小，用来存放诸如常量字符串等东西</div><div class="line">spark.yarn.driver.memoryOverhead					driver内存7%					同上</div><div class="line">spark.yarn.am.memoryOverhead						AM内存7%						同上</div><div class="line">spark.yarn.am.port									随机							application master端口</div><div class="line">spark.yarn.jar										无								spark jar文件的位置</div><div class="line">spark.yarn.access.namenodes							无								spark作业能访问的hdfs namenode地址</div><div class="line">spark.yarn.containerLauncherMaxThreads				25								application master能用来启动executor container的最大线程数量</div><div class="line">spark.yarn.am.extraJavaOptions						无								application master的jvm参数</div><div class="line">spark.yarn.am.extraLibraryPath						无								application master的额外库路径</div><div class="line">spark.yarn.maxAppAttempts															提交spark作业最大的尝试次数</div><div class="line">spark.yarn.submit.waitAppCompletion					true							cluster模式下，client是否等到作业运行完再退出</div></pre></td></tr></table></figure>
<p>以上这些参数可以在spark-submit中配置,使用–conf配置</p>
<h1 id="spark-submit详解"><a href="#spark-submit详解" class="headerlink" title="spark-submit详解"></a>spark-submit详解</h1><p>spark-submit可以通过一个统一的接口,将spark应用程序提交到所有spark支持的集群管理器上(Standalone(mater),Yarn(ResourceManager)等),所以我们并不需要为每种集群管理器都做特殊的配置</p>
<p>–master<br>1.如果不设置,那么就是local模式<br>2.如果设置spark://开头的URL,那么就是Standalone模式,会提交到指定的URL的Mater进程上去<br>3.如果设置yarn-client/yarn-cluster,那么就是yarn模式,会读取hadoop配置文件,然后连接ResourceManager</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/" data-id="cj290scbm0131ssqqok9n643p" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/" class="article-date">
  <time datetime="2017-04-16T04:47:25.174Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/">spark core之worker节点配置以及spark-env.sh参数详解</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="worker节点配置"><a href="#worker节点配置" class="headerlink" title="worker节点配置"></a>worker节点配置</h1><p>场景:如果在已有的spark集群中,你想要加入一台新的worker节点</p>
<p>如果你想将某台机器部署成Standalone集群架构中的worker节点,那么就必须在该机器上部署spark安装包,并修改配置文件如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#修改配置文件</div><div class="line">cd spark/conf/</div><div class="line">mv spark-env.sh.template spark-env.sh</div><div class="line">vim spark-env.sh  //添加</div><div class="line">export JAVA_HOME=/home/hadoop/app/jdk1.7.0_80</div><div class="line">export SPARK_MASTER_IP=hdp-node-01            //配置master的机器</div><div class="line">export SPARK_MASTER_PORT=7077</div><div class="line">#######################################################</div><div class="line">mv slaves.template slaves  </div><div class="line">vim slaves      //添加worker的节点</div><div class="line">hdp-node-01</div><div class="line">hdp-node-02</div><div class="line">#######################################################</div><div class="line">// 注意要配置多个机器之间的ssh免密码登录</div></pre></td></tr></table></figure></p>
<h1 id="spark-env-sh参数详解"><a href="#spark-env-sh参数详解" class="headerlink" title="spark-env.sh参数详解"></a>spark-env.sh参数详解</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SPARK_MASTER_IP					#指定master进程所在的机器的ip地址</div><div class="line">SPARK_MASTER_PORT				#指定master监听的端口号（默认是7077）</div><div class="line">SPARK_MASTER_WEBUI_PORT			#指定master web ui的端口号（默认是8080）</div><div class="line"></div><div class="line">#其实使用spark-env.sh配置的参数和我们手动启动master时指定的参数是一样的,sbin/start-master.sh --port 7078，类似这种方式，貌似可以指定一样的配置属性</div><div class="line">我明确告诉大家，这个作用的确是一模一样的</div><div class="line"></div><div class="line">#你可以在spark-evn.sh中就去配置好,但是有时呢，可能你会遇到需要临时更改配置，并启动master或worker进程的情况</div><div class="line">#此时就比较适合，用sbin/start-master.sh这种脚本的命令行参数，来设置这种配置属性</div><div class="line">#但是通常来说呢，还是推荐在部署的时候，通过spark-env.sh来设定</div><div class="line">脚本命令行参数通常用于临时的情况</div><div class="line"></div><div class="line">SPARK_MASTER_OPTS				#设置master的额外参数，使用&quot;-Dx=y&quot;设置各个参数(x对应的是参数名,y对应的是参数的值)</div><div class="line">比如说export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=1&quot;</div><div class="line"></div><div class="line">参数名											默认值						含义</div><div class="line">spark.deploy.retainedApplications				200							在spark web ui上最多显示多少个application的信息</div><div class="line">spark.deploy.retainedDrivers					200							在spark web ui上最多显示多少个driver的信息</div><div class="line">spark.deploy.spreadOut							true						资源调度策略，spreadOut会尽量将application的executor进程分布在更多worker上，适合基于hdfs文件计算的情况，提升数据本地化概率；非spreadOut会尽量将executor分配到一个worker上，适合计算密集型的作业</div><div class="line">spark.deploy.defaultCores						无限大						每个spark作业最多在standalone集群中使用多少个cpu core，默认是无限大，有多少用多少</div><div class="line">spark.deploy.timeout							60							单位秒，一个worker多少时间没有响应之后，master认为worker挂掉了</div><div class="line"></div><div class="line">------------------------------------------------------------</div><div class="line"></div><div class="line"></div><div class="line">SPARK_LOCAL_DIRS				spark的工作目录，包括了shuffle map输出文件，以及持久化到磁盘的RDD等</div><div class="line"></div><div class="line">SPARK_WORKER_PORT				worker节点的端口号，默认是随机的</div><div class="line">SPARK_WORKER_WEBUI_PORT			worker节点的web ui端口号，默认是8081</div><div class="line">SPARK_WORKER_CORES				worker节点上，允许spark作业使用的最大cpu数量，默认是机器上所有的cpu core</div><div class="line">SPARK_WORKER_MEMORY				worker节点上，允许spark作业使用的最大内存量，格式为1000m，2g等，默认最小是1g内存</div><div class="line"></div><div class="line">就是说，有些master和worker的配置，可以在spark-env.sh中部署时即配置，但是也可以在start-slave.sh脚本启动进程时命令行参数设置</div><div class="line">但是命令行参数的优先级比较高，会覆盖掉spark-env.sh中的配置</div><div class="line">比如说，上一讲我们的实验，worker的内存默认是1g，但是我们通过--memory 500m，是可以覆盖掉这个属性的</div><div class="line"></div><div class="line">SPARK_WORKER_INSTANCES			当前机器上的worker进程数量，默认是1，可以设置成多个，但是这时一定要设置SPARK_WORKER_CORES，限制每个worker的cpu数量</div><div class="line">SPARK_WORKER_DIR				spark作业的工作目录，包括了作业的日志等，默认是spark_home/work</div><div class="line">SPARK_WORKER_OPTS				worker的额外参数，使用&quot;-Dx=y&quot;设置各个参数</div><div class="line"></div><div class="line">参数名											默认值						含义</div><div class="line">spark.worker.cleanup.enabled					false						是否启动自动清理worker工作目录，默认是false</div><div class="line">spark.worker.cleanup.interval					1800						单位秒，自动清理的时间间隔，默认是30分钟</div><div class="line">spark.worker.cleanup.appDataTtl					7 * 24 * 3600				默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天</div><div class="line">-----------------------------------------------------------------</div><div class="line"></div><div class="line">SPARK_DAEMON_MEMORY				分配给master和worker进程自己本身的内存，默认是1g</div><div class="line">SPARK_DAEMON_JAVA_OPTS			设置master和worker自己的jvm参数，使用&quot;-Dx=y&quot;设置各个参数</div><div class="line">SPARK_PUBLISC_DNS				master和worker的公共dns域名，默认是没有的</div><div class="line"></div><div class="line">这里提示一下，大家可以观察一下，咱们的内存使用情况</div><div class="line">在没有启动spark集群之前，我们的内存使用是1个多g，启动了spark集群之后，就一下子耗费到2个多g</div><div class="line">每次又执行一个作业时，可能会耗费到3个多g左右</div><div class="line"></div><div class="line">所以大家就明白了，为什么之前用分布式的集群，每个worker节点才1个g内存，根本是没有办法使用standalone模式和yarn模式运行作业的</div></pre></td></tr></table></figure>
<p>下面是给大家列出spark所有的启动和关闭shell脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sbin/start-all.sh				根据配置，在集群中各个节点上，启动一个master进程和多个worker进程</div><div class="line">sbin/stop-all.sh				在集群中停止所有master和worker进程</div><div class="line">sbin/start-master.sh			在本地启动一个master进程</div><div class="line">sbin/stop-master.sh				关闭master进程</div><div class="line">sbin/start-slaves.sh			根据conf/slaves文件中配置的worker节点，启动所有的worker进程</div><div class="line">sbin/stop-slaves.sh				关闭所有worker进程</div><div class="line">sbin/start-slave.sh				在本地启动一个worker进程</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/" data-id="cj290scbj012yssqq9v7g8xzb" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/" class="article-date">
  <time datetime="2017-04-16T04:47:25.173Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/">spark core之Thrift JDBC_ODBC server</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Spark SQL的Thrift JDBC/ODBC server是基于Hive 0.13的HiveServer2实现的。这个服务启动之后，最主要的功能就是可以让我们通过<br>Java JDBC来以编程的方式调用Spark SQL。此外，在启动该服务之后，可以通过Spark或Hive 0.13自带的beeline工具来进行测试。</p>
        
          <p class="article-more-link">
            <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/#more">Read More</a>
          </p>
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/" data-id="cj290scaw012dssqq7q2i2ypr" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/spark core之standalone集群架构" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/" class="article-date">
  <time datetime="2017-04-16T04:47:25.171Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/">spark core之standalone集群架构</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="spark-core之standalone集群架构"><a href="#spark-core之standalone集群架构" class="headerlink" title="spark core之standalone集群架构"></a>spark core之standalone集群架构</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构.png" alt=""></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/" data-id="cj290scbg012vssqqtgzg3y5x" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/46/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/45/">45</a><a class="page-number" href="/page/46/">46</a><span class="page-number current">47</span><a class="page-number" href="/page/48/">48</a><a class="page-number" href="/page/49/">49</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/48/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>