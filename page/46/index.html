<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Chen&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/46/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">
  
    <link rel="alternate" href="/atom.xml" title="Chen&#39;s Blog" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Chen&#39;s Blog</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">一个技术渣的自说自话</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL数据源之JDBC数据源" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之JDBC数据源/" class="article-date">
  <time datetime="2017-04-16T04:47:25.199Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之JDBC数据源/">SparkSQL数据源之JDBC数据源</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>spark sql支持使用JDBC从关系型数据库(比如Mysql)中读取数据,读取的数据,依然由DataFrame表示,可以很方便的使用spark core提供的各种算子进行处理</p>
<p>实际上使用spark sql处理JDBC中的数据是非常有用的,比如说,你的mysql业务数据库中,有大量的数据,比如1000万,然后,你现在需要编写一个程序,对线上的脏数据进行某种复杂业务逻辑的处理,甚至复杂到可能涉及到要用spark sql反复查询hive中的数据,来进行关联处理</p>
<p>此时,用spark sql来通过JDBC数据源,加载mysql中的数据,然后通过各种算子进行处理,是最好的选择,因为spark是分布式的计算框架,对于1000万数据,肯定是分布式处理的,而如果你自己手工编写一个java程序,那么你只能分批次处理了,首先处理2万条,再处理2万条,可能运行完你的java程序,已经是好久之后的事情了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sqlContext.read.format(&quot;jdbc&quot;).options(</div><div class="line">Map(</div><div class="line">&quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;,</div><div class="line">&quot;dbtable&quot;-&gt;&quot;students&quot;</div><div class="line">)).load()</div></pre></td></tr></table></figure>
<p>案例:查询分数大于80分的学生信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">create database testdb;</div><div class="line">use testdb;</div><div class="line">create table student_infos(name varchar(20), age int);</div><div class="line">create table student_scores(name varchar(20), score int);</div><div class="line"></div><div class="line">insert into student_infos values(&quot;leo&quot;,18),(&quot;marry&quot;,17),(&quot;jack&quot;,19);</div><div class="line"></div><div class="line">insert into student_scores values(&quot;leo&quot;,88),(&quot;marry&quot;,77),(&quot;jack&quot;,99);</div><div class="line"></div><div class="line"></div><div class="line">create table good_student_infos(name varchar(20),  age  int, score int);</div></pre></td></tr></table></figure>
<p>完整的代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div></pre></td><td class="code"><pre><div class="line">val data2Mysql = (iterator: Iterator[(String, Int, Int)])=&gt;&#123;</div><div class="line">  val conn:Connection = null</div><div class="line">  val ps:PreparedStatement = null</div><div class="line">  var sql = &quot;insert into good_student_infos (name,age,scores) values (?,?,?)&quot;</div><div class="line">  try&#123;</div><div class="line">    //Class.forName(&quot;com.mysql.jdbc.Driver&quot;)</div><div class="line">    val conn = DriverManager.getConnection(&quot;jdbc:mysql://spark1:3306/testdb&quot;,&quot;&quot;,&quot;&quot;)</div><div class="line">    iterator.foreach&#123;</div><div class="line">      case (name,age,scores) =&gt;</div><div class="line">        ps.setString(1,name)</div><div class="line">        ps.setInt(2,age)</div><div class="line">        ps.setInt(3,age)</div><div class="line">        ps.executeUpdate()</div><div class="line">      case _ =&gt;</div><div class="line">    &#125;</div><div class="line">  &#125;catch&#123;</div><div class="line">    case e:Exception =&gt; println(&quot;mysql exception&quot;)</div><div class="line">  &#125;finally &#123;</div><div class="line">    if (ps != null)</div><div class="line">      ps.close()</div><div class="line">    if (conn != null)</div><div class="line">      conn.close()</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">def RDD2DataFrameByReflection(): Unit =&#123;</div><div class="line">  val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">  val sc = new SparkContext(sparkConf)</div><div class="line"></div><div class="line">  val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">  // 通过jdbc构建DataFrame</div><div class="line">  val studentInfosDF = sqlContext.read.format(&quot;jdbc&quot;)</div><div class="line">    .options(Map(</div><div class="line">      &quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;,</div><div class="line">      &quot;dbtable&quot;-&gt;&quot;student_infos&quot;</div><div class="line">    )).load()</div><div class="line"></div><div class="line">  val studentScoresDF = sqlContext.read.format(&quot;jdbc&quot;)</div><div class="line">    .options(Map(</div><div class="line">      &quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;,</div><div class="line">      &quot;dbtable&quot;-&gt;&quot;student_scores&quot;</div><div class="line">    )).load()</div><div class="line"></div><div class="line">  // 将两个DataFrame转换为rdd,执行join操作</div><div class="line">  val studentInfosRdd = studentInfosDF.rdd.map(row=&gt;(row(0).toString,row(1).toString.toInt))</div><div class="line">  val studentScoresRdd = studentScoresDF.rdd.map(row=&gt;(row(0).toString,row(1).toString.toInt))</div><div class="line">  val studentInfoScoresRdd = studentInfosRdd.join(studentScoresRdd).map(t=&gt;(t._1, t._2._1, t._2._2))</div><div class="line"></div><div class="line">  // 过滤学生成绩大于80的学生信息</div><div class="line">  val goodStudentRdd = studentInfoScoresRdd.filter(_._3 &gt; 80)</div><div class="line"></div><div class="line">  goodStudentRdd.foreachPartition(data2Mysql(_))</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之JDBC数据源/" data-id="cj290sc7d00z9ssqqr8teh54t" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL数据源之hive数据源" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之hive数据源/" class="article-date">
  <time datetime="2017-04-16T04:47:25.198Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之hive数据源/">SparkSQL数据源之hive数据源</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>spark sql支持对hive中存储的数据进行读写,操作hive中的数据时,必须创建HiveContext,而不是SQLContext,HiveContext继承自SQLContext,但是增加了在hive元数据库中查找表,以及用HiveQL语法编写SQL的功能,除了sql()方法,HiveContext还提供了hql()方法,从而用hive语法来编译sql</p>
<p>使用HiveContext,可以执行hive的大部分功能,包括创建表,往表里到入数据以及用sql语句查询表中的数据,查询出来的数据是一个Row数组</p>
<p>将hive-site.xml拷贝到spark/conf目录下,将mysql connector拷贝到spark/lib目录下</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">val sqlContext = new HiveContext(sc)</div><div class="line">sqlContext.sql(&quot;create table if not exists students (name String, age Int)&quot;)</div><div class="line"></div><div class="line">sqlContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/students.txt&apos; into table students&quot;)</div><div class="line"></div><div class="line">sqlContext.sql(&quot;select name, age from students where age&lt;=18&quot;).collect</div></pre></td></tr></table></figure>
<p>spark sql还允许将数据保存到hive表中,调用DataFrame的saveAsTable命令,即可将DataFrame中的数据保存到hive表中,与registerTempTable不同,saveAsTable是会将DataFrame中的数据物化到hive表中的,而且还会在hive元数据库中创建表的元数据</p>
<p>默认情况下,saveAsTable会创建一张hive Managed Table,也就是说,数据的位置都是由元数据库中国你的信息控制的,当managed Table被删除时,表中的数据也会一并被物理删除</p>
<p>registerTempTable只是注册一个临时的表,只要按spark Application重启或者停止了,那么表就没了,而saveAsTable创建的是物化的表,无论spark Application重启或停止,表都会一直存在</p>
<p>调用hiveContext.table()方法还可以直接针对hive中的表,创建一个DataFrame</p>
<p>案例:查询分数大于80分的学生的信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line"></div><div class="line">val hiveContext = new HiveContext(sc)</div><div class="line"></div><div class="line">// 判断是否存在student_info表,如果存在就删除</div><div class="line">hiveContext.sql(&quot;drop if exists table student_info&quot;)</div><div class="line">// 创建表</div><div class="line">hiveContext.sql(&quot;create if not exists table studnet_info (name STRNG, age INT&quot;)</div><div class="line"></div><div class="line">// 将学生基本信息数据导入student_info表</div><div class="line">hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/student_info.txt&apos; into table student_info &quot;)</div><div class="line"></div><div class="line">// 同样的步骤:创建表student_score,并加载数据</div><div class="line">hiveContext.sql(&quot;drop if exists table student_score&quot;)</div><div class="line">hiveContext.sql(&quot;create if not exists table student_score (name STRNG, score INT&quot;)</div><div class="line">hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/student_score.txt&apos; into table student_score &quot;)</div><div class="line"></div><div class="line">// 执行sql查询,关联2张表,查询成绩大于80分的学生成绩</div><div class="line">val goodStudentDF = hiveContext.sql(&quot;select info.name,info.age,score.score&quot; +</div><div class="line">                        &quot;from student_info info&quot; +</div><div class="line">                        &quot;join student_score score on info.name=score.name&quot; +</div><div class="line">                        &quot;where score.score&gt;=80&quot;)</div><div class="line"></div><div class="line">// 接着将DataFrame中的数据保存到good_student_info表中</div><div class="line">hiveContext.sql(&quot;drop if exists table good_student_info&quot;)</div><div class="line">goodStudentDF.saveAsTable(&quot;good_student_info&quot;)</div><div class="line"></div><div class="line">// 然后针对good_student_info表直接创建DataFrame</div><div class="line">val goodStudentRows = hiveContext.table(&quot;good_student_info&quot;).collect</div><div class="line">for(goodStudentRow &lt;- goodStudentRows)&#123;</div><div class="line">  println(goodStudentRow)</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">进入到hive的shell中</div><div class="line"></div><div class="line">&gt;hive</div><div class="line">&gt;show tables;</div><div class="line">&gt;select * from good_student_info;</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之hive数据源/" data-id="cj290sc7j00zfssqq9n1whayz" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL数据源之DataFrame的使用" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之DataFrame的使用/" class="article-date">
  <time datetime="2017-04-16T04:47:25.196Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之DataFrame的使用/">SparkSQL数据源之DataFrame的使用</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>DataFrame介绍</p>
<p>DataFrame可以理解为是以列的形式组织的,分布式的数据集合,他其实和关系型数据库中的表非常类似,但是底层做了很多的优化,DataFrame可以通过很多来源进行构建,包括:结构胡的数据文件,hive中的表,外部的关系型数据库,以及RDD</p>
<p>SQLContext<br>要使用Spark SQL,首先就得创建一个SQLContext对象,或者是他的子类的对象,比如HiveContext对象</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val sc:SparkContext = ...</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line">import sqlContext.implicits._</div></pre></td></tr></table></figure>
<p>HiveContext<br>除了基本你的SQLContext以外,还可以使用它的子类—–HiveContext,HiveContext的功能除了包含SQLContext提供的所有功能之外,还包含了额外的专门针对Hive的一些功能,这些额外的功能包括:使用hiveSQL语法来编写和执行sql;使用hive中的UDF函数;从hive表中读取数据</p>
<p>要使用hiveContext,就必须预先安装好hive,SQLContext支持的数据源,hiveContext也同样支持—–而不只是支持hive,对于spark1.3.x以上的版本,都推荐使用hiveContext,因为其功能更加丰富和完善</p>
<p>spark sql还支持用spark.sql.dialect参数来设置sql的方言,使用SQLContext的setConf()即可进行设置,对于SQLContext,他只支持”sql”一种方言,对于hiveContext,他默认的方言是”hiveql”</p>
<p>创建DataFrame<br>使用SQLContext,可以从RDD,hive表或者其他数据源,来创建一个DataFrame,以下是一个使用json文件创建DataFrame的例子<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">val sc:SparkContext = ...</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line">val df = sqlContext.read.json(&quot;hdfs://spark1:9000/students.json&quot;)</div><div class="line">df.show()</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">=======students.json===========</div><div class="line">&#123;&quot;id&quot;:1, &quot;name&quot;:&quot;leo&quot;, &quot;age&quot;:18&#125;</div><div class="line">&#123;&quot;id&quot;:2, &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:19&#125;</div><div class="line">&#123;&quot;id&quot;:3, &quot;name&quot;:&quot;merry&quot;, &quot;age&quot;:17&#125;</div></pre></td></tr></table></figure></p>
<p>下面的代码是在windows本地的完整测试代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">object TopNBasic &#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line"></div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    testMethod(sc)</div><div class="line">    sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //在实际的生成中,我们是封装函数来进行逻辑的组织</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line">    //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def testMethod(sc: SparkContext): Unit =&#123;</div><div class="line">    val sqlContext = new SQLContext(sc)</div><div class="line">    val df = sqlContext.read.json(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">    df.show()</div><div class="line"></div><div class="line">    /*</div><div class="line">    打印结果:</div><div class="line">    +---+---+-----+</div><div class="line">    |age| id| name|</div><div class="line">    +---+---+-----+</div><div class="line">    | 18|  1|  leo|</div><div class="line">    | 19|  2| jack|</div><div class="line">    | 17|  3|merry|</div><div class="line">    +---+---+-----+</div><div class="line">     */</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>提交到spark集群的shell<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">/usr/local/spark/bin/spark-submit \</div><div class="line">--class cn.spark.study.sql.DataFrameCreate \</div><div class="line">--num-executors 3 \</div><div class="line">--driver-memory 100m \</div><div class="line">--executor-cores 3 \</div><div class="line">--files /usr/local/hive/conf/hive-site.xml \</div><div class="line">--driver-class-path /usr/local/hive/lib/mysql-connctor-java-5.1.17.jar \</div><div class="line">/root/spark-test-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</div></pre></td></tr></table></figure></p>
<p>其中会用到了hive的conf文件,和mysql的驱动jar包</p>
<p>DataFrame的常用操作<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div></pre></td><td class="code"><pre><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line">val df = sqlContext.read.json(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">// 打印DataFrame中的所有的数据</div><div class="line">df.show()</div><div class="line">// 打印DataFrame中的元数据信息(Schema)</div><div class="line">df.printSchema</div><div class="line">// 查询某一列所有的数据</div><div class="line">df.select(&quot;name&quot;).show</div><div class="line">// 查询某几列所有的数据,并对列进行计算(将age列加1)</div><div class="line">df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(1)).show</div><div class="line">//df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show</div><div class="line"></div><div class="line">// 对于某一列的值进行过滤</div><div class="line">df.filter(df.col(&quot;age&quot;).gt(18)).show</div><div class="line">//df.filter(df(&quot;age&quot;) &gt; 18).show</div><div class="line"></div><div class="line">// 根据某一列进行分组,然后进行聚合</div><div class="line">df.groupBy(df.col(&quot;age&quot;)).count.show</div><div class="line">//df.groupBy(&quot;age&quot;).count.show</div><div class="line"></div><div class="line"></div><div class="line">/*</div><div class="line">打印结果:</div><div class="line">// 打印DataFrame中的所有的数据</div><div class="line">df.show()</div><div class="line">  +---+---+-----+</div><div class="line">  |age| id| name|</div><div class="line">  +---+---+-----+</div><div class="line">  | 18|  1|  leo|</div><div class="line">  | 19|  2| jack|</div><div class="line">  | 17|  3|merry|</div><div class="line">  +---+---+-----+</div><div class="line">  </div><div class="line">  // 打印DataFrame中的元数据信息(Schema)</div><div class="line"> df.printSchema</div><div class="line">  root</div><div class="line">   |-- age: long (nullable = true)</div><div class="line">   |-- id: long (nullable = true)</div><div class="line">   |-- name: string (nullable = true)</div><div class="line">  </div><div class="line">// 查询某一列所有的数据</div><div class="line">df.select(&quot;name&quot;).show</div><div class="line">  +-----+</div><div class="line">  | name|</div><div class="line">  +-----+</div><div class="line">  |  leo|</div><div class="line">  | jack|</div><div class="line">  |merry|</div><div class="line">  +-----+</div><div class="line">  </div><div class="line">// 查询某几列所有的数据,并对列进行计算(将age列加1)</div><div class="line">df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(1)).show</div><div class="line">  +-----+---------+</div><div class="line">  | name|(age + 1)|</div><div class="line">  +-----+---------+</div><div class="line">  |  leo|       19|</div><div class="line">  | jack|       20|</div><div class="line">  |merry|       18|</div><div class="line">  +-----+---------+</div><div class="line">  </div><div class="line">// 对于某一列的值进行过滤</div><div class="line">df.filter(df.col(&quot;age&quot;).gt(18)).show</div><div class="line">  +---+---+----+</div><div class="line">  |age| id|name|</div><div class="line">  +---+---+----+</div><div class="line">  | 19|  2|jack|</div><div class="line">  +---+---+----+</div><div class="line">  </div><div class="line">// 根据某一列进行分组,然后进行聚合</div><div class="line">df.groupBy(df.col(&quot;age&quot;)).count.show</div><div class="line">  +---+-----+</div><div class="line">  |age|count|</div><div class="line">  +---+-----+</div><div class="line">  | 19|    1|</div><div class="line">  | 17|    1|</div><div class="line">  | 18|    1|</div><div class="line">  +---+-----+</div><div class="line"> */</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之DataFrame的使用/" data-id="cj290sc7a00z6ssqqgmsarv9h" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL工作原理剖析及性能优化" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL工作原理剖析及性能优化/" class="article-date">
  <time datetime="2017-04-16T04:47:25.194Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL工作原理剖析及性能优化/">SparkSQL工作原理流程剖析及性能优化</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="SparkSQL工作原理流程"><a href="#SparkSQL工作原理流程" class="headerlink" title="SparkSQL工作原理流程"></a>SparkSQL工作原理流程</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/SparkSQL工作原理流程.png" alt=""></p>
<h1 id="spark-sql核心源码"><a href="#spark-sql核心源码" class="headerlink" title="spark sql核心源码"></a>spark sql核心源码</h1><h1 id="spark-sql优化的点"><a href="#spark-sql优化的点" class="headerlink" title="spark sql优化的点"></a>spark sql优化的点</h1><p>1.设置shuffle过程中的并行度:spark.sql.shuffle.partitions(SqlContext.setconf())<br>2.在hive数据仓库建设的过程中,合理设置数据类型,比如能设置为int的,就不要设置为BigInt,减少数据类型导致的不必要的内存开销<br>3.编写sql时尽量列出列名,不要写select *<br>4.并行处理查询结果,对于spark sql查询的结果,如果数据量比较大,比如超过1000条,那么就不要一次性collect到Driver里面,使用foreach()算子,并行处理查询结果<br>5.缓存表:对于一条sql语句中可能多次使用到的表,可以对其进行缓存,使用<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">SQLContext.cacheTable(tableName)</div><div class="line">//或者</div><div class="line">DataFrame.cache()</div></pre></td></tr></table></figure></p>
<p>spark sql会用内存列存储的格式进行表的缓存,然后spark sql就可以仅仅扫描需要使用的列,并且自动优化压缩,来最小化内存使用和GC开销,SQLContext.uncacheTable(tableName)可以将表从缓存中移除,用SQLContext.setConf(),设置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">spark.sql.inMemoryColumnarStorage.batchSize  </div><div class="line">//默认是1000</div></pre></td></tr></table></figure></p>
<p>可以配置列存储的单位<br>6.广播join表:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">spark.sql.autoBroadcastJoinThreshold </div><div class="line">//默认10485760(10M) ,会将10M以内的表自动广播出去</div></pre></td></tr></table></figure></p>
<p>在内存够用的情况下,可以增加其大小,可以让更多的表被自动广播出去,可以将join中的较小的表广播出去,而不用进行网络数据传输了<br>7.钨丝计划:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">spark.sql.tungsten.enabled</div><div class="line">//默认是true</div></pre></td></tr></table></figure></p>
<p>自动管理内存</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL工作原理剖析及性能优化/" data-id="cj290sc7700z3ssqqrcaexkqy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL实战之统计每日top3热点搜索词" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL实战之统计每日top3热点搜索词/" class="article-date">
  <time datetime="2017-04-16T04:47:25.193Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL实战之统计每日top3热点搜索词/">SparkSQL实战之统计每日top3热点搜索词</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>数据格式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">日期,用户,搜索词,程序,平台,版本</div><div class="line">2015-10-01	leo	water	beijing	android 1.0</div></pre></td></tr></table></figure></p>
<p>要求:<br>1.筛选出符合查询条件的数据<br>2.统计出每天搜索uv排名前3的搜索词<br>3.按照每天的top3搜索词的搜索uv总次数,倒序排序<br>4.将数据保存到hive表中</p>
<p>实现思路<br>1.针对原始数据(HDFS文件),获取输入的rdd<br>2.使用filter,去针对输入RDD中的数据进行过滤,过滤出符合查询条件的数据<br>2.1.普通的做法:直接在filter算子函数中,使用外部的查询条件(Map),但是这样做的话,会将查询条件Map发送给每个task一个副本<br>2.2.优化后的做法:将查询条件,封装为Broadcast广播变量,在filter算子中使用Broadcast广播变量<br>3.将数据转换为”(日期<em>搜索词, 用户)” ,然后对其进行分组,其次再对分组后的数据在组内进行去重操作,得到每天每个搜索词的uv,最后的数据格式为:(日期</em>搜索词, uv)<br>4.将3中得到的Rdd转成Rdd[Row],将该Rdd转换成DataFrame<br>5.将DataFrame注册成临时表,使用spark sql开窗函数,来统计每天的uv数量排名前3的搜索词,以及他的搜索uv,最后获取的是一个DataFrame<br>6.将DataFrame转换为rdd,继续操作,按照每天日期进行分组,并进行映射,计算出每天的top3搜索词的搜索uv总数,然后将uv总数作为key,将每天的top3搜索词以及搜索次数拼接为一个字符串<br>7.按照每天的top3搜索总uv,进行倒序排列<br>8.整理格式:日期_搜索词_uv<br>9.再次映射为DataFrame,并将数据保存到hive表中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new HiveContext(sc)</div><div class="line"></div><div class="line">import sqlContext.implicits._</div><div class="line"></div><div class="line">// 构造一份数据:查询条件(在实际的开发过程中,通过JavaWeb将查询条件入库到mysql中</div><div class="line">// 然后这里从mysql中提取查询条件</div><div class="line">val queryParamMap = Map(</div><div class="line">  &quot;city&quot;-&gt;Array(&quot;beijing&quot;,&quot;hubei&quot;),</div><div class="line">  &quot;platform&quot;-&gt;Array(&quot;android&quot;),</div><div class="line">  &quot;version&quot;-&gt;Array(&quot;1.0&quot;,&quot;1.2&quot;)</div><div class="line">)</div><div class="line">// 将查询条件广播</div><div class="line">val queryParamMapBroadCast = sc.broadcast(queryParamMap)</div><div class="line"></div><div class="line">//val lineRdd = sc.textFile(&quot;hdfs://spark1:9000/keyword.txt&quot;)</div><div class="line">//val lineRdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\sousuo.txt&quot;)</div><div class="line">// 使用查询条件,进行filter</div><div class="line">val filterRdd = lineRdd.filter&#123;</div><div class="line">  line=&gt;</div><div class="line">    val arr = line.split(&quot;\t&quot;)</div><div class="line">    /*</div><div class="line">    日期,用户,搜索词,程序,平台,版本</div><div class="line">    2015-10-01	leo	water	beijing	android 1.0</div><div class="line">     */</div><div class="line">    val city, platform, version = (arr(2),arr(3), arr(4))</div><div class="line"></div><div class="line">    val map = queryParamMapBroadCast.value</div><div class="line">    val cities = map.get(&quot;city&quot;)</div><div class="line">    if(!cities.toList.contains(city))&#123;</div><div class="line">      false</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    val platforms = map.get(&quot;platform&quot;)</div><div class="line">    if(!platforms.toList.contains(platform))&#123;</div><div class="line">      false</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    val versions = map.get(&quot;version&quot;)</div><div class="line">    if(!versions.toList.contains(version))&#123;</div><div class="line">      false</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    true</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 改变格式:(日期_搜索词,用户)</div><div class="line">val dateKeywordUserRdd = filterRdd.map&#123;</div><div class="line">    /*</div><div class="line">    日期,用户,搜索词,程序,平台,版本</div><div class="line">    2015-10-01	leo	water	beijing	android 1.0</div><div class="line">     */</div><div class="line">  line=&gt;</div><div class="line">    val arr = line.split(&quot;\t&quot;)</div><div class="line">    val (date,user,keyword) = (arr(0),arr(1), arr(2))</div><div class="line">    // 返回格式:(日期_搜索词,用户)</div><div class="line">    (date+&quot;_&quot;+keyword, user)</div><div class="line">&#125;</div><div class="line">// 进行分组,获取每天每个搜索词,有哪些有用(没有对用户去重)</div><div class="line">val dateKeywordUsesRdd = dateKeywordUserRdd.groupByKey()</div><div class="line">// 对每天每个搜索词的搜索用户,执行去重,获取其uv</div><div class="line">val dateKeywordUvsRdd = dateKeywordUsesRdd.map&#123;</div><div class="line">  line=&gt;</div><div class="line">    val dateKeyword = line._1</div><div class="line">    val iter = line._2.toIterator</div><div class="line">    val distinctUsers = mutable.Set[String]()</div><div class="line">    while(iter.hasNext)&#123;</div><div class="line">      val user = iter.next</div><div class="line">      distinctUsers+=user</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    // 返回:(日期_搜索词,uv)</div><div class="line">    val uv = distinctUsers.size</div><div class="line">    (dateKeyword, uv)</div><div class="line">&#125;</div><div class="line"></div><div class="line">// 返回:Row(日期,搜索词,uv)</div><div class="line">val dateKeywordUsesRowRdd = dateKeywordUvsRdd.map(line=&gt;Row(line._1.split(&quot;_&quot;)(0),line._1.split(&quot;_&quot;)(1),line._2.toString.toInt))</div><div class="line"></div><div class="line">// 将每天每个搜索词的uv数据转成DataFrame</div><div class="line">val structFields = StructType(Array(</div><div class="line">  StructField(&quot;date&quot;, StringType, true),</div><div class="line">  StructField(&quot;keyword&quot;, StringType, true),</div><div class="line">  StructField(&quot;uv&quot;, IntegerType, true)</div><div class="line">))</div><div class="line"></div><div class="line">val dateKeywordUvDF = sqlContext.createDataFrame(dateKeywordUsesRowRdd, structFields)</div><div class="line"></div><div class="line">// 使用spark sql的开窗函数,统计每天搜索uv排名前3的热点搜索词</div><div class="line">dateKeywordUvDF.registerTempTable(&quot;daily_keyword_uv&quot;)</div><div class="line">val dailyTop3KeywordDF = sqlContext.sql(&quot;&quot; +</div><div class="line">  &quot;select date, keyword, uv&quot; +</div><div class="line">  &quot;from (&quot; +</div><div class="line">  &quot;   select &quot; +</div><div class="line">  &quot;       date,&quot; +</div><div class="line">  &quot;       keyworkd,&quot; +</div><div class="line">  &quot;       uv,&quot; +</div><div class="line">  &quot;       row_number() over (partition by date order by uv desc) rank&quot; +</div><div class="line">  &quot;   from daily_keyword_uv&quot; +</div><div class="line">  &quot;) tmp&quot; +</div><div class="line">  &quot;where rank &lt;= 3&quot;)</div><div class="line"></div><div class="line">dailyTop3KeywordDF.show</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL实战之统计每日top3热点搜索词/" data-id="cj290sc6z00yxssqqznhisw7f" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL函数之开窗函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之开窗函数/" class="article-date">
  <time datetime="2017-04-16T04:47:25.192Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之开窗函数/">SparkSQL函数之开窗函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>分组取topN的案例</p>
<p>商品在某个分类下的topN</p>
<p>代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">val hiveContext = new HiveContext(sc)</div><div class="line">hiveContext.sql(&quot;drop table if not exists sales&quot;)</div><div class="line">hiveContext.sql(&quot;create table if not exists sales (&quot; +</div><div class="line">  &quot;product STRING&quot; +</div><div class="line">  &quot;category SRING&quot; +</div><div class="line">  &quot;revenue BIGINT&quot; +</div><div class="line">  &quot;)&quot;)</div><div class="line">hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resources/sales.txt&apos; into table sales&quot;)</div><div class="line"></div><div class="line">// 使用row_number()开窗函数:给每个分组内的数据,按照其排序的顺序,打上一个分组内的行号,行号从1开始</div><div class="line">val top3SalesDF = hiveContext.sql(&quot;select product,category,revenue &quot; +</div><div class="line">  &quot;from (&quot; +</div><div class="line">  &quot;   select product,category,revenue,&quot; +</div><div class="line">  &quot;       row_number() over (partition by category order by revenue desc) rank&quot; +</div><div class="line">  &quot;   from sales&quot; +</div><div class="line">  &quot;) tmp_sales&quot; +</div><div class="line">  &quot;where rank &lt;= 3&quot;)</div><div class="line"></div><div class="line">/* row_number函数使用说明:</div><div class="line">1.row_number函数后面跟的是over关键字</div><div class="line">2.括号中是partition by 表示根据哪个字段进行分组</div><div class="line">3.order by表示在组内按照指定的字段进行排序</div><div class="line">4.row_number就能给每个组内的每行一个组内行号</div><div class="line">5.在子查询的外部,取组内排名前3</div><div class="line"> */</div><div class="line"></div><div class="line">hiveContext.sql(&quot;drop table if not exists top3_sales&quot;)</div><div class="line">top3SalesDF.saveAsTable(&quot;top3_sales&quot;)</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之开窗函数/" data-id="cj290sc7200z0ssqqgrwujt4l" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL函数之内置函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之内置函数/" class="article-date">
  <time datetime="2017-04-16T04:47:25.190Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之内置函数/">SparkSQL函数之内置函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>在spark1.5.x版本中,增加了一系列内置函数到DataFrame API中,并且实现了code generation的优化,与普通的函数不同,DataFrame的函数并不会执行后立即返回一个值,而是返回一个Column对象,用于在并行作业中进行求值,Column可以用在DataFrame的操作之中,比如select,filter,groupBy等,函数的输入值,也可以是Column</p>
<table>
<thead>
<tr>
<th style="text-align:left">种类</th>
<th style="text-align:left">函数</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">聚合函数</td>
<td style="text-align:left">approxCountDistinct,avg,count,countDistinct,first,last,max,mean,min,sum,sumDistinct</td>
</tr>
<tr>
<td style="text-align:left">集合函数</td>
<td style="text-align:left">array_contains,explode,size,sort_array</td>
</tr>
<tr>
<td style="text-align:left">日期/时间转换函数</td>
<td style="text-align:left">日期时间转换unix_timestamp,from_unixtime,to_date,quarter,day,dayofyear,weekofyear,from_utc_timestamp,to_utc_timestamp;(从日期时间中提取字段:year,month,dayofmonth,hour,minute,second;(日期/时间计算)datediff,date_add,date_sub,add_months,last_day,next_day,months_between;(获取当前时间)current_date,current_timestamp,trunc,date_format</td>
</tr>
<tr>
<td style="text-align:left">混合函数</td>
<td style="text-align:left">array,isNaN,isnotnull,isnull,not,when,if,rand</td>
</tr>
<tr>
<td style="text-align:left">字符串函数</td>
<td style="text-align:left">concat,decode,encode,format_number,format_string,get_json_object,length,lpad,ltrim,lower,rpad,upper</td>
</tr>
<tr>
<td style="text-align:left">窗口函数</td>
<td style="text-align:left">cumeDist,denseRank,lag,lead,ntile,percentRank,rank,rowNumber</td>
</tr>
</tbody>
</table>
<p>案例:根据每天的用户访问日志和购买日志,统计每日的uv和销售额</p>
<p>统计每日的UV</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">// 导入隐式转换</div><div class="line">import sqlContext.implicits._</div><div class="line"></div><div class="line">// 构造用户访问日志数据,创建DataFrame</div><div class="line">val userAccessLog = Array(//date , userid</div><div class="line">  &quot;2015-10-01,1122&quot;,</div><div class="line">  &quot;2015-10-01,1123&quot;,</div><div class="line">  &quot;2015-10-01,1124&quot;,</div><div class="line">  &quot;2015-10-02,1122&quot;,</div><div class="line">  &quot;2015-10-02,1123&quot;,</div><div class="line">  &quot;2015-10-02,1122&quot;</div><div class="line">)</div><div class="line"></div><div class="line">val userAccessLogRDD = sc.parallelize(userAccessLog)</div><div class="line"></div><div class="line">// 将RDD转成DataFrame,需要处理RDD的元素变成Row</div><div class="line">val userAccessLogRowRdd = userAccessLogRDD.map&#123;</div><div class="line">  line=&gt;</div><div class="line">    val arr = line.split(&quot;,&quot;)</div><div class="line">    Row(arr(0), arr(1).toInt)</div><div class="line">&#125;</div><div class="line"></div><div class="line">val structType = StructType(Array(</div><div class="line"> StructField(&quot;date&quot;, StringType, true),</div><div class="line"> StructField(&quot;userid&quot;, IntegerType, true)</div><div class="line">))</div><div class="line"></div><div class="line">// 构建DataFrame</div><div class="line">val userAccessLogRowDF = sqlContext.createDataFrame(userAccessLogRowRdd, structType)</div><div class="line"></div><div class="line">// 内置函数的使用</div><div class="line">// 1.统计uv</div><div class="line">userAccessLogRowDF.groupBy(&quot;date&quot;)</div><div class="line">  // 按照date聚合,在每组中对userid进行去重统计</div><div class="line">  //.agg(&apos;date, functions.countDistinct(&apos;userid)).show</div><div class="line">  /* 打印结果</div><div class="line">  +----------+----------+----------------------+</div><div class="line">  |      date|      date|count(DISTINCT userid)|</div><div class="line">  +----------+----------+----------------------+</div><div class="line">  |2015-10-02|2015-10-02|                     2|</div><div class="line">  |2015-10-01|2015-10-01|                     3|</div><div class="line">  +----------+----------+----------------------+</div><div class="line">   */</div><div class="line">// 如果我们要取date,userid列</div><div class="line">  .agg(&apos;date, functions.countDistinct(&apos;userid)).rdd</div><div class="line">  .map&#123;row=&gt; (row(1),row(2))&#125;</div><div class="line">  .collect</div><div class="line">  .foreach(println)</div><div class="line">/* 打印结果</div><div class="line">(2015-10-02,2)</div><div class="line">(2015-10-01,3)</div><div class="line"> */</div><div class="line"></div><div class="line">/*</div><div class="line">聚合函数总结:</div><div class="line">1.对DataFrame调用groupBy()方法,对某一列进行分组</div><div class="line">2.调用agg()方法,第一个参数,必须是之前groupBy()方法中出现的字段</div><div class="line">  agg()方法的第一个参数是用单引号开头的</div><div class="line">3.第二个参数,传入countDistinct,sum,first等,spark提供的内置函数</div><div class="line">  内置函数中传入的参数也是用:单引号作为参数的</div><div class="line">4.所有的统计函数在:org.apache.spark.sql.functions中,所以要指定functions.xx</div><div class="line"> */</div></pre></td></tr></table></figure>
<p>统计销售额<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">// 导入隐式转换</div><div class="line">import sqlContext.implicits._</div><div class="line"></div><div class="line">val userSaleLog = Array(</div><div class="line">  &quot;2015-10-01,55.55&quot;,</div><div class="line">  &quot;2015-10-01,66.55&quot;,</div><div class="line">  &quot;2015-10-02,77.55&quot;,</div><div class="line">  &quot;2015-10-02,55.55&quot;,</div><div class="line">  &quot;2015-10-03,55.55&quot;,</div><div class="line">  &quot;2015-10-03,55.55&quot;</div><div class="line">)</div><div class="line"></div><div class="line">val userSaleLogRdd = sc.parallelize(userSaleLog)</div><div class="line">val userSaleLogRowRdd = userSaleLogRdd.map&#123;</div><div class="line">  line=&gt;</div><div class="line">    val arr = line.split(&quot;,&quot;)</div><div class="line">    Row(arr(0), arr(1).toDouble)</div><div class="line">&#125;</div><div class="line"></div><div class="line">val structType = StructType(Array(</div><div class="line">  StructField(&quot;date&quot;, StringType, true),</div><div class="line">  StructField(&quot;sale_amount&quot;, DoubleType, true)</div><div class="line">))</div><div class="line"></div><div class="line">val userSaleLogDF = sqlContext.createDataFrame(userSaleLogRowRdd, structType)</div><div class="line"></div><div class="line">// 每日销售额的统计</div><div class="line">userSaleLogDF.groupBy(&quot;date&quot;)</div><div class="line">  .agg(&apos;date, functions.sum(&apos;sale_amount)).rdd</div><div class="line">  .map(row=&gt; (row(1),row(2).toString.toDouble))</div><div class="line">  .collect</div><div class="line">  .foreach(println)</div><div class="line">/* 打印结果</div><div class="line">(2015-10-02,133.1)</div><div class="line">(2015-10-01,122.1)</div><div class="line">(2015-10-03,111.1)</div><div class="line"> */</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之内置函数/" data-id="cj290sc6w00yussqqdl2txy6u" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL函数之UDF" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDF/" class="article-date">
  <time datetime="2017-04-16T04:47:25.188Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDF/">SparkSQL函数之UDF</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">// 构造模拟数据rdd</div><div class="line">val names = Array(&quot;leo&quot;, &quot;marry&quot;, &quot;jack&quot;)</div><div class="line">val namesRdd = sc.parallelize(names)</div><div class="line"></div><div class="line">// 将RDD转成DataFrame</div><div class="line">val namesRowRdd = namesRdd.map(Row(_))</div><div class="line">val structType = StructType(Array(StructField(&quot;name&quot;,StringType,true)))</div><div class="line">val namesDF = sqlContext.createDataFrame(namesRowRdd, structType)</div><div class="line"></div><div class="line">// 注册一张表</div><div class="line">namesDF.registerTempTable(&quot;names&quot;)</div><div class="line"></div><div class="line">// 定义和注册自定义函数</div><div class="line">// 函数名: strLen</div><div class="line">// 函数体:这里是一个匿名函数:(str:String)=&gt; str.length</div><div class="line">sqlContext.udf.register(&quot;strLen&quot;, (str:String)=&gt; str.length)</div><div class="line"></div><div class="line">// 使用自定义函数</div><div class="line">sqlContext.sql(&quot;select name, strLen(name) from names&quot;)</div><div class="line">  .collect</div><div class="line">  .foreach(println)</div><div class="line">/* 打印结果</div><div class="line">[leo,3]</div><div class="line">[marry,5]</div><div class="line">[jack,4]</div><div class="line"> */</div></pre></td></tr></table></figure>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDF/" data-id="cj290sc6t00yrssqq4giboh2f" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL函数之UDAF自定义聚合函数" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDAF自定义聚合函数/" class="article-date">
  <time datetime="2017-04-16T04:47:25.187Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDAF自定义聚合函数/">SparkSQL函数之UDAF自定义聚合函数</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>UDF(自定义函数),针对的是单行输入,返回一个输出<br>UDAF(自定义聚合函数),针对的是多行输入,进行聚合计算,返回一个输出</p>
<p>下面是一个自定义的分组统计字符串的个数的函数</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div></pre></td><td class="code"><pre><div class="line">package org.dt.spark</div><div class="line"></div><div class="line">import org.apache.spark.sql.Row</div><div class="line">import org.apache.spark.sql.types._</div><div class="line">import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;</div><div class="line"></div><div class="line">/**</div><div class="line">  */</div><div class="line">class StringCount extends UserDefinedAggregateFunction&#123;</div><div class="line">  // inputSchema:指的是输入数据的类型</div><div class="line">  override def inputSchema: StructType = &#123;</div><div class="line">    StructType(Array(StructField(&quot;str&quot;, StringType, true)))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // bufferSchema:中间进行聚合时,所处理的数据的类型</div><div class="line">  override def bufferSchema: StructType = &#123;</div><div class="line">    StructType(Array(StructField(&quot;count&quot;, IntegerType, true)))</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // dataType:是函数返回值的类型</div><div class="line">  override def dataType: DataType = &#123;</div><div class="line">    IntegerType</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  override def deterministic: Boolean = true</div><div class="line"></div><div class="line">  // 为每个分组的数据执行初始化操作</div><div class="line">  override def initialize(buffer: MutableAggregationBuffer): Unit = &#123;</div><div class="line">    buffer(0) = 0</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // 每个分组有新的值进来的时候,如何进行分组对应的聚合值的计算</div><div class="line">  override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123;</div><div class="line">    buffer(0) = buffer.getAs[Int](0) + 1</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // 由于spark是分布式的,所以一个分组的数据,可能会在不同的节点上进行局部聚合,这个过程就是update</div><div class="line">  // 但是,最后,在各个节点上的聚合值,要进行merge,也就是合并</div><div class="line">  override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123;</div><div class="line">    buffer1(0) = buffer1.getAs[Int](0) + buffer2.getAs[Int](0)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  // 个分组的聚合值,如何通过中间的缓存聚合值,返回一个最终的聚合值</div><div class="line">  override def evaluate(buffer: Row): Any = &#123;</div><div class="line">    // 这里是直接将数据返回,没有做任何的处理</div><div class="line">    buffer.getAs[Int](0)</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>测试自定义聚合函数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line">val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">// 构造模拟数据rdd:</div><div class="line">val names = Array(&quot;leo&quot;, &quot;marry&quot;, &quot;jack&quot;, &quot;marry&quot;, &quot;jack&quot;, &quot;marry&quot;, &quot;jack&quot;)</div><div class="line">val namesRdd = sc.parallelize(names)</div><div class="line"></div><div class="line">// 将RDD转成DataFrame</div><div class="line">val namesRowRdd = namesRdd.map(Row(_))</div><div class="line">val structType = StructType(Array(StructField(&quot;name&quot;,StringType,true)))</div><div class="line">val namesDF = sqlContext.createDataFrame(namesRowRdd, structType)</div><div class="line"></div><div class="line">// 注册一张表</div><div class="line">namesDF.registerTempTable(&quot;names&quot;)</div><div class="line"></div><div class="line">// 定义和注册自定义函数</div><div class="line">// 函数名: strCount</div><div class="line">// 函数体: 是一个自定义的函数</div><div class="line">sqlContext.udf.register(&quot;strCount&quot;, new StringCount)</div><div class="line"></div><div class="line">// 使用自定义函数:统计相同名字出现的次数</div><div class="line">sqlContext.sql(&quot;select name, strCount(name) from names group by name&quot;)</div><div class="line">  .collect</div><div class="line">  .foreach(println)</div><div class="line">/* 打印结果</div><div class="line">  [jack,3]</div><div class="line">  [leo,1]</div><div class="line">  [marry,3]</div><div class="line"> */</div></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDAF自定义聚合函数/" data-id="cj290sc6q00yossqqjeaukumy" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-bigdata/spark从入门到精通_笔记/SparkSQL之通用的load和save操作" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之通用的load和save操作/" class="article-date">
  <time datetime="2017-04-16T04:47:25.186Z" itemprop="datePublished">2017-04-16</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/spark/">spark</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之通用的load和save操作/">SparkSQL之通用的load和save操作</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="通用的load和save操作"><a href="#通用的load和save操作" class="headerlink" title="通用的load和save操作"></a>通用的load和save操作</h1><p>对于spark sql的DataFrame来说,无论是从什么数据源创建出来的DataFrame,都有一些共同的load和save操作,load操作主要用于加载数据,创建出来DataFrame;save操作,主要用于将DataFrame中的数据保存到文件中</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">// users.parquet是使用parquet面向列存储的文件,用文本打开是乱码</div><div class="line">val df = sqlContext.read.load(&quot;users.parquet&quot;)</div><div class="line"></div><div class="line">df.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save()</div></pre></td></tr></table></figure>
<p>完整的代码如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">  val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">  val sc = new SparkContext(sparkConf)</div><div class="line">  val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">  val userDF = sqlContext.read.load(&quot;C:\\Users\\Administrator\\Desktop\\users.parquet&quot;)</div><div class="line">  userDF.printSchema</div><div class="line">  userDF.show</div><div class="line">  /*</div><div class="line">  打印结果:</div><div class="line">  root</div><div class="line">   |-- name: string (nullable = true)</div><div class="line">   |-- favorite_color: string (nullable = true)</div><div class="line">   |-- favorite_numbers: array (nullable = true)</div><div class="line">   |    |-- element: integer (containsNull = true)</div><div class="line"></div><div class="line">  +------+--------------+----------------+</div><div class="line">  |  name|favorite_color|favorite_numbers|</div><div class="line">  +------+--------------+----------------+</div><div class="line">  |Alyssa|          null|  [3, 9, 15, 20]|</div><div class="line">  |   Ben|           red|              []|</div><div class="line">  +------+--------------+----------------+</div><div class="line">   */</div><div class="line"></div><div class="line">  userDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;C:\\Users\\Administrator\\Desktop\\nameAndFavoriteColor.parquet&quot;)</div><div class="line">// 可以看到在桌面生成了一个nameAndFavoriteColor.parquet文件夹</div></pre></td></tr></table></figure></p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/nameAndFavoriteColor.parquet.png" alt=""></p>
<p>验证上面的保存文件是否保存成功<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">  val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line">  val sc = new SparkContext(sparkConf)</div><div class="line">  val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line">// 使用上面的保存路径</div><div class="line">  val userDF = sqlContext.read.load(&quot;C:\\Users\\Administrator\\Desktop\\nameAndFavoriteColor.parquet&quot;)</div><div class="line">  userDF.printSchema</div><div class="line">  userDF.show</div><div class="line"></div><div class="line">/*</div><div class="line">打印结果:</div><div class="line">root</div><div class="line"> |-- name: string (nullable = true)</div><div class="line"> |-- favorite_color: string (nullable = true)</div><div class="line"></div><div class="line">+------+--------------+</div><div class="line">|  name|favorite_color|</div><div class="line">+------+--------------+</div><div class="line">|Alyssa|          null|</div><div class="line">|   Ben|           red|</div><div class="line">+------+--------------+</div><div class="line">所以说明上面的保存是成功的</div><div class="line">*/</div></pre></td></tr></table></figure></p>
<h1 id="手动指定数据源的类型"><a href="#手动指定数据源的类型" class="headerlink" title="手动指定数据源的类型"></a>手动指定数据源的类型</h1><p>可以手动指定用来操作的数据源类型,数据源通常需要使用其全限定名来指定,比如parquet是org.apache.spark.sql.parquet,但是spark sql 内置了一些数据源类型,比如json,parquet,jdbc等等,实际上,通过这个功能,就可以在不同类型的数据源之间进行转换了,比如将json文件中的数据保存到parquet文件中,默认情况下,如果不指定数据源的类型,那么就是parquet</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"> val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)</div><div class="line"> val sc = new SparkContext(sparkConf)</div><div class="line"> val sqlContext = new SQLContext(sc)</div><div class="line"></div><div class="line"> val userDF = sqlContext.read.format(&quot;json&quot;).load(&quot;C:\\Users\\Administrator\\Desktop\\people.json&quot;)</div><div class="line">//保存为parquet</div><div class="line"> userDF.select(&quot;name&quot;).write.format(&quot;parquet&quot;).save(&quot;C:\\Users\\Administrator\\Desktop\\peopleName.parquet&quot;)</div></pre></td></tr></table></figure>
<h1 id="save-mode"><a href="#save-mode" class="headerlink" title="save mode"></a>save mode</h1><p>spark sql对于save操作,提供了不同的savemode,主要用来处理,当目标位置,已经有数据时,应该如何处理,而且save操作并不会执行行锁操作,并且不是原子的,因此是有一定风险出现脏数据的</p>
<table>
<thead>
<tr>
<th style="text-align:left">Save Mode</th>
<th style="text-align:left">意义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">SaveMode.ErrorIfExists(默认)</td>
<td style="text-align:left">如果目标位置已经存在数据,那么就抛出一个异常</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Append</td>
<td style="text-align:left">如果目标位置已经存在数据,那么将数据追加进去</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Overwrite</td>
<td style="text-align:left">如果目标位置已经存在数据,那么就将已经存在的数据删除,用新数据进行覆盖</td>
</tr>
<tr>
<td style="text-align:left">SaveMode.Ignore</td>
<td style="text-align:left">如果目标位置已经存在数据,那么就忽略,不做任何操作</td>
</tr>
</tbody>
</table>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之通用的load和save操作/" data-id="cj290sc6k00ylssqqo66wnbt9" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/spark/">spark</a></li></ul>

    </footer>
  </div>
  
</article>


  


  <nav id="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/45/">&laquo; Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><a class="page-number" href="/page/45/">45</a><span class="page-number current">46</span><a class="page-number" href="/page/47/">47</a><a class="page-number" href="/page/48/">48</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/47/">Next &raquo;</a>
  </nav>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/IDEA/">IDEA</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Linux/">Linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/NFS/">NFS</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tachyon/">Tachyon</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/azkaban/">azkaban</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/echarts/">echarts</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/flume/">flume</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hadoop/">hadoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hbase/">hbase</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/hive/">hive</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/inotify/">inotify</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/java/">java</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/kafka/">kafka</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/linux/">linux</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/logstash/">logstash</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/markdown/">markdown</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/memcached/">memcached</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mongodb/">mongodb</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/mysql/">mysql</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nginx/">nginx</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/project/">project</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/python/">python</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/redis/">redis</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/rsync/">rsync</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/scala/">scala</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/shell/">shell</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/socket/">socket</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/spark/">spark</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/sqoop/">sqoop</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/storm/">storm</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/IDEA/">IDEA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux基础命令/">Linux基础命令</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linux重要配置文件/">Linux重要配置文件</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NFS/">NFS</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/NIO/">NIO</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/azkaban/">azkaban</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/echarts/">echarts</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/flume/">flume</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/">hadoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hbase/">hbase</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/hive/">hive</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/inotify/">inotify</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/java/">java</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/kafka/">kafka</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/linux/">linux</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/logstash/">logstash</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mapreduce/">mapreduce</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/markdown/">markdown</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/memcached/">memcached</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb/">mongodb</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mysql/">mysql</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/netty/">netty</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/nginx/">nginx</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/project/">project</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/python/">python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/redis/">redis</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rpc/">rpc</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/rsync/">rsync</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala/">scala</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala函数式编程/">scala函数式编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/scala编程/">scala编程</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/shell/">shell</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/socket/">socket</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/spark/">spark</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/sqoop/">sqoop</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/storm/">storm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/">zookeeper</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据仓库/">数据仓库</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/IDEA/" style="font-size: 10px;">IDEA</a> <a href="/tags/Linux基础命令/" style="font-size: 19.52px;">Linux基础命令</a> <a href="/tags/Linux重要配置文件/" style="font-size: 14.76px;">Linux重要配置文件</a> <a href="/tags/NFS/" style="font-size: 10px;">NFS</a> <a href="/tags/NIO/" style="font-size: 11.43px;">NIO</a> <a href="/tags/azkaban/" style="font-size: 10.48px;">azkaban</a> <a href="/tags/echarts/" style="font-size: 10.95px;">echarts</a> <a href="/tags/flume/" style="font-size: 10.95px;">flume</a> <a href="/tags/hadoop/" style="font-size: 18.57px;">hadoop</a> <a href="/tags/hbase/" style="font-size: 13.33px;">hbase</a> <a href="/tags/hive/" style="font-size: 18.1px;">hive</a> <a href="/tags/inotify/" style="font-size: 10px;">inotify</a> <a href="/tags/java/" style="font-size: 12.38px;">java</a> <a href="/tags/kafka/" style="font-size: 12.86px;">kafka</a> <a href="/tags/linux/" style="font-size: 13.33px;">linux</a> <a href="/tags/logstash/" style="font-size: 10.48px;">logstash</a> <a href="/tags/mapreduce/" style="font-size: 16.67px;">mapreduce</a> <a href="/tags/markdown/" style="font-size: 10px;">markdown</a> <a href="/tags/memcached/" style="font-size: 13.81px;">memcached</a> <a href="/tags/mongodb/" style="font-size: 14.76px;">mongodb</a> <a href="/tags/mysql/" style="font-size: 17.14px;">mysql</a> <a href="/tags/netty/" style="font-size: 10.95px;">netty</a> <a href="/tags/nginx/" style="font-size: 14.29px;">nginx</a> <a href="/tags/project/" style="font-size: 10.48px;">project</a> <a href="/tags/python/" style="font-size: 19.05px;">python</a> <a href="/tags/redis/" style="font-size: 17.14px;">redis</a> <a href="/tags/rpc/" style="font-size: 10.48px;">rpc</a> <a href="/tags/rsync/" style="font-size: 10px;">rsync</a> <a href="/tags/scala/" style="font-size: 17.62px;">scala</a> <a href="/tags/scala函数式编程/" style="font-size: 11.9px;">scala函数式编程</a> <a href="/tags/scala编程/" style="font-size: 15.71px;">scala编程</a> <a href="/tags/shell/" style="font-size: 17.62px;">shell</a> <a href="/tags/socket/" style="font-size: 11.9px;">socket</a> <a href="/tags/spark/" style="font-size: 20px;">spark</a> <a href="/tags/sqoop/" style="font-size: 10.95px;">sqoop</a> <a href="/tags/storm/" style="font-size: 15.24px;">storm</a> <a href="/tags/zookeeper/" style="font-size: 16.19px;">zookeeper</a> <a href="/tags/数据仓库/" style="font-size: 11.43px;">数据仓库</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/01/">January 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/">Tachyon</a>
          </li>
        
          <li>
            <a href="/2017/04/30/数据仓库/数据仓库2/">数据仓库</a>
          </li>
        
          <li>
            <a href="/2017/04/29/IDEA/IDEA/">IDEA</a>
          </li>
        
          <li>
            <a href="/2017/04/29/数据仓库/ETL/">ETL</a>
          </li>
        
          <li>
            <a href="/2017/04/28/数据仓库/PowderDesigner/">PowderDesigner的使用</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Mr. Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>