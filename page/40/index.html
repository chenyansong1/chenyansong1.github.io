<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/40/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"right","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/40/"/>





  <title> Chen's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-right 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一个技术渣的自说自话</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocapitalize="off" autocomplete="off" autocorrect="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark内核解密/spark的job/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark内核解密/spark的job/" itemprop="url">
                  spark的job
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="job"><a href="#job" class="headerlink" title="job"></a>job</h1><p>在一个Executor中一次性最多能够运行多少并发的Task取决于当前Executor能够使用的Cores数量</p>
<p>如果有88个文件,而每个文件的大小小于128M,那么将会有88个Partition,所以会启动88个task,而由于数据本地性,假如有的机器上有54个文件,那么在该机器上会启动54个task</p>
<p>如果在进行cache时,cache的数据放置在哪台机器上,那么后续的操作会在那台机器上进行,这就是数据本地性</p>
<p><img src="/assert/img/bigdata/spark内核解密/job的执行过程.png" alt=""></p>
<h1 id="rdd的依赖关系"><a href="#rdd的依赖关系" class="headerlink" title="rdd的依赖关系"></a>rdd的依赖关系</h1><p><img src="/assert/img/bigdata/spark内核解密/stage.png" alt=""></p>
<p>窄依赖:每个父RDD的Partition最多被子RDD的一个Partition所使用;例如map,filter<br>宽依赖:多个子RDD的Partition会依赖同一个父RDD的Partition;例如groupByKey,reduceByKey</p>
<p>特别说明:对join操作有两种情况,如果说join操作的使用每个Partition仅仅和已知的Partition进行join,这次是join操作就是窄依赖</p>
<p>每个stage里面的Task的数量是由该stage中最后一个RDD的Partition的数量决定的<br>从后往前推理,遇到宽依赖就断开,遇到窄依赖就把当前的RDD加入到该stage中</p>
<p>最后一个stage里面的任务类型是ResultTask,前面其他所有的Stage里面的任务的类型为ShuffleMapTask</p>
<p>hadoop中的MapReduce中的Mapper和Reducer在spark中等量的算子是:map和reduceByKey</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark内核解密/spark内核架构/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark内核解密/spark内核架构/" itemprop="url">
                  spark内核架构
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p><img src="/assert/img/bigdata/spark内核解密/spark内核解密.png" alt=""></p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2017/04/16/bigdata/spark内核解密/spark内核架构/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark内核解密/spark RDD的持久化,广播,累加器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark内核解密/spark RDD的持久化,广播,累加器/" itemprop="url">
                  spark RDD的持久化,广播,累加器
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="常见的Action"><a href="#常见的Action" class="headerlink" title="常见的Action"></a>常见的Action</h1><p>凡是Action级别的操作都会触发:sc.runJob</p>
<p>reduce<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">val numbers = sc.parallelize(1 to 100)</div><div class="line">numbers.reduce(_+_)	//会将上一次计算的结果作为下一次的第一个参数</div><div class="line"></div><div class="line"></div><div class="line">def reduce(f: (T, T) =&gt; T): T = withScope &#123;</div><div class="line">	//....</div><div class="line">	sc.runJob(this, reducePartition, mergeResult)</div><div class="line">	jobResult.getOrElse(throw new UnsupportedOperationException(&quot;empty collection&quot;))</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>collect</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">numbers.collect		</div><div class="line"></div><div class="line">def collect(): Array[T] = withScope &#123;</div><div class="line">	val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray)</div><div class="line">	Array.concat(results: _*)</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p><img src="/assert/img/bigdata/spark内核解密/collect工作机制.png" alt=""></p>
<p>count</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">val numbers = sc.parallelize(1 to 100)</div><div class="line">numbers.count	</div><div class="line"></div><div class="line">/*</div><div class="line">//Return the number of elements in the RDD.</div><div class="line">def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum</div><div class="line">*/</div></pre></td></tr></table></figure>
<p>take:取结果的一批元素<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">val numbers = sc.parallelize(1 to 10)</div><div class="line">val arr = numbers.map(_*2).take(5)</div><div class="line">arr.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">2</div><div class="line">4</div><div class="line">6</div><div class="line">8</div><div class="line">10</div><div class="line"> */</div><div class="line"></div><div class="line">源码:</div><div class="line">  def take(num: Int): Array[T] = withScope &#123;</div><div class="line">    if (num == 0) &#123;</div><div class="line">      new Array[T](0)</div><div class="line">    &#125; else &#123;</div><div class="line">		//......</div><div class="line">        val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p)</div><div class="line"></div><div class="line">        res.foreach(buf ++= _.take(num - buf.size))</div><div class="line">        partsScanned += numPartsToTry</div><div class="line">      &#125;</div><div class="line">      buf.toArray</div><div class="line">    &#125;</div><div class="line">  &#125;</div></pre></td></tr></table></figure></p>
<p>countByKey:统计Tuple中key的次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">val numbers = sc.parallelize(Seq(1,2,3,5,1,3,5))</div><div class="line">val rdd = numbers.map((_,1)).countByKey()</div><div class="line">rdd.foreach(println)</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">(1,2)</div><div class="line">(3,2)</div><div class="line">(5,2)</div><div class="line">(2,1)</div><div class="line"></div><div class="line">*/</div><div class="line"></div><div class="line">//源码:</div><div class="line">  def countByKey(): Map[K, Long] = self.withScope &#123;</div><div class="line">    self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap</div><div class="line">  &#125;</div><div class="line"></div><div class="line">因为因countByKey也是进行了collect,而collect是Action,所以countByKey也是Action</div></pre></td></tr></table></figure></p>
<p>saveAsTextFile<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">在源码中有这样的一句:self.context.runJob(self, writeToFile)</div></pre></td></tr></table></figure></p>
<h1 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h1><p>1.某步骤计算特别耗时<br>2.计算链条特别长的情况<br>3.checkpoint所在的RDD也一定要持久化数据(checkpoint之前persist或cache)<br>4.Shuffle之后要进行persist(因为Shuffle要进行网络传输,如果失败,数据丢失,那么又要进行网络传输)<br>5.Shuffle之前(框架默认帮我们把数据持久化到磁盘)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">/** Persist this RDD with the default storage level (`MEMORY_ONLY`).默认是内存中缓存 */</div><div class="line">def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</div><div class="line"></div><div class="line">/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */</div><div class="line">def cache(): this.type = persist()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line">几种持久化的方式</div><div class="line"></div><div class="line">object StorageLevel &#123;</div><div class="line"> 	 //不持久化</div><div class="line">  val NONE = new StorageLevel(false, false, false, false)</div><div class="line"></div><div class="line">	//持久化到磁盘2分</div><div class="line">  val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)</div><div class="line">  val DISK_ONLY = new StorageLevel(true, false, false, false)</div><div class="line"></div><div class="line"></div><div class="line">	//只是缓存到内存</div><div class="line">  val MEMORY_ONLY = new StorageLevel(false, true, false, true)</div><div class="line">  val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)</div><div class="line"></div><div class="line">	//缓存到内存并序列化,这样存储就会小,但是反序列化的时候,耗CPU</div><div class="line">  val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)</div><div class="line">  val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)</div><div class="line">	</div><div class="line">	//优先考虑内存,然后再写入到磁盘,这样防止内存溢出</div><div class="line">  val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)</div><div class="line">  val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)</div><div class="line"></div><div class="line">  val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)</div><div class="line">  val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)</div><div class="line">  val OFF_HEAP = new StorageLevel(false, false, true, false)</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//上述情况可以看到,有些情况下有2分副本,为什么要有两份?</div><div class="line">因为如果一份内存崩溃掉了,那么另外一份可以立即顶上,虽然2分副本占用了空间,但这就是使用空间(2份副本)换时间</div></pre></td></tr></table></figure>
<p>cache:是persist的一种特殊情况</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cache实际上是调用的是下面的persist:即只是缓存在内存中,并且只是缓存一份</div><div class="line">def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</div></pre></td></tr></table></figure>
<p>执行的效果说明:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\test.txt&quot;)</div><div class="line">val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).cache</div><div class="line">rdd2.collect.foreach(println)</div><div class="line"></div><div class="line">下面是连续2次执行上述代码所用的时间:</div><div class="line">//第一次</div><div class="line">17/03/24 14:41:55 INFO DAGScheduler: Job 0 finished: collect at MakeActionRDD.scala:30, took 1.117844 s</div><div class="line"></div><div class="line">//第二次</div><div class="line">17/03/24 14:42:46 INFO DAGScheduler: Job 0 finished: collect at MakeActionRDD.scala:30, took 0.600663 s</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">cache之后一定不能立即有其他算子</div><div class="line">val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).cache.count</div><div class="line">是没有对计算结果缓存</div></pre></td></tr></table></figure></p>
<p>cache从内存中清除<br>1.使用unpersist,强制从内存中去掉</p>
<h1 id="Spark广播"><a href="#Spark广播" class="headerlink" title="Spark广播"></a>Spark广播</h1><p>广播的应用场景:<br>1.大变量: 每次task在执行任务的时候都要拷贝数据副本,因为函数式编程,即变量不变val,因为要将变量拷贝一份到task中,一个Executor保存一份,Executor中所有的task只读共享这个大变量<br>广播是由Driver发给当前Application分配的所有Executor内存级别的全局只读变量,Executor中的线程池中共享该全局变量,极大的减少了网络传输,否则的话每个task都要传输一次该变量,并极大的节省了内存,当也隐式的提高了CPU的有效工作</p>
<p>图示广播:</p>
<p><img src="/assert/img/bigdata/spark内核解密/广播.png" alt=""></p>
<p>代码:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">val number = 10</div><div class="line">val broadcastNumber = sc.broadcast(number)</div><div class="line"></div><div class="line">val dataRdd = sc.parallelize(1 to 100)</div><div class="line">val dataMap = dataRdd.map(_ * broadcastNumber.value)</div><div class="line"></div><div class="line">dataMap.collect.foreach(println)</div></pre></td></tr></table></figure>
<h1 id="Spark累加器"><a href="#Spark累加器" class="headerlink" title="Spark累加器"></a>Spark累加器</h1><p>Accumulator:对于Executor只能修改,但不可读,只对Driver可读,在记录集群的状态,尤其是全局唯一的状态的时候很重要,即一个作业中的所有的Executor共享</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">val sum = sc.accumulator(0)</div><div class="line">//sc.accumulator(0,&quot;test_Acc&quot;) 指定Acc的名称</div><div class="line"></div><div class="line">val dataRdd = sc.parallelize(1 to 100)</div><div class="line">dataRdd.foreach(sum += _)</div><div class="line">println(sum)</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark内核解密/HA下的spark集群工作原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark内核解密/HA下的spark集群工作原理/" itemprop="url">
                  HA下的spark集群工作原理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/assert/img/bigdata/spark内核解密/ha-spark.png" alt=""></p>
<h1 id="添加zk来解决master的单点问题"><a href="#添加zk来解决master的单点问题" class="headerlink" title="添加zk来解决master的单点问题"></a>添加zk来解决master的单点问题</h1><p>到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：<br>Spark集群规划：node1，node2是Master；node3，node4，node5是Worker  , 安装配置zk集群，并启动zk集群, 停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&quot;</div></pre></td></tr></table></figure></p>
<ol>
<li>在node1节点上修改slaves配置文件内容指定worker节点</li>
<li>在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master</li>
</ol>
<p>启动集群的时候,我们需要向zk要master<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--master spark://node1:7077,node2:7077</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/基于Yarn的两种提交模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/基于Yarn的两种提交模式/" itemprop="url">
                  基于Yarn的两种提交模式
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>spark的三种提交模式<br>1.Standalone模式,基于spark自己的Master-Worker集群<br>2.基于Yarn的yarn-cluster模式<br>3.基于Yarn的yarn-client模式</p>
<p>在我们之前提交的spark应用程序的spark-submit脚本,加上–master参数,设置yarn-cluster,或yarn-client,即可,如果你没有设置,那么就是Standalone模式,同时需要在spark-env.sh中补充配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export HADOOP_HOME=XXXXX</div></pre></td></tr></table></figure></p>
<p>yarn-cluster和yarn-client的提交模式</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/基于yarn的两种提交模式.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/共享变量/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/共享变量/" itemprop="url">
                  共享变量
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>默认情况下,如果一个算子的函数中使用到了某个外部的变量,那么这个变量的值会被拷贝到每个task中,此时每个task只能操作自己的那份变量副本,如果多个task想要共享某个变量,那么这种方式是做不到的</p>
<p>spark为此提供了两种共享变量,一种是Broadcast Variable(广播变量),另一种是Accumulator(累加变量),Broadcast Variable会将使用到的变量,仅仅为每个节点拷贝一份,更大的作用是优化性能,减少网络传输以及内存消耗,主要用于共享读</p>
<p>Accumulator则可以让多个task共同操作同一份变量,主要可以进行累加操作</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2017/04/16/bigdata/spark从入门到精通_笔记/共享变量/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/wordcount程序原理解析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/wordcount程序原理解析/" itemprop="url">
                  wordcount程序原理解析
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>程序如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">val conf = new SparkConf().setAppName(&quot;WordCount&quot;)</div><div class="line">val sc = new SparkContext(conf)</div><div class="line">val linesRdd = sc.textFile(&quot;/etc/hosts&quot;)</div><div class="line">val wordsRdd = linesRdd.flatMap(_.split(&quot; &quot;))</div><div class="line">val pairsRdd = wordRdd.map((_,1))</div><div class="line">val wordCountRdd = pairsRdd.reduceByKey(_+_)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">//打印</div><div class="line">wordCountRdd.foreach(println)</div></pre></td></tr></table></figure></p>
<p>下面的图是通过RDD的产生流程进行解析的</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/wordcount程序原理解析.png" alt=""></p>
<p>下面的图是通过源码的角度进行解析的</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/wordcount的运行原理及源码解读.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的排序/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的排序/" itemprop="url">
                  spark的排序
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>这里使用的是sortByKey对Tuple按照key进行排序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">val lineRdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">val reducedRdd = lineRdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)</div><div class="line"></div><div class="line">//将排序的key转换到Tuple的key列</div><div class="line">val countWords = reducedRdd.map(count=&gt;(count._2,count._1))</div><div class="line"></div><div class="line">val sortedRdd = countWords.sortByKey(false)</div><div class="line"></div><div class="line">//重新组织</div><div class="line">val result = sortedRdd.map(sort=&gt;(sort._2,sort._1))</div><div class="line"></div><div class="line">result.foreach(println)</div><div class="line"></div><div class="line">/*执行结果:</div><div class="line">(spark,19)</div><div class="line">(hadoop,13)</div><div class="line">(88,6)</div><div class="line">(100,6)</div><div class="line">(56,6)</div><div class="line">(22,4)</div><div class="line">(33,4)</div><div class="line">(99,4)</div><div class="line">(94,2)</div><div class="line">(94spark,1)</div><div class="line"> */</div></pre></td></tr></table></figure>
<p>二次排序<br>也是使用sortByKey,只不过此时key为我们自定义的Bean作为key来进行排序,而Bean中有比较的方法<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">class SecondarySortKey(val first:Int, val second: Int ) extends Ordered[SecondarySortKey] with Serializable&#123;</div><div class="line">  def compare(other: SecondarySortKey): Int = &#123;</div><div class="line">    if(this.first - other.first !=0)&#123;</div><div class="line">      this.first - other.first</div><div class="line">    &#125;else&#123;</div><div class="line">      this.second - other.second</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">object SecondarySortKey&#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    val line = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">    /*xx.txt</div><div class="line">    1 11</div><div class="line">    2 22</div><div class="line">    3 33</div><div class="line">    2 11</div><div class="line">    1 22</div><div class="line">     */</div><div class="line">    val pairWithSortKey = line.map(line=&gt;&#123;</div><div class="line">      val arr = line.split(&quot; &quot;)</div><div class="line">      val first = arr(0).toInt</div><div class="line">      val second = arr(1).toInt</div><div class="line">      (new SecondarySortKey(first,second), line)  //指定Tuple的key为SecondarySortKey</div><div class="line">    &#125;).sortByKey(false).map(pair=&gt;pair._2)//sortByKey排序的时候会以SecondarySortKey为key排序</div><div class="line"></div><div class="line">    pairWithSortKey.collect.foreach(println)</div><div class="line">    /*</div><div class="line">    打印结果:</div><div class="line">    3 33</div><div class="line">    2 22</div><div class="line">    2 11</div><div class="line">    1 22</div><div class="line">    1 11</div><div class="line">     */</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //在实际的生成中,我们是封装函数来进行逻辑的组织</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line"></div><div class="line">    //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>其实对基本类型spark有对基本类型的比较方法的实现,所以不用我们实现比较方法也能实现排序</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/" itemprop="url">
                  spark的常见的rdd
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h1><ul>
<li>使用程序中的集合创建rdd<br>主要用于进行测试,可以在实际部署到集群运行之前,自己使用集合构造册数数据,来测试后面的spark应用的流程</li>
<li>使用本地文件创建rdd</li>
<li>使用HDFS文件创建rdd<br>主要可以针对HDFS上存储的大数据,进行离线批处理操作</li>
</ul>
<p>使用程序中的集合创建rdd</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">object MakeRDD &#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line"></div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    test(sc)</div><div class="line">    sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def test(sc:SparkContext)=&#123;</div><div class="line">    val numberRdd = sc.parallelize(Seq(1,2,3,8,22))</div><div class="line">    val reducedRdd = numberRdd.reduce(_+_)</div><div class="line">    println(reducedRdd)</div><div class="line"></div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //构建SparkContext</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line"></div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>使用本地文件创建rdd和HDFS文件创建rdd<br>1.如果是针对本地文件的话,如果是在Windows上本地测试,Windows上有一份文件即可;如果是在spark集群上针对linux本地文件,那么需要将文件拷贝到所有worker节点上<br>2.spark的textFile方法支持针对目录,压缩文件以及通配符进行rdd的创建<br>3.spark默认会为HDFS的每一个block创建一个partition,但是也可以通过textFile()的第二个参数手动设置分区数量,只能比block数量多,不能比block数量少</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">object MakeRDD &#123;</div><div class="line">  def main(args: Array[String]): Unit = &#123;</div><div class="line">    System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\Users\\Administrator\\Desktop\\hadoop\\&quot;)</div><div class="line"></div><div class="line">    val sc = sparkContext(&quot;Transformation Operations&quot;)</div><div class="line">    test(sc)</div><div class="line">    sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  def test(sc:SparkContext)=&#123;</div><div class="line">    val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">    val rddreuslt = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)</div><div class="line">    rddreuslt.foreach(println)</div><div class="line">  &#125;</div><div class="line"></div><div class="line">  //在实际的生成中,我们是封装函数来进行逻辑的组织</div><div class="line">  def sparkContext(name:String)=&#123;</div><div class="line">    val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;)</div><div class="line">    val sc = new SparkContext(conf)</div><div class="line"></div><div class="line">    sc</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line">//如果是使用HDFS文件创建rdd,只要把textFile的文件路径修改为HDFS文件路径即可</div><div class="line">val rdd = sc.textFile(&quot;hdfs://spark1:9000/xx.txt&quot;)</div><div class="line"></div><div class="line">//如果是在集群中运行的时候,那么需要将.setMaster(&quot;local&quot;)去掉</div></pre></td></tr></table></figure>
<h1 id="Transformation和action"><a href="#Transformation和action" class="headerlink" title="Transformation和action"></a>Transformation和action</h1><p>spark支持两种rdd操作:Transformation和action,Transformation操作会针对已有的rdd创建一个新的rdd,而action则主要是对rdd进行最后的操作,比如遍历,reduce,保存到文件等,并可以返回结果给Driver程序</p>
<p>例如map就是一种Transformation操作,他用于将已有的rdd的每个元素传入一个自定义的函数,并获取一个新的元素,然后将所有的新元素组成一个新的rdd;而reduce就是一种action操作,它用于对rdd中的所有元素进行聚合操作,并获取一个最终的结果,然后返回给Driver程序</p>
<p>Transformation的特点就是lazy特性,lazy特性的指的是:如果一个spark应用中只定义了Transformation操作,那么即使你执行该应用,这些操作也不会执行,也就是说,Transformation是不会触发spark程序的执行的,他只是记录了对rdd的所作的操作,但是不会自发的执行,只有当Transformation之后,接着执行一个action操作,那么所有的Transformation才会被执行,spark通过这种lazy特性,来进行底层的spark应用执行的优化,避免产生过多的中间结果</p>
<p>action操作执行,会触发一个spark job的运行,从而触发这个action之前所有的Transformation的执行,这是action的特性</p>
<p>综上:Transformation会产生rdd,而action会产生结果而不是rdd</p>
<p>下图是程序提交的流程上来反映spark的Transformation的lazy特性</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/Transformation_action.png" alt=""></p>
<p>案例:统计文件中每行出现的次数<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">val rdd = sc.textFile(&quot;C:\\Users\\Administrator\\Desktop\\xx.txt&quot;)</div><div class="line">var reducedByKeyRdd = rdd.map((_,1)).reduceByKey(_+_)</div><div class="line">reducedByKeyRdd.foreach(println)</div></pre></td></tr></table></figure></p>
<p>常用的Transformation介绍</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">map</td>
<td style="text-align:left">将rdd中的每个元素传入自定义函数,获取一个新的元素,然后用新的元素组成的新的rdd</td>
</tr>
<tr>
<td style="text-align:left">filter</td>
<td style="text-align:left">对RDD中的每一个元素进行判断,如果返回true就保留</td>
</tr>
<tr>
<td style="text-align:left">flatMap</td>
<td style="text-align:left">与map类似,大那是对每个元素都可以返回一个或多个新元素,然后对所有的元素flat</td>
</tr>
<tr>
<td style="text-align:left">groupByKey</td>
<td style="text-align:left">根据key进行分组,每个key对应一个Iterator<value></value></td>
</tr>
<tr>
<td style="text-align:left">reduceByKey</td>
<td style="text-align:left">对每个key对应的value进行reduce操作</td>
</tr>
<tr>
<td style="text-align:left">sortByKey</td>
<td style="text-align:left">对每个key对应的value进行排序操作</td>
</tr>
<tr>
<td style="text-align:left">join</td>
<td style="text-align:left">对两个包含<key,value>对的rdd进行join操作,每个keyjoin上的pair,都会传入自定义函数进行处理</key,value></td>
</tr>
<tr>
<td style="text-align:left">cogroup</td>
<td style="text-align:left">同join,但是是每个key对应的Iterable<value>都会传入自定义的函数进行处理</value></td>
</tr>
</tbody>
</table>
<p>常用的action操作</p>
<table>
<thead>
<tr>
<th style="text-align:left">操作</th>
<th style="text-align:left">介绍</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">reduce</td>
<td style="text-align:left">将rdd中的所有元素进行聚合操作,第一个和第二个元素聚合产生的值,和第三个元素聚合,产生的值和第四个元素聚合,一次类推</td>
</tr>
<tr>
<td style="text-align:left">collect</td>
<td style="text-align:left">将rdd中的所有元素获取到本地客户端</td>
</tr>
<tr>
<td style="text-align:left">count</td>
<td style="text-align:left">获取rdd元素总数</td>
</tr>
<tr>
<td style="text-align:left">take(n)</td>
<td style="text-align:left">后rdd前n个元素</td>
</tr>
<tr>
<td style="text-align:left">saveAsTextFile</td>
<td style="text-align:left">将rdd元素保存到文件中,对每个元素调用toString方法</td>
</tr>
<tr>
<td style="text-align:left">countByKey</td>
<td style="text-align:left">对每个key对应的值进行count计数</td>
</tr>
<tr>
<td style="text-align:left">foreach</td>
<td style="text-align:left">遍历rdd中的每一个元素</td>
</tr>
</tbody>
</table>
<h1 id="Transformation实例"><a href="#Transformation实例" class="headerlink" title="Transformation实例"></a>Transformation实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div></pre></td><td class="code"><pre><div class="line">#map:将集合中每个元素乘以2</div><div class="line">val numberRdd = sc.parallelize(1 to 7)</div><div class="line">val resultRdd = numberRdd.map(_*2)</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">2</div><div class="line">4</div><div class="line">6</div><div class="line">8</div><div class="line">10</div><div class="line">12</div><div class="line">14</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#filter:过滤出集合中的偶数</div><div class="line">val numberRdd = sc.parallelize(1 to 7)</div><div class="line">val resultRdd = numberRdd.filter(_%2==0)</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">2</div><div class="line">4</div><div class="line">6</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#flatMap:将行拆分为单词</div><div class="line">val linesRdd = sc.parallelize(Seq(&quot;zhangsna 88&quot;,&quot;lisi 99&quot;))</div><div class="line">val resultRdd = linesRdd.flatMap(_.split(&quot; &quot;))</div><div class="line"></div><div class="line">resultRdd.foreach(println)</div><div class="line">/*打印结果:</div><div class="line">zhangsna</div><div class="line">88</div><div class="line">lisi</div><div class="line">99</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#groupByKey:将每个班级的成绩进行分组</div><div class="line"></div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))</div><div class="line">val resultRdd = linesRdd.groupByKey()//返回: RDD[(K, Iterable[V])]</div><div class="line">resultRdd.foreach&#123;</div><div class="line">  score=&gt;&#123;</div><div class="line">    print(score._1+&quot; :&quot;)</div><div class="line"></div><div class="line">	//println(score._2.toList)</div><div class="line">    score._2.foreach(sco=&gt;print(sco.toString + &quot; &quot;))</div><div class="line">    println</div><div class="line">  &#125;</div><div class="line"></div><div class="line">&#125;</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">cls1 :80 82</div><div class="line">cls2 :88 98</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#reduceByKey:统计每个班级的总分</div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))</div><div class="line">val resultRdd = linesRdd.reduceByKey(_+_)</div><div class="line">resultRdd.foreach(println)</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">(cls1,162)</div><div class="line">(cls2,186)</div><div class="line"> */</div><div class="line"></div><div class="line">#sortByKey:将学生分数进行排序</div><div class="line"></div><div class="line">val linesRdd = sc.parallelize(Seq((&quot;zhangsna&quot;,80),(&quot;lisi&quot;,88),(&quot;wangwu&quot;,82),(&quot;zhaoliu&quot;,98)))</div><div class="line"></div><div class="line">//要将key放在tuple2的第一个位置,这就是第一个map的作用,而最后一个map的作用就是调整打印的顺序</div><div class="line">val resultRdd = linesRdd.map(t=&gt;(t._2,t._1)).sortByKey(false).map(t=&gt;(t._2,t._1))</div><div class="line">resultRdd.foreach(println)</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">(zhaoliu,98)</div><div class="line">(lisi,88)</div><div class="line">(wangwu,82)</div><div class="line">(zhangsna,80)</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line">#join:打印每个学生的成绩</div><div class="line"></div><div class="line">val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(3,82)))</div><div class="line">val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(3,&quot;wangwu&quot;)))</div><div class="line"></div><div class="line">val joinedRdd = scoreRdd.join(studRdd)</div><div class="line">/*</div><div class="line">def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]</div><div class="line">返回的是:(K, (V, W))</div><div class="line"> */</div><div class="line">joinedRdd.foreach&#123;</div><div class="line">  t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取</div><div class="line">    val snu = t._1</div><div class="line">    val (score,name) = t._2</div><div class="line">    println(name + &quot;:&quot; + score + &quot;:&quot; + snu)</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line"></div><div class="line">/*打印结果:</div><div class="line">  zhangsna:80:1</div><div class="line">  wangwu:82:3</div><div class="line">  lsii:88:2</div><div class="line"> */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#cogroup</div><div class="line"></div><div class="line">    val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(1,80),(2,88),(3,82)))</div><div class="line">    val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(1,&quot;zhangsna2&quot;),(3,&quot;wangwu&quot;)))</div><div class="line"></div><div class="line">    val joinedRdd = scoreRdd.cogroup(studRdd)</div><div class="line">    /*</div><div class="line">    def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))]</div><div class="line">    返回的是:(K, (Iterable[V], Iterable[W]))</div><div class="line">     */</div><div class="line">    joinedRdd.foreach&#123;</div><div class="line">      t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取</div><div class="line">        val snu = t._1</div><div class="line">        val (score,name) = t._2</div><div class="line">        println(snu + &quot;: &quot; + score.toList.toString + &quot;  &quot; + name.toList.toString)</div><div class="line">      &#125;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    /*打印结果:</div><div class="line">      1: List(80, 80)  List(zhangsna, zhangsna2)</div><div class="line">      3: List(82)  List(wangwu)</div><div class="line">      2: List(88, 88)  List(lsii)</div><div class="line">     */</div></pre></td></tr></table></figure>
<h1 id="action实例"><a href="#action实例" class="headerlink" title="action实例"></a>action实例</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div></pre></td><td class="code"><pre><div class="line">#reduce操作</div><div class="line"> val scoreRdd = sc.parallelize(Seq(1,2,3,4,5))</div><div class="line"> val result = scoreRdd.reduce(_+_)</div><div class="line"> println(result)</div><div class="line"></div><div class="line"> /*打印结果:</div><div class="line"> 15</div><div class="line">  */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#collect</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">//使用collect操作将分布在远程的数据拉取到本地,对大数据量不要这么做,测试可以,因为可能造成本地内存溢出,还可能因为将远程的数据拉倒本地,走网络的话,性能会很差</div><div class="line">val result = scoreRdd.collect</div><div class="line">result.foreach(println)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#count</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">val result = scoreRdd.count</div><div class="line">println(result)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#take</div><div class="line"></div><div class="line"> val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line"> //从远程获取指定数量的数据,返回:Array[T]</div><div class="line"> val result = scoreRdd.take(3)</div><div class="line"> result.foreach(println)</div><div class="line"> </div><div class="line"> /*结果打印:</div><div class="line"> 1</div><div class="line"> 2</div><div class="line"> 3</div><div class="line">  */</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#saveAsTextFile</div><div class="line">val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)</div><div class="line">//从远程获取指定数量的数据,返回:Array[T]</div><div class="line">val result = scoreRdd.saveAsTextFile(&quot;C:\\Users\\Administrator\\Desktop\\hadoop\\result&quot;)</div><div class="line"></div><div class="line">/*结果:</div><div class="line">在C:\\Users\\Administrator\\Desktop\\hadoop\\result目录下,有下面的文件:</div><div class="line"></div><div class="line">._SUCCESS.crc</div><div class="line">.part-00000.crc</div><div class="line">.part-00001.crc</div><div class="line">.part-00002.crc</div><div class="line">_SUCCESS</div><div class="line">part-00000</div><div class="line">part-00001</div><div class="line">part-00002</div><div class="line"></div><div class="line">因为在parallelize的指定的分区为3,所以会生成3个part,其中的在</div><div class="line">part-00000文件中存在的数据:1</div><div class="line">part-00000文件中存在的数据:2\n3</div><div class="line">part-00000文件中存在的数据:4\n5\n6</div><div class="line"></div><div class="line">*/</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">#countByKey</div><div class="line"> val scoreRdd = sc.parallelize(Seq((&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls3&quot;,&quot;zhangsan3&quot;),(&quot;cls2&quot;,&quot;zhangsan2&quot;)),3)</div><div class="line"></div><div class="line"> val result = scoreRdd.countByKey() //返回:Map[K, Long]</div><div class="line"> for((k,v)&lt;-result)&#123;</div><div class="line">   println(k+&quot;:&quot;+v.toString)</div><div class="line"> &#125;</div><div class="line"> /*结果打印:</div><div class="line"> cls2:1</div><div class="line"> cls3:1</div><div class="line"> cls1:3</div><div class="line">  */</div></pre></td></tr></table></figure>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/" itemprop="url">
                  spark的基本工作原理
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>下面的这张图表示的是spark的基本的工作原理图(简图)<br><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark的基本工作原理.png" alt=""></p>
<p>下面是对RDD的概念解释<br>1.RDD在抽象上来说是一种元素的集合,包含了数据,他是被分区的,分为多个分区,每个分区分布在集群中的不同节点上,从而让RDD中的数据可以被并行操作(分布式数据集)<br>2.RDD的创建:通过HDFS文件或hive表创建;通过应用程序的集合来创建<br>3.RDD的数据默认情况下存放在内存中,但是在内存资源不足时,spark会自动将RDD数据写入磁盘(弹性)<br>4.RDD最重要的特性是:提供了容错性,可以自动从节点失败中恢复过来,即:如果某个节点山的RDD partition因为节点故障,导致数据丢了,那么RDD会自动通过自己的数据来源重新计算该partition,这一切对使用者是透明的<br><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/rdd的概念理解.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/39/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/39/">39</a><span class="page-number current">40</span><a class="page-number" href="/page/41/">41</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/41/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/header.jpg"
               alt="Mr. Chen" />
          <p class="site-author-name" itemprop="name">Mr. Chen</p>
           
              <p class="site-description motion-element" itemprop="description">一个技术渣的自说自话</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">576</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">30</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">37</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr. Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>





  

  

  

  

  

</body>
</html>
