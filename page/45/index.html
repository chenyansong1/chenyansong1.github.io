<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="一个技术渣的自说自话">
<meta property="og:type" content="website">
<meta property="og:title" content="Chen's Blog">
<meta property="og:url" content="http://yoursite.com/page/45/index.html">
<meta property="og:site_name" content="Chen's Blog">
<meta property="og:description" content="一个技术渣的自说自话">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chen's Blog">
<meta name="twitter:description" content="一个技术渣的自说自话">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"right","display":"always","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/page/45/"/>





  <title> Chen's Blog </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container sidebar-position-right 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Chen's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">一个技术渣的自说自话</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/" itemprop="url">
                  spark core之新闻网站关键指标离线统计
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h1><p>新闻网站<br>1、版块<br>2、新闻页面<br>3、新用户注册<br>4、用户跳出</p>
<p>案例需求分析</p>
<p>每天每个页面的PV：PV是Page View，是指一个页面被所有用户访问次数的总和，页面被访问一次就被记录1次PV<br>每天每个页面的UV：UV是User View，是指一个页面被多少个用户访问了，一个用户访问一次是1次UV，一个用户访问多次还是1次UV<br>新用户注册比率：当天注册用户数 / 当天未注册用户数<br>用户跳出率：IP只浏览了一个页面就离开网站的次数/网站总访问数（PV）<br>版块热度排行榜：根据每个版块每天被访问的次数，做出一个排行榜</p>
<h1 id="网站日志格式"><a href="#网站日志格式" class="headerlink" title="网站日志格式"></a>网站日志格式</h1><p>date timestamp userid pageid section action </p>
<p>日志字段说明<br>date: 日期，yyyy-MM-dd格式<br>timestamp: 时间戳<br>userid: 用户id<br>pageid: 页面id<br>section: 新闻版块<br>action: 用户行为，两类，点击页面和注册</p>
<p>模拟数据生成程序<br>模式数据演示</p>
<h1 id="创建hive表和数据导入"><a href="#创建hive表和数据导入" class="headerlink" title="创建hive表和数据导入"></a>创建hive表和数据导入</h1><p>在hive中创建访问日志表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">create table news_access (</div><div class="line">  date string,</div><div class="line">  timestamp bigint,</div><div class="line">  userid bigint,</div><div class="line">  pageid bigint,</div><div class="line">  section string,</div><div class="line">  action string)</div></pre></td></tr></table></figure></p>
<p>将模拟数据导入hive表中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">load data local inpath &apos;/usr/local/test/news_access.log&apos; into table news_access;</div></pre></td></tr></table></figure></p>
<h1 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div><div class="line">142</div><div class="line">143</div><div class="line">144</div><div class="line">145</div><div class="line">146</div><div class="line">147</div><div class="line">148</div><div class="line">149</div><div class="line">150</div><div class="line">151</div><div class="line">152</div><div class="line">153</div><div class="line">154</div><div class="line">155</div><div class="line">156</div><div class="line">157</div><div class="line">158</div><div class="line">159</div><div class="line">160</div><div class="line">161</div><div class="line">162</div><div class="line">163</div><div class="line">164</div><div class="line">165</div><div class="line">166</div><div class="line">167</div><div class="line">168</div><div class="line">169</div><div class="line">170</div><div class="line">171</div><div class="line">172</div><div class="line">173</div><div class="line">174</div><div class="line">175</div><div class="line">176</div><div class="line">177</div><div class="line">178</div><div class="line">179</div><div class="line">180</div><div class="line">181</div><div class="line">182</div><div class="line">183</div><div class="line">184</div><div class="line">185</div><div class="line">186</div><div class="line">187</div><div class="line">188</div><div class="line">189</div><div class="line">190</div><div class="line">191</div><div class="line">192</div><div class="line">193</div><div class="line">194</div><div class="line">195</div><div class="line">196</div><div class="line">197</div><div class="line">198</div><div class="line">199</div><div class="line">200</div><div class="line">201</div><div class="line">202</div><div class="line">203</div><div class="line">204</div><div class="line">205</div><div class="line">206</div><div class="line">207</div><div class="line">208</div><div class="line">209</div><div class="line">210</div><div class="line">211</div><div class="line">212</div><div class="line">213</div><div class="line">214</div><div class="line">215</div><div class="line">216</div><div class="line">217</div><div class="line">218</div><div class="line">219</div><div class="line">220</div><div class="line">221</div><div class="line">222</div><div class="line">223</div><div class="line">224</div><div class="line">225</div><div class="line">226</div><div class="line">227</div><div class="line">228</div><div class="line">229</div><div class="line">230</div><div class="line">231</div><div class="line">232</div><div class="line">233</div><div class="line">234</div><div class="line">235</div><div class="line">236</div><div class="line">237</div><div class="line">238</div><div class="line">239</div><div class="line">240</div><div class="line">241</div><div class="line">242</div><div class="line">243</div><div class="line">244</div><div class="line">245</div><div class="line">246</div><div class="line">247</div><div class="line">248</div><div class="line">249</div><div class="line">250</div><div class="line">251</div><div class="line">252</div><div class="line">253</div><div class="line">254</div></pre></td><td class="code"><pre><div class="line">package cn.spark.study.sql.upgrade.news;</div><div class="line"></div><div class="line">import java.math.BigDecimal;</div><div class="line">import java.text.SimpleDateFormat;</div><div class="line">import java.util.Calendar;</div><div class="line">import java.util.Date;</div><div class="line"></div><div class="line">import org.apache.spark.SparkConf;</div><div class="line">import org.apache.spark.api.java.JavaSparkContext;</div><div class="line">import org.apache.spark.sql.DataFrame;</div><div class="line">import org.apache.spark.sql.hive.HiveContext;</div><div class="line"></div><div class="line">/**</div><div class="line"> * 新闻网站关键指标离线统计Spark作业</div><div class="line"> * @author Administrator</div><div class="line"> *</div><div class="line"> */</div><div class="line">public class NewsOfflineStatSpark &#123;</div><div class="line"></div><div class="line">	public static void main(String[] args) &#123;</div><div class="line">		// 一般来说，在小公司中，可能就是将我们的spark作业使用linux的crontab进行调度</div><div class="line">			// 将作业jar放在一台安装了spark客户端的机器上，并编写了对应的spark-submit shell脚本</div><div class="line">			// 在crontab中可以配置，比如说每天凌晨3点执行一次spark-submit shell脚本，提交一次spark作业</div><div class="line">			// 一般来说，离线的spark作业，每次运行，都是去计算昨天的数据</div><div class="line">		// 大公司总，可能是使用较为复杂的开源大数据作业调度平台，比如常用的有azkaban、oozie等</div><div class="line">			// 但是，最大的那几个互联网公司，比如说BAT、美团、京东，作业调度平台，都是自己开发的</div><div class="line">			// 我们就会将开发好的Spark作业，以及对应的spark-submit shell脚本，配置在调度平台上，几点运行</div><div class="line">			// 同理，每次运行，都是计算昨天的数据</div><div class="line">		</div><div class="line">		// 一般来说，每次spark作业计算出来的结果，实际上，大部分情况下，都会写入mysql等存储</div><div class="line">		// 这样的话，我们可以基于mysql，用java web技术开发一套系统平台，来使用图表的方式展示每次spark计算</div><div class="line">		// 出来的关键指标</div><div class="line">		// 比如用折线图，可以反映最近一周的每天的用户跳出率的变化</div><div class="line">		</div><div class="line">		// 也可以通过页面，给用户提供一个查询表单，可以查询指定的页面的最近一周的pv变化</div><div class="line">		// date pageid pv</div><div class="line">		// 插入mysql中，后面用户就可以查询指定日期段内的某个page对应的所有pv，然后用折线图来反映变化曲线</div><div class="line">		</div><div class="line">		// 拿到昨天的日期，去hive表中，针对昨天的数据执行SQL语句</div><div class="line">		String yesterday = getYesterday();</div><div class="line">		</div><div class="line">		// 创建SparkConf以及Spark上下文</div><div class="line">		SparkConf conf = new SparkConf()</div><div class="line">				.setAppName(&quot;NewsOfflineStatSpark&quot;)    </div><div class="line">				.setMaster(&quot;local&quot;);  </div><div class="line">		JavaSparkContext sc = new JavaSparkContext(conf);</div><div class="line">		HiveContext hiveContext = new HiveContext(sc.sc());</div><div class="line">	</div><div class="line">		// 开发第一个关键指标：页面pv统计以及排序</div><div class="line">		calculateDailyPagePv(hiveContext, yesterday);  </div><div class="line">		// 开发第二个关键指标：页面uv统计以及排序</div><div class="line">		calculateDailyPageUv(hiveContext, yesterday);</div><div class="line">		// 开发第三个关键指标：新用户注册比率统计</div><div class="line">		calculateDailyNewUserRegisterRate(hiveContext, yesterday);</div><div class="line">		// 开发第四个关键指标：用户跳出率统计</div><div class="line">		calculateDailyUserJumpRate(hiveContext, yesterday);</div><div class="line">		// 开发第五个关键指标：版块热度排行榜</div><div class="line">		calculateDailySectionPvSort(hiveContext, yesterday);</div><div class="line">		</div><div class="line">		// 关闭Spark上下文</div><div class="line">		sc.close();</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	/**</div><div class="line">	 * 获取昨天的字符串类型的日期</div><div class="line">	 * @return 日期</div><div class="line">	 */</div><div class="line">	private static String getYesterday() &#123;</div><div class="line">		Calendar cal = Calendar.getInstance();</div><div class="line">		cal.setTime(new Date());</div><div class="line">		cal.add(Calendar.DAY_OF_YEAR, -1);  </div><div class="line">		</div><div class="line">		Date yesterday = cal.getTime();</div><div class="line">		</div><div class="line">		SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;);</div><div class="line">		return sdf.format(yesterday);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天每个页面的pv以及排序</div><div class="line">	 *   排序的好处：排序后，插入mysql，java web系统要查询每天pv top10的页面，直接查询mysql表limit 10就可以</div><div class="line">	 *   如果我们这里不排序，那么java web系统就要做排序，反而会影响java web系统的性能，以及用户响应时间</div><div class="line">	 */</div><div class="line">	private static void calculateDailyPagePv(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;pageid,&quot;</div><div class="line">					+ &quot;pv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;pageid,&quot;</div><div class="line">						+ &quot;count(*) pv &quot;</div><div class="line">					+ &quot;FROM news_access &quot;</div><div class="line">					+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">					+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot; </div><div class="line">					+ &quot;GROUP BY date,pageid &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY pv DESC &quot;;  </div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">	</div><div class="line">		// 在这里，我们也可以转换成一个RDD，然后对RDD执行一个foreach算子</div><div class="line">		// 在foreach算子中，将数据写入mysql中</div><div class="line">		</div><div class="line">		df.show();  </div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天每个页面的uv以及排序</div><div class="line">	 *   Spark SQL的count(distinct)语句，有bug，默认会产生严重的数据倾斜</div><div class="line">	 *   只会用一个task，来做去重和汇总计数，性能很差</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyPageUv(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;pageid,&quot;</div><div class="line">					+ &quot;uv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;pageid,&quot;</div><div class="line">						+ &quot;count(*) uv &quot;</div><div class="line">					+ &quot;FROM ( &quot;</div><div class="line">						+ &quot;SELECT &quot;</div><div class="line">							+ &quot;date,&quot;</div><div class="line">							+ &quot;pageid,&quot;</div><div class="line">							+ &quot;userid &quot;</div><div class="line">						+ &quot;FROM news_access &quot;</div><div class="line">						+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">						+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot;</div><div class="line">						+ &quot;GROUP BY date,pageid,userid &quot;</div><div class="line">					+ &quot;) t2 &quot;</div><div class="line">					+ &quot;GROUP BY date,pageid &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY uv DESC &quot;;</div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">		</div><div class="line">		df.show();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的新用户注册比例</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyNewUserRegisterRate(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		// 昨天所有访问行为中，userid为null，新用户的访问总数</div><div class="line">		String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NULL&quot;;</div><div class="line">		// 昨天的总注册用户数</div><div class="line">		String sql2 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;register&apos; AND date=&apos;&quot; + date + &quot;&apos; &quot;;</div><div class="line">	</div><div class="line">		// 执行两条SQL，获取结果</div><div class="line">		Object result1 = hiveContext.sql(sql1).collect()[0].get(0);</div><div class="line">		long number1 = 0L;</div><div class="line">		if(result1 != null) &#123;</div><div class="line">			number1 = Long.valueOf(String.valueOf(result1));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		Object result2 = hiveContext.sql(sql2).collect()[0].get(0);</div><div class="line">		long number2 = 0L;</div><div class="line">		if(result2 != null) &#123;</div><div class="line">			number2 = Long.valueOf(String.valueOf(result2));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 计算结果</div><div class="line">		System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;);  </div><div class="line">		System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;);  </div><div class="line">		double rate = (double)number2 / (double)number1;</div><div class="line">		System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;);  </div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的用户跳出率</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailyUserJumpRate(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		// 计算已注册用户的昨天的总的访问pv</div><div class="line">		String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL &quot;;</div><div class="line">		</div><div class="line">		// 已注册用户的昨天跳出的总数</div><div class="line">		String sql2 = &quot;SELECT count(*) FROM ( SELECT count(*) cnt FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL GROUP BY userid HAVING cnt=1 ) t &quot;;</div><div class="line">		</div><div class="line">		// 执行两条SQL，获取结果</div><div class="line">		Object result1 = hiveContext.sql(sql1).collect()[0].get(0);</div><div class="line">		long number1 = 0L;</div><div class="line">		if(result1 != null) &#123;</div><div class="line">			number1 = Long.valueOf(String.valueOf(result1));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		Object result2 = hiveContext.sql(sql2).collect()[0].get(0);</div><div class="line">		long number2 = 0L;</div><div class="line">		if(result2 != null) &#123;</div><div class="line">			number2 = Long.valueOf(String.valueOf(result2));  </div><div class="line">		&#125;</div><div class="line">		</div><div class="line">		// 计算结果</div><div class="line">		System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;);  </div><div class="line">		System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;);  </div><div class="line">		double rate = (double)number2 / (double)number1;</div><div class="line">		System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;);</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 计算每天的版块热度排行榜</div><div class="line">	 * @param hiveContext</div><div class="line">	 * @param date</div><div class="line">	 */</div><div class="line">	private static void calculateDailySectionPvSort(</div><div class="line">			HiveContext hiveContext, String date) &#123;</div><div class="line">		String sql = </div><div class="line">				&quot;SELECT &quot;</div><div class="line">					+ &quot;date,&quot;</div><div class="line">					+ &quot;section,&quot;</div><div class="line">					+ &quot;pv &quot;</div><div class="line">				+ &quot;FROM ( &quot;</div><div class="line">					+ &quot;SELECT &quot;</div><div class="line">						+ &quot;date,&quot;</div><div class="line">						+ &quot;section,&quot;</div><div class="line">						+ &quot;count(*) pv &quot;</div><div class="line">					+ &quot;FROM news_access &quot;</div><div class="line">					+ &quot;WHERE action=&apos;view&apos; &quot;</div><div class="line">					+ &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot;</div><div class="line">					+ &quot;GROUP BY date,section &quot;</div><div class="line">				+ &quot;) t &quot;</div><div class="line">				+ &quot;ORDER BY pv DESC &quot;;</div><div class="line">		</div><div class="line">		DataFrame df = hiveContext.sql(sql);</div><div class="line">		  </div><div class="line">		df.show();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">	/**</div><div class="line">	 * 格式化小数</div><div class="line">	 * @param str 字符串</div><div class="line">	 * @param scale 四舍五入的位数</div><div class="line">	 * @return 格式化小数</div><div class="line">	 */</div><div class="line">	private static double formatDouble(double num, int scale) &#123;</div><div class="line">		BigDecimal bd = new BigDecimal(num);  </div><div class="line">		return bd.setScale(scale, BigDecimal.ROUND_HALF_UP).doubleValue();</div><div class="line">	&#125;</div><div class="line">	</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h1>
          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/" itemprop="url">
                  spark core之单独启动master和worker脚本
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="单独启动master和worker脚本"><a href="#单独启动master和worker脚本" class="headerlink" title="单独启动master和worker脚本"></a>单独启动master和worker脚本</h1><p>sbin/start-all.sh脚本可以直接启动集群中的master进程和worker进程</p>
<p>这里讲的是单独启动master和worker进程</p>
<p>因为worker进程启动之后,会向master进程去注册,所以需要先启动master进程</p>
<p>为什么有的时候要单独启动master和worker进程?<br>因为可以通过命令行参数为进程配置一些独特的参数,如监听的端口号,web ui 的端口号,使用的cpu和内存等,比如:你可能向单独给某个节点配置不同的cpu和内存资源的使用限制,那么就可以使用脚本单独启动worker进程的时候,通过命令行参数来设置</p>
<p>手动启动master进程<br>需要在某个部署了spark安装包的节点上,使用sbin/start-master.sh启动,master启动之后,启动日志就会打印一行spark://HOST:PORT出来,这就是master的url地址,worker进程就会通过这个地址来连接到master进程,并且进行注册</p>
<p>另外，除了worker进程要使用这个URL以外，我们自己在编写spark代码时，也可以给SparkContext的setMaster()方法，传入这个URL地址<br>然后我们的spark作业，就会使用standalone模式连接master，并提交作业</p>
<p>此外，还可以通过<a href="http://MASTER_HOST:8080" target="_blank" rel="external">http://MASTER_HOST:8080</a> URL来访问master集群的监控web ui，那个web ui上，也会显示master的URL地址</p>
<p>手动启动worker进程<br>在部署了spark安装包的前提下,在你希望作为worker node的节点上,使用sbin/start-slave.sh <master-spark-url>在当前节点上启动,启动worker的时候需要指定master的url</master-spark-url></p>
<p>启动worker进程之后,再访问:http:MASTER_HOST:8080,在集群web ui上,就可以看到新启动的节点,包括该节点的cpu和内存资源</p>
<p>此外,以下参数是可以在手动启动master和worker的时候指定的:<br>-h host, –host host     在哪台机器上启动,默认都是本机<br>-p port, –port port 在机器上启动后,使用哪个端口对外提供服务,master默认是7077,worker默认是随机的<br>–webui-port port  web ui端口,master默认的是8080,worker默认的是8081<br>-c cores, –cores cores 仅限于worker,总共能让spark Application使用多少个cpu core,默认是当前机器上的所有的cpu core<br>-m mem, –memory mem  仅限于worker,总共能让spark Application使用多少内存,是100M或者1G这样的格式<br>-d dir, –worker-dir dir    仅限于worker,工作目录,默认是spark home/work目录<br>–properties-file file master和worker加载配置文件的地址,默认是spark_home/conf/spark-defaults.conf</p>
<p>咱们举个例子，比如说小公司里面，物理集群可能就一套，同一台机器上面，可能要部署Storm的supervisor进程，可能还要同时部署Spark的worker进程机器，cpu和内存，既要供storm使用，还要供spark使用<br>这个时候，可能你就需要限制一下worker节点能够使用的cpu和内存的数量</p>
<p>小公司里面，搭建spark集群的机器可能还不太一样，有的机器比如说是有5个g内存，有的机器才1个g内存那你对于1个g内存的机器，是不是得限制一下内存使用量，比如说500m</p>
<p>实例:<br>1、启动master: 日志和web ui，观察master url</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/单独启动master.png" alt=""></p>
<p>2、启动worker: 观察web ui，是否有新加入的worker节点，以及对应的信息</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/单独启动worker.png" alt=""></p>
<p>3、单独关闭master和worker,此时的顺序得反过来,先关闭worker,再去关闭master</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/stop_worker_master.png" alt=""></p>
<p>4、再次单独启动master和worker，给worker限定，就使用500m内存，跟之前看到的worker信息比对一下内存最大使用量</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/worker_allocate_memory.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/" itemprop="url">
                  spark core之主要的几个术语
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <table>
<thead>
<tr>
<th style="text-align:center">术语</th>
<th style="text-align:left">解释</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Application</td>
<td style="text-align:left">spark应用程序,就是用户基于spark API开发的程序,一定是通过一个有main方法的类执行的</td>
</tr>
<tr>
<td style="text-align:center">Application jar</td>
<td style="text-align:left">这个就是把写好的spark工程，打包成一个jar包，其中包括了所有的第三方jar依赖包，比如java中，就用maven+assembly插件打包最方便</td>
</tr>
<tr>
<td style="text-align:center">Driver</td>
<td style="text-align:left">在使用spark-submit提交应用的时候,会指定一个主类,这个类有一个main方法,driver进程就是在运行程序中的main方法的进程,这就是driver</td>
</tr>
<tr>
<td style="text-align:center">cluster manager</td>
<td style="text-align:left">集群管理器,就是为每个spark application，在集群中调度和分配资源的组件，比如Spark Standalone、YARN、Mesos等</td>
</tr>
<tr>
<td style="text-align:center">deploy mode</td>
<td style="text-align:left">部署模式，无论是基于哪种集群管理器(Standalone、YARN、Mesos)，spark作业部署或者运行模式，都分为两种，client和cluster，client模式下driver运行在提交spark作业的机器上,即执行应用的main类(主要用于测试)；cluster模式下，driver运行在spark集群中的某一个节点上</td>
</tr>
<tr>
<td style="text-align:center">Worker Node</td>
<td style="text-align:left">集群中的工作节点，能够运行executor进程，运行作业代码的节点</td>
</tr>
<tr>
<td style="text-align:center">Executor</td>
<td style="text-align:left">集群管理器为application分配的进程，运行在worker节点上，负责执行作业的任务，并将数据保存在内存或磁盘中，每个application都有自己的executor</td>
</tr>
<tr>
<td style="text-align:center">Job</td>
<td style="text-align:left">每个spark application，根据你执行了多少次action操作，就会有多少个job</td>
</tr>
<tr>
<td style="text-align:center">Stage</td>
<td style="text-align:left">根据是否有shuffle,每个job都会划分为多个stage（阶段），每个stage都会有对应的一批task，分配到executor上去执行</td>
</tr>
<tr>
<td style="text-align:center">Task</td>
<td style="text-align:left">driver发送到executor上执行的计算单元，每个task负责在一个阶段（stage），处理一小片数据，计算出对应的结果</td>
</tr>
</tbody>
</table>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/" itemprop="url">
                  spark core之yarn模式
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="yarn-client模式原理"><a href="#yarn-client模式原理" class="headerlink" title="yarn-client模式原理"></a>yarn-client模式原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/yarn-client模式原理.png" alt=""></p>
<h1 id="yarn-cluster模式原理"><a href="#yarn-cluster模式原理" class="headerlink" title="yarn-cluster模式原理"></a>yarn-cluster模式原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/yarn-cluster模式原理.png" alt=""></p>
<h1 id="yarn-client模式提交spark作业"><a href="#yarn-client模式提交spark作业" class="headerlink" title="yarn-client模式提交spark作业"></a>yarn-client模式提交spark作业</h1><h2 id="yarn运行spark作业的前提"><a href="#yarn运行spark作业的前提" class="headerlink" title="yarn运行spark作业的前提"></a>yarn运行spark作业的前提</h2><p>如果想要让spark作业可以运行在yarn上面,那么首先就必须在spark-env.sh文件中,配置HADOOP_CONF_DIR或者YARN_CONF_DIR属性,值为hadoop的配置文件的目录,即:HADOOP_HOME/etc/hadoop,其中包含了hadoop和yarn所有的配置文件,比如:hdfs-site.xml,yarn-site.xml等,spark需要这些配置来读写HDFS,以及连接到yarn ResourceManager上,这个目录中包含的配置文件都会被分发到yarn集群中去的</p>
<p>vim spark/conf/spark-env.sh<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop</div><div class="line"></div><div class="line">/*在/usr/local/hadoop/etc/hadoop目录下有:</div><div class="line">yarn-site.xml(其中可以找到ResourceManager所在的机器)</div><div class="line">还有一些其他的配置文件</div><div class="line">*/</div></pre></td></tr></table></figure></p>
<p>跟spark standalone模式不同,通常不需要使用–master指定master URL<br>因为spark会从hadoop的配置文件中去读ResourceManager的配置,这样就知道了ResourceManager所在的机器(master),所以不需要我们指定,但是我们需要指定deploy mode,如下示例:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">/export/servers/spark/bin/spark-submit \</div><div class="line">--class cn.spark.study.core.WordCount \</div><div class="line">--master yarn-cluster</div><div class="line">#--master yarn-client</div><div class="line">--num-executors 1 \</div><div class="line">--driver-memory 100m \</div><div class="line">--executor-memory 100m \</div><div class="line">--executor-cores 1 \</div><div class="line">--queue hadoop队列</div><div class="line">/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</div></pre></td></tr></table></figure></p>
<p>–queue,在不同的部门,或者是不同的大数据项目,共用一个yarn集群,运行spark作业,推荐一定要用–queue,指定不同的hadoop队列,做项目或者部门之间的队列隔离</p>
<p>与Standalone模式类似,yarn-client模式通常建议在测试的时候使用,方便你直接在提交作业的机器上查看日志,但是作业实际部署到生产环境中进行运行的时候,还是使用yarn-cluster模式</p>
<p>yarn模式下需要观察的点:<br>1.日志<br>命令行日志<br>web ui日志</p>
<p>2.web ui的地址不再是spark://192.168.0.108:8080这种URL了,因为那是Standalone模式下的监控web ui,在yarn模式下,要看yarn的web ui: <a href="http://192.168.0.108:8088/" target="_blank" rel="external">http://192.168.0.108:8088/</a> 这是yarn的URL地址</p>
<p>3.进程<br>driver是什么进程</p>
<p>AppLicationMaster是什么进程</p>
<p>executor进程</p>
<h1 id="yarn模式下的日志查看"><a href="#yarn模式下的日志查看" class="headerlink" title="yarn模式下的日志查看"></a>yarn模式下的日志查看</h1><p>在yarn模式下,spark作业运行相关的executor和ApplicationMaster都是运行在yarn的container中的,一个作业运行完了以后,yarn有两种方式来处理spark作业打印出的日志</p>
<p>1.聚合日志方式(推荐,比较常用)<br>这种格式将散落在集群中各个机器上的日志,最后都给聚合起来,让我们可以统一查看,如果yarn的日志聚合的选项打开了,即:yarn.log-aggregation-enable(yarn-site.xml文件中配置), container的日志会拷贝到HDFS上去,并从机器中删除</p>
<p>然后我们使用yarn logs -applicationId <app id=""> 命令来查看日志(app Id在yarn的web ui上看:resourceManager_host:8088)</app></p>
<p>yarn logs命令,会打印出application对应的所有container的日志出来,当然,因为日志是在HDFS上的,我们自然可以通过HDFS的命令行来直接从HDFS中查看日志,日志在HDFS中的目录,可以通过查看yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix属性来获知</p>
<p>2.web ui<br>日志也可以通过spark web ui来查看executor的输出日志<br>但是此时需要启动History Server,需要让spark history server和mapreduce history server运行着;并且在yarn-site.xml文件中,配置yarn.log.server.url属性<br>spark history server web ui中的log url,会将你重新定向到mapreduce history server上去查看日志</p>
<p>3.分散查看(通常不推荐)<br>如果没有打开聚合日志选项,那么日志默认就是散落在各个机器上的本次磁盘目录中的,在YARN_APP_LOGS_DIR目录下,根据hadoop版本的不同,通常在/tmp/logs目录下,或者在$HADOOP_HOME/logs/userlogs目录下,如果你要查看某个container的日志,那么就得登录到那台机器上去,然后到指定的目录下如,找到那个日志文件,然后才能查看</p>
<h1 id="yarn模式相关的参数"><a href="#yarn模式相关的参数" class="headerlink" title="yarn模式相关的参数"></a>yarn模式相关的参数</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">yarn模式运行spark作业所有属性详解</div><div class="line"></div><div class="line">属性名称											默认值							含义</div><div class="line">spark.yarn.am.memory								512m							client模式下，YARN Application Master使用的内存总量</div><div class="line">spark.yarn.am.cores									1								client模式下，Application Master使用的cpu数量</div><div class="line">spark.driver.cores									1								cluster模式下，driver使用的cpu core数量，driver与Application Master运行在一个进程中，所以也控制了Application Master的cpu数量</div><div class="line">spark.yarn.am.waitTime								100s							cluster模式下，Application Master要等待SparkContext初始化的时长; client模式下，application master等待driver来连接它的时长</div><div class="line">spark.yarn.submit.file.replication					hdfs副本数						作业写到hdfs上的文件的副本数量，比如工程jar，依赖jar，配置文件等，最小一定是1</div><div class="line">spark.yarn.preserve.staging.files					false							如果设置为true，那么在作业运行完之后，会避免工程jar等文件被删除掉</div><div class="line">spark.yarn.scheduler.heartbeat.interval-ms			3000							application master向resourcemanager发送心跳的间隔，单位ms</div><div class="line">spark.yarn.scheduler.initial-allocation.interval	200ms							application master在有pending住的container分配需求时，立即向resourcemanager发送心跳的间隔</div><div class="line">spark.yarn.max.executor.failures					executor数量*2，最小3			整个作业判定为失败之前，executor最大的失败次数</div><div class="line">spark.yarn.historyServer.address					无								spark history server的地址</div><div class="line">spark.yarn.dist.archives							无								每个executor都要获取并放入工作目录的archive</div><div class="line">spark.yarn.dist.files								无								每个executor都要放入的工作目录的文件</div><div class="line">spark.executor.instances							2								默认的executor数量</div><div class="line">spark.yarn.executor.memoryOverhead					executor内存10%					每个executor的堆外内存大小，用来存放诸如常量字符串等东西</div><div class="line">spark.yarn.driver.memoryOverhead					driver内存7%					同上</div><div class="line">spark.yarn.am.memoryOverhead						AM内存7%						同上</div><div class="line">spark.yarn.am.port									随机							application master端口</div><div class="line">spark.yarn.jar										无								spark jar文件的位置</div><div class="line">spark.yarn.access.namenodes							无								spark作业能访问的hdfs namenode地址</div><div class="line">spark.yarn.containerLauncherMaxThreads				25								application master能用来启动executor container的最大线程数量</div><div class="line">spark.yarn.am.extraJavaOptions						无								application master的jvm参数</div><div class="line">spark.yarn.am.extraLibraryPath						无								application master的额外库路径</div><div class="line">spark.yarn.maxAppAttempts															提交spark作业最大的尝试次数</div><div class="line">spark.yarn.submit.waitAppCompletion					true							cluster模式下，client是否等到作业运行完再退出</div></pre></td></tr></table></figure>
<p>以上这些参数可以在spark-submit中配置,使用–conf配置</p>
<h1 id="spark-submit详解"><a href="#spark-submit详解" class="headerlink" title="spark-submit详解"></a>spark-submit详解</h1><p>spark-submit可以通过一个统一的接口,将spark应用程序提交到所有spark支持的集群管理器上(Standalone(mater),Yarn(ResourceManager)等),所以我们并不需要为每种集群管理器都做特殊的配置</p>
<p>–master<br>1.如果不设置,那么就是local模式<br>2.如果设置spark://开头的URL,那么就是Standalone模式,会提交到指定的URL的Mater进程上去<br>3.如果设置yarn-client/yarn-cluster,那么就是yarn模式,会读取hadoop配置文件,然后连接ResourceManager</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/" itemprop="url">
                  spark core之worker节点配置以及spark-env.sh参数详解
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="worker节点配置"><a href="#worker节点配置" class="headerlink" title="worker节点配置"></a>worker节点配置</h1><p>场景:如果在已有的spark集群中,你想要加入一台新的worker节点</p>
<p>如果你想将某台机器部署成Standalone集群架构中的worker节点,那么就必须在该机器上部署spark安装包,并修改配置文件如下:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">#修改配置文件</div><div class="line">cd spark/conf/</div><div class="line">mv spark-env.sh.template spark-env.sh</div><div class="line">vim spark-env.sh  //添加</div><div class="line">export JAVA_HOME=/home/hadoop/app/jdk1.7.0_80</div><div class="line">export SPARK_MASTER_IP=hdp-node-01            //配置master的机器</div><div class="line">export SPARK_MASTER_PORT=7077</div><div class="line">#######################################################</div><div class="line">mv slaves.template slaves  </div><div class="line">vim slaves      //添加worker的节点</div><div class="line">hdp-node-01</div><div class="line">hdp-node-02</div><div class="line">#######################################################</div><div class="line">// 注意要配置多个机器之间的ssh免密码登录</div></pre></td></tr></table></figure></p>
<h1 id="spark-env-sh参数详解"><a href="#spark-env-sh参数详解" class="headerlink" title="spark-env.sh参数详解"></a>spark-env.sh参数详解</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SPARK_MASTER_IP					#指定master进程所在的机器的ip地址</div><div class="line">SPARK_MASTER_PORT				#指定master监听的端口号（默认是7077）</div><div class="line">SPARK_MASTER_WEBUI_PORT			#指定master web ui的端口号（默认是8080）</div><div class="line"></div><div class="line">#其实使用spark-env.sh配置的参数和我们手动启动master时指定的参数是一样的,sbin/start-master.sh --port 7078，类似这种方式，貌似可以指定一样的配置属性</div><div class="line">我明确告诉大家，这个作用的确是一模一样的</div><div class="line"></div><div class="line">#你可以在spark-evn.sh中就去配置好,但是有时呢，可能你会遇到需要临时更改配置，并启动master或worker进程的情况</div><div class="line">#此时就比较适合，用sbin/start-master.sh这种脚本的命令行参数，来设置这种配置属性</div><div class="line">#但是通常来说呢，还是推荐在部署的时候，通过spark-env.sh来设定</div><div class="line">脚本命令行参数通常用于临时的情况</div><div class="line"></div><div class="line">SPARK_MASTER_OPTS				#设置master的额外参数，使用&quot;-Dx=y&quot;设置各个参数(x对应的是参数名,y对应的是参数的值)</div><div class="line">比如说export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=1&quot;</div><div class="line"></div><div class="line">参数名											默认值						含义</div><div class="line">spark.deploy.retainedApplications				200							在spark web ui上最多显示多少个application的信息</div><div class="line">spark.deploy.retainedDrivers					200							在spark web ui上最多显示多少个driver的信息</div><div class="line">spark.deploy.spreadOut							true						资源调度策略，spreadOut会尽量将application的executor进程分布在更多worker上，适合基于hdfs文件计算的情况，提升数据本地化概率；非spreadOut会尽量将executor分配到一个worker上，适合计算密集型的作业</div><div class="line">spark.deploy.defaultCores						无限大						每个spark作业最多在standalone集群中使用多少个cpu core，默认是无限大，有多少用多少</div><div class="line">spark.deploy.timeout							60							单位秒，一个worker多少时间没有响应之后，master认为worker挂掉了</div><div class="line"></div><div class="line">------------------------------------------------------------</div><div class="line"></div><div class="line"></div><div class="line">SPARK_LOCAL_DIRS				spark的工作目录，包括了shuffle map输出文件，以及持久化到磁盘的RDD等</div><div class="line"></div><div class="line">SPARK_WORKER_PORT				worker节点的端口号，默认是随机的</div><div class="line">SPARK_WORKER_WEBUI_PORT			worker节点的web ui端口号，默认是8081</div><div class="line">SPARK_WORKER_CORES				worker节点上，允许spark作业使用的最大cpu数量，默认是机器上所有的cpu core</div><div class="line">SPARK_WORKER_MEMORY				worker节点上，允许spark作业使用的最大内存量，格式为1000m，2g等，默认最小是1g内存</div><div class="line"></div><div class="line">就是说，有些master和worker的配置，可以在spark-env.sh中部署时即配置，但是也可以在start-slave.sh脚本启动进程时命令行参数设置</div><div class="line">但是命令行参数的优先级比较高，会覆盖掉spark-env.sh中的配置</div><div class="line">比如说，上一讲我们的实验，worker的内存默认是1g，但是我们通过--memory 500m，是可以覆盖掉这个属性的</div><div class="line"></div><div class="line">SPARK_WORKER_INSTANCES			当前机器上的worker进程数量，默认是1，可以设置成多个，但是这时一定要设置SPARK_WORKER_CORES，限制每个worker的cpu数量</div><div class="line">SPARK_WORKER_DIR				spark作业的工作目录，包括了作业的日志等，默认是spark_home/work</div><div class="line">SPARK_WORKER_OPTS				worker的额外参数，使用&quot;-Dx=y&quot;设置各个参数</div><div class="line"></div><div class="line">参数名											默认值						含义</div><div class="line">spark.worker.cleanup.enabled					false						是否启动自动清理worker工作目录，默认是false</div><div class="line">spark.worker.cleanup.interval					1800						单位秒，自动清理的时间间隔，默认是30分钟</div><div class="line">spark.worker.cleanup.appDataTtl					7 * 24 * 3600				默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天</div><div class="line">-----------------------------------------------------------------</div><div class="line"></div><div class="line">SPARK_DAEMON_MEMORY				分配给master和worker进程自己本身的内存，默认是1g</div><div class="line">SPARK_DAEMON_JAVA_OPTS			设置master和worker自己的jvm参数，使用&quot;-Dx=y&quot;设置各个参数</div><div class="line">SPARK_PUBLISC_DNS				master和worker的公共dns域名，默认是没有的</div><div class="line"></div><div class="line">这里提示一下，大家可以观察一下，咱们的内存使用情况</div><div class="line">在没有启动spark集群之前，我们的内存使用是1个多g，启动了spark集群之后，就一下子耗费到2个多g</div><div class="line">每次又执行一个作业时，可能会耗费到3个多g左右</div><div class="line"></div><div class="line">所以大家就明白了，为什么之前用分布式的集群，每个worker节点才1个g内存，根本是没有办法使用standalone模式和yarn模式运行作业的</div></pre></td></tr></table></figure>
<p>下面是给大家列出spark所有的启动和关闭shell脚本<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">sbin/start-all.sh				根据配置，在集群中各个节点上，启动一个master进程和多个worker进程</div><div class="line">sbin/stop-all.sh				在集群中停止所有master和worker进程</div><div class="line">sbin/start-master.sh			在本地启动一个master进程</div><div class="line">sbin/stop-master.sh				关闭master进程</div><div class="line">sbin/start-slaves.sh			根据conf/slaves文件中配置的worker节点，启动所有的worker进程</div><div class="line">sbin/stop-slaves.sh				关闭所有worker进程</div><div class="line">sbin/start-slave.sh				在本地启动一个worker进程</div></pre></td></tr></table></figure></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/" itemprop="url">
                  spark core之Thrift JDBC_ODBC server
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>Spark SQL的Thrift JDBC/ODBC server是基于Hive 0.13的HiveServer2实现的。这个服务启动之后，最主要的功能就是可以让我们通过<br>Java JDBC来以编程的方式调用Spark SQL。此外，在启动该服务之后，可以通过Spark或Hive 0.13自带的beeline工具来进行测试。</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/" itemprop="url">
                  spark core之standalone集群架构
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="spark-core之standalone集群架构"><a href="#spark-core之standalone集群架构" class="headerlink" title="spark core之standalone集群架构"></a>spark core之standalone集群架构</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构.png" alt=""></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Standalone模式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Standalone模式/" itemprop="url">
                  spark core之Standalone模式
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Standalone-client模式提交作业"><a href="#Standalone-client模式提交作业" class="headerlink" title="Standalone client模式提交作业"></a>Standalone client模式提交作业</h1><p>通常情况来说,部署在测试机器上去,进行测试运行spark作业的时候,都是使用的是client模式,client模式下,提交作业以后,driver在本地启动,可以实时看到详细的日志信息,方便你追踪和排查错误</p>
<p>client的三种模式:<br>1.硬编码:SparkConf.setMaster(“spark://IP:PORT”)<br>2.spark-submit提交的时候设置一下:–master spark://IP:PORT<br>3.spark-shell去启动的时候可以指定:–master spark://IP:PORT</p>
<p>上面三种写法,使用第二种,是最合适的</p>
<p>在Standalone模式下中,在spark-submit提交脚本中,用–master指定Master的URL的,使用Standalone client模式或者是cluster模式,是要在spark-submit中使用–deploy-mode client/cluster来设置,但是如果不设置,默认的–deploy-mode为client模式</p>
<p>使用spark-submit脚本来提交application时，application jar是会自动被分发到所有worker节点上去的,对于你的application依赖的额外jar包，可以通过spark-submit脚本中的–jars标识，来指定，可以使用逗号分隔多个jar,比如说，你写spark-sql的时候，有的时候，在作业中，要往mysql中写数据，此时可能会出现找不到mysql驱动jar包的情况,此时，就需要你手动在spark-submit脚本中，使用–jars，加入一些依赖的jar包</p>
<p>提交的脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">/export/servers/spark/bin/spark-submit \</div><div class="line">--master spark://hdp-node-01:7077 \</div><div class="line">--deploy-mode client \</div><div class="line">--class cn.spark.study.core.WordCount \</div><div class="line">--num-executors 1 \</div><div class="line">--driver-memory 100m \</div><div class="line">--executor-memory 100m \</div><div class="line">--executor-cores 1 \</div><div class="line">/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</div></pre></td></tr></table></figure>
<p>提交Standalone client模式的作业<br>1.–master 和 –deploy-mode 来提交作业<br>2.在web UI查看,可以看到completed applications一栏中,有刚刚提交的作业,对比一下UI上的ApplicationID和driver机器上打印的日志的ApplicationID<br>3.使用jps查看进程,看Standalone client模式提交作业的时候,当前机器上会有哪些进程(会看到一个sparksubmit相当于driver进程,还是一个进程是CoarseGrainedExecutorBackend进程,这个进程就是在worker机器上的executor进程)</p>
<h1 id="Standalone-cluster模式提交作业"><a href="#Standalone-cluster模式提交作业" class="headerlink" title="Standalone cluster模式提交作业"></a>Standalone cluster模式提交作业</h1><p>Standalone cluster模式,通常用于,spark作业部署到生产环境中去使用,Standalone client模式下,在spark-submit脚本执行的机器上,会启动driver进程,然后去进行整个作业的调度,通常来说,你的spark-submit脚本能够执行的机器,也就是,作为一个开发人员能够登录的机器,通常不会直接是spark集群部署的机器,因为不是随便谁都能够登录到spark集群中某个机器上去执行一个脚本,这是没有安全性可言的,用client模式,你的机器可能与spark集群部署的机器,都不在一个机房,或者是举例很远,那么此时远距离的频繁的网络通信会影响整个作业的执行性能,所以在生产环境中是使用的Standalone cluster模式,因为在这种模式下,会由master在集群中的一个节点上来启动driver,然后driver会进行频繁的作业调度,此时driver和集群是在一起的,这样性能是比较高的</p>
<p>此外,在Standalone cluster模式下,还支持监控你的driver进程,并且在driver进程挂掉的时候,自动重启该进程,要使用这个功能,在spark-submit脚本中,使用–supervise参数即可,这个参数其实在spark streaming中作为HA高可用来的,配置driver的高可用</p>
<p>如果想要杀掉反复挂掉的driver进程,使用以下即可:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt;</div><div class="line"></div><div class="line">#如果要查看driver id，通过http://&lt;maser url&gt;:8080即可查看到</div></pre></td></tr></table></figure></p>
<p>提交的脚本</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">/export/servers/spark/bin/spark-submit \</div><div class="line">--master spark://hdp-node-01:7077 \</div><div class="line">--deploy-mode cluster \</div><div class="line">--class cn.spark.study.core.WordCount \</div><div class="line">--num-executors 1 \</div><div class="line">--driver-memory 100m \</div><div class="line">--executor-memory 100m \</div><div class="line">--executor-cores 1 \</div><div class="line">/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \</div></pre></td></tr></table></figure>
<h1 id="Standalone模式下的多作业资源调度"><a href="#Standalone模式下的多作业资源调度" class="headerlink" title="Standalone模式下的多作业资源调度"></a>Standalone模式下的多作业资源调度</h1><p>Standalone集群对于同时提交上来的多个作业(Application),仅仅支持FIFO调度策略,也就是先入先出</p>
<p>默认情况下,集群对多作业同时执行的支持是不好的,没有办法同时执行多个作业,因为先提交上来的每一个作业都会尝试使用集群中的所有可用的cpu资源(spreadOut),此时相当于只能支持Application串行一个一个运行了,因此如果我们希望能够支持多作业同时运行,那么就需要调整一些资源参数了</p>
<p>我们需要调整的资源参数是:spark.cores.max,来限制每个作业能够使用的最大的cpu core数量,这样先提交上来的作业不会使用所有的cpu资源,后面提交上来的作业就可以获取到资源了,这样就可以同时运行多个Application</p>
<p>比如说,如果集群一共有20个节点，每个节点是8核，160 cpu core,那么，如果你不限制每个作业获取的最大cpu资源大小，而且在你spark-submit的时候，或者说，你就设置了num-executors，total-cores，160 此时，你的作业是会使用所有的cpu core资源的,所以，如果我们可以通过设置全局的一个参数，让每个作业最多只能获取到一部分cpu core资源,那么，后面提交上来的作业，就也可以获取到一部分资源,standalone集群，才可以支持同时执行多个作业</p>
<p>使用SparkConf或spark-submit中的–conf标识，设置参数即可</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">SparkConf conf = new SparkConf()</div><div class="line">		.set(&quot;spark.cores.max&quot;, &quot;10&quot;)</div></pre></td></tr></table></figure>
<p>通常不建议使用SparkConf，硬编码，来设置一些属性，不够灵活,建议使用spark-submit来设置属性<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">--conf spark.cores.max=10</div></pre></td></tr></table></figure></p>
<p>此外，还可以直接通过spark-env.sh配置每个application默认能使用的最大cpu数量来进行限制，默认是无限大，此时就不需要每个application都自己手动设置了<br>在spark-env.sh中配置spark.deploy.defaultCores即可<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=10&quot;</div></pre></td></tr></table></figure></p>
<h1 id="Standalone模式下的作业监控与日志记录"><a href="#Standalone模式下的作业监控与日志记录" class="headerlink" title="Standalone模式下的作业监控与日志记录"></a>Standalone模式下的作业监控与日志记录</h1><p>spark standalone模式，提供了一个web界面来让我们监控集群，并监控所有的作业的运行,web界面上,提供了master和worker的相关信息,默认的话,我们的web界面运行在master机器上的8080端口</p>
<p>spark web ui<br>1.哪些作业在跑<br>2.哪些作业跑完了,花了多长时间,使用了多少资源<br>3.哪些作业跑失败了</p>
<p>Application web ui<br>1.可以看到job,stage,task的详细运行信息<br>2.shuffle read,shuffle write,gc,运行时间,每个task分配的数据量<br>3.定位很多性能问题、troubleshooting等等，如task数据分布不允许，那么就是数据倾斜<br>4.哪个stage运行的时间最慢，通过之前讲解的stage划分算法，去你的代码里定位到，那个stage对应的是哪一块儿代码，你的那段代码为什么会运行太慢,使用优化策略去优化性能</p>
<p>但是有个问题，作业运行完了以后，我们就看不到了,此时跟history server有关，需要我们开启</p>
<p>日志记录<br>1、系统级别的，spark自己的日志记录<br>2、我们在程序里面，用log4j，或者System.out.println打印出来的日志</p>
<p>spark web ui中可以看到<br>1、看每个application在每个executor上的日志<br>2、stdout，可以显示我们用System.out.println打印出来的日志，stderr，可以显示我们用System.err.println打印出来的日志</p>
<p>此外，我们自己在spark作业代码中，打出来的日志，比如用System.out.println()等，是打到每个作业在每个节点的工作目录中去的,默认是SPARK_HOME/work目录下,这个目录下，每个作业都有两个文件，一个是stdout，一个是stderr，分别代表了标准输出流和异常输出流</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子的闭包原理详解/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子的闭包原理详解/" itemprop="url">
                  spark core之spark算子的闭包原理详解
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <p>通常来说，这个问题跟在RDD的算子中操作作用域外部的变量有关,所谓RDD算子中，操作作用域外部的变量，指的是，类似下面的语句: var a = 0; rdd.foreach(i -&gt; a += i),此时，对rdd执行的foreach算子的作用域，其实仅仅是它的内部代码，但是这里却操作了作用域外部的a变量,根据不同的编程语言的语法，这种功能是可以做到的，而这种现象就叫做闭包</p>
          <!--noindex-->
          <div class="post-button text-center">
            <a class="btn" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子的闭包原理详解/#more" rel="contents">
              阅读全文 &raquo;
            </a>
          </div>
          <!--/noindex-->
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子汇总/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Mr. Chen">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Chen's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子汇总/" itemprop="url">
                  spark core之spark算子汇总
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-16T12:47:25+08:00">
                2017-04-16
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/spark/" itemprop="url" rel="index">
                    <span itemprop="name">spark</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">val sparkConf = new SparkConf()</div><div class="line">      .setAppName(&quot;Rdd&quot;)</div><div class="line">      .setMaster(&quot;local&quot;)</div><div class="line">val sc = new SparkContext(sparkConf)</div><div class="line"></div><div class="line">val studentNames = Array(&quot;张三&quot;, &quot;李四&quot;, &quot;王五&quot;)</div><div class="line">val studentNamesRdd = sc.parallelize(studentNames, 2)</div><div class="line"></div><div class="line"></div><div class="line">//mapPartitions类似于map</div><div class="line">// 不同之处在于map算子一次处理一个partition中的一条数据,</div><div class="line">// mapPartitions算子,一次处理一个partition中的所有的数据</div><div class="line"></div><div class="line">// 推荐使用场景</div><div class="line">// 如果你的Rdd的数据量不是特别大,那么建议采用mapPartitions算子代替map算子,这样可以加快处理速度</div><div class="line">// 但是如果你的rdd的数据量特别大,比如10条数据,不建议使用mapPartitions,因为可能会内存溢出</div><div class="line"></div><div class="line">studentNamesRdd.mapPartitions&#123;</div><div class="line">  ite=&gt;</div><div class="line">    var arr = mutable.ArrayBuffer&lt;String&gt;()</div><div class="line">    while(ite.hasNext)&#123;</div><div class="line">      val studentName = ite.next</div><div class="line">      val studentScore = studentScoresMap.get(studentName)</div><div class="line"></div><div class="line">    &#125;</div><div class="line">    null</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h1 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">map是对每个元素操作, mapPartitions是对其中的每个partition操作</div><div class="line">------------------------------------------------------------</div><div class="line">mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码</div><div class="line">val func = (index: Int, iter: Iterator[(Int)]) =&gt; &#123; //index是分区的索引,Iterator是一个分区中的数据,可以迭代</div><div class="line">  iter.toList.map(x =&gt; &quot;[partID:&quot; +  index + &quot;, val: &quot; + x + &quot;]&quot;).iterator</div><div class="line">&#125;</div><div class="line">val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)</div><div class="line">rdd1.mapPartitionsWithIndex(func).collect</div></pre></td></tr></table></figure>
<h1 id="groupByKey算子原理"><a href="#groupByKey算子原理" class="headerlink" title="groupByKey算子原理"></a>groupByKey算子原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/groupByKey算子原理.png" alt=""></p>
<h1 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">#Example</div><div class="line">sc.textFile(args(0)).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).</div><div class="line"></div><div class="line">#源码</div><div class="line">  def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123;</div><div class="line">    combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</div><div class="line">  &#125;</div><div class="line">/*</div><div class="line">在reduceByKey的内部是调用的combineByKeyWithClassTag, 由上面的源码知道combineByKeyWithClassTag的第一个参数是对value原样输出,第二个,第三个参数是调用reduceByKey中指定的函数</div><div class="line">第二个参数的功能:是在partition内进行操作</div><div class="line">第三个参数的功能是对各个partition的所有的结果进行操作</div><div class="line">*/</div></pre></td></tr></table></figure>
<p>reduceByKey算子原理</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/reduceByKey算子原理.png" alt=""></p>
<h1 id="distinct算子原理"><a href="#distinct算子原理" class="headerlink" title="distinct算子原理"></a>distinct算子原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/distinct算子原理.png" alt=""></p>
<h1 id="cogroup算子原理"><a href="#cogroup算子原理" class="headerlink" title="cogroup算子原理"></a>cogroup算子原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/cogroup算子原理.png" alt=""></p>
<h1 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">val rdd6 = sc.parallelize(List(5,6,4,7))</div><div class="line">val rdd7 = sc.parallelize(List(1,2,3,4))</div><div class="line">#intersection求交集</div><div class="line">val rdd9 = rdd6.intersection(rdd7)</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">//对rdd中的数据进行去重</div><div class="line">distinct([numTasks]))      </div><div class="line">//Return a new dataset that contains the distinct elements of the source dataset.</div></pre></td></tr></table></figure>
<p>intersection算子原理</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/intersection算子原理.png" alt=""></p>
<h1 id="join算子原理"><a href="#join算子原理" class="headerlink" title="join算子原理"></a>join算子原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/join算子原理.png" alt=""></p>
<h1 id="sortByKey原理"><a href="#sortByKey原理" class="headerlink" title="sortByKey原理"></a>sortByKey原理</h1><p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/sortByKey原理.png" alt=""></p>
<h1 id="aggregate"><a href="#aggregate" class="headerlink" title="aggregate"></a>aggregate</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">aggregate(参数1)(参数2,参数3)   //参数1是初始化的值,参数2是一个函数,对每一个partition的数据聚合,参数3 是对所有partition的结果进行聚合</div><div class="line"></div><div class="line">###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]</div><div class="line">###0 + (0+1+2+3+4/*局部求和*/   +   0+5+6+7+8+9/*局部求和*/)</div><div class="line">rdd1.aggregate(0)(_+_, _+_/*全局求和*/)</div><div class="line">rdd1.aggregate(0)(math.max(_, _), _ + _) //传给第一个参数的是:itera ,所以使用math.max(_, _) 去迭代</div><div class="line"></div><div class="line"></div><div class="line">###5和1比, 得5再和234比得5 --&gt; 5和6789比,得9 --&gt; 5 + (5+9)</div><div class="line">rdd1.aggregate(5)(math.max(_, _), _ + _)</div><div class="line"> </div><div class="line">add1.reduce(math.max(_,_))//第一个下划线是上一次求max的值,第二个下划线是循环add1中的一个元素</div><div class="line"> </div><div class="line"> </div><div class="line">aggregate:先进行局部的(partition)操作(循环迭代所有的局部元素),然后进行全局的操作(循环迭代所有的分区)</div><div class="line"> </div><div class="line"> </div><div class="line">val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)</div><div class="line">def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = &#123;</div><div class="line">  iter.toList.map(x =&gt; &quot;[partID:&quot; +  index + &quot;, val: &quot; + x + &quot;]&quot;).iterator</div><div class="line">&#125;</div><div class="line">rdd2.aggregate(&quot;&quot;)(_ + _, _ + _)  //abcdef</div><div class="line">rdd2.aggregate(&quot;=&quot;)(_ + _, _ + _)  //==abc=def  :局部求和为 =abc 和 =def 最后整体求和:==abc=def</div><div class="line"> </div><div class="line">val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)</div><div class="line">rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)//24 或者是42 因为是并行的任务,所以可能先返回2,也有可能先返回4</div><div class="line"> </div><div class="line">val rdd4 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;&quot;),2)</div><div class="line">rdd4.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)//返回10 或者01</div><div class="line">/*因为有初始值(空字符串)的存在,所以如下过程:</div><div class="line">分区0:</div><div class="line"> math.min(&quot;&quot;.length,&quot;12&quot;.length)  ==&gt;0.toString  &quot;0&quot;</div><div class="line"> math.min(&quot;0&quot;.length,&quot;23&quot;.length) ==&gt;1.toString  &quot;1&quot; 最终结果===&gt;1</div><div class="line"> </div><div class="line">分区1:</div><div class="line"> math.min(&quot;&quot;.length,&quot;345&quot;.length) ==&gt;0.toString  &quot;0&quot;</div><div class="line"> math.min(&quot;0&quot;.length,&quot;&quot;.length)  ==&gt;0.toString  &quot;0&quot; 最终结果===&gt;0</div><div class="line"> </div><div class="line">分区全局聚合:&quot;&quot;+&quot;1&quot;+&quot;0&quot;  或者   &quot;&quot;+&quot;0&quot;+&quot;1&quot;</div><div class="line">*/</div><div class="line"> </div><div class="line">val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)</div><div class="line">rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) //结果: &quot;11&quot;</div></pre></td></tr></table></figure>
<h1 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">val rand = studentNamesRdd.sample(false,0.1,9)</div><div class="line"></div><div class="line">/*</div><div class="line">  /**</div><div class="line">   * Return a sampled subset of this RDD.</div><div class="line">   *</div><div class="line">   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)</div><div class="line">   * @param fraction expected size of the sample as a fraction of this RDD&apos;s size</div><div class="line">   *  without replacement: probability that each element is chosen; fraction must be [0, 1]</div><div class="line">   *  with replacement: expected number of times each element is chosen; fraction must be &gt;= 0</div><div class="line">   * @param seed seed for the random number generator</div><div class="line">   */</div><div class="line">  def sample(</div><div class="line">      withReplacement: Boolean,</div><div class="line">      fraction: Double,</div><div class="line">      seed: Long = Utils.random.nextLong): RDD[T] = withScope &#123;</div><div class="line">    require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction)</div><div class="line">    if (withReplacement) &#123;</div><div class="line">      new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)</div><div class="line">    &#125; else &#123;</div><div class="line">      new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">*/</div></pre></td></tr></table></figure>
<h1 id="union"><a href="#union" class="headerlink" title="union"></a>union</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">#union求并集，注意类型要一致</div><div class="line">val rdd6 = sc.parallelize(List(5,6,4,7))</div><div class="line">val rdd7 = sc.parallelize(List(1,2,3,4))</div><div class="line">val rdd8 = rdd6.union(rdd7)</div><div class="line">rdd8.distinct.sortBy(x=&gt;x).collect</div></pre></td></tr></table></figure>
<p>union算子原理</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/union算子原理.png" alt=""></p>
<h1 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"> val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2)</div><div class="line"></div><div class="line"> def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123;</div><div class="line">   iter.toList.map(x =&gt; &quot;[partID:&quot; +  index + &quot;, val: &quot; + x + &quot;]&quot;).iterator</div><div class="line"> &#125;</div><div class="line"> pairRDD.mapPartitionsWithIndex(func2).foreach(println)     //查看分区的结果</div><div class="line"></div><div class="line"> pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).foreach(println)  //在局部可以将key相同的放在一起迭代,math.max(_, _) 就是取一个分区中key相同的元素中的最大的值</div><div class="line"> pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).foreach(println)//会在每个分区中有一个初始化的值:100</div><div class="line"></div><div class="line">/*</div><div class="line">第一个参数:每个key的初始值</div><div class="line">第二个参数:如何进行shuffle map-side的本地聚合</div><div class="line">第三个参数:如何进行shuffle reduce-side的全局聚合</div><div class="line">*/</div><div class="line"></div><div class="line">/*</div><div class="line">//这里是分区信息</div><div class="line">[partID:0, val: (cat,2)]</div><div class="line">[partID:0, val: (cat,5)]</div><div class="line">[partID:0, val: (mouse,4)]</div><div class="line"></div><div class="line">[partID:1, val: (cat,12)]</div><div class="line">[partID:1, val: (dog,12)]</div><div class="line">[partID:1, val: (mouse,2)]</div><div class="line"></div><div class="line">//这里是aggregate的结果</div><div class="line">(dog,12)</div><div class="line">(cat,17)</div><div class="line"></div><div class="line">(mouse,6)</div><div class="line"></div><div class="line">(dog,100)</div><div class="line">(cat,200)</div><div class="line">(mouse,200)</div><div class="line">*/</div></pre></td></tr></table></figure>
<h1 id="cartesian笛卡尔积"><a href="#cartesian笛卡尔积" class="headerlink" title="cartesian笛卡尔积"></a>cartesian笛卡尔积</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">#cartesian笛卡尔积</div><div class="line"></div><div class="line">val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))</div><div class="line">val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))</div><div class="line">rdd1.cartesian(rdd2).foreach(println)</div><div class="line"></div><div class="line">/*</div><div class="line">(tom,tom)</div><div class="line">(tom,kitty)</div><div class="line">(tom,shuke)</div><div class="line">(jerry,tom)</div><div class="line">(jerry,kitty)</div><div class="line">(jerry,shuke)</div><div class="line">*/</div></pre></td></tr></table></figure>
<p>cartesian算子原理</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/cartesian算子原理.png" alt=""></p>
<h1 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line">coalesce算子,功能:将RDD的partition缩减</div><div class="line">将一定量的数据缩减到更少的partition中去</div><div class="line"></div><div class="line">使用场景,配合filter算子使用</div><div class="line">使用filter算子过滤掉很多数据以后,比如30%的数据,出现很多partition中的数据不均匀的情况</div><div class="line">此时建议使用coalesce算子,压缩rdd的partition的数量,从而让各个partition中的数据更加紧促</div><div class="line">*/</div><div class="line"></div><div class="line"></div><div class="line"> val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 3)</div><div class="line"></div><div class="line"> def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123;</div><div class="line">   iter.toList.map(x =&gt; &quot;[partID:&quot; +  index + &quot;, val: &quot; + x + &quot;]&quot;).iterator</div><div class="line"> &#125;</div><div class="line"> pairRDD.mapPartitionsWithIndex(func2).foreach(println)     //查看分区的结果</div><div class="line"></div><div class="line"> pairRDD.coalesce(2).mapPartitionsWithIndex(func2).foreach(println)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">/*</div><div class="line">------------第一次查看分区的结果-----------------------</div><div class="line">[partID:0, val: (cat,2)]</div><div class="line">[partID:0, val: (cat,5)]</div><div class="line"></div><div class="line">[partID:1, val: (mouse,4)]</div><div class="line">[partID:1, val: (cat,12)]</div><div class="line"></div><div class="line">[partID:2, val: (dog,12)]</div><div class="line">[partID:2, val: (mouse,2)]</div><div class="line"></div><div class="line">------------第二次查看分区的结果-----------------------</div><div class="line">[partID:0, val: (cat,2)]</div><div class="line">[partID:0, val: (cat,5)]</div><div class="line"></div><div class="line">[partID:1, val: (mouse,4)]</div><div class="line">[partID:1, val: (cat,12)]</div><div class="line">[partID:1, val: (dog,12)]</div><div class="line">[partID:1, val: (mouse,2)]</div><div class="line">*/</div></pre></td></tr></table></figure>
<p>coalesce算子原理</p>
<p><img src="http://ols7leonh.bkt.clouddn.com//assert/img/bigdata/spark从入门到精通_笔记/coalesce算子原理.png" alt=""></p>
<h1 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line">/*</div><div class="line">repartition可以将RDD的partition增多或者减少</div><div class="line">而coalesce仅仅能将rdd的partition减少</div><div class="line"></div><div class="line">reparation的使用场景</div><div class="line">使用spark sql从hive中查询数据时,spark sql会根据hive对应的HDFS文件的block数量来决定加载出来的数据rdd有多少个partition,这里的partition数量,是我们根本无法设置的,有些时候产生的partition数量少了,此时就可以在spark sql加载hive数据到rdd中以后,立即使用reparation算在,将rdd的partition数量变多</div><div class="line">*/</div><div class="line"></div><div class="line"> val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 1)</div><div class="line"></div><div class="line"> def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123;</div><div class="line">   iter.toList.map(x =&gt; &quot;[partID:&quot; +  index + &quot;, val: &quot; + x + &quot;]&quot;).iterator</div><div class="line"> &#125;</div><div class="line"> pairRDD.mapPartitionsWithIndex(func2).foreach(println)     //查看分区的结果</div><div class="line"></div><div class="line"> pairRDD.repartition(3).mapPartitionsWithIndex(func2).foreach(println)</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">/*</div><div class="line">------------第一次查看分区的结果-----------------------</div><div class="line">[partID:0, val: (cat,2)]</div><div class="line">[partID:0, val: (cat,5)]</div><div class="line">[partID:0, val: (mouse,4)]</div><div class="line">[partID:0, val: (cat,12)]</div><div class="line">[partID:0, val: (dog,12)]</div><div class="line">[partID:0, val: (mouse,2)]</div><div class="line"></div><div class="line">------------第二次查看分区的结果-----------------------</div><div class="line">[partID:0, val: (mouse,4)]</div><div class="line">[partID:0, val: (mouse,2)]</div><div class="line"></div><div class="line">[partID:1, val: (cat,2)]</div><div class="line">[partID:1, val: (cat,12)]</div><div class="line"></div><div class="line">[partID:2, val: (cat,5)]</div><div class="line">[partID:2, val: (dog,12)]</div><div class="line">*/</div></pre></td></tr></table></figure>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/44/"><i class="fa fa-angle-left"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/44/">44</a><span class="page-number current">45</span><a class="page-number" href="/page/46/">46</a><span class="space">&hellip;</span><a class="page-number" href="/page/56/">56</a><a class="extend next" rel="next" href="/page/46/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/header.jpg"
               alt="Mr. Chen" />
          <p class="site-author-name" itemprop="name">Mr. Chen</p>
           
              <p class="site-description motion-element" itemprop="description">一个技术渣的自说自话</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">555</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">28</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">36</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Mr. Chen</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  






  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  


  




	





  





  





  






  





  

  

  

  

  

</body>
</html>
