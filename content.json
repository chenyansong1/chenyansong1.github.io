[{"title":"Tachyon","date":"2017-05-02T08:25:41.834Z","path":"2017/05/02/bigdata/spark从入门到精通_笔记/Tachyon/","text":"不同的Application之间共享数据","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"数据仓库","date":"2017-04-30T01:16:03.044Z","path":"2017/04/30/数据仓库/数据仓库2/","text":"数据库:是一种逻辑概念,通过数据库软件来实现,数据库由很多表组成, 数据仓库:是一种逻辑的概念,从数据量来说,数据仓库要比数据库更庞大的多,数据仓库主要用于数据挖掘和数据分析,辅助领导做决策使用. 数据库和数据仓库的区别就是:OLTP和OLAP的区别:操作型处理(联机事务处理),他是针对具体业务在数据库联机的日常操作,通常对少数记录进行查询,修改,用户关心操作的相应时间数据的安全性,完整性和并发性支持的用户数等问题,传统的数据库作为数据管理的主要手段,主要用于操作型处理(如增删改查),主要面向应用 分析型处理(联机分析处理OLAP)一般针对某些主题的历史数据进行分析,支持管理决策","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://yoursite.com/tags/数据仓库/"}]},{"title":"IDEA","date":"2017-04-29T11:38:37.546Z","path":"2017/04/29/IDEA/IDEA/","text":"IntelliJ Idea 常用快捷键列表 Ctrl+Alt+t 选择代码块 try catch Alt+回车 导入包,自动修正Ctrl+N 查找类Ctrl+Shift+N 查找文件Ctrl+Alt+L 格式化代码Ctrl+Alt+O 优化导入的类和包Alt+Insert 生成代码(如get,set方法,构造函数等) mac系统 fn+alt+回车Ctrl+E或者Alt+Shift+C 最近更改的代码Ctrl+R 替换文本Ctrl+F 查找文本Ctrl+Shift+Space 自动补全代码Ctrl+空格 代码提示Ctrl+Alt+Space 类名或接口名提示Ctrl+P 方法参数提示Ctrl+Shift+Alt+N 查找类中的方法或变量Alt+Shift+C 对比最近修改的代码 Ctrl+Alt+H打开方法调用结构 &gt;&gt;&gt; Shift+F6 重构-重命名Ctrl+Shift+先上键Ctrl+X 删除行Ctrl+D 复制行Ctrl+/ 或 Ctrl+Shift+/ 注释（// 或者/…/ ）Ctrl+J 自动代码Ctrl+E 最近打开的文件Ctrl+H 显示类结构图Ctrl+Q 显示注释文档Alt+F1 查找代码所在位置Alt+1 快速打开或隐藏工程面板Ctrl+Alt+ left/right 返回至上次浏览的位置Alt+ left/right 切换代码视图Alt+ Up/Down 在方法间快速移动定位Ctrl+Shift+Up/Down 代码向上/下移动。F2 或Shift+F2 高亮错误或警告快速定位 代码标签输入完成后，按Tab，生成代码。选中文本，按Ctrl+Shift+F7 ，高亮显示所有该文本，按Esc高亮消失。Ctrl+W 选中代码，连续按会有其他效果选中文本，按Alt+F3 ，逐个往下查找相同文本，并高亮显示。Ctrl+Up/Down 光标跳转到第一行或最后一行下Ctrl+B 快速打开光标处的类或方法","tags":[{"name":"IDEA","slug":"IDEA","permalink":"http://yoursite.com/tags/IDEA/"}]},{"title":"ETL","date":"2017-04-29T04:13:07.272Z","path":"2017/04/29/数据仓库/ETL/","text":"etl的过程:数据的抽取,转换,加载,在设计ETL的时候我们也是从这三部分出发 数据的抽取:从各个不同的数据源抽取到ODS(Operational Data Store操作型数据存储)中,这个过程也可以做一些数据的清洗和转换,在抽取的过程中需要挑选不同的抽取方法,尽可能的提高ETL的运行效率 看数据从哪些业务系统来的(ERP,OA,CRM等) 增量更新的问题:1.时间戳的方式2.自增长id的方式 数据转换:这里涉及到无效数据的过滤,格式的转换 数据的加载:一般在数据清洗完之后直接写入DW(数据仓库)中去 大多数据仓库的数据架构可以概括为： 数据源–&gt;ODS(操作型数据存储)–&gt;DW–&gt;DM(data mart) ETL贯穿其各个环节。 ​一、数据抽取： 可以理解为是把源数据的数据抽取到ODS或者DW中。 源数据类型： 关系型数据库，如Oracle,Mysql,Sqlserver等; 文本文件，如用户浏览网站产生的日志文件，业务系统以文件形式提供的数据等； 其他外部数据，如手工录入的数据等； 抽取的频率： 大多是每天抽取一次，​也可以根据业务需求每小时甚至每分钟抽取，当然得考虑源数据库系统能否承受； 抽取策略： 个人感觉这是数据抽取中最重要的部分，可分为全量抽取和增量抽取。 全量抽取适用于那些数据量比较小，并且不容易判断其数据发生改变的诸如关系表，维度表，配置表等； 增量抽取，一般是由于数据量大，不可能采用全量抽取，或者为了节省抽取时间而采用的抽取策略； 如何判断增量，这是增量抽取中最难的部分，一般包括以下几种情况： a) 通过时间标识字段抽取增量；源数据表中有明确的可以标识当天数据的字段的流水表， 如createtime，updatetime等； b) 根据上次抽取结束时候记录的自增长ID来抽取增量；无createtime,但有自增长类型字段的流水表， 如自增长的ID，抽取完之后记录下最大的ID， 下次抽取可根据上次记录的ID来抽取； c) 通过分析数据库日志获取增量数据，无时间标识字段，无自增长ID的关系型数据库中的表； d) 通过与前一天数据的Hash比较，比较出发生变化的数据，这种策略比较复杂，在这里描述一下， 比如一张会员表，它的主键是memberID,而会员的状态是有可能每天都更新的， 我们在第一次抽取之后，生成一张备用表A，包含两个字段，第一个是memberID, 第二个是除了memberID之外其他所有字段拼接起来，再做个Hash生成的字段， 在下一次抽取的时候，将源表同样的处理,生成表B,将B和A左关联，Hash字段不相等的 为发生变化的记录，另外还有一部分新增的记录， 根据这两部分记录的memberID去源表中抽取对应的记录； e) 由源系统主动推送增量数据；例如订单表，交易表， 有些业务系统在设计的时候，当一个订单状态发生变化的时候，是去源表中做update， 而我们在数据仓库中需要把一个订单的所有状态都记录下来， 这时候就需要在源系统上做文章，数据库​触发器一般不可取。我能想到的方法是在业务系统上做些变动， 当订单状态发生变化时候，记一张流水表，可以是写进数据库，也可以是记录日志文件。 当然肯定还有其他抽取策略，至于采取哪种策略，需要考虑源数据系统情况， 抽取过来的数据在数据仓库中的存储和处理逻辑，抽取的时间窗口等等因素。 二、数据清洗： 顾名思义​，就是把不需要的，和不符合规范的数据进行处理。数据清洗最好放在抽取的环节进行， 这样可以节约后续的计算和存储成本； 当源数据为数据库时候，其他抽取数据的SQL中就可以进行很多数据清洗的工作了。 ​数据清洗主要包括以下几个方面： 空值处理；根据业务需要，可以将空值替换为特定的值或者直接过滤掉； 验证数据正确性；主要是把不符合​业务含义的数据做一处理，比如，把一个表示数量的字段中的字符串 替换为0，把一个日期字段的非日期字符串过滤掉等等； 规范数据格式；比如，把所有的日期都格式化成YYYY-MM-DD的格式等； ​数据转码；把一个源数据中用编码表示的字段，通过关联编码表，转换成代表其真实意义的值等等； 数据标准，统一；比如在源数据中表示男女的方式有很多种，在抽取的时候，直接根据模型中定义的值做转化， 统一表示男女； 其他业务规则定义的数据清洗。。。 三、数据转换和加载： 很多人理解的ETL是在经过前两个部分之后，加载到数据仓库的数据库中就完事了。 数据转换和加载不仅仅是在源数据–&gt;ODS这一步，ODS–&gt;DW, DW–&gt;DM包含更为重要和复杂的ETL过程。 什么是ODS？ ODS（Operational Data Store）是数据仓库体系结构中的一个可选部分， ODS具备数据仓库的部分特征和OLTP系统的部分特征， 它是“面向主题的、集成的、当前或接近当前的、 不断变化的”数据。​—摘自百度百科 其实大多时候，ODS只是充当了一个数据临时存储，数据缓冲的角色。一般来说， 数据由源数据加载到ODS之后，会保留一段时间，当后面的数据处理逻辑有问题，需要重新计算的时候， 可以直接从ODS这一步获取，而不用再从源数据再抽取一次，减少对源系统的压力。 另外，ODS还会直接给DM或者前端报表提供数据，比如一些维表或者不需要经过计算和处理的数据； 还有，ODS会完成一些其他事情，比如，存储一些明细数据以备不时之需等等； 数据转换(刷新)： 数据转换，更多的人把它叫做数据刷新，就是用ODS中的增量或者全量数据来刷新DW中的表。 DW中的表基本都是按照事先设计好的模型创建的，如事实表，维度表，汇总表等， 每天都需要把新的数据更新到这些表中。 更新这些表的过程(程序)都是刚开始的时候开发好的，每天只需要传一些参数,如日期，来运行这些程序即可。 数据加载： 个人认为，每insert数据到一张表，都可以称为数据加载，至于是delete+insert、truncate+insert、 还是merge，这个是由业务规则决定的，这些操作也都是嵌入到数据抽取、转换的程序中的。 四、ETL工具： 在传统行业的数据仓库项目中，大多会采用一些现成的ETL工具，如Informatica、Datastage、微软SSIS等。 这三种工具我都使用过，优点有：图形界面，开发简单，数据流向清晰；缺点：局限性，不够灵活， 处理大数据量比较吃力，查错困难，昂贵的费用； 选择ETL工具需要充分考虑源系统和数据仓库的环境，当然还有成本，如果源数据系统和数据仓库都采用 ORACLE，那么我觉得所有的ETL，都可以用存储过程来完成了。。 在大一点的互联网公司，由于数据量大，需求特殊，ETL工具大多为自己开发， 或者在开源工具上再进行一些二次开发，在实际工作中， 一个存储过程，一个shell/perl脚本，一个java程序等等，都可以作为ETL工具。 ​ 五、ETL过程中的元数据： 试想一下，你作为一个新人接手别人的工作，没有文档，程序没有注释， 数据库中的表和字段也没有任何comment，你是不是会骂娘了？ 业务系统发生改变，删除了一个字段，需要数据仓库也做出相应调整的时候， 你如何知道改这个字段会对哪些程序产生影响？ 源系统表的字段及其含义，源系统数据库的IP、接口人，数据仓库表的字段及其含义， 源表和目标表的对应关系，一个任务对应的源表和目标表，任务之间的依赖关系， 任务每次执行情况等等等等，这些元数据如果都能严格的管控起来，上面的问题肯定不会是问题了","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://yoursite.com/tags/数据仓库/"}]},{"title":"PowderDesigner的使用","date":"2017-04-28T08:41:26.804Z","path":"2017/04/28/数据仓库/PowderDesigner/","text":"为什么要先有概念数据模型,再通过概念数据模型生成物理数据模型? 以为概念数据模型为实际事务的实体抽象,通过在概念中指定实体与实体之间的关系(一对一,一对多,多对多)),这样使用概念数据模型生成物理数据模型的时候直接在物理数据模型的实体中有实体间的属性关系字段生成,而不用我们指定对应的关系字段 通过生成的物理数据模型可以根据我们指定的数据库的类型生成对应的数据库sql脚本,这样我们那到这些数据库脚本去数据库中执行,就在数据库中生成了对应的表,视图等","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://yoursite.com/tags/数据仓库/"}]},{"title":"hive实战","date":"2017-04-26T01:29:54.618Z","path":"2017/04/26/bigdata/hive/hive_new/hive实战/","text":"hive数据库创建时的编码问题12javax.jdo.option.connectionURLjdbc:mysql://192.168.0.11:3306/metastore_hive_db?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8 如果数据库不存在,那么创建的库为utf-8,而hive不支持utf-8的格式,将会出现乱码的情况,此时我们需要手动在mysql库中创建metastore_hive_db,编码为latin1格式,这样就能解决这个问题 1.手动创建元数据库,设置编码为latin11jdbc:mysql://192.168.0.11:3306/metastore_hive_db?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8 2.启动hive让其生成元数据库中的表 3.修改表字段注解和表注解 123alter table COLUMNS_V2 modify column comment varchar(256) character set utf8;alter table TABLE_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; 4.修改分区字段注解123alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(4000) character set utf8; 5.修改索引注释1alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8; 6.去hive中创建建库,建表 123456789101112131415161718192021222324252627create database ods;create database dw;create database dim;create database app;#然后去每个库下面建我们需要的表,如:CREATE TABLE `dim.dim_category` ( `tree_id` int COMMENT &apos;分类树ID&apos;, `tree_name` varchar(64) COMMENT &apos;分类树名称&apos;, `category_id` int COMMENT &apos;分类ID&apos;, `category_name` varchar(64) COMMENT &apos;分类名称&apos;, `category_type` int COMMENT &apos;分类类型0后台分类1前台分类&apos;, `parent_id` int COMMENT &apos;上级分类树根的是0&apos;, `parent_name` varchar(64) COMMENT &apos;父分类名称&apos;, `layer` tinyint COMMENT &apos;层级&apos;, `sort` int COMMENT &apos;分类排序&apos;, `path` varchar(255) COMMENT &apos;所有上级分类&apos;, `is_fmcg` tinyint comment &apos;是否快消品1是0否&apos;, `is_open` tinyint COMMENT &apos;是否打开1是0否&apos;, `is_show` tinyint COMMENT &apos;app是否显示1是0否&apos;, `visibility` tinyint COMMENT &apos;商户端是否显示1是0否&apos;) comment &apos;分类表&apos;ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;\\t&apos;STORED AS TEXTFILE; 远程连接hive元数据库启动元数据 #这个服务可以让远程去连接hive的元数据库bin/hive –service metastore &amp; #如果是jdbc去连接hive的话需要启动这个服务bin/hive –service hiveserver2 &amp; 如果是本地去连接远程,则在本地客户端需要配置12345&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://192.168.0.11:9083&lt;/value&gt; &lt;description&gt;运行hive的主机地址及端口&lt;/description&gt;&lt;/property&gt; hive-env.sh 配置12HADOOP_HOME=export HIVE_CONF_DIR= hive的日志12hive -hiveconf hive.root.logger=DEBUG,console#将DEBUG以上的所有级别的日志打印到控制台,在调试的时候可以使用 常用工作语句123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172create database ods;create database dw;create database app;create database dim;show databases;use ods;show tables;show tables like &apos;ods_*&apos;;#查看表的信息desc formatted ods.ods_order;可以看表的字段, 分区信息, 表所在的location, 表的类型#可以看文件的数据hive&gt;dfs -ls /user/hive/warehourse/ods.db/ods_order#查看文件的大小hive (ods)&gt; dfs -du -h /user/hive/warehouse/ods.db/;#查看执行计划explain select * from ods.ods_order ;#查看执行计划中的执行了哪些分区(对于查询语句中,分区没有卡死的情况下,我们可以很容易的知道)explain dependency select * from test where dt=&apos;20151010&apos; ;hive (ods)&gt; explain dependency select * from ods_order where dt=&apos;20151010&apos;;OKExplain&#123;&quot;input_partitions&quot;:[&#123;&quot;partitionName&quot;:&quot;ods@ods_order@dt=20151010&quot;&#125;],&quot;input_tables&quot;:[&#123;&quot;tablename&quot;:&quot;ods@ods_order&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;#可以看到输入分区,表,表类型#查看表的分区的信息show partitions app_order_city_d;#查看hadoop的任务yarn application -list|grep 用户名#有时候,知道hive中出错了,不要让他跑了,此时手动杀掉任务yarn application -kill application_143432432432423_86621#查看hive的函数show functions;#找到某个我们不记得名字的函数show functions like &apos;xpath*&apos;;#查看某个函数的用法,其中有例子desc function extended upper ;#hive查看表存储实际路径,找到locationdesc extended dw_order ;#添加分区alter table ods_order add partition (dt=&apos;20151111&apos;);#删除分区alter table ods_order drop if exists partition (dt=&apos;20151111&apos;);#开启mapreduceset hive.fetch.task.conversion=minimal,more 创建分区加载数据1234567891011121314151617181920hive (ods)&gt; alter table ods_customer add partition (dt=&apos;20151210&apos;);OKTime taken: 0.19 secondshive (ods)&gt; show partitions ods_customer;OKpartitiondt=20151210Time taken: 0.151 seconds, Fetched: 1 row(s)hive (ods)&gt; load data local inpath &apos;/home/hadoop/app/hive/script/t_customer.txt&apos; overwrite into table ods_customer partition (dt=&apos;20151210&apos;);hive (ods)&gt; dfs -du -h /user/hive/warehouse/ods.db/ods_customer/ ;7.6 M /user/hive/warehouse/ods.db/ods_customer/dt=20151210#有的时候采用追加的方式去加载数据load data local inpath &apos;/home/hadoop/app/hive/script/t_customer.txt&apos; into table ods_customer partition (dt=&apos;20151210&apos;); 一些业务逻辑1234567891011121314151617select city_id,sum(case when order_status=5 then 1 else 0 end) as cnt_ord_succ_d,sum(case when order_status=3 then 1 else 0 end) as cnt_ord_cacel_d,sum(1) as cnt_ord_d, &lt;----每天的每个城市的下单数count(distinct CUST_ID) as cnt_ord_user &lt;----- 下单的用户数FROM dw.dw_order WHERE dt=&apos;$&#123;day_01&#125;&apos; and city_id is not nullgroup by city_id ;1.以城市id作为分组2.case when 进行转换 case when order_status=5 then 1 else 0 end3.sum对case when的结果进行求和4.统计每天下单的用户数,因为用户存在多次购买,所以使用distinct去重,然后使用count去统计去重之后的个数count(distinct CUST_ID) 求商品的复购率 123456789101112131415161718192021222324252627282930313233343536需求列出的商品的7日,15日,30复购率，目的了解这几款商品的周期.计算口径:当日购买部分商品的用户数/7日重复购买此商品的用户数。每天查看每个城市每个商品当日购买用户数，7日15日30日复购率。SELECT t3.atdate AS cdate,t3.city_id,t3.goods_id,COUNT(DISTINCT CASE WHEN days=0 THEN t3.cust_id END) AS cnt_buy_cust_d, #取当前的购买人数COUNT(DISTINCT CASE WHEN days&gt;0 AND days&lt;=7 THEN t3.cust_id END) AS cnt_buy_cust_7_d, #取7天的购买人数,同一个用户多次购买,只取一次COUNT(DISTINCT CASE WHEN days&gt;0 AND days&lt;=15 THEN t3.cust_id END) AS cnt_buy_cust_15_d, #取15天的购买人数COUNT(DISTINCT CASE WHEN days&gt;0 AND days&lt;=30 THEN t3.cust_id END) AS cnt_buy_cust_30_dFROM ( SELECT t1.atdate,t1.city_id,t1.cust_id,t1.goods_id, DATEDIFF(t2.atdate, t1.atdate) days ###第3步 FROM (###第一步 SELECT o.order_date AS atdate,o.city_id, o.cust_id,og.goods_id FROM dw.dw_order o INNER JOIN dw.dw_order_goods og ON o.order_id=og.order_id AND o.ORDER_STATUS = 5 AND og.source_id=1 AND o.dt = &apos;20151010&apos; ) t1 INNER JOIN (###第2步 SELECT o.order_date AS atdate,o.city_id, o.cust_id,og.goods_id, og.goods_name FROM dw.dw_order o INNER JOIN dw.dw_order_goods og ON o.order_id=og.order_id AND o.ORDER_STATUS = 5 AND og.source_id=1 ) t2 ON t1.cust_id=t2.cust_id AND t1.goods_id=t2.goods_id) t3 GROUP BY t3.atdate,t3.city_id,t3.goods_id;#这样在做数据展现的时候,求7日复购率select round(cnt_buy_cust_d/cnt_buy_cust_7_d) from t1其实在O2O中复购率可以看一些像:蔬菜,水果,生鲜等的上架周期 月平均日客户数 1234567891011121314目前有一个合作资源，北京某度假酒店，价值几百到8000不等的酒店套房，一共100套，可以给到购买200元以上订单用户，用于抽奖奖品，比如设置的获奖条件：凡在9月,10月,11月的用户，下单200元以上的订单，即可获得北京某度假酒店。目的带动销量，刺激用户参与活动，同时给合作方导流。合作方需要知道我们订单金额在200以上的每天平均的用户量是多少.#客户id是int类型 需注意用countSELECT SUM(CASE WHEN t.COMPLETION_DATE&gt;=&apos;20151001&apos; AND t.COMPLETION_DATE&lt;=&apos;20151031&apos; THEN 1 ELSE 0 END) AS cnt_ord_10_m //返回的是订单数量,COUNT(DISTINCT CASE WHEN t.COMPLETION_DATE&gt;=&apos;20151001&apos; AND t.COMPLETION_DATE&lt;=&apos;20151031&apos; THEN CUST_ID END) AS cnt_cust_10_m //返回的是客户数量FROM dw.dw_order t WHERE t.COMPLETION_DATE&gt;=&apos;20151001&apos; AND t.COMPLETION_DATE&lt;=&apos;20151031&apos; AND CITY_ID=2AND ORDER_TYPE &lt;&gt;6AND PAYABLE_AMOUNT&gt;100AND t.ORDER_STATUS=5;#注意这里的sum和count分别的作用 求每个用户累计订单数，累计应付金额 123456789101112#1.在dw_customer表中有累计的订单数和累计的订单总额,这样就可以避免去全表扫描dw_order表,因为订单表很大的,这样很耗时;#2.为什么要用full join?因为dw_customer表中并不是最新的用户表,如果当天有新增的用户,而新增的用户还不存在于dw_customer表中,而此时新增用户还有购买行为,那么如果使用left join就会将新增的用户过滤掉了,所以此处使用的是full joinselect nvl(t1.cust_id,t2.cust_id), nvl(t2.order_cnt,0)+nvl(t1.order_cnt,0) as order_cnt,nvl(t2.amount_sum,0)+nvl(t1.amount_sum,0) as amount_sumfrom dw.dw_customer t1full outer join (select cust_id,count(1) as order_cnt,sum(payable_amount) as amount_sum from dw.dw_order where dt=&apos;20151011&apos; and order_status=5group by cust_id) t2 on t1.cust_id=t2.cust_idand t1.dt=20151210 limit 100; 新用户统计信息Hql分析(日粒度)123456789101112131415161718#查看所有的时间函数show functions like &quot;*time*&quot;;/*这里涉及到两个时间的转换:1.将时间字符串转成毫秒数2015/5/1 21:46 转成毫秒数2.将毫秒数转成指定格式的时间字符串from_unixtime将毫秒数转成指定格式的字符串其实一般处理时间会使用我们写的UDF函数去统一处理*/select count(1) from dw.dw_customerwhere dt=&apos;20151210&apos; and from_unixtime(unix_timestamp(register_time,&apos;yyyy/MM/dd HH:mm&apos;),&apos;yyyyMMdd&apos;)=&apos;20140610&apos;; 求5,6月各个渠道带来的新用户(注册时间)，以此来考核运营部门的kpi 123456select source_no,count(1) from dw.dw_customerwhere dt=20151211 andfrom_unixtime(unix_timestamp(register_time,&apos;yyyy/MM/dd HH:mm&apos;),&apos;yyyyMMdd&apos;)&gt;=&apos;20141201&apos;and from_unixtime(unix_timestamp(register_time,&apos;yyyy/MM/dd HH:mm&apos;),&apos;yyyyMMdd&apos;)&lt;=&apos;20150131&apos; and source_no is not nullgroup by source_no; 统计各个渠道带来的用户，top10完成订单数 12345678#求分组中的top10,使用row_number()函数:partition by 表示分组的字段, order by 表示组内排序的字段select source_no,mobile,order_cnt,rn from (select source_no,order_cnt,mobile,row_number() over(partition by source_no order by order_cnt desc) as rn from dw.dw_customer where dt=20151211 and source_no is not null and order_cnt is not null) t2 where rn &lt;10; etl开发模板1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#!/bin/bash#这里是需要用到的函数. /home/anjianbing/soft/functions/wait4FlagFile.sh# ===========================================================================# 程序名称: # 功能描述: 城市每日完成订单数# 输入参数: 运行日期 20151010# 目标表名: app.app_order_city_d 到哪张表来# 数据源表: dw.dw_order 从哪张表来# 创建人: 安坚兵# 创建日期: 2015-12-21# 版本说明: v1.0# 代码审核: # 修改人名:# 修改日期:# 修改原因:# 修改列表: # ===========================================================================### 1.参数加载exe_hive=&quot;hive&quot;# 因为有的时候,可能会运行脚本的时候指定参数:sh test.sh 20151010,此时就是运行指定天的数,否则默认是运行前一天的数据if [ $# -eq 1 ]then day_01=`date --date=&quot;$&#123;1&#125;&quot; +%Y-%m-%d`else day_01=`date -d&apos;-1 day&apos; +%Y-%m-%d`fi#拿到时间的年月日syear=`date --date=$day_01 +%Y`smonth=`date --date=$day_01 +%m`sday=`date --date=$day_01 +%d`#目标数据库和表TARGET_DB=app.dbTARGET_TABLE=app_order_city_d### 2.定义执行HQLHQL=&quot;insert overwrite table app.app_order_city_d partition (dt=&apos;$&#123;day_01&#125;&apos;)SELECT city_id,COUNT(1) FROM dw.dw_order WHERE dt=&apos;$&#123;day_01&#125;&apos; AND order_status=5 GROUP BY city_id;&quot;### 3.检查依赖 (因为只要等ods层的表跑完了,才能跑dw层的表,同样只有等dw层的表跑完了,才能跑app层的表)wait4FlagFile HDFS /user/hive/warehouse/dw.db/dw_order/dt=$&#123;day_01&#125; _SUCCESS 15801152142#### 4.执行HQL$exe_hive -e &quot;$HQL&quot;#### 5. 判断代码是否执行成功，touch控制文件result=`hadoop fs -ls /user/hive/warehouse/$&#123;TARGET_DB&#125;/$&#123;TARGET_TABLE&#125;/dt=$&#123;day_01&#125; | wc -l`if [[ $result -gt 0 ]]; then #为下面的依赖做准备 hadoop fs -touchz /user/hive/warehouse/$&#123;TARGET_DB&#125;/$&#123;TARGET_TABLE&#125;/dt=$&#123;day_01&#125;/_SUCCESSelse echo &quot;失败发送预警短信和邮件&quot;fi 给已经排好序的表添加序列号的字段12345#给已经排好序的表添加序列号的字段select student_nu, score, row_number() over() as numfrom student order by score; 行转列123456789101112name class scorezhangsan math 99zhangsan chinese 88lisi math 99lisi chinese 100#行转列selectname,max(case when class=&apos;math&apos; then score) as math_score,max(case when class=&apos;chinese&apos; then score) as math_score,from student group by name ;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive项目实战之服访问日志处理","date":"2017-04-25T09:12:51.610Z","path":"2017/04/25/bigdata/hive/hive_new/hive项目实战之服访问日志处理/","text":"思路 创建原表 针对不同的业务创建不同的子表 数据存储格式:orcfile/parquet 数据压缩:snappy map output 数据压缩:snappy 外部表 分区表 创建原表123456789101112create table if not exists default.log_src(remote_add string,remote_user string,request string,host string//...其他的字段)comment &apos;access log&apos;row format delimited fields terminated by &apos; &apos;stored as textfile ; 加载数据1load data local inpath &apos;/datas/access.log&apos; into table default.log_src ; 会发现,我们的数据是不会正确的加载到表中的,因为使用空格的时候会出现错乱 hive的官网对web log推荐使用正则表达式 这里是hive官网的例子: https://issues.apache.org/jira/browse/HIVE-662 123456789101112131415161718192021CREATE TABLE serde_regex( host STRING, identity STRING, user STRING, time STRING, request STRING, status STRING, size STRING, referer STRING, agent STRING)ROW FORMAT SERDE &apos;org.apache.hadoop.hive.contrib.serde2.RegexSerDe&apos;WITH SERDEPROPERTIES ( &quot;input.regex&quot; = &quot;([^ ]*) ([^ ]*) ([^ ]*) (-|\\\\[[^\\\\]]*\\\\]) ([^ \\&quot;]*|\\&quot;[^\\&quot;]*\\&quot;) (-|[0-9]*) (-|[0-9]*)(?: ([^ \\&quot;]*|\\&quot;[^\\&quot;]*\\&quot;) ([^ \\&quot;]*|\\&quot;[^\\&quot;]*\\&quot;))?&quot;, &quot;output.format.string&quot; = &quot;%1$s %2$s %3$s %4$s %5$s %6$s %7$s %8$s %9$s&quot;)STORED AS TEXTFILE;LOAD DATA LOCAL INPATH &quot;../data/files/apache.access.log&quot; INTO TABLE serde_regex;LOAD DATA LOCAL INPATH &quot;../data/files/apache.access.2.log&quot; INTO TABLE serde_regex;SELECT * FROM serde_regex ORDER BY time; 对于不同的日志格式,我们可能需要些不同的正则表达式去匹配,下面的网站是校验你的正则表达式是否正确的网站,需要提供一条日志和你写的正则表达式 http://wpjam.qiniudn.com/tool/regexpal/ 写正则表达式的规律:一个()就是一个字段,每个字段之间使用空格隔开,这样针对每一个字段在括号中写需要匹配的正则表达式 如果没有正则表达式,那么我们要写MapReduce做预处理了 根据不同的业务创建不同的子表1234567891011121314create table if not exists default.log_comm(remote_add string,remote_user string,request string,host string)comment &apos;access log common&apos;row format delimited fields terminated by &apos;\\t&apos;stored as orc tblproperties (&quot;orc.compress&quot;=&quot;SNAPPY&quot;) ;insert into table default.log_comm select remote_add, remote_user, request, host from default.log_src ; 自定义UDF进行数据清洗定义UDF对原表数据进行清洗 定义UDF去除引号1.UDF编码 1234567891011121314#下面是伪代码class RemoveQuotesUDF extends UDF&#123; public TEXT evaluate(Text str)&#123; if(null==str.toString())&#123; return null; &#125; //将所有的引号替换为空 return new Text(str.toString().replaceAll(&quot;\\&quot;&quot;, &quot;&quot;)); &#125;&#125; 2.将上面的代码打包导出/opt/jars/udf.jar 3.add jar /opt/jars/udf.jar ; 4.create temporary function my_removequotes as “com.study.udf.RemoveQuotesUDF” ; 5.list jars ; 6.12insert overwrite table default.log_comm select my_removequotes(remote_add), remote_user, request, host from default.log_src ; 定义UDF日期转换源数据中的日期的格式为:31/Aug/2015:00:04:37 +0800udf之后的数据为:20150110437 udf代码12345678910111213141516171819202122232425262728293031SimpleDateFormat inputFormat = new SimpleDateFormat(&quot;dd/MM/yyyy:HH:mm:ss&quot;, Locale.ENGLISH);SimpleDateFormat outputFormat = new SimpleDateFormat(&quot;yyyyMMddHHmmss&quot;, Locale.ENGLISH);public Text evalue(Text input)&#123; Text output = new Text(); if(null==str||null==str.toString())&#123; return null; &#125; String inputDate = input.toString().trim(); if(null == inputDate)&#123; return null; &#125; try&#123; //parse Date parseDate = inputFormat.parse(inputDate); //transform String outputDate = outputFormat.format(parseDate); //set output.set(outputDate); &#125;catch&#123;Exception e)&#123; e.printStackTrace(); &#125; return output;&#125; 2.将上面的代码打包导出/opt/jars/udf2.jar 3.add jar /opt/jars/udf2.jar ; 4.create temporary function my_datetransform as “com.study.udf.DateTransformUDF” ; 5.list jars ; 6.12insert overwrite table default.log_comm select my_removequotes(remote_add), my_datetransform(time_local), remote_user, request, host from default.log_src ; 根据业务编写hiveQL统计每天每小时的访问量,并排序12345678select substring(time_local,9,2) hour from default.log_comm;select t.hour, count(*) cnt from (select substring(time_local,9,2) hour from default.log_comm) tgroup by t.hour order by cnt desc ; 根据访问的ip地址进行地域统计 12345select t.prex_ip, count(*) cnt from (select substring(ip, 1, 7) prex_ip from log_comm ) tgroup by t.prex_ip order by cnt desc ; 使用python脚本进行数据清洗和统计12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#数据的格式userid movieid rate time196 33 4 881250949#创建表create table if not exists u_data(userid int,movieid int,rating int,unixtime string)row format delimitedfields terminated by &apos;\\t&apos;stored as textfile#导入数据load data local inpath &apos;/datas/u.data&apos; overwrite into table u_data ;#创建一个新的表(此时时间变成weekday星期几)create table if not exists u_data_new(userid int,movieid int,rating int,weekday int)row format delimitedfields terminated by &apos;\\t&apos;stored as textfile#加载python文件add file weekday_mapper.py ;#向新表中插入数据insert overwrite table u_data_new select transform(userid,movieid, rating, unixtime) using &apos;python weekday_mapper.py&apos; as(user, movieid, rating, weekday)from u_data ;/*transform方法中传递过去的是u_data表中的指定的字段using 表示使用的是哪个python脚本as表示调用脚本传递出来的数据*/#按星期分组统计select weekd, count(*) from u_data_new group by weekday ; python脚本12345678import sysimport datetimefor line in sys.stdin: line = line.strip() userid,movieid, rating, unixtime = line.split(&apos;\\t&apos;) weekday = datetime.datetime.fromtimestamp(float(unixtime)) print &apos;\\t&apos;.join([userid,movieid, rating, str(weekday)])","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中的优化","date":"2017-04-25T03:02:57.947Z","path":"2017/04/25/bigdata/hive/hive_new/hive中的优化/","text":"hive.fetch.task.conversion有些情况不走MapReduce,这样可以更快 1234567891011121314151617181920212223242526272829vim conf/hive-site.xml &lt;property&gt; &lt;name&gt;hive.fetch.task.conversion&lt;/name&gt; &lt;value&gt;more&lt;/value&gt; &lt;description&gt; 对于当前查询只是单表,没有子查询和聚合以及distinct和join下面的情况不会走mapreduce Some select queries can be converted to single FETCH task minimizing latency. Currently the query should be single sourced not having any subquery and should not have any aggregations or distincts (which incurs RS), lateral views and joins. 0. none : disable hive.fetch.task.conversion 1. minimal : SELECT STAR, FILTER on partition columns, LIMIT only 2. more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) &lt;/description&gt; &lt;/property&gt;1.minimal 对于select* 和where条件是分区字段以及limit等不会走MapReduceselect * from test;select * from test where monday=&apos;201509&apos; ; (此时test表有monday=&apos;201509&apos;这个分区)select * from test limit 5;2.more : SELECT, FILTER, LIMIT only (support TABLESAMPLE and virtual columns) SELECT,filter,limit不走MapReduceselect job, ename from emp ;select job, ename from emp_partition where ename=&apos;SMITH&apos;; 大表的拆分将大表(字段很多的表),我们只需要对其中的某些字段进行操作,那么我们可以创建子表(取我们需要的字段) 12345create table if not exists test_subrow format delimited fields terminated by &apos;,&apos;stored as orcas select id,name from test 外部表和分区表通常这两个表是结合使用的 12345678910create external table if not exists test_sub(id int,name string)comment &apos;xx&apos;partitioned by (month string, day string)row format delimited fields tereminated by &apos;,&apos;location &apos;xx.txt&apos; ; 数据的存储格式和数据的压缩 存储格式:orcfitle,parquet 数据压缩:snappy sql优化 优化sql语句 join Reduce join大表对大表:每个表的数据都是从文件中读取的 Map join 12#如果设置了这个参数,那么程序会识别map joinset hive.auto.convert.join=true ; 小表对大表;大表的数据是从文件从读取的,小表的数据放入到内存中DistributedCache 类将小表缓存起来 SMB join Sort-Merge-bucket join 是对上面的大表对大表的优化策略 123set hive.auto.convert.sortmerge.join=true;set hive.optimize.bucketmapjoin = true;set hive.optimize.bucketmapjoin.sortedmerge=true; group by出现数据倾斜的问题:对于group by (col_name) ,如果进行group的col_name字段很多为空,那么会出现数据倾斜的问题,此时最好将该字段变成非空,然后就不会数据倾斜 count(distinct xx)也是会出现数据倾斜的问题 执行计划12345explain select * from emp ;explain select deptno, avg(sal) avg_sal from emp group by deptno;explain extended select deptno, avg(sal) avg_sal from emp group by deptno; 并行执行1234567job1 a join b ==&gt; aajob2 c join d ==&gt; ccjob3 aa join cc#因为job1和job2没有依赖关系,所以可以并行执行 可以设置如下的参数进行配置 12345678910&lt;property&gt; &lt;name&gt;hive.exec.parallel&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;description&gt;Whether to execute jobs in parallel&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.parallel.thread.number&lt;/name&gt; &lt;value&gt;8&lt;/value&gt; &lt;description&gt;How many jobs at most can be executed in parallel&lt;/description&gt;&lt;/property&gt; jvm重用123hive&gt;set mapreduce.job.jvm.numtasks;#可以设置jvm中跑多少个MapReduce reduce数目12set mapreduce.job.reduces; 推测执行关掉推测执行,不然有的时候,对同一个任务会启动两个MapReduce12340: jdbc:hive2://hdp-node-01:10000&gt; set hive.mapred.reduce.tasks.speculative.execution=false;0: jdbc:hive2://hdp-node-01:10000&gt; set mapreduce.reduce.speculative=false;0: jdbc:hive2://hdp-node-01:10000&gt; set mapreduce.map.speculative=false; 动态分区Strict Mode在分区表进行查询,在where子句中没有加分区过滤的话,将禁止提交任务(默认是nostrict) 123456789101112131415161718192021222324252627282930313233343536373839404142430: jdbc:hive2://hdp-node-01:10000&gt; set hive.mapred.mode ;+-----------------------------+--+| set |+-----------------------------+--+| hive.mapred.mode=nonstrict |+-----------------------------+--+1 row selected (0.017 seconds)0: jdbc:hive2://hdp-node-01:10000&gt; set hive.mapred.mode=strict;No rows affected (0.016 seconds)0: jdbc:hive2://hdp-node-01:10000&gt; set hive.mapred.mode ;+--------------------------+--+| set |+--------------------------+--+| hive.mapred.mode=strict |+--------------------------+--+1 row selected (0.01 seconds)#对分区表执行查询数据0: jdbc:hive2://hdp-node-01:10000&gt; select * from emp_partition;Error: Error while compiling statement: FAILED: SemanticException [Error 10041]: No partition predicate found for Alias &quot;emp_partition&quot; Table &quot;emp_partition&quot; (state=42000,code=10041)0: jdbc:hive2://hdp-node-01:10000&gt; 0: jdbc:hive2://hdp-node-01:10000&gt; show partitions emp_partition;+---------------+--+| partition |+---------------+--+| month=201509 || month=201510 || month=201511 |+---------------+--+3 rows selected (0.165 seconds)0: jdbc:hive2://hdp-node-01:10000&gt; select * from emp_partition where month=&apos;201509&apos; limit 3;+----------------------+----------------------+--------------------+--------------------+-------------------------+--------------------+---------------------+-----------------------+----------------------+--+| emp_partition.empno | emp_partition.ename | emp_partition.job | emp_partition.mgr | emp_partition.hiredate | emp_partition.sal | emp_partition.comm | emp_partition.deptno | emp_partition.month |+----------------------+----------------------+--------------------+--------------------+-------------------------+--------------------+---------------------+-----------------------+----------------------+--+| 7369 | SMITH | CLERK | 7902 | 1980-12-17 | 800.0 | NULL | 20 | 201509 || 7499 | ALLEN | SALESMAN | 7698 | 1981-2-20 | 1600.0 | 300.0 | 30 | 201509 || 7521 | WARD | SALESMAN | 7698 | 1981-2-22 | 1250.0 | 500.0 | 30 | 201509 |+----------------------+----------------------+--------------------+--------------------+-------------------------+--------------------+---------------------+-----------------------+----------------------+--+3 rows selected (0.19 seconds)0: jdbc:hive2://hdp-node-01:10000&gt; 使用严格模式可以禁止3种类型的查询: 对于分区表,不加分区字段过滤条件的不能执行 对于order by, 没有使用limit的不能执行 对于笛卡尔积的查询,使用where但是没有使用on的不能进行查询","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中数据的存储格式和Snappy压缩","date":"2017-04-25T01:50:12.762Z","path":"2017/04/25/bigdata/hive/hive_new/hive中使用Snappy压缩/","text":"12345hive&gt;set mapreduce.map.output.compress=true;hive&gt;set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec ;//同时需要在hadoop按照的时候支持Snappy 在hive中一般使用的存储格式为:orc,parquet数据压缩一般使用的是snappy 123456789101112131415161718192021222324create table if not exists test_textfile(...)stored as textfilecreate table if not exists test_orc_snappy(...)stored as orc tblproperties(&quot;orc.compress&quot;=&quot;SNAPPY&quot;)#储存使用的是orc#压缩使用的是SNAPPY向压缩的表中插入数据:insert into tble test_orc_snappy select * from test_textfile","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"HiveServer2, Beeline, JDBC的使用","date":"2017-04-24T08:22:48.029Z","path":"2017/04/24/bigdata/hive/hive_new/HiveServer2,Beeline,JDBC的使用/","text":"启动hiveserver2服务hiveServer2是将hive作为一个服务启动,这样可以有更多的客户端连接过来,他启动的进程服务叫RunJar 1bin/hiveserver2 beeline连接hiveserver212345678910111213141516171819202122232425262728293031323334bin/beelinebeeline&gt;!connec jdbc:hive2://hdp-node-01:10000用户名输入当前用户(root),密码是空(回车即可)#这样就相当于进入了cli中[root@hdp-node-01 hive]# bin/hiveserver2 &amp;[root@hdp-node-01 hive]# bin/beeline Beeline version 1.2.1 by Apache Hive#去连接服务beeline&gt; !connec jdbc:hive2://hdp-node-01:10000Connecting to jdbc:hive2://hdp-node-01:10000#输入用户名和密码Enter username for jdbc:hive2://hdp-node-01:10000: rootEnter password for jdbc:hive2://hdp-node-01:10000: Connected to: Apache Hive (version 1.2.1)Driver: Hive JDBC (version 1.2.1)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://hdp-node-01:10000&gt; #执行查询语句0: jdbc:hive2://hdp-node-01:10000&gt; select * from default.emp;OK+------------+------------+------------+----------+---------------+----------+-----------+-------------+--+| emp.empno | emp.ename | emp.job | emp.mgr | emp.hiredate | emp.sal | emp.comm | emp.deptno |+------------+------------+------------+----------+---------------+----------+-----------+-------------+--+| 7369 | SMITH | CLERK | 7902 | 1980-12-17 | 800.0 | NULL | 20 || 7499 | ALLEN | SALESMAN | 7698 | 1981-2-20 | 1600.0 | 300.0 | 30 || 7521 | WARD | SALESMAN | 7698 | 1981-2-22 | 1250.0 | 500.0 | 30 || 7566 | JONES | MANAGER | 7839 | 1981-4-2 | 2975.0 | NULL | 20 || 7654 | MARTIN | SALESMAN | 7698 | 1981-9-28 | 1250.0 | 1400.0 | 30 |其实beeline显示的数据相比较于hive 的cli更加的规整,我们更容易观察,但是他不会显示执行的日志信息,日志信息在hiveserver中 JDBC连接hiveserver2服务123456789#下面是连接的步骤:Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); Connection cnct = DriverManager.getConnection(&quot;jdbc:hive2://&lt;host&gt;:&lt;port&gt;&quot;, &quot;&lt;user&gt;&quot;, &quot;&lt;password&gt;&quot;);//Connection cnct = DriverManager.getConnection(&quot;jdbc:hive2://&lt;host&gt;:&lt;port&gt;&quot;, &quot;&lt;user&gt;&quot;, &quot;&quot;); Statement stmt = cnct.createStatement();ResultSet rset = stmt.executeQuery(&quot;SELECT foo FROM bar&quot;); 下面是官网的例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970import java.sql.SQLException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.Statement;import java.sql.DriverManager; public class HiveJdbcClient &#123; private static String driverName = &quot;org.apache.hive.jdbc.HiveDriver&quot;; /** * @param args * @throws SQLException */ public static void main(String[] args) throws SQLException &#123; try &#123; //加载驱动 Class.forName(driverName); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); System.exit(1); &#125; //获取JDBC连接 Connection con = DriverManager.getConnection(&quot;jdbc:hive2://localhost:10000/default&quot;, &quot;hive&quot;, &quot;&quot;); Statement stmt = con.createStatement(); String tableName = &quot;testHiveDriverTable&quot;; stmt.execute(&quot;drop table if exists &quot; + tableName); stmt.execute(&quot;create table &quot; + tableName + &quot; (key int, value string)&quot;); // show tables String sql = &quot;show tables &apos;&quot; + tableName + &quot;&apos;&quot;; System.out.println(&quot;Running: &quot; + sql); ResultSet res = stmt.executeQuery(sql); if (res.next()) &#123; System.out.println(res.getString(1)); &#125; // describe table sql = &quot;describe &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1) + &quot;\\t&quot; + res.getString(2)); &#125; // load data into table // NOTE: filepath has to be local to the hive server // NOTE: /tmp/a.txt is a ctrl-A separated file with two fields per line String filepath = &quot;/tmp/a.txt&quot;; sql = &quot;load data local inpath &apos;&quot; + filepath + &quot;&apos; into table &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); stmt.execute(sql); // select * query sql = &quot;select * from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(String.valueOf(res.getInt(1)) + &quot;\\t&quot; + res.getString(2)); &#125; // regular hive query sql = &quot;select count(1) from &quot; + tableName; System.out.println(&quot;Running: &quot; + sql); res = stmt.executeQuery(sql); while (res.next()) &#123; System.out.println(res.getString(1)); &#125; &#125;&#125;#注意上面的代码对于jdbc的连接关闭没有做,我们在实际生产中要有的 hiveserver2和jdbc的使用场景:将hive中的分析结果存储在hive表(result),前端通过dao代码,进行数据的查询(因为结果的数据集很少,所以很快) hiveserver2的并发有点问题","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中自带function的使用以及自定义UDF使用","date":"2017-04-24T07:33:38.129Z","path":"2017/04/24/bigdata/hive/hive_new/hive中自带function的使用以及自定义UDF使用/","text":"hive自带的函数12345678910111213141516171819202122232425262728#查看所有的show functions;#如果一个函数的名称记不全了,可以使用like去模糊匹配hive (test_database)&gt; show functions like &apos;spli*&apos;;OKtab_namesplitTime taken: 0.03 seconds, Fetched: 1 row(s)hive (test_database)&gt; #查看一个具体的函数的使用desc function split;desc function extended split;hive (test_database)&gt; desc function extended split;OKtab_namesplit(str, regex) - Splits str around occurances that match regex#我们可以参看这里的例子Example: &gt; SELECT split(&apos;oneAtwoBthreeC&apos;, &apos;[ABC]&apos;) FROM src LIMIT 1; [&quot;one&quot;, &quot;two&quot;, &quot;three&quot;]Time taken: 0.02 seconds, Fetched: 4 row(s)hive (test_database)&gt; 用户自定义函数123456789101112package com.example.hive.udf; import org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text; public final class Lower extends UDF &#123; public Text evaluate(final Text s) &#123; if (s == null) &#123; return null; &#125; return new Text(s.toString().toLowerCase()); &#125;&#125; 步骤:1.继承org.apache.hadoop.hive.sl.exec.UDF2.需要实现evaluate函数,evaluate函数支持重载 注意事项:1.UDF必须要有返回值类型,可以返回null,但是返回类型不能为void2.UDF中常用的Text/LongWritable等类型,不推荐使用java类型 UDF开发实例12345678910111213141516171819202122232425262728293031323334&apos;1.先开发一个java类，继承UDF，并重载evaluate方法&apos;##############################################################package cn.itcast.bigdata.udfimport org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text; public final class Lower extends UDF&#123;public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase());&#125;&#125;##############################################################&apos;2.打成jar包上传到服务器&apos;&apos;3.将jar包添加到hive的classpath&apos;hive&gt;add JAR /home/hadoop/udf.jar;&apos;4.创建临时函数与开发好的java class关联&apos;Hive&gt;create temporary function tolowercase as &apos;cn.itcast.bigdata.udf.Lower&apos;; #tolowercase 是临时函数的名字，as后面指定的是临时函数对应的java类&apos;5.即可在hql中使用自定义的函数tolowercase &apos;hive&gt;select tolowercase(name) from student ;#在0.13.x之后,另外一种方式是1.将jar文件放入hdfs中hive&gt;dfs -mkdir -p /user/study/hive/jars/ ;hive&gt;dfs -put /opt/datas/udf.jar /user/study/hive/jars/ ;2.将add jar 和创建函数的的步骤放在一起了CREATE FUNCTION myfunc AS &apos;cn.itcast.bigdata.udf.Lower&apos; USING JAR &apos;hdfs://hadoop-node-01:9000/user/study/hive/jars/udf.jar&apos;;hive&gt;show functions like &apos;myfunc&apos;; Json数据解析UDF开发12345678910111213141516public class JsonParser extends UDF &#123; public String evaluate(String jsonLine) &#123; ObjectMapper objectMapper = new ObjectMapper(); try &#123; MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class); return bean.toString(); &#125; catch (Exception e) &#123; &#125; return &quot;&quot;;&#125; &#125; 电话号码转换函数123456789101112131415161718public class ToProvince extends UDF&#123; static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;();static&#123; provinceMap.put(&quot;138&quot;, &quot;beijing&quot;); provinceMap.put(&quot;139&quot;, &quot;shanghai&quot;); provinceMap.put(&quot;137&quot;, &quot;dongjing&quot;); provinceMap.put(&quot;156&quot;, &quot;huoxing&quot;);&#125; //我们需要重载这个方法，来适应我们的业务逻辑public String evaluate(String phonenbr)&#123; String res = provinceMap.get(phonenbr.substring(0, 3)); return res==null?&quot;wukong&quot;:res;&#125;&#125;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中数据导入导出Import和Export","date":"2017-04-24T06:42:51.484Z","path":"2017/04/24/bigdata/hive/hive_new/hive中数据导入导出Import和Export/","text":"Export将hive表中的数据导出到外部12345678910111213141516171819202122232425262728293031export table tablename [partition (part_coloumn=&apos;value&apos; [,...]]to &apos;export_target_path&apos;#export_target_path指的是hdfs上的路径#其实会将hive表指向的目录中的文件copy到指定的hdfs目录下,同时会将hive表的元数据也copy到hdfs的目录下hive (default)&gt; export table emp to &apos;/tmp/export/emp_exp&apos;;Copying data from file:/tmp/root/9950c21d-9397-4e81-a3fa-032ffe368fa7/hive_2017-04-24_14-47-28_645_5478278646286661256-1/-local-10000/_metadataCopying file: file:/tmp/root/9950c21d-9397-4e81-a3fa-032ffe368fa7/hive_2017-04-24_14-47-28_645_5478278646286661256-1/-local-10000/_metadataCopying data from hdfs://hdp-node-01:9000/user/hive/warehouse/empCopying file: hdfs://hdp-node-01:9000/user/hive/warehouse/emp/employee.txtCopying file: hdfs://hdp-node-01:9000/user/hive/warehouse/emp/employee2.txtOKTime taken: 1.856 seconds#hive表中的数据hive (default)&gt; dfs -ls /user/hive/warehouse/emp/ ;Found 2 items-rwxr-xr-x 3 root supergroup 652 2017-04-23 22:50 /user/hive/warehouse/emp/employee.txt-rwxr-xr-x 3 root supergroup 652 2017-04-24 00:59 /user/hive/warehouse/emp/employee2.txt#导出之后的目录hive (default)&gt; dfs -ls /tmp/export/emp_exp/ ;Found 2 items-rwx-wx-wx 3 root supergroup 1601 2017-04-24 14:47 /tmp/export/emp_exp/_metadatadrwx-wx-wx - root supergroup 0 2017-04-24 14:47 /tmp/export/emp_exp/datahive (default)&gt; dfs -ls /tmp/export/emp_exp/data/;Found 2 items-rwx-wx-wx 3 root supergroup 652 2017-04-24 14:47 /tmp/export/emp_exp/data/employee.txt-rwx-wx-wx 3 root supergroup 652 2017-04-24 14:47 /tmp/export/emp_exp/data/employee2.txt Import导入将外部数据导入到hive表中12345678910111213141516import [[external] table new_or_original_tablename [partition (part_column=&quot;value&quot;[,...])]]from &apos;souce_path&apos;[location &apos;import_target_path&apos;]hive (test_database)&gt; create table if not exists test_database.emp like default.emp;import table test_database.emp from &apos;/tmp/export/emp_exp&apos; ;hive (test_database)&gt; dfs -ls /user/hive/warehouse/test_database.db/emp/;Found 2 items-rwxr-xr-x 3 root supergroup 652 2017-04-24 15:01 /user/hive/warehouse/test_database.db/emp/employee.txt-rwxr-xr-x 3 root supergroup 652 2017-04-24 15:01 /user/hive/warehouse/test_database.db/emp/employee2.txt`","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中常见的查询","date":"2017-04-24T06:05:50.012Z","path":"2017/04/24/bigdata/hive/hive_new/hive中常见的查询/","text":"常见的查询语句1234567891011121314151617181920212223242526272829SELECT [ALL | DISTINCT] select_expr, select_expr, ... FROM table_reference [WHERE where_condition] [GROUP BY col_list] [ORDER BY col_list] [CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY col_list] ] [LIMIT number]hive (default)&gt; select * from emp limit 5;hive (default)&gt; select * from emp t where t.sal between 800 and 1000;hive (default)&gt; select * from emp t where t.comm is null;hive (default)&gt; select count(*) cnt from emp;hive (default)&gt; select max(sal) max_sal from emp;hive (default)&gt; select sum(sal) sum_sal from emp;hive (default)&gt; select avg(sal) from emp; group by / having123456789101112131415#每个部门的平均工资select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno#每个部门中每个岗位的最高薪水select t.deptno,t.job,max(t.sal) max_sal from emp t group by t.deptno, t.job;where是针对单条记录进行筛选的having是针对分组结果进行组内筛选的#每个部门的平均薪水大于2000的部门select t.deptno, avg(t.sal) avg_sal from emp t group by t.deptno having avg_sal &gt; 2000 ; join操作1234567891011121314151617181920#等值joinselect e.empno, e.ename, d.deptno, d.name from emp e join dept d on e.deptno=d.deptno ;#左连接:left joinselect e.empno, e.ename, d.deptno, d.name from emp e left join dept d on e.deptno=d.deptno ;#右连接:right joinselect e.empno, e.ename, d.deptno, d.name from emp e right join dept d on e.deptno=d.deptno ;#全连接full joinselect e.empno, e.ename, d.deptno, d.name from emp e full join dept d on e.deptno=d.deptno ; order by是对全局数据的排序,仅仅只有一个reduce,如果查询的结果集比较大的时候,会出现内存溢出1select * from emp order by empno desc ; sort by对每一个reduce内部数据进行排序,对全局的结果集来说不是排序的 123456789101112131415161718set mapreduce.job.reduces=3;select * from emp sort by empno asc ;insert overwrite local directory &apos;/tmp/datas/sortby-res&apos;select * from emp sort by empno asc ;[root@hdp-node-01 sortby-res]# ll /tmp/datas/sortby-res/total 12-rw-r--r-- 1 root root 573 Apr 24 15:18 000000_0-rw-r--r-- 1 root root 468 Apr 24 15:18 000001_0-rw-r--r-- 1 root root 281 Apr 24 15:18 000002_0[root@hdp-node-01 sortby-res]# cat 000000_0 7369\u0001SMITH\u0001CLERK\u00017902\u00011980-12-17\u0001800.0\u0001\\N\u0001207566\u0001JONES\u0001MANAGER\u00017839\u00011981-4-2\u00012975.0\u0001\\N\u0001207654\u0001MARTIN\u0001SALESMAN\u00017698\u00011981-9-28\u00011250.0\u00011400.0\u0001307654\u0001MARTIN\u0001SALESMAN\u00017698\u00011981-9-28\u00011250.0\u00011400.0\u000130 distribute by类似于MapReduce中的分区partition的功能,对数据进行分区,通常结果sort by进行使用 123456789101112131415161718192021222324252627282930313233set mapreduce.job.reduces=3;#按部门尽心分区,那么相同的部门将分到一个reduce中,然后sort 排序,就是在一个部门中进行排序了select * from emp distribute by deptno sort by empno asc ;insert overwrite local directory &apos;/tmp/datas/distributeby-res&apos;select * from emp distribute by deptno sort by empno asc ;#注意distribute by要在sort by之前#查看结果[root@hdp-node-01 datas]# ll /tmp/datas/distributeby-res/total 12-rw-r--r-- 1 root root 586 Apr 24 15:26 000000_0-rw-r--r-- 1 root root 278 Apr 24 15:26 000001_0-rw-r--r-- 1 root root 458 Apr 24 15:26 000002_0#可以看到在000000_0中的都是部门号为30的,而且empno是升序排列的[root@hdp-node-01 datas]# cat /tmp/datas/distributeby-res/000000_0 7499\u0001ALLEN\u0001SALESMAN\u00017698\u00011981-2-20\u00011600.0\u0001300.0\u0001307499\u0001ALLEN\u0001SALESMAN\u00017698\u00011981-2-20\u00011600.0\u0001300.0\u0001307521\u0001WARD\u0001SALESMAN\u00017698\u00011981-2-22\u00011250.0\u0001500.0\u0001307521\u0001WARD\u0001SALESMAN\u00017698\u00011981-2-22\u00011250.0\u0001500.0\u0001307654\u0001MARTIN\u0001SALESMAN\u00017698\u00011981-9-28\u00011250.0\u00011400.0\u0001307654\u0001MARTIN\u0001SALESMAN\u00017698\u00011981-9-28\u00011250.0\u00011400.0\u0001307698\u0001BLAKE\u0001MANAGER\u00017839\u00011981-5-1\u00012850.0\u0001\\N\u0001307698\u0001BLAKE\u0001MANAGER\u00017839\u00011981-5-1\u00012850.0\u0001\\N\u0001307844\u0001TURNER\u0001SALESMAN\u00017698\u00011981-9-8\u00011500.0\u00010.0\u0001307844\u0001TURNER\u0001SALESMAN\u00017698\u00011981-9-8\u00011500.0\u00010.0\u0001307900\u0001JAMES\u0001CLERK\u00017698\u00011981-12-3\u0001950.0\u0001\\N\u0001307900\u0001JAMES\u0001CLERK\u00017698\u00011981-12-3\u0001950.0\u0001\\N\u000130 cluster by当distribute by 和sort by字段相同时,可以使用cluster by代替 1select * from emp cluster by deptno ;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"导出hive表数据的几种方式","date":"2017-04-24T02:42:35.392Z","path":"2017/04/24/bigdata/hive/hive_new/导出hive表数据的几种方式/","text":"将hive表中的数据导入到本地123456789101112131415161718192021222324252627282930313233343536#方式一(导出到本地目录下)insert overwrite local directory &apos;/tmp/datas/hive_exp_emp&apos;row format delimited fields terminated by &apos;,&apos;select * from default.emp ;#方式二bin/hive -e &quot;select * from default.emp ;&quot; &gt; /tmp/datas/exp_res.txt#方式三(导出到hdfs中)insert overwrite directory &apos;/tmp/datas/hive_exp_emp&apos;row format delimited fields terminated by &apos;,&apos;select * from default.emp ;#到hdfs上查看导出的数据hive (default)&gt; dfs -ls /tmp/datas/hive_exp_emp ;Found 1 items-rwx-wx-wx 3 root supergroup 1322 2017-04-24 01:32 /tmp/datas/hive_exp_emp/000000_0hive (default)&gt; dfs -text /tmp/datas/hive_exp_emp/000000_0 ;7369,SMITH,CLERK,7902,1980-12-17,800.0,\\N,207499,ALLEN,SALESMAN,7698,1981-2-20,1600.0,300.0,307521,WARD,SALESMAN,7698,1981-2-22,1250.0,500.0,307566,JONES,MANAGER,7839,1981-4-2,2975.0,\\N,207654,MARTIN,SALESMAN,7698,1981-9-28,1250.0,1400.0,307698,BLAKE,MANAGER,7839,1981-5-1,2850.0,\\N,307782,CLARK,MANAGER,7839,1981-7-9,2450.0,\\N,10#然后我们可以将hdfs上的数据拿到本地hdfs dfs -get /tmp/datas/hive_exp_emp/000000_0 /user/datas/test.txt","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"导入数据到hive表的六大方式","date":"2017-04-24T02:13:52.455Z","path":"2017/04/24/bigdata/hive/hive_new/导入数据到hive表的六大方式/","text":"加载文件到表中1234567891011load data [local] inpath &apos;filepath&apos; [overwrite] into table tablename [partition (partition1=val1, partition2=val2...)]#原始文件存储的位置1.本地: 此时需要加上local2.hdfs: 那么什么不用加#对表中的数据是否覆盖1.覆盖:overwrite2.追加:不用overwrite#分区表需要加载上:partition (partition1=val1, partition2=val2...) 加载本地文件到hive表1load data local inpath &apos;xx/emp.txt&apos; into table default.emp ; 加载hdfs文件到hive表1load data inpath &apos;/user/hive/xx/emp.txt&apos; into table default.emp ; 加载数据覆盖表中已有的数据1load data inpath &apos;/user/hive/xx/emp.txt&apos; overwrite into table default.emp ; 通过as创建表的时候,加载数据123create table if not exists default.emp_as asselect * from default.emp ; 创建表的时通过insert加载12create table if not exists default.emp_cli like emp ;insert into table default.emp_cli select * from default.emp ; 创建表的时候通过location指定加载1234567891011create external table if not exists default.emp_external(id int,name string)comment &apos;external table&apos;row format delimited fields terminated by &apos;,&apos;location &apos;xx.txt&apos; ;因为location中的数据已经存在,那么在我们创建表的时候就直接有了表的数据 对于hdfs上的数据,加载完之后就会被删除1234567891011121314151617#将数据上传到hdfshive (default)&gt; dfs -put /home/hadoop/app/hive/script/employee2.txt /user/; hive (default)&gt; dfs -ls /user/ ;Found 3 items-rw-r--r-- 3 root supergroup 652 2017-04-24 00:59 /user/employee2.txtdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /user/hivedrwxr-xr-x - root supergroup 0 2017-04-23 23:23 /user/zhangsan#加载数到hive表hive (default)&gt; load data inpath &apos;/user/employee2.txt&apos; into table emp ;#再次查看hdfs中的数据(数据被删除了)hive (default)&gt; dfs -ls /user/ ;Found 2 itemsdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /user/hivedrwxr-xr-x - root supergroup 0 2017-04-23 23:23 /user/zhangsanhive (default)&gt;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中的分区表","date":"2017-04-24T01:23:09.895Z","path":"2017/04/24/bigdata/hive/hive_new/hive中的分区表/","text":"分区表实际上就是对应一个hdfs文件系统上的独立的文件夹,该文件夹下是该分区所有的数据文件,hive中的分区就是分目录,把一个大的数据集根据业务需求要分割成更小的数据集 在查询时通过where子句中的表达式来选择查询所需要的指定的分区,这样的查询效率会提高很多 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253create table dept_partition(deptno int,dname string,loc string)comment &apos;partition table&apos;partitioned by (event_month string)row format delimited fields terminated by &apos;,&apos; ;create external table if not exists default.emp_partition(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)comment &apos;emp partition table&apos;partitioned by (month string)row format delimitedfields terminated by &apos;,&apos; ;#查看分区的信息hive (default)&gt; desc formatted emp_partition;OKcol_name data_type comment# col_name data_type comment empno int ename string job string mgr int hiredate string sal double comm double deptno int #这里可以看到分区的字段名和字段类型 # Partition Information # col_name data_type comment month string .....hive (default)&gt; 加载数据到分区表123456load data local inpath &apos;/home/hadoop/app/hive/script/employee2.txt&apos; into table emp_partition partition (month=&apos;201509&apos;)hive (default)&gt; dfs -ls /user/hive/warehouse/emp_partition/ ;Found 1 itemsdrwxr-xr-x - root supergroup 0 2017-04-24 00:10 /user/hive/warehouse/emp_partition/month=201509 插叙分区表中的数据1select * from emp_partition where month=&apos;201509&apos; ; 指定多个分区字段12345678910111213141516171819202122create external table if not exists default.emp_partition(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)comment &apos;emp partition table&apos;partitioned by (month string, day string)row format delimitedfields terminated by &apos;,&apos; ;#加载数据load data local inpath &apos;/home/hadoop/app/hive/script/employee2.txt&apos; into table emp_partition partition (month=&apos;201509&apos;, day=&apos;13&apos;)#查询数据select * from emp_partition where month=&apos;201509&apos; and day=&apos;13&apos; ; 注意事项1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465create table if not exists default.dept_nopartition(deptno int,dname string,loc string)comment &apos;dept no partition table&apos;row format delimitedfields terminated by &apos;,&apos; ;hive&gt;dfs -put /xx/dept.txt /user/hive/warehouse/dept_nopartition ;select * from dept_nopartition#此时是有数据的----------------------------------create table if not exists default.dept_partition(deptno int,dname string,loc string)comment &apos;dept no partition table&apos;partitioned by (day string)row format delimitedfields terminated by &apos;,&apos; ;hive&gt;dfs -mkdir -p /user/hive/warehouse/dept_partition/day=20150913 ;hive&gt;dfs -put /xx/dept.txt /user/hive/dept_partition/day=20150913 ;hive&gt;select * from dept_partition#此时没有数据登录到mysql中去看,表的元数据信息,mysql&gt;select * from paritions;#可以看到其中并没有表dept_partition对应的分区信息,所以当我们select查询的时候,根本不知道分区的存在既然数据我们已经手动放入了我们创建的分区目录中,那么我们又要查询到数据,怎么解决呢?#方式一:hive&gt;msck repair table dept_partitionOKPartitions not in metastore: dept_part:day=20150913Repair: Added partition to metastore dept_part:day=20150913此时再去mysql中查看表的分区信息mysql&gt;select * from paritions;#此时就可以看到分区信息了#方式二:hive&gt;dfs -mkdir -p /user/hive/warehouse/dept_partition/day=20150914 ;hive&gt;dfs -put /xx/dept.txt /user/hive/dept_partition/day=20150914 ;解决:alter table dept_partition add partition(day=&apos;20150914&apos;) ;此时查看mysql中的分区信息mysql&gt;select * from paritions;#此时就可以看到分区信息了 查看表的所有的分区12345678hive (default)&gt; show partitions emp_partition;OKpartitionmonth=201509month=201510month=201511Time taken: 0.137 seconds, Fetched: 3 row(s)hive (default)&gt;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive中的外部表","date":"2017-04-24T00:38:18.056Z","path":"2017/04/24/bigdata/hive/hive_new/外部表/","text":"查看表的类型1234567891011121314151617181920212223#可以看表是管理表还是外部表desc formatted test_table;hive (default)&gt; desc formatted test_table;OKcol_name data_type comment# col_name data_type comment id int name string # Detailed Table Information Database: default Owner: root CreateTime: Sun Apr 23 11:14:21 CST 2017 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hdp-node-01:9000/user/hive/warehouse/test_table #可以看表的类型Table Type: MANAGED_TABLE 创建外部表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960create external table if not exists default.emp_external(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int)comment &apos;employee external table&apos;row format delimitedfields terminated by &apos;,&apos; location &apos;/user/zhangsan/hive/warehouse/emp_external&apos; ;#查看表在HDFS中的目录是否创建hive (default)&gt; dfs -ls /user/zhangsan/hive/warehouse/;Found 1 itemsdrwxr-xr-x - root supergroup 0 2017-04-23 23:23 /user/zhangsan/hive/warehouse/emp_external#查看表的类型hive (default)&gt; desc emp_external;OKcol_name data_type commentempno int ename string job string mgr int hiredate string sal double comm double deptno int Time taken: 0.159 seconds, Fetched: 8 row(s)hive (default)&gt; desc formatted emp_external;OKcol_name data_type comment# col_name data_type comment empno int ename string job string mgr int hiredate string sal double comm double deptno int # Detailed Table Information Database: default Owner: root CreateTime: Sun Apr 23 23:12:23 CST 2017 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://hdp-node-01:9000/user/hive/warehouse/emp_external #此时表的类型是外部表Table Type: EXTERNAL_TABLE 外部表和内部表(管理表)的区别内部表也称为管理表,默认存储在/user/hive/warehouse下,也可以使用location去指定,删除表时,会删除表中的数和元数据 外部表也称为External_table,在创建表时可以自己指定目录的位置(location),删除表时,会删除表的元数据,不会删除表数据 创建外部表,指向已经存在的数据12345678910111213141516171819202122232425262728293031323334# 向外部表的location中放入数据hive (default)&gt; dfs -put /home/hadoop/app/hive/script/employee.txt /user/zhangsan/hive/warehouse/emp_external ;hive (default)&gt; dfs -ls /user/zhangsan/hive/warehouse/emp_external;Found 1 items-rw-r--r-- 3 root supergroup 652 2017-04-23 23:29 /user/zhangsan/hive/warehouse/emp_external/employee.txthive (default)&gt; select * from emp_external;OKemp_external.empno emp_external.ename emp_external.job emp_external.mgr emp_external.hiredate emp_external.sal emp_external.comm emp_external.deptno7369 SMITH CLERK 7902 1980-12-17 800.0 NULL 207499 ALLEN SALESMAN 7698 1981-2-20 1600.0 300.0 307521 WARD SALESMAN 7698 1981-2-22 1250.0 500.0 307566 JONES MANAGER 7839 1981-4-2 2975.0 NULL 207654 MARTIN SALESMAN 7698 1981-9-28 1250.0 1400.0 307698 BLAKE MANAGER 7839 1981-5-1 2850.0 NULL 307782 CLARK MANAGER 7839 1981-7-9 2450.0 NULL 107788 SCOTT ANALYST 7566 1982-12-9 3000.0 NULL 207839 KING PRESIDENT NULL 1981-11-17 5000.0 NULL 107844 TURNER SALESMAN 7698 1981-9-8 1500.0 0.0 307876 ADAMS CLERK 7788 1983-1-12 1100.0 NULL 207900 JAMES CLERK 7698 1981-12-3 950.0 NULL 307902 FORD ANALYST 7566 1981-12-3 3000.0 NULL 207934 MILLER CLERK 7782 1982-1-23 1300.0 NULL 10Time taken: 0.665 seconds, Fetched: 14 row(s)hive (default)&gt; #其实内部表也是一样的,在创建完表之后,我们使用load data local inpath &apos;xx.txt&apos; into table test,向表中添加数据,本质就是将xx.txt拷贝到表在HDFS中的目录下#我们不用load data的方式,使用手动的hdfs -put xx.txt /user/hive/warehouse/test/ 这样做也是可以的,所以对于外部表,只不过是表的目录在另外的hdfs上,这样在我们创建表的时候指定了location,那么当表创建完成之后,使用select去查询的时候,表中就有了数据因为外部表的数据已经存在,所以当我们创建外部表的时候,就可以直接查询数据,不用我们load数据当然我们可以先创建外部表,然后再向其中放入数据,放入数据的方式有2种,第一种是通过我们的load,第二种是直接通过hdfs -put 放入数据","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"markdown的快捷键","date":"2017-04-23T11:27:29.595Z","path":"2017/04/23/markdown/markdown的快捷键/","text":"File Shortcut Description COMMAND (CTRL) - N new window COMMAND (CTRL) - S save COMMAND (CTRL) - SHIFT - S save as COMMAND (CTRL) - O open file COMMAND (CTRL) - W close window COMMAND (CTRL) - F4 close Editing Shortcut Description COMMAND (CTRL) - A Select All COMMAND (CTRL) - D Delete Line COMMAND (CTRL) - Z Undo COMMAND (CTRL) - SHIFT - Z Redo COMMAND (CTRL) - Up Go to Doc Start COMMAND (CTRL) - Down Go to Doc End CTRL-Left Go to word End CTRL - Q Folding Finding Shortcut Description COMMAND (CTRL) - F Starting search COMMAND (CTRL) - G Find next COMMAND (CTRL) - SHIFT + G Find previous COMMAND - ALT - F Replace (Mac) SHIFT - CTRL - F Replace (linux, win) COMMAND - SHIFT - ALT - F Replace all (Mac) SHIFT - CTRL - R Replace all (linux, win) Ctrl + I ： 斜体 Ctrl + B ： 粗体 Ctrl + G ： 图片 Ctrl + Q ： 引用 Ctrl + 1 ： 标题 1 Ctrl + 2 ： 标题 2 Ctrl + 3 ： 标题 3 Ctrl + K ： 代码块 Ctrl + L ： 超链接 Ctrl + T ： 时间戳 Ctrl + U ： 无序列表 Ctrl + R ： 水平标尺 F4 ： 启用水平布局 F5 ： 启用实时预览 F6 ： 在浏览器中预览","tags":[{"name":"markdown","slug":"markdown","permalink":"http://yoursite.com/tags/markdown/"}]},{"title":"hive的DDL操作","date":"2017-04-23T11:23:53.688Z","path":"2017/04/23/bigdata/hive/hive_new/hive的DDL操作/","text":"创建数据库及数据库相关操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name [COMMENT database_comment] [LOCATION hdfs_path] [WITH DBPROPERTIES (property_name=property_value, ...)];#注意一定要使用if如果存在就创建,这样更加的专业create database if not exists db_hive_01;create database if not exists db_hive_02location &apos;/user/study/hive/warehouse/db_hive_02.db&apos;#如果不指定数据库在HDFS中的位置,默认是在HDFS中的/user/hive/warehouse/以数据库名.db命名的目录#在数据库下创建表,那么将在数据库的目录下创建一个以表名为名字的目录create table if not exist db_hive_02.user#查看数据库show databases;#查看以db_hive开头的数据库(有时候我们只是记得数据库的大致的名称的时候,这个命令有用)show databases like &apos;db_hive*&apos;;#查看数据库的相关信息hive (test_database)&gt; desc database test_database;OKdb_name comment location owner_name owner_type parameterstest_database hdfs://hdp-node-01:9000/user/hive/warehouse/test_database.db root USERTime taken: 0.052 seconds, Fetched: 1 row(s)#查看数据库的扩展信息hive (test_database)&gt; desc database extended test_database;OKdb_name comment location owner_name owner_type parameterstest_database hdfs://hdp-node-01:9000/user/hive/warehouse/test_database.db root USERTime taken: 0.068 seconds, Fetched: 1 row(s)hive (test_database)&gt; #删除数据库(如果已经存在的话)drop database if exists test_database;hive (test_database)&gt; drop database test_database;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. InvalidOperationException(message:Database test_database is not empty. One or more tables exist.)#数据库中有表存在的数据库,是不能删除的#可以使用级联删除的方式,进行删除,删除的时候,数据库对应的在HDFS中的目录也是会被删除的drop database test_database cascad; hive官网完整的建表语句12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name -- (Note: TEMPORARY available in Hive 0.14.0 and later) [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [SKEWED BY (col_name, col_name, ...) -- (Note: Available in Hive 0.10.0 and later)] ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)] -- (Note: Available in Hive 0.6.0 and later) ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] -- (Note: Available in Hive 0.6.0 and later) [AS select_statement]; -- (Note: Available in Hive 0.5.0 and later; not supported for external tables) CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; data_type : primitive_type | array_type | map_type | struct_type | union_type -- (Note: Available in Hive 0.7.0 and later) primitive_type : TINYINT | SMALLINT | INT | BIGINT | BOOLEAN | FLOAT | DOUBLE | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later) | STRING | BINARY -- (Note: Available in Hive 0.8.0 and later) | TIMESTAMP -- (Note: Available in Hive 0.8.0 and later) | DECIMAL -- (Note: Available in Hive 0.11.0 and later) | DECIMAL(precision, scale) -- (Note: Available in Hive 0.13.0 and later) | DATE -- (Note: Available in Hive 0.12.0 and later) | VARCHAR -- (Note: Available in Hive 0.12.0 and later) | CHAR -- (Note: Available in Hive 0.13.0 and later) array_type : ARRAY &lt; data_type &gt; map_type : MAP &lt; primitive_type, data_type &gt; struct_type : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt; union_type : UNIONTYPE &lt; data_type, data_type, ... &gt; -- (Note: Available in Hive 0.7.0 and later) row_format : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char] [NULL DEFINED AS char] -- (Note: Available in Hive 0.13 and later) | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] file_format: : SEQUENCEFILE | TEXTFILE -- (Default, depending on hive.default.fileformat configuration) | RCFILE -- (Note: Available in Hive 0.6.0 and later) | ORC -- (Note: Available in Hive 0.11.0 and later) | PARQUET -- (Note: Available in Hive 0.13.0 and later) | AVRO -- (Note: Available in Hive 0.14.0 and later) | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname constraint_specification: : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE ] [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE 几种创建表的方法自己指定字段建表1234567891011121314151617create table if not exists default.test_log_2015_0913( ip string comment &apos;remote ip address&apos;, user string , request_url string comment &apos;user request url&apos;)#对表进行commentCOMMENT &apos;WEB Access Logs&apos;#指定格式化ROW FORMAT DELIMITED# 指定字段的分割符fields terminated by &apos;,&apos;#文件的存储格式(默认的是textFile的格式,但是在实际生产环境中,我们使用的是一般是Parquet或者是ORCSTORED AS textFile #指定数据在HDFS中存储的位置,默认是在对应的数据库下面创建一个以表名的目录,但是我们可以指定这个目录,就是下面这样的操作LOCATION &apos;/user/hive/warehouse/test_log_2015_0913&apos; 根据另外一张表的查询结果创建一张表这种建表的方式将查询表中查询的数据导入到了新建的表中 123456789101112131415161718192021222324252627282930313233343536create table if not exists default.test_log_2015_0913( ip string comment &apos;remote ip address&apos;, user string , request_url string comment &apos;user request url&apos;)#对表进行commentCOMMENT &apos;WEB Access Logs&apos;#指定格式化ROW FORMAT DELIMITED# 指定字段的分割符fields terminated by &apos;,&apos;#文件的存储格式(默认的是textFile的格式,但是在实际生产环境中,我们使用的是一般是Parquet或者是ORCSTORED AS textFile #指定数据在HDFS中存储的位置,默认是在对应的数据库下面创建一个以表名的目录,但是我们可以指定这个目录,就是下面这样的操作LOCATION &apos;/user/hive/warehouse/test_log_2015_0913&apos;as select * from xxx# 使用查询的结果创建表,此时新创建的表中有数据hive (default)&gt; create table if not exists default.test_log &gt; as select id, name from default.test_table;hive (default)&gt; select * from default.test_log;OKtest_log.id test_log.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwuTime taken: 0.294 seconds, Fetched: 6 row(s)hive (default)&gt; 使用其他的表或者视图创建表结构1234567891011121314151617181920212223242526272829CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name LIKE existing_table_or_view_name [LOCATION hdfs_path]; #查询test_log2的数据hive (default)&gt; select * from test_log2;OKtest_log2.id test_log2.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwuTime taken: 0.108 seconds, Fetched: 6 row(s)#使用test_log2去创建新的表,这里只是构建新表的结构hive (default)&gt; create table if not exists default.test_log6 &gt; like test_log2;OKTime taken: 0.16 seconds#新表中没有数据hive (default)&gt; select * from test_log6;OKtest_log6.id test_log6.nameTime taken: 0.106 secondshive (default)&gt; 对表的增删改操作123456789# 清除表的数据truncate table dept_cats;# 修改表的名称alter table dept_like rename to dept_like_rename;#删除表drop table if exists dept_like_rename;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive交互式命令行中的一些常用操作","date":"2017-04-23T09:00:49.269Z","path":"2017/04/23/bigdata/hive/hive_new/hive交互式命令行中的一些常用操作/","text":"hive中常用的操作语句123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354create table student(id int, name string) ROW FORMAT delimited fields terminated by &apos;\\t&apos;lines terminatd by &apos;\\n&apos;stored as textfile;ROW FORMAT 表示行格式化delimited 表示行加上限制fields terminated by &apos;\\t&apos;类似的限制还有:lines terminated by &apos;\\n&apos;#加载数据load data local inpath &apos;xx.txt&apos; into table db_hive.student;#通常在表的名称前面加上数据库名称,这样,避免我们忘记使用use db_hiveselect *from student;#查看表的结构desc formatted student;#创建数据库show databases;create database db_hive;use db_hive;desc test_table;desc extended test_table;#相对于上面一个,显示的格式更加让我们容易观察desc formatted test_table;#hive中提供的函数show functions;#看一个函数怎么使用,如upper函数desc function upper#可以看到函数的使用例子desc function extended upper 在 hive cli命令窗口中如何查看HDFS文件系统123456789101112131415#可以在Hive命令行中直接操作HDFShive (default)&gt; dfs -ls /user/hive/warehouse;Found 5 itemsdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /user/hive/warehouse/mytable1drwxr-xr-x - root supergroup 0 2016-11-28 13:49 /user/hive/warehouse/stu_buckdrwxr-xr-x - root supergroup 0 2016-11-28 10:12 /user/hive/warehouse/studentdrwxr-xr-x - root supergroup 0 2017-04-23 16:05 /user/hive/warehouse/test_database.dbdrwxr-xr-x - root supergroup 0 2017-04-23 15:00 /user/hive/warehouse/test_tablehive (default)&gt; dfs -text /user/hive/warehouse/test_table/student.txt;11,zhangsan22,lisi33,wangwu 在hive cli命令窗口中如何查看本地(linux)文件系统123456789101112131415161718192021# 在hive shell中操作linux文件系统的命令(在命令前加一个!号)使用上述命令就可以在hive shell中不用退出的情况下使用linux命令hive (default)&gt; ! ls /etc/hosts;/etc/hostshive (default)&gt; #注意不要使用命令的别名hive (default)&gt; ! ll /etc/hosts;Exception raised from Shell command Failed to execute ll /etc/hostshive (default)&gt; ! cat /tmp/hivef.txt;test_table.id test_table.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwu 在hive cli中的所有的历史命令12345678910111213#在当前用户的目录下有一个 .hivehistory 文件,其中就是我们在hive cli中的所有的操作[root@hdp-node-01 ~]# pwd/root[root@hdp-node-01 ~]# cat .hivehistory show databases;quit;create database hadoop_hive;use hadoop_hive;create table student(id int,name string);show tables;.....","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive的几种交互式操作","date":"2017-04-23T08:40:19.599Z","path":"2017/04/23/bigdata/hive/hive_new/hive的几种交互式操作/","text":"12345678910111213141516[root@hdp-node-01 hive]# ./bin/hive -helpusage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)[root@hdp-node-01 hive]# bin/hive -e12345678910111213141516bin/hive -e &lt;quoted-query-string&gt; # 带引号的查询字符串#他是不会进入hive的交互式命令的[root@hdp-node-01 hive]# ./bin/hive -e &quot;select * from default.test_table ;&quot;....Logging initialized using configuration in file:/home/hadoop/app/apache-hive-1.2.1-bin/conf/hive-log4j.propertiesOKtest_table.id test_table.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwuTime taken: 4.049 seconds, Fetched: 6 row(s)[root@hdp-node-01 hive]# bin/hive -f在实际的开发过程中,就是使用这样方式 123456789101112131415161718192021222324252627282930313233343536373839-f &lt;filename&gt; #将我们的sql语句放入一个文件中touch hivef.sql select * from default.test_table ;bin/hive -f hivef.sql[root@hdp-node-01 hive]# bin/hive -f hivef.sql Logging initialized using configuration in file:/home/hadoop/app/apache-hive-1.2.1-bin/conf/hive-log4j.propertiesOKtest_table.id test_table.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwuTime taken: 2.28 seconds, Fetched: 6 row(s)#将执行的结果写入一个文件中[root@hdp-node-01 hive]# bin/hive -f hivef.sql &gt; /tmp/hivef.txtLogging initialized using configuration in file:/home/hadoop/app/apache-hive-1.2.1-bin/conf/hive-log4j.propertiesOKTime taken: 2.258 seconds, Fetched: 6 row(s)[root@hdp-node-01 hive]# [root@hdp-node-01 hive]# cat /tmp/hivef.txt test_table.id test_table.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwu[root@hdp-node-01 hive]# bin/hive -i1234-i &lt;filename&gt; Initialization SQL file (初始化的sql文件)通常与用户自定义的udf相互使用","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive常见属性配置","date":"2017-04-23T08:25:01.513Z","path":"2017/04/23/bigdata/hive/hive_new/hive常见属性配置/","text":"hive数据仓库位置配置12345678910111213141516171819202122232425262728293031在conf/hive-default.xml.template 中有如下的配置,如果我们想要配置这个参数,可以在hive-site.xml中添加: &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt;在该目录下,没有对默认的数据库(default)创建文件夹,如果某张表数据属于default数据库,那么直接在/user/hive/warehouse以表的名称创建文件夹[root@hdp-node-01 hive]# hdfs dfs -ls /user/hive/warehouseFound 5 itemsdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /user/hive/warehouse/mytable1drwxr-xr-x - root supergroup 0 2016-11-28 13:49 /user/hive/warehouse/stu_buckdrwxr-xr-x - root supergroup 0 2016-11-28 10:12 /user/hive/warehouse/studentdrwxr-xr-x - root supergroup 0 2017-04-23 16:05 /user/hive/warehouse/test_database.dbdrwxr-xr-x - root supergroup 0 2017-04-23 15:00 /user/hive/warehouse/test_table上面的test_table表就是在default数据库下创建,所以直接在HDFS的/user/hive/warehouse目录下而如果我们创建了一个数据库,如:test_database,然后在该数据库下创建表:test_table,那么就会在对应的数据库下创建表的目录,如下:[root@hdp-node-01 hive]# hdfs dfs -ls /user/hive/warehouse/test_database.dbFound 1 itemsdrwxr-xr-x - root supergroup 0 2017-04-23 16:05 /user/hive/warehouse/test_database.db/test_table#test_database.db就是我们创建数据库对应的目录,而该目录下的test_table就是我们创建的表对应的目录 hive运行日志的信息位置配置及运行日志配置hive运行日志的信息位置配置1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#cp hive-log4j.properties.template hive-log4j.properties# Define some default values that can be overridden by system propertieshive.log.threshold=ALL#日志的级别hive.root.logger=INFO,DRFA#配置的是日志的存放目录,默认是在/tmp/当前用户/目录下面hive.log.dir=$&#123;java.io.tmpdir&#125;/$&#123;user.name&#125;#当前的日志文件hive.log.file=hive.log#默认存放日志文件的目录[root@hdp-node-01 root]# ll /tmp/root/ total 1228drwx------ 2 root root 4096 Dec 17 20:54 09fbac45-c45a-45de-bc40-2aeb96a383fc-rw-r--r-- 1 root root 0 Dec 17 20:54 09fbac45-c45a-45de-bc40-2aeb96a383fc3011878711548453.pipeoutdrwx------ 2 root root 4096 Dec 17 20:35 3c42d672-333f-4a13-8251-3bd4426979d3-rw-r--r-- 1 root root 0 Dec 17 20:35 3c42d672-333f-4a13-8251-3bd4426979d3415307667270826455.pipeoutdrwx------ 2 root root 4096 Nov 28 10:36 95635832-e09c-47a2-a00c-7a30e7ad6a0f-rw-r--r-- 1 root root 0 Nov 28 10:36 95635832-e09c-47a2-a00c-7a30e7ad6a0f7702146714156298728.pipeoutdrwx------ 2 root root 4096 Dec 17 20:33 b024d3ea-59d7-4a68-a73a-e6d8b6f18ed8-rw-r--r-- 1 root root 0 Dec 17 20:33 b024d3ea-59d7-4a68-a73a-e6d8b6f18ed87234035665673320168.pipeoutdrwx------ 2 root root 4096 Nov 28 09:59 cb503457-399e-46c5-b3fe-8fa18c676db5-rw-r--r-- 1 root root 0 Nov 28 08:19 cb503457-399e-46c5-b3fe-8fa18c676db52063284649208370642.pipeoutdrwx------ 2 root root 4096 Dec 17 20:54 e45f3d4c-5ad9-48f5-99cf-2a2424b0883d-rw-r--r-- 1 root root 0 Dec 17 20:54 e45f3d4c-5ad9-48f5-99cf-2a2424b0883d2746165294995632557.pipeout#日志文件-rw-r--r-- 1 root root 20054 Apr 23 15:25 hive.log -rw-r--r-- 1 root root 40632 Nov 23 00:30 hive.log.2016-11-23-rw-r--r-- 1 root root 546704 Nov 27 23:41 hive.log.2016-11-27-rw-r--r-- 1 root root 612388 Nov 28 13:50 hive.log.2016-11-28hive.log.threshold=ALLhive.root.logger=INFO,DRFA#改变日志的目录hive.log.dir=/home/hadoop/app/hive/loghive.log.file=hive.log[root@hdp-node-01 hive]# ll /home/hadoop/app/hive/logtotal 4-rw-r--r-- 1 root root 3065 Apr 23 15:33 hive.log 配置hive运行日志在控制台打印,方便调试12345678910111213141516171819202122232425#查看启动hive命令的帮助[root@hdp-node-01 root]# /home/hadoop/app/hive/bin/hive -helpusage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)#可以在启动的时候带参数/home/hadoop/app/hive/bin/hive --hiveconf hive.root.logger=INFO,console#这样就可以将log信息打印到控制台,我们可以详细的看到(INFO,console 是将INFO以上的log信息打印到console上)#这样在我们程序出错的时候,我们可以通过这样的调试方式进行直观的错误排查 显示当前数据库以及表头的信息123456789101112131415161718192021222324252627282930313233343536373839404142434445#打印表头#需要在hive-site.xml中添加&lt;property&gt;&lt;name&gt;hive.cli.print.current.db&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;#默认的数据库时defaulthive (default)&gt; show databases;OKdatabase_namedefaulthadoop_hivehive (default)&gt; use hadoop_hive;OK#改变数据库为hadoop_hivehive (hadoop_hive)&gt; #打印当前的数据库信息#需要在hive-site.xml中添加&lt;property&gt;&lt;name&gt;hive.cli.print.header&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;hive (hadoop_hive)&gt; select *from default.test_table;OK#会将表名+字段名带上test_table.id test_table.name11 zhangsan22 lisi33 wangwu11 zhangsan22 lisi33 wangwu 启动hive时指定参数1bin/hive --hiveconf &lt;property=value&gt; 1234567891011121314151617181920212223#查看启动hive命令的帮助[root@hdp-node-01 root]# /home/hadoop/app/hive/bin/hive -helpusage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --database &lt;databasename&gt; Specify the database to use -e &lt;quoted-query-string&gt; SQL from command line -f &lt;filename&gt; SQL from files -H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property --hivevar &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file -S,--silent Silent mode in interactive shell -v,--verbose Verbose mode (echo executed SQL to the console)#可以在启动的时候带参数/home/hadoop/app/hive/bin/hive --hiveconf hive.root.logger=INFO,console#这样就可以将log信息打印到控制台,我们可以详细的看到(INFO,console 是将INFO以上的log信息打印到console上)#这样在我们程序出错的时候,我们可以通过这样的调试方式进行直观的错误排查 查看当前所有的命令行配置信息123456789101112131415161718192021#查看所有的设置的环境变量的信息hive (default)&gt; set;system:sun.boot.library.path=/home/hadoop/app/jdk1.7.0_80/jre/lib/amd64system:sun.cpu.endian=littlesystem:sun.cpu.isalist=system:sun.io.unicode.encoding=UnicodeLittlesystem:sun.java.command=org.apache.hadoop.util.RunJar /home/hadoop/app/hive/lib/hive-cli-1.2.1.jar org.apache.hadoop.hive.cli.CliDriversystem:sun.java.launcher=SUN_STANDARDsystem:sun.jnu.encoding=UTF-8system:sun.management.compiler=HotSpot 64-Bit Tiered Compilerssystem:sun.os.patch.level=unknownsystem:user.country=USsystem:user.dir=/tmp/rootsystem:user.home=/rootsystem:user.language=ensystem:user.name=rootsystem:user.timezone=Asia/Shanghaihive (default)&gt; set;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive简介","date":"2017-04-22T10:20:23.134Z","path":"2017/04/22/bigdata/hive/hive_new/hive简介/","text":"什么是Hive？ Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射成一张表，并提供类SQL查询功能； 构建在Hadoop之上的数据仓库 使用HQL作为查询接口 使用HDFS存储 使用MapReduce计算 本质是： 将HQL转化成MapReduce程序 适合离线数据处理 Hive 架构 用户接口: ClientCLI(hive shell)、 JDBC/ODBC(java访问hive)， WEBUI(浏览器访问hive) 元数据: Metastore元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等； 默认存储在自带的derby数据库中，推荐使用采用MySQL存储Metastore； Hadoop 使用HDFS进行存储，使用MapReduce进行计算； 驱动器: Driver 包含：解析器、编译器、优化器、执行器； 解析器：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、 SQL语义是否有误(比如select中被判定为聚合的字段在group by中是否有出现) 编译器：将AST编译生成逻辑执行计划 优化器：对逻辑执行计划进行优化； 执行器：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/TEZ/Spark","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"spark性能优化之四十之数据倾斜解决方案之使用随机数以及扩容表进行join","date":"2017-04-22T06:13:47.682Z","path":"2017/04/22/bigdata/spark从入门到精通_笔记/spark性能优化之四十之数据倾斜解决方案之使用随机数以及扩容表进行join/","text":"使用随机数以及扩容表进行join的步骤如下 1、选择一个RDD，要用flatMap，进行扩容，将每条数据，映射为多条数据，每个映射出来的数据，都带了一个n以内的随机数，通常来说，会选择10。2、将另外一个RDD，做普通的map映射操作，每条数据，都打上一个10以内的随机数。3、最后，将两个处理后的RDD，进行join操作。 局限性： 1、因为你的两个RDD都很大，所以你没有办法去将某一个RDD扩的特别大，一般咱们就是10倍。2、如果就是10倍的话，那么数据倾斜问题，的确是只能说是缓解和减轻，不能说彻底解决。 使用随机数以及扩容表进行join和上一节sample采样倾斜key并单独进行join的区别: sample采样倾斜key并单独进行join将key，从另外一个RDD中过滤出的数据，可能只有一条，或者几条，此时，咱们可以任意进行扩容，扩成1000倍。将从第一个RDD中拆分出来的那个倾斜key RDD，打上1000以内的一个随机数。这种情况下，还可以配合上，提升shuffle reduce并行度，join(rdd, 1000)。通常情况下，效果还是非常不错的。打散成100份，甚至1000份，2000份，去进行join，那么就肯定没有数据倾斜的问题了吧。 123456789101112131415161718192021222324// rdd.join(userid2InfoRDD)会产生数据倾斜//对rdd的每条数据加上10以内的前缀val expandRdd1 = rdd.map&#123; tuple=&gt;&#123; val prefix = Random.nextInt(10) (prefix+&quot;_&quot;+tuple._1, tuple._2) &#125;&#125;//将userid2InfoRDD中的每条数据扩容为10条,并对扩容后的每条数据加上10以内的前缀val expandRdd2 = userid2InfoRDD.flatMap&#123; tuple=&gt;&#123; for(i&lt;- 1 t0 10)&#123; list.add((&quot;0_&quot;+tuple._1, tuple._2)) &#125; list &#125;&#125;//joinexpandRdd1.join(expandRdd2)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之四十之数据倾斜解决方案之sample采样倾斜key单独进行join","date":"2017-04-22T02:18:31.961Z","path":"2017/04/22/bigdata/spark从入门到精通_笔记/spark性能优化之四十之数据倾斜解决方案之sample采样倾斜key单独进行join/","text":"方案的思路图解 这个方案的实现思路，跟大家解析一下：其实关键之处在于，将发生数据倾斜的key，单独拉出来，放到一个RDD中去；就用这个原本会倾斜的key RDD跟其他RDD，单独去join一下，这个时候，key对应的数据，可能就会分散到多个task中去进行join操作。 就不至于说是，这个key跟之前其他的key混合在一个RDD中时，肯定是会导致一个key对应的所有数据，都到一个task中去，就会导致数据倾斜。如下图: 这种方案什么时候适合使用？ 优先对于join，肯定是希望能够采用上一讲讲的，reduce join转换map join。两个RDD数据都比较大，那么就不要那么搞了。 针对你的RDD的数据，你可以自己把它转换成一个中间表，或者是直接用countByKey()的方式，你可以看一下这个RDD各个key对应的数据量；此时如果你发现整个RDD就一个，或者少数几个key，是对应的数据量特别多；尽量建议，比如就是一个key对应的数据量特别多。 此时可以采用咱们的这种方案，单拉出来那个最多的key；单独进行join，尽可能地将key分散到各个task上去进行join操作。 什么时候不适用呢？ 如果一个RDD中，导致数据倾斜的key，特别多；那么此时，最好还是不要这样了；还是使用我们最后一个方案，终极的join数据倾斜的解决方案。 1234567891011121314151617181920212223242526272829303132333435363738//假设rdd.join(rdd2)会产生数据倾斜//假设rdd的数据格式为(&quot;hell&quot;,1)这样的格式//采样10%的数据val sampleRDD = rdd.sample(false,0.1,9)//对采样数据进行reduceByKeyval reduceByKeyRdd = sampleRDD.map((_,1)).reduceByKey(_+_)//反转Tuple,对key进行排序val reversedSampleRdd = reduceByKeyRdd.map(tup=&gt;(tup._2, tup._1))val wkewedKeyList = reversedSampleRdd.sortByKey(false).take(1)//take返回的是一个list,这里是去取list中Tuple,然后取Tuple的key,这样就拿到了导致倾斜的keyval keyStr = wkewedKeyList.toList.get(0)._2//------------上面的过程是去拿到导致数据倾斜的key----------------//拿到产生数据倾斜key对应的RDDval skewedRdd = rdd.filter&#123; tuple=&gt; tuple._1.equals(keyStr)&#125;//拿到非数据倾斜的key对应的RDDval noSkewedRdd = rdd.filter&#123; tuple=&gt; !tuple._1.equals(keyStr)&#125;//------上面的过程是去拿到产生数据倾斜key对应的RDD 和非数据倾斜的key对应的RDD-----//对上面产生的RDD分别去进行joinval joinedRdd1 = skewedRdd.join(rdd2)val joinedRdd2 = noSkewedRdd.join(rdd2)val joinedRdd = joinedRdd1.union(joinedRdd2) 在上面的基础上我们可以进行更进一步的优化: 就是说，咱们单拉出来了，一个或者少数几个可能会产生数据倾斜的key，然后还可以进行更加优化的一个操作； 对于那个key，从另外一个要join的表中，也过滤出来一份数据，比如可能就只有一条数据。userid2infoRDD，一个userid key，就对应一条数据。 然后呢，采取对那个只有一条数据的RDD，进行flatMap操作，打上100个随机数，作为前缀，返回100条数据。 单独拉出来的可能产生数据倾斜的RDD，给每一条数据，都打上一个100以内的随机数，作为前缀。 再去进行join，是不是性能就更好了。肯定可以将数据进行打散，去进行join。join完以后，可以执行map操作，去将之前打上的随机数，给去掉，然后再和另外一个普通RDD join以后的结果，进行union操作。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768//假设rdd.join(rdd2)会产生数据倾斜//假设rdd的数据格式为(&quot;hell&quot;,1)这样的格式//采样10%的数据val sampleRDD = rdd.sample(false,0.1,9)//对采样数据进行reduceByKeyval reduceByKeyRdd = sampleRDD.map((_,1)).reduceByKey(_+_)//反转Tuple,对key进行排序val reversedSampleRdd = reduceByKeyRdd.map(tup=&gt;(tup._2, tup._1))val wkewedKeyList = reversedSampleRdd.sortByKey(false).take(1)//take返回的是一个list,这里是去取list中Tuple,然后取Tuple的key,这样就拿到了导致倾斜的keyval keyStr = wkewedKeyList.toList.get(0)._2//------------上面的过程是去拿到导致数据倾斜的key----------------//拿到产生数据倾斜key对应的RDDval skewedRdd = rdd.filter&#123; tuple=&gt; tuple._1.equals(keyStr)&#125;//拿到非数据倾斜的key对应的RDDval noSkewedRdd = rdd.filter&#123; tuple=&gt; !tuple._1.equals(keyStr)&#125;//------上面的过程是去拿到产生数据倾斜key对应的RDD 和非数据倾斜的key对应的RDD-----val skewDataInRdd2 = rdd2.filter(_.equals(keyStr)val skewData100Rdd2 = skewDataInRdd2.flatMap&#123; tuple=&gt; list = Array() for(i&lt;- 1 to 100)&#123; val prefix = Random.nextInt(100) list.add((prefix+&quot;_&quot;+tuple._1,tuple._2)) &#125; list&#125;//------上面的过程对另外一个需要join的rdd进行过滤,拿到也过滤出来一份数据，比如可能就只有一条数据。然后呢，采取对那个只有一条数据的RDD，进行flatMap操作，打上100个随机数，作为前缀，返回100条数据val joinedRdd1 = skewedRdd.map&#123; tuple=&gt; val prefix = Random.nextInt(100) (prefix+&quot;_&quot;+tuple._1,tuple._2)&#125;//将两个都加上前缀的RDD进行join,因为有了前缀,这样他们就能打散.join(skewData100Rdd2)//去掉前缀.map&#123; tuple=&gt; val arr = tuple._1.split(&quot;_&quot;) (arr(1),tuple._2)&#125;val joinedRdd2 = noSkewedRdd.join(rdd2)val joinedRdd = joinedRdd1.union(joinedRdd2)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十九之数据倾斜解决方案之将reduce join转换为map join","date":"2017-04-22T01:44:50.285Z","path":"2017/04/22/bigdata/spark从入门到精通_笔记/spark性能优化之三十九之数据倾斜解决方案之将reduce join转换为map join/","text":"map端join和reduce端join的对比 reduce join转换为map join，适合在什么样的情况下，可以来使用？ 如果两个RDD要进行join，其中一个RDD是比较小的。一个RDD是100万数据，一个RDD是1万数据。（一个RDD是1亿数据，一个RDD是100万数据） 其中一个RDD必须是比较小的，broadcast出去那个小RDD的数据以后，就会在每个executor的block manager中都驻留一份。要确保你的内存足够存放那个小RDD中的数据 这种方式下，根本不会发生shuffle操作，肯定也不会发生数据倾斜；从根本上杜绝了join操作可能导致的数据倾斜的问题； 对于join中有数据倾斜的情况，大家尽量第一时间先考虑这种方式，效果非常好；如果某个RDD比较小的情况下。 不适合的情况： 两个RDD都比较大，那么这个时候，你去将其中一个RDD做成broadcast，就很笨拙了。很可能导致内存不足。最终导致内存溢出，程序挂掉。 而且其中某些key（或者是某个key），还发生了数据倾斜；此时可以采用最后两种方式。 对于join这种操作，不光是考虑数据倾斜的问题；即使是没有数据倾斜问题，也完全可以优先考虑，用我们讲的这种高级的reduce join转map join的技术，不要用普通的join，去通过shuffle，进行数据的join；完全可以通过简单的map，使用map join的方式，牺牲一点内存资源；在可行的情况下，优先这么使用。 不走shuffle，直接走map，是不是性能也会高很多？这是肯定的。 12345678910111213val userInfosBroadcast = sc.broadcast(userinfos)rdd.map&#123; tuple=&gt; val userInfosList = userInfosBroadcast.value() //伪代码:将userInfosList转成userInfoMap userInfoMap.put(userInfosList) //实现自己的业务逻辑 (tuple._1,userInfoMap(tuple._1)) &#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十八之数据倾斜解决方案之使用随机key实现双重聚合","date":"2017-04-22T01:24:06.260Z","path":"2017/04/22/bigdata/spark从入门到精通_笔记/spark性能优化之三十八之数据倾斜解决方案之使用随机key实现双重聚合/","text":"使用随机key实现双重聚合 原理 使用场景1.groupByKey2.reduceByKey 比较适合使用这种方式；join，咱们通常不会这样来做，后面会讲三种，针对不同的join造成的数据倾斜的问题的解决方案。 第一轮聚合的时候，对key进行打散，将原先一样的key，变成不一样的key，相当于是将每个key分为多组；先针对多个组，进行key的局部聚合；接着，再去除掉每个key的前缀，然后对所有的key，进行全局的聚合。 对groupByKey、reduceByKey造成的数据倾斜，有比较好的效果。如果说，之前的第一、第二、第三种方案，都没法解决数据倾斜的问题，那么就只能依靠这一种方式了。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十七之数据倾斜解决方案之提高shuffle操作的并行度","date":"2017-04-22T00:58:27.693Z","path":"2017/04/22/bigdata/spark从入门到精通_笔记/spark性能优化之三十七之数据倾斜解决方案之提高shuffle操作的并行度/","text":"提高shuffle操作的并行度原理图 提升shuffle reduce端并行度，怎么来操作？ 很简单，主要给我们所有的shuffle算子，比如groupByKey、countByKey、reduceByKey。在调用的时候，传入进去一个参数。一个数字。那个数字，就代表了那个shuffle操作的reduce端的并行度。那么在进行shuffle操作的时候，就会对应着创建指定数量的reduce task。 这样的话，就可以让每个reduce task分配到更少的数据。基本可以缓解数据倾斜的问题。 比如说，原本某个task分配数据特别多，直接OOM，内存溢出了，程序没法运行，直接挂掉。按照log，找到发生数据倾斜的shuffle操作，给它传入一个并行度数字，这样的话，原先那个task分配到的数据，肯定会变少。就至少可以避免OOM的情况，程序至少是可以跑的。 提升shuffle reduce并行度的缺陷 治标不治本，因为，它没有从根本上改变数据倾斜的本质和问题。不像第一个和第二个方案（直接避免了数据倾斜的发生）。原理没有改变，只是说，尽可能地去缓解和减轻shuffle reduce task的数据压力，以及数据倾斜的问题。 实际生产环境中的经验。 1、如果最理想的情况下，提升并行度以后，减轻了数据倾斜的问题，或者甚至可以让数据倾斜的现象忽略不计，那么就最好。就不用做其他的数据倾斜解决方案了。 2、不太理想的情况下，就是比如之前某个task运行特别慢，要5个小时，现在稍微快了一点，变成了4个小时；或者是原先运行到某个task，直接OOM，现在至少不会OOM了，但是那个task运行特别慢，要5个小时才能跑完。 那么，如果出现第二种情况的话，各位，就立即放弃第三种方案(提高shuffle操作的并行度)，开始去尝试和选择后面的四种方案。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十六之数据倾斜解决方案之聚合源数据以及过滤导致倾斜的key","date":"2017-04-21T14:17:18.997Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十六之数据倾斜解决方案之聚合源数据以及过滤导致倾斜的key/","text":"第一个方案：聚合源数据咱们现在，做一些聚合的操作，groupByKey、reduceByKey；groupByKey，说白了，就是拿到每个key对应的values；reduceByKey，说白了，就是对每个key对应的values执行一定的计算。 现在这些操作，比如groupByKey和reduceByKey，包括之前说的join。都是在spark作业中执行的。 spark作业的数据来源，通常是哪里呢？90%的情况下，数据来源都是hive表（hdfs，大数据分布式存储系统）。hdfs上存储的大数据。hive表，hive表中的数据，通常是怎么出来的呢？有了spark以后，hive比较适合做什么事情？hive就是适合做离线的，晚上凌晨跑的，ETL（extract transform load，数据的采集、清洗、导入），hive sql，去做这些事情，从而去形成一个完整的hive中的数据仓库；说白了，数据仓库，就是一堆表。 spark作业的源表，hive表，其实通常情况下来说，也是通过某些hive etl生成的。hive etl可能是晚上凌晨在那儿跑。今天跑昨天的数九。 数据倾斜，某个key对应的80万数据，某些key对应几百条，某些key对应几十条；现在，咱们直接在生成hive表的hive etl中，对数据进行聚合。比如按key来分组，将key对应的所有的values，全部用一种特殊的格式，拼接到一个字符串里面去，比如“key=sessionid, value: action_seq=1|user_id=1|search_keyword=火锅|category_id=001;action_seq=2|user_id=1|search_keyword=涮肉|category_id=001”。 对key进行group，在spark中，拿到key=sessionid，values；hive etl中，直接对key进行了聚合。那么也就意味着，每个key就只对应一条数据。在spark中，就不需要再去执行groupByKey+map这种操作了。直接对每个key对应的values字符串，map操作，进行你需要的操作即可。key,values串。 spark中，可能对这个操作，就不需要执行shffule操作了，也就根本不可能导致数据倾斜。 或者是，对每个key在hive etl中进行聚合，对所有values聚合一下，不一定是拼接起来，可能是直接进行计算。reduceByKey，计算函数，应用在hive etl中，每个key的values。 聚合源数据方案，第二种做法你可能没有办法对每个key，就聚合出来一条数据； 那么也可以做一个妥协；对每个key对应的数据，10万条；有好几个粒度，比如10万条里面包含了几个城市、几天、几个地区的数据，现在放粗粒度；直接就按照城市粒度，做一下聚合，几个城市，几天、几个地区粒度的数据，都给聚合起来。比如说 city_id date area_id select … from … group by city_id 尽量去聚合，减少每个key对应的数量，也许聚合到比较粗的粒度之后，原先有10万数据量的key，现在只有1万数据量。减轻数据倾斜的现象和问题。 第二个方案：过滤导致倾斜的key如果你能够接受某些数据，在spark作业中直接就摒弃掉，不使用。比如说，总共有100万个key。只有2个key，是数据量达到10万的。其他所有的key，对应的数量都是几十。 这个时候，你自己可以去取舍，如果业务和需求可以理解和接受的话，在你从hive表查询源数据的时候，直接在sql中用where条件，过滤掉某几个key。 那么这几个原先有大量数据，会导致数据倾斜的key，被过滤掉之后，那么在你的spark作业中，自然就不会发生数据倾斜了。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十六之数据倾斜之原理及现象说明","date":"2017-04-21T13:32:28.830Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十六之数据倾斜之原理及现象说明/","text":"需要看的blog:https://www.iteblog.com/archives/2061.htmlhttps://www.iteblog.com/archives/1671.htmlhttps://www.iteblog.com/archives/1672.html 数据倾斜的原理 想象一下，出现数据倾斜以后的运行的情况。很糟糕！第一个和第二个task，各分配到了1万数据；那么可能1万条数据，需要10分钟计算完毕；第一个和第二个task，可能同时在10分钟内都运行完了；第三个task要88万条，88 * 10 = 880分钟 = 14.5个小时；大家看看，本来另外两个task很快就运行完毕了（10分钟），但是由于一个拖后腿的家伙，第三个task，要14.5个小时才能运行完，就导致整个spark作业，也得14.5个小时才能运行完。导致spark作业，跑的特别特别特别特别慢！！！像老牛拉破车！数据倾斜，一旦出现，是不是性能杀手。。。。 数据倾斜的现象发生数据倾斜以后的现象： spark数据倾斜，有两种表现： 1、你的大部分的task，都执行的特别特别快，刷刷刷，就执行完了（你要用client模式，standalone client，yarn client，本地机器主要一执行spark-submit脚本，就会开始打印log），task175 finished；剩下几个task，执行的特别特别慢，前面的task，一般1s可以执行完5个；最后发现1000个task，998，999 task，要执行1个小时，2个小时才能执行完一个task。出现数据倾斜了 还算好的，因为虽然老牛拉破车一样，非常慢，但是至少还能跑。 2、运行的时候，其他task都刷刷刷执行完了，也没什么特别的问题；但是有的task，就是会突然间，啪，报了一个OOM，JVM Out Of Memory，内存溢出了，task failed，task lost，resubmitting task。反复执行几次都到了某个task就是跑不通，最后就挂掉。 某个task就直接OOM，那么基本上也是因为数据倾斜了，task分配的数量实在是太大了！！！所以内存放不下，然后你的task每处理一条数据，还要创建大量的对象。内存爆掉了。 出现数据倾斜了,这种就不太好了，因为你的程序如果不去解决数据倾斜的问题，压根儿就跑不出来。 作业都跑不完，还谈什么性能调优这些东西。扯淡。。。 数据倾斜的产生原因与定位定位原因与出现问题的位置: 如果出现了数据倾斜,那么使用client模式去跑程序,这样就可以根据本地打印的log去定位导致数据倾斜的程序代码,具体如下: 出现数据倾斜的原因，基本只可能是因为发生了shuffle操作，在shuffle的过程中，出现了数据倾斜的问题。因为某个，或者某些key对应的数据，远远的高于其他的key。 1、你在自己的程序里面找找，哪些地方用了会产生shuffle的算子，groupByKey、countByKey、reduceByKey、join 2、看log log一般会报是在你的哪一行代码，导致了OOM异常；或者呢，看log，看看是执行到了第几个stage！！！ 我们这里不会去剖析stage的划分算法，spark代码，是怎么划分成一个一个的stage的。哪一个stage，task特别慢，就能够自己用肉眼去对你的spark代码进行stage的划分，就能够通过stage定位到你的代码，哪里发生了数据倾斜 去找找，代码那个地方，是哪个shuffle操作。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十六之troubleshooting之持久化以及checkpoint的使用","date":"2017-04-21T09:06:58.925Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十六之troubleshooting之持久化以及checkpoint的使用/","text":"错误的持久化使用方式 usersRDD，想要对这个RDD做一个cache，希望能够在后面多次使用这个RDD的时候，不用反复重新计算RDD；可以直接使用通过各个节点上的executor的BlockManager管理的内存 / 磁盘上的数据，避免重新反复计算RDD。123usersRDD.cache()usersRDD.count()usersRDD.take() 上面这种方式，不要说会不会生效了，实际上是会报错的。会报什么错误呢？会报一大堆file not found的错误。 正确的持久化使用方式：123usersRDDusersRDD = usersRDD.cache()val cachedUsersRDD = usersRDD.cache() 之后再去使用usersRDD，或者cachedUsersRDD，就可以了。就不会报错了。所以说，这个是咱们的持久化的正确的使用方式。 checkpoint原理： 1、在代码中，用SparkContext，设置一个checkpoint目录，可以是一个容错文件系统的目录，比如hdfs；2、在代码中，对需要进行checkpoint的rdd，执行RDD.checkpoint()；3、RDDCheckpointData（spark内部的API），接管你的RDD，会标记为marked for checkpoint，准备进行checkpoint4、你的job运行完之后，会调用一个finalRDD.doCheckpoint()方法，会顺着rdd lineage，回溯扫描，发现有标记为待checkpoint的rdd，就会进行二次标记，inProgressCheckpoint，正在接受checkpoint操作5、job执行完之后，就会启动一个内部的新job，去将标记为inProgressCheckpoint的rdd的数据，都写入hdfs文件中。（备注，如果rdd之前cache过，会直接从缓存中获取数据，写入hdfs中；如果没有cache过，那么就会重新计算一遍这个rdd，再checkpoint）6、将checkpoint过的rdd之前的依赖rdd，改成一个CheckpointRDD*，强制改变你的rdd的lineage。后面如果rdd的cache数据获取失败，直接会通过它的上游CheckpointRDD，去容错的文件系统，比如hdfs，中，获取checkpoint的数据。 说一下checkpoint的使用 1、SparkContext，设置checkpoint目录 1sc.checkpointFile(“hdfs://...”) 2、对RDD执行checkpoint操作 1rdd.checkpoint","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十五之troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题","date":"2017-04-21T08:32:41.847Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十五之troubleshooting之解决yarn-cluster模式的JVM栈内存溢出问题/","text":"yarn-client模式提交的图示 实践经验，碰到的yarn-cluster的问题： 有的时候，运行一些包含了spark sql的spark作业，可能会碰到yarn-client模式下，可以正常提交运行；yarn-cluster模式下，可能是无法提交运行的，会报出JVM的PermGen（永久代）的内存溢出，OOM。 yarn-client模式下，driver是运行在本地机器上的，spark使用的JVM的PermGen的配置，是本地的spark-class文件（spark客户端是默认有配置的），JVM的永久代的大小是128M，这个是没有问题的；但是呢，在yarn-cluster模式下，driver是运行在yarn集群的某个节点上的，使用的是没有经过配置的默认设置（PermGen永久代大小），82M。 spark-sql，它的内部是要进行很复杂的SQL的语义解析、语法树的转换等等，特别复杂，在这种复杂的情况下，如果说你的sql本身特别复杂的话，很可能会比较导致性能的消耗，内存的消耗。可能对PermGen永久代的占用会比较大。 所以，此时，如果对永久代的占用需求，超过了82M的话，但是呢又在128M以内；就会出现如上所述的问题，yarn-client模式下，默认是128M，这个还能运行；如果在yarn-cluster模式下，默认是82M，就有问题了。会报出PermGen Out of Memory error log。 如何解决这种问题？ 既然是JVM的PermGen永久代内存溢出，那么就是内存不够用。咱们呢，就给yarn-cluster模式下的，driver的PermGen多设置一些。 spark-submit脚本中，加入以下配置即可： 1--conf spark.driver.extraJavaOptions=&quot;-XX:PermSize=128M -XX:MaxPermSize=256M&quot; 这个就设置了driver永久代的大小，默认是128M，最大是256M。那么，这样的话，就可以基本保证你的spark作业不会出现上述的yarn-cluster模式导致的永久代内存溢出的问题。 spark sql，sql，要注意，一个问题 sql，有大量的or语句。比如where keywords1=’’ or keywords2=’’ or keywords3=’’当达到or语句，有成百上千的时候，此时可能就会出现一个driver端的jvm stack overflow (JVM栈内存溢出的问题) JVM栈内存溢出，基本上就是由于调用的方法层级过多，因为产生了大量的，非常深的，超出了JVM栈深度限制的，递归。递归方法。我们的猜测，spark sql，有大量or语句的时候，spark sql内部源码中，在解析sql，比如转换成语法树，或者进行执行计划的生成的时候，对or的处理是递归。or特别多的话，就会发生大量的递归。 JVM Stack Memory Overflow，栈内存溢出。 这种时候，建议不要搞那么复杂的spark sql语句。采用替代方案：将一条sql语句，拆解成多条sql语句来执行。每条sql语句，就只有100个or子句以内；一条一条SQL语句来执行。根据生产环境经验的测试，一条sql语句，100个or子句以内，是还可以的。通常情况下，不会报那个栈内存溢出。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十四之troubleshooting之解决yarn-client模式导致的网卡流量激增问题","date":"2017-04-21T08:04:54.680Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十四之troubleshooting之解决yarn-client模式导致的网卡流量激增问题/","text":"yarn-client模式下图解 yarn-client模式下，会产生什么样的问题呢？ 由于咱们的driver是启动在本地机器的，而且driver是全权负责所有的任务的调度的，也就是说要跟yarn集群上运行的多个executor进行频繁的通信（中间有task的启动消息、task的执行统计消息、task的运行状态、shuffle的输出结果）。 咱们来想象一下。比如你的executor有100个，stage有10个，task有1000个。每个stage运行的时候，都有1000个task提交到executor上面去运行，平均每个executor有10个task。接下来问题来了，driver要频繁地跟executor上运行的1000个task进行通信。通信消息特别多，通信的频率特别高。运行完一个stage，接着运行下一个stage，又是频繁的通信。 在整个spark运行的生命周期内，都会频繁的去进行通信和调度。所有这一切通信和调度都是从你的本地机器上发出去的，和接收到的。这是最要人命的地方。你的本地机器，很可能在30分钟内（spark作业运行的周期内），进行频繁大量的网络通信。那么此时，你的本地机器的网络通信负载是非常非常高的。会导致你的本地机器的网卡流量会激增！！！ 你的本地机器的网卡流量激增，当然不是一件好事了。因为在一些大的公司里面，对每台机器的使用情况，都是有监控的。不会允许单个机器出现耗费大量网络带宽等等这种资源的情况。运维人员。可能对公司的网络，或者其他（你的机器还是一台虚拟机），对其他机器，都会有负面和恶劣的影响。 解决的方法： 实际上解决的方法很简单，就是心里要清楚，yarn-client模式是什么情况下，可以使用的？yarn-client模式，通常咱们就只会使用在测试环境中，你写好了某个spark作业，打了一个jar包，在某台测试机器上，用yarn-client模式去提交一下。因为测试的行为是偶尔为之的，不会长时间连续提交大量的spark作业去测试。还有一点好处，yarn-client模式提交，可以在本地机器观察到详细全面的log。通过查看log，可以去解决线上报错的故障（troubleshooting）、对性能进行观察并进行性能调优。 实际上线了以后，在生产环境中，都得用yarn-cluster模式，去提交你的spark作业。 yarn-cluster模式，就跟你的本地机器引起的网卡流量激增的问题，就没有关系了。也就是说，就算有问题，也应该是yarn运维团队和基础运维团队之间的事情了。使用了yarn-cluster模式以后，就不是你的本地机器运行Driver，进行task调度了。是yarn集群中，某个节点会运行driver进程，负责task调度。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十三之troubleshooting之解决算子函数返回null导致的问题","date":"2017-04-21T07:59:25.747Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十三之troubleshooting之解决算子函数返回null导致的问题/","text":"123456789101112//在算子函数中，返回null// return actionRDD.mapToPair(new PairFunction&lt;Row, String, Row&gt;() &#123;//// private static final long serialVersionUID = 1L;// // @Override// public Tuple2&lt;String, Row&gt; call(Row row) throws Exception &#123;// return new Tuple2&lt;String, Row&gt;(&quot;-999&quot;, RowFactory.createRow(&quot;-999&quot;)); // &#125;// // &#125;); 大家可以看到，在有些算子函数里面，是需要我们有一个返回值的。但是，有时候，我们可能对某些值，就是不想有什么返回值。我们如果直接返回NULL的话，那么可以不幸的告诉大家，是不行的，会报错的。 Scala.Math(NULL)，异常 如果碰到你的确是对于某些值，不想要有返回值的话，有一个解决的办法： 1、在返回的时候，返回一些特殊的值，不要返回null，比如“-999”2、在通过算子获取到了一个RDD之后，可以对这个RDD执行filter操作，进行数据过滤。filter内，可以对数据进行判定，如果是-999，那么就返回false，给过滤掉就可以了。3、大家不要忘了，之前咱们讲过的那个算子调优里面的coalesce算子，在filter之后，可以使用coalesce算子压缩一下RDD的partition的数量，让各个partition的数据比较紧凑一些。也能提升一些性能。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十二之troubleshooting之解决各种序列化导致的报错","date":"2017-04-21T07:50:38.093Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十二之troubleshooting之解决各种序列化导致的报错/","text":"你会看到什么样的序列化导致的报错？ 用client模式去提交spark作业，观察本地打印出来的log。如果出现了类似于Serializable、Serialize等等字眼，报错的log，那么恭喜大家，就碰到了序列化问题导致的报错。 虽然是报错，但是序列化报错，应该是属于比较简单的了，很好处理。 序列化报错要注意的三个点： 1、你的算子函数里面，如果使用到了外部的自定义类型的变量，那么此时，就要求你的自定义类型，必须是可序列化的 123456789101112131415final Teacher teacher = new Teacher(&quot;leo&quot;);studentsRDD.foreach(new VoidFunction() &#123; public void call(Row row) throws Exception &#123; String teacherName = teacher.getName(); .... &#125;&#125;);public class Teacher implements Serializable &#123; &#125; 2、如果要将自定义的类型，作为RDD的元素类型，那么自定义的类型也必须是可以序列化的1234567891011JavaPairRDD&lt;Integer, Teacher&gt; teacherRDDJavaPairRDD&lt;Integer, Student&gt; studentRDDstudentRDD.join(teacherRDD)public class Teacher implements Serializable &#123; &#125;public class Student implements Serializable &#123; &#125; 3、不能在上述两种情况下，去使用一些第三方的，不支持序列化的类型1234567891011Connection conn = studentsRDD.foreach(new VoidFunction() &#123; public void call(Row row) throws Exception &#123; conn.....&#125;&#125;);//Connection是不支持序列化的","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十一之troubleshooting之解决YARN队列资源不足导致的Application直接失败","date":"2017-04-21T07:31:04.918Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十一之troubleshooting之解决YARN队列资源不足导致的Application直接失败/","text":"yarn资源不足导致的Application提交失败的现象说明如果说，你是基于yarn来提交spark。比如yarn-cluster或者yarn-client。你可以指定提交到某个hadoop队列上的。每个队列都是可以有自己的资源的。 跟大家说一个生产环境中的，给spark用的yarn资源队列的情况：500G内存，200个cpu core。 比如说，某个spark application，在spark-submit里面你自己配了，executor，80个；每个executor，4G内存；每个executor，2个cpu core。你的spark作业每次运行，大概要消耗掉320G内存，以及160个cpu core。 乍看起来，咱们的队列资源，是足够的，500G内存，280个cpu core。 首先，第一点，你的spark作业实际运行起来以后，耗费掉的资源量，可能是比你在spark-submit里面配置的，以及你预期的，是要大一些的。400G内存，190个cpu core。 那么这个时候，的确，咱们的队列资源还是有一些剩余的。但是问题是，如果你同时又提交了一个spark作业上去，一模一样的。那就可能会出问题。 第二个spark作业，又要申请320G内存+160个cpu core。结果，发现队列资源不足。。。。 此时，可能会出现两种情况：（备注，具体出现哪种情况，跟你的YARN、Hadoop的版本，你们公司的一些运维参数，以及配置、硬件、资源肯能都有关系） 1、YARN，发现资源不足时，你的spark作业，并没有hang在那里，等待资源的分配，而是直接打印一行fail的log，直接就fail掉了。2、YARN，发现资源不足，你的spark作业，就hang在那里。一直等待之前的spark作业执行完，等待有资源分配给自己来执行。 采用如下方案1、在你的J2EE（我们这个项目里面，spark作业的运行，之前说过了，J2EE平台触发的，执行spark-submit脚本），限制，同时只能提交一个spark作业到yarn上去执行，确保一个spark作业的资源肯定是有的。 2、你应该采用一些简单的调度区分的方式，比如说，你有的spark作业可能是要长时间运行的，比如运行30分钟；有的spark作业，可能是短时间运行的，可能就运行2分钟。此时，都提交到一个队列上去，肯定不合适。很可能出现30分钟的作业卡住后面一大堆2分钟的作业。分队列，可以申请（如果你的集群不是你管理的,跟你们的YARN、Hadoop运维的同学申请）。(如果集群是自己管理的)你自己给自己搞两个调度队列。每个队列的根据你要执行的作业的情况来设置。在你的J2EE程序里面，要判断，如果是长时间运行的作业，就干脆都提交到某一个固定的队列里面去把；如果是短时间运行的作业，就统一提交到另外一个队列里面去。这样，避免了长时间运行的作业，阻塞了短时间运行的作业。 3、你的队列里面，无论何时，只会有一个作业在里面运行。那么此时，就应该用我们之前讲过的性能调优的手段，去将每个队列能承载的最大的资源，分配给你的每一个spark作业，比如80个executor；6G的内存；3个cpu core。尽量让你的spark作业每一次运行，都达到最满的资源使用率，最快的速度，最好的性能；并行度，240个cpu core，720个task。 4、在J2EE中，通过线程池的方式（一个线程池对应一个资源队列），来实现上述我们说的方案。12345678910ExecutorService threadPool = Executors.newFixedThreadPool(1);threadPool.submit(new Runnable() &#123; @Overridepublic void run() &#123;&#125; &#125;);","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之三十之troubleshooting之解决JVM GC导致的shuffle文件拉取失败","date":"2017-04-21T06:58:02.221Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之三十之troubleshooting之解决JVM GC导致的shuffle文件拉取失败/","text":"task拉取上一个stage数据的过程 问题描述比如，executor的JVM进程，可能内存不是很够用了。那么此时可能就会执行GC。minor GC or full GC。总之一旦发生了JVM之后，就会导致executor内，所有的工作线程全部停止，比如BlockManager，基于netty的网络通信。 下一个stage的executor，可能是还没有停止掉的，task想要去上一个stage的task所在的exeuctor，去拉取属于自己的数据，结果由于对方正在gc，就导致拉取了半天没有拉取到。 就很可能会报出，shuffle file not found。但是，可能下一个stage又重新提交了stage或task以后，再执行就没有问题了，因为可能第二次就没有碰到JVM在gc了。 解决的办法1spark.shuffle.io.maxRetries 3 第一个参数，意思就是说，shuffle文件拉取的时候，如果没有拉取到（拉取失败），最多或重试几次（会重新拉取几次文件），默认是3次。1spark.shuffle.io.retryWait 5s 第二个参数，意思就是说，每一次重试拉取文件的时间间隔，默认是5s钟。 默认情况下，假如说第一个stage的executor正在进行漫长的full gc。第二个stage的executor尝试去拉取文件，结果没有拉取到，默认情况下，会反复重试拉取3次，每次间隔是五秒钟。最多只会等待3 * 5s = 15s。如果15s内，没有拉取到shuffle file。就会报出shuffle file not found。 针对这种情况，我们完全可以进行预备性的参数调节。增大上述两个参数的值，达到比较大的一个值，尽量保证第二个stage的task，一定能够拉取到上一个stage的输出文件。避免报shuffle file not found。然后可能会重新提交stage和task去执行。那样反而对性能也不好。12spark.shuffle.io.maxRetries 60spark.shuffle.io.retryWait 60s 最多可以忍受1个小时没有拉取到shuffle file。只是去设置一个最大的可能的值。full gc不可能1个小时都没结束吧。 这样呢，就可以尽量避免因为gc导致的shuffle file not found，无法拉取到的问题。 查看问题的方法有时会出现的一种情况，非常普遍，在spark的作业中；shuffle file not found。（spark作业中，非常非常常见的）而且，有的时候，它是偶尔才会出现的一种情况。有的时候，出现这种情况以后，会重新去提交stage、task。重新执行一遍，发现就好了。没有这种错误了。 log怎么看？用client模式去提交你的spark作业。比如standalone client；yarn client。一提交作业，直接可以在本地看到刷刷刷更新的log。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十九之troubleshooting之控制shuffle reduce端缓冲大小以避免OOM","date":"2017-04-21T06:38:19.313Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十九之troubleshooting之控制shuffle reduce端缓冲大小以避免OOM/","text":"shuffle reduce端缓冲图示 map端的task是不断的输出数据的，数据量可能是很大的。但是，其实reduce端的task，并不是等到map端task将属于自己的那份数据全部写入磁盘文件之后，再去拉取的。map端写一点数据，reduce端task就会拉取一小部分数据，立即进行后面的聚合、算子函数的应用。 每次reduece能够拉取多少数据，就由buffer来决定。因为拉取过来的数据，都是先放在buffer中的。然后才用后面的executor分配的堆内存占比（0.2），hashmap，去进行后续的聚合、函数的执行。 reduce端缓冲（buffer），可能会出什么问题？ 可能是会出现，默认是48MB，也许大多数时候，reduce端task一边拉取一边计算，不一定一直都会拉满48M的数据。可能大多数时候，拉取个10M数据，就计算掉了。 大多数时候，也许不会出现什么问题。但是有的时候，map端的数据量特别大，然后写出的速度特别快。reduce端所有task，拉取的时候，全部达到自己的缓冲的最大极限值，缓冲，48M，全部填满。 这个时候，再加上你的reduce端执行的聚合函数的代码，可能会创建大量的对象。也许，一下子，内存就撑不住了，就会OOM。reduce端的内存中，就会发生内存溢出的问题。 针对上述的可能出现的问题，我们该怎么来解决呢？ 这个时候，就应该减少reduce端task缓冲的大小。我宁愿多拉取几次，但是每次同时能够拉取到reduce端每个task的数量，比较少，就不容易发生OOM内存溢出的问题。（比如，可以调节成12M） 在实际生产环境中，我们都是碰到过这种问题的。这是典型的以性能换执行的原理。reduce端缓冲小了，不容易OOM了，但是，性能一定是有所下降的，你要拉取的次数就多了。就走更多的网络传输开销。 这种时候，只能采取牺牲性能的方式了，spark作业，首先，第一要义，就是一定要让它可以跑起来。分享一个经验，曾经写过一个特别复杂的spark作业，写完代码以后，半个月之内，就是跑不起来，里面各种各样的问题，需要进行troubleshooting。调节了十几个参数，其中就包括这个reduce端缓冲的大小。总算作业可以跑起来了,然后才去考虑性能的调优。 再来说说，reduce端缓冲大小的另外一面，关于性能调优的一面： 咱们假如说，你的Map端输出的数据量也不是特别大，然后你的整个application的资源也特别充足。200个executor、5个cpu core、10G内存。 其实可以尝试去增加这个reduce端缓冲大小的，比如从48M，变成96M。那么这样的话，每次reduce task能够拉取的数据量就很大。需要拉取的次数也就变少了。比如原先需要拉取100次，现在只要拉取50次就可以执行完了。对网络传输性能开销的减少，以及reduce端聚合操作执行的次数的减少，都是有帮助的。 最终达到的效果，就应该是性能上的一定程度上的提升。一定要注意，资源足够的时候，再去做这个事儿。 12spark.reducer.maxSizeInFlight，48spark.reducer.maxSizeInFlight，24","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十八之算子调优之reduceByKey本地聚合介绍","date":"2017-04-21T06:17:52.191Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十八之算子调优之reduceByKey本地聚合介绍/","text":"reduceByKey实现原理图 reduceByKey，相较于普通的shuffle操作（比如groupByKey），它的一个特点，就是说，会进行map端的本地聚合。 对map端给下个stage每个task创建的输出文件中，写数据之前，就会进行本地的combiner操作，也就是说对每一个key，对应的values，都会执行你的算子函数（) + _） 用reduceByKey对性能的提升： 1、在本地进行聚合以后，在map端的数据量就变少了，减少磁盘IO。而且可以减少磁盘空间的占用。2、下一个stage，拉取数据的量，也就变少了。减少网络的数据传输的性能消耗。3、在reduce端进行数据缓存的内存占用变少了。4、reduce端，要进行聚合的数据量也变少了。 总结： reduceByKey在什么情况下使用呢？ 1、非常普通的，比如说，就是要实现类似于wordcount程序一样的，对每个key对应的值，进行某种数据公式或者算法的计算（累加、类乘）2、对于一些类似于要对每个key进行一些字符串拼接的这种较为复杂的操作，可以自己衡量一下，其实有时，也是可以使用reduceByKey来实现的。但是不太好实现。如果真能够实现出来，对性能绝对是有帮助的。（shuffle基本上就占了整个spark作业的90%以上的性能消耗，主要能对shuffle进行一定的调优，都是有价值的）","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十七之算子调优之foreachPartition优化写数据库性能","date":"2017-04-21T03:28:06.179Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十七之算子调优之foreachPartition优化写数据库性能/","text":"默认的foreach的性能缺陷在哪里？ 首先，对于每条数据，都要单独去调用一次function，task为每个数据，都要去执行一次function函数。如果100万条数据，（一个partition），调用100万次。性能比较差。另外一个非常非常重要的一点,如果每个数据，你都去创建一个数据库连接的话，那么你就得创建100万次数据库连接。但是要注意的是，数据库连接的创建和销毁，都是非常非常消耗性能的。虽然我们之前已经用了数据库连接池，只是创建了固定数量的数据库连接。你还是得多次通过数据库连接，往数据库（MySQL）发送一条SQL语句，然后MySQL需要去执行这条SQL语句。如果有100万条数据，那么就是100万次发送SQL语句。以上两点（数据库连接，多次发送SQL语句），都是非常消耗性能的。 用了foreachPartition算子之后，好处在哪里？ 1、对于我们写的function函数，就调用一次，一次传入一个partition所有的数据2、主要创建或者获取一个数据库连接就可以3、只要向数据库发送一次SQL语句和多组参数即可 在实际生产环境中，清一色，都是使用foreachPartition操作；但是有个问题，跟mapPartitions操作一样，如果一个partition的数量真的特别特别大，比如真的是100万，那基本上就不太靠谱了。一下子进来，很有可能会发生OOM，内存溢出的问题。 一组数据的对比：生产环境 一个partition大概是1千条左右用foreach，跟用foreachPartition，性能的提升达到了2~3分钟。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十六之算子调优之reparation解决spark sql低并行度的性能问题","date":"2017-04-21T02:57:00.223Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十六之算子调优之reparation解决spark sql低并行度的性能问题/","text":"并行度：之前说过，并行度是自己可以调节，或者说是设置的。 1、spark.default.parallelism2、textFile()，传入第二个参数，指定partition数量（比较少用） 咱们的项目代码中，没有设置并行度，实际上，在生产环境中，是最好自己设置一下的。官网有推荐的设置方式，你的spark-submit脚本中，会指定你的application总共要启动多少个executor，100个；每个executor多少个cpu core，2~3个；对于一个application，总共有cpu core，200个。 官方推荐，根据你的application的总cpu core数量（在spark-submit中可以指定，200个），自己手动设置spark.default.parallelism参数，指定为cpu core总数的2~3倍。400~600个并行度。600。 你设置的这个并行度，在哪些情况下会生效？哪些情况下，不会生效？如果你压根儿没有使用Spark SQL（DataFrame），那么你整个spark application默认所有stage的并行度都是你设置的那个参数。（除非你使用coalesce算子缩减过partition数量） 问题来了，Spark SQL，用了。用Spark SQL的那个stage的并行度，你没法自己指定。Spark SQL自己会默认根据hive表对应的hdfs文件的block，自动设置Spark SQL查询所在的那个stage的并行度。你自己通过spark.default.parallelism参数指定的并行度，只会在没有Spark SQL的stage中生效。 比如你第一个stage，用了Spark SQL从hive表中查询出了一些数据，然后做了一些transformation操作，接着做了一个shuffle操作（groupByKey）；下一个stage，在shuffle操作之后，做了一些transformation操作。hive表，对应了一个hdfs文件，有20个block；你自己设置了spark.default.parallelism参数为100。 你的第一个stage的并行度，是不受你的控制的，就只有20个task；第二个stage，才会变成你自己设置的那个并行度，100。 问题在哪里？ Spark SQL默认情况下，它的那个并行度，咱们没法设置。可能导致的问题，也许没什么问题，也许很有问题。Spark SQL所在的那个stage中，后面的那些transformation操作，可能会有非常复杂的业务逻辑，甚至说复杂的算法。如果你的Spark SQL默认把task数量设置的很少，20个，然后每个task要处理为数不少的数据量，然后还要执行特别复杂的算法。 这个时候，就会导致第一个stage的速度，特别慢。第二个stage，1000个task，刷刷刷，非常快。 解决上述Spark SQL无法设置并行度和task数量的办法，是什么呢？ repartition算子，你用Spark SQL这一步的并行度和task数量，肯定是没有办法去改变了。但是呢，可以将你用Spark SQL查询出来的RDD，使用repartition算子，去重新进行分区，此时可以分区成多个partition，比如从20个partition，分区成100个。 然后呢，从repartition以后的RDD，再往后，并行度和task数量，就会按照你预期的来了。就可以避免跟Spark SQL绑定在一个stage中的算子，只能使用少量的task去处理大量数据以及复杂的算法逻辑。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十五之算子调优之filter过后使用coalesce减少分区数量","date":"2017-04-21T02:37:55.550Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十五之算子调优之filter过后使用coalesce减少分区数量/","text":"默认情况下，经过了这种filter之后，RDD中的每个partition的数据量，可能都不太一样了。（原本每个partition的数据量可能是差不多的） 问题： 1、每个partition数据量变少了，但是在后面进行处理的时候，还是要跟partition数量一样数量的task，来进行处理；有点浪费task计算资源 2、每个partition的数据量不一样，会导致后面的每个task处理每个partition的时候，每个task要处理的数据量就不同，这个时候很容易发生什么问题？数据倾斜 比如说，第二个partition的数据量才100；但是第三个partition的数据量是900；那么在后面的task处理逻辑一样的情况下，不同的task要处理的数据量可能差别达到了9倍，甚至10倍以上；同样也就导致了速度的差别在9倍，甚至10倍以上。 这样的话呢，就会导致有些task运行的速度很快；有些task运行的速度很慢。这，就是数据倾斜。 针对上述的两个问题，我们希望应该能够怎么样？ 1、针对第一个问题，我们希望可以进行partition的压缩吧，因为数据量变少了，那么partition其实也完全可以对应的变少。比如原来是4个partition，现在完全可以变成2个partition。那么就只要用后面的2个task来处理即可。就不会造成task计算资源的浪费。（不必要，针对只有一点点数据的partition，还去启动一个task来计算） 2、针对第二个问题，其实解决方案跟第一个问题是一样的；也是去压缩partition，尽量让每个partition的数据量差不多。那么这样的话，后面的task分配到的partition的数据量也就差不多。不会造成有的task运行速度特别慢，有的task运行速度特别快。避免了数据倾斜的问题。 有了解决问题的思路之后，接下来，我们该怎么来做呢？实现？ coalesce算子 主要就是用于在filter操作之后，针对每个partition的数据量各不相同的情况，来压缩partition的数量。减少partition的数量，而且让每个partition的数据量都尽量均匀紧凑。 从而便于后面的task进行计算操作，在某种程度上，能够一定程度的提升性能。 1rdd.filter(...).coalesce(100)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十四之算子调优之MapPartitions提升Map操作性能","date":"2017-04-21T02:20:59.371Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十四之算子调优之MapPartitions提升Map操作性能/","text":"spark中，最基本的原则，就是每个task处理一个RDD的partition MapPartitions操作的优点： 如果是普通的map，比如一个partition中有1万条数据；ok，那么你的function要执行和计算1万次,但是，使用MapPartitions操作之后，一个task仅仅会执行一次function，function一次接收所有的partition数据。只要执行一次就可以了，性能比较高。 MapPartitions的缺点：一定是有的。 如果是普通的map操作，一次function的执行就处理一条数据；那么如果内存不够用的情况下，比如处理了1千条数据了，那么这个时候内存不够了，那么就可以将已经处理完的1千条数据从内存里面垃圾回收掉，或者用其他方法，腾出空间来吧。所以说普通的map操作通常不会导致内存的OOM异常。但是MapPartitions操作，对于大量数据来说，比如甚至一个partition，100万数据，一次传入一个function以后，那么可能一下子内存不够，但是又没有办法去腾出内存空间来，可能就OOM，内存溢出。 什么时候比较适合用MapPartitions系列操作，就是说，数据量不是特别大的时候，都可以用这种MapPartitions系列操作，性能还是非常不错的，是有提升的。比如原来是15分钟，（曾经有一次性能调优），12分钟。10分钟-&gt;9分钟。但是也有过出问题的经验，MapPartitions只要一用，直接OOM，内存溢出，崩溃。在项目中，自己先去估算一下RDD的数据量，以及每个partition的量，还有自己分配给每个executor的内存资源。看看一下子内存容纳所有的partition数据，行不行。如果行，可以试一下，能跑通就好。性能肯定是有提升的。但是试了一下以后，发现，不行，OOM了，那就放弃吧。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十三之HashShuffleManager与SortShuffleManager","date":"2017-04-21T01:22:19.216Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十三之HashShuffleManager与SortShuffleManager/","text":"几种spark shuffle manager 12spark.shuffle.manager：hash、sort、tungsten-sort（自己实现内存管理）2.0.x之后就没有了这个配置参数 在spark1.2.x之前shuffle Manager是HashShuffleManager；这种manager，实际上，从spark 1.2.x版本以后，就不再是默认的选择了。 spark 1.2.x版本以后，默认的shuffle manager，是什么呢？SortShuffleManager。 下面是SortShuffleManager的原理图 和shuffle manager相关的另外一个参数1spark.shuffle.sort.bypassMergeThreshold：200 自己可以设定一个阈值，默认是200，当reduce task数量少于等于200；map task创建的输出文件小于等于200的；最后会将所有的输出文件合并为一份文件。 这样做的好处，就是避免了sort排序，节省了性能开销。而且还能将多个reduce task的文件合并成一份文件。节省了reduce task拉取数据的时候的磁盘IO的开销。 在spark 1.5.x以后，对于shuffle manager又出来了一种新的manager，tungsten-sort（钨丝），钨丝sort shuffle manager。官网上说，钨丝sort shuffle manager，效果跟sort shuffle manager是差不多的。 但是，唯一的不同之处在于，钨丝manager，是使用了自己实现的一套内存管理机制，性能上有很大的提升， 而且可以避免shuffle过程中产生的大量的OOM，GC，等等内存相关的异常。 总结: 对于hash、sort、tungsten-sort。如何来选择？ 1、需不需要数据默认就让spark给你进行排序？就好像mapreduce，默认就是有按照key的排序。如果不需要的话，其实还是建议搭建就使用最基本的HashShuffleManager，因为最开始就是考虑的是不排序，换取高性能； 2、什么时候需要用sort shuffle manager？如果你需要你的那些数据按key排序了，那么就选择这种吧，而且要注意，reduce task的数量应该是超过200的，这样sort、merge（多个文件合并成一个）的机制，才能生效把。但是这里要注意，你一定要自己考量一下，有没有必要在shuffle的过程中，就做这个事情，毕竟对性能是有影响的。 3、如果你不需要排序，而且你希望你的每个task输出的文件最终是会合并成一份的，你自己认为可以减少性能开销；可以去调节bypassMergeThreshold这个阈值，比如你的reduce task数量是500，默认阈值是200，所以默认还是会进行sort和直接merge的；可以将阈值调节成550，不会进行sort，按照hash的做法，每个reduce task创建一份输出文件，最后合并成一份文件。（一定要提醒大家，这个参数，其实我们通常不会在生产环境里去使用，也没有经过验证说，这样的方式，到底有多少性能的提升） 4、如果你想选用sort based shuffle manager，而且你们公司的spark版本比较高，是1.5.x版本的，那么可以考虑去尝试使用tungsten-sort shuffle manager。看看性能的提升与稳定性怎么样。 1234567spark.shuffle.manager：hash、sort、tungsten-sortnew SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;hash&quot;)new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;tungsten-sort&quot;)// 默认就是，new SparkConf().set(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)new SparkConf().set(&quot;spark.shuffle.sort.bypassMergeThreshold&quot;, &quot;550&quot;)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十二shuffle调优之map端内存缓冲和reduce端内存占比","date":"2017-04-21T01:21:01.052Z","path":"2017/04/21/bigdata/spark从入门到精通_笔记/spark性能优化之二十二shuffle调优之map端内存缓冲和reduce端内存占比/","text":"下面是shuffle过程中的shuffle write和shuffle read的原理图 原理说完之后，来看一下，默认情况下，不调优，可能会出现什么样的问题？ 默认，map端内存缓冲是每个task，32kb。默认，reduce端聚合内存比例，是0.2，也就是20%。 如果map端的task，处理的数据量比较大，但是呢，你的内存缓冲大小是固定的。可能会出现什么样的情况？ 每个task就处理320kb，32kb，总共会向磁盘溢写320 / 32 = 10次。每个task处理32000kb，32kb，总共会向磁盘溢写32000 / 32 = 1000次。 在map task处理的数据量比较大的情况下，而你的task的内存缓冲默认是比较小的，32kb。可能会造成多次的map端往磁盘文件的spill溢写操作，发生大量的磁盘IO，从而降低性能。 reduce端聚合内存，占比。默认是0.2。如果数据量比较大，reduce task拉取过来的数据很多，那么就会频繁发生reduce端聚合内存不够用，频繁发生spill操作，溢写到磁盘上去。而且最要命的是，磁盘上溢写的数据量越大，后面在进行聚合操作的时候，很可能会多次读取磁盘中的数据，进行聚合。 默认不调优，在数据量比较大的情况下，可能频繁地发生reduce端的磁盘文件的读写。 这两个点之所以放在一起讲，是因为他们俩是有关联的。数据量变大，map端肯定会出点问题；reduce端肯定也会出点问题；出的问题是一样的，都是磁盘IO频繁，变多，影响性能。 调优： 调节map task内存缓冲：spark.shuffle.file.buffer，默认32k（spark 1.3.x不是这个参数，后面还有一个后缀，kb；spark 1.5.x以后，变了，就是现在这个参数）调节reduce端聚合内存占比：spark.shuffle.memoryFraction，0.2 在实际生产环境中，我们在什么时候来调节两个参数？ 看Spark UI，如果你的公司是决定采用standalone模式，那么很简单，你的spark跑起来，会显示一个Spark UI的地址，4040的端口，进去看，依次点击进去，可以看到，你的每个stage的详情，有哪些executor，有哪些task，每个task的shuffle write和shuffle read的量，shuffle的磁盘和内存，读写的数据量；如果是用的yarn模式来提交，课程最前面，从yarn的界面进去，点击对应的application，进入Spark UI，查看详情。 如果发现shuffle 磁盘的write和read，很大。这个时候，就意味着最好调节一些shuffle的参数。进行调优。首先当然是考虑开启map端输出文件合并机制。 调节上面说的那两个参数。调节的时候的原则。spark.shuffle.file.buffer，每次扩大一倍，然后看看效果，64，128；spark.shuffle.memoryFraction，每次提高0.1，看看效果。 不能调节的太大，太大了以后过犹不及，因为内存资源是有限的，你这里调节的太大了，其他环节的内存使用就会有问题了。 调节了以后，效果？map task内存缓冲变大了，减少spill到磁盘文件的次数；reduce端聚合内存变大了，减少spill到磁盘的次数，而且减少了后面聚合读取磁盘文件的数量。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十一shuffle调优之合并map端输出文件","date":"2017-04-20T13:53:17.279Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化之二十一shuffle调优原理/","text":"shuffle的原理什么样的情况下，会发生shuffle？ 在spark中，主要是以下几个算子：groupByKey、reduceByKey、countByKey、join，等等。 什么是shuffle？ groupByKey，要把分布在集群各个节点上的数据中的同一个key，对应的values，都给集中到一块儿，集中到集群中同一个节点上，更严密一点说，就是集中到一个节点的一个executor的一个task中。 然后呢，集中一个key对应的values之后，才能交给我们来进行处理，","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化之二十JVM调优中的调节executor堆外内存与连接等待时长","date":"2017-04-20T12:39:31.344Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化之二十JVM调优中的调节executor堆外内存与连接等待时长/","text":"executor堆外内存executor堆外内存报错的场景有时候,如果你的spark作业处理的数据量特别大,几亿数据量,然后spark作业一运行,时不时的报错123shuffle file cannot find executor,task lost out of memory(内存溢出) 可能是说executor的堆外内存不太够用,导致executor在运行的过程中,可能会内存溢出,然后导致后续的stage的task的在运行的时候,可能要从executor中拉取shuffle map output文件,但是executor可能已经挂掉了,关联的BlockManager也没有了,所以可能回报123shuffle output file not findresubmitting taskexecutor lost 此时spark作业彻底崩溃 上述情况下,就可以考虑调节一下executor的堆外内存,也许就可以避免报错,此外有时,堆外内存调节的比较大的时候,对于性能来说,也会带来一定的提升 executor堆外内存报错的原理 executor堆外内存配置1--conf spark.yarn.executor.memoryOverhead=2048 spark-submit脚本里面，去用–conf的方式，去添加配置；一定要注意！！！切记，不是在你的spark作业代码中，用new SparkConf().set()这种方式去设置，不要这样去设置，是没有用的！一定要在spark-submit脚本中去设置。 spark.yarn.executor.memoryOverhead（看名字，顾名思义，针对的是基于yarn的提交模式） 默认情况下，这个堆外内存上限大概是300多M；后来我们通常项目中，真正处理大数据的时候，这里都会出现问题，导致spark作业反复崩溃，无法运行；此时就会去调节这个参数，到至少1G（1024M），甚至说2G、4G 通常这个参数调节上去以后，就会避免掉某些JVM OOM的异常问题，同时呢，会让整体spark作业的性能，得到较大的提升。 连接等待时长 总结为什么在这里讲这两个参数呢？ 因为比较实用，在真正处理大数据（不是几千万数据量、几百万数据量），几亿，几十亿，几百亿的时候。很容易碰到executor堆外内存，以及gc引起的连接超时的问题。file not found，executor lost，task lost。 调节上面两个参数，还是很有帮助的。 这是实际生产中提交的spark-submit1234567891011121314/usr/local/spark/bin/spark-submit \\--class com.ibeifeng.sparkstudy.WordCount \\--num-executors 80 \\--driver-memory 6g \\--executor-memory 6g \\--executor-cores 3 \\--master yarn-cluster \\--queue root.default \\--conf spark.yarn.executor.memoryOverhead=2048 \\--conf spark.core.connection.ack.wait.timeout=300 \\/usr/local/spark/spark.jar \\$&#123;1&#125;//$&#123;1&#125;筛选条件","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十九之JVM调优概述以及降低cache操作的内存占比","date":"2017-04-20T09:13:13.838Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十九之JVM调优概述以及降低cache操作的内存占比/","text":"几种调优的点1、常规性能调优：分配资源,并行度,序列化,localwait(数据本地化),广播变量等 2、JVM调优（Java虚拟机）：JVM相关的参数，通常情况下，如果你的硬件配置、基础的JVM的配置，都ok的话，JVM通常不会造成太严重的性能问题；反而更多的是，在troubleshooting中，JVM占了很重要的地位；JVM造成线上的spark作业的运行报错，甚至失败（比如OOM）。 3、shuffle调优（相当重要）：spark在执行groupByKey、reduceByKey等操作时的，shuffle环节的调优。这个很重要。shuffle调优，其实对spark作业的性能的影响，是相当之高！！！经验：在spark作业的运行过程中，只要一牵扯到有shuffle的操作，基本上shuffle操作的性能消耗，要占到整个spark作业的50%~90%。10%用来运行map等操作，90%耗费在两个shuffle操作。groupByKey、countByKey。 4、spark操作调优（spark算子调优，比较重要）：groupByKey，countByKey或aggregateByKey来重构实现。有些算子的性能，是比其他一些算子的性能要高的。foreachPartition替代foreach。如果一旦遇到合适的情况，效果还是不错的。 1、分配资源、并行度、RDD重构与缓存2、shuffle调优3、spark算子调优4、JVM调优、广播大变量。。。 JVM调优原理 每一次放对象的时候，都是放入eden区域，和其中一个survivor区域；另外一个survivor区域是空闲的。 当eden区域和一个survivor区域放满了以后（spark运行过程中，产生的对象实在太多了），就会触发minor gc，小型垃圾回收。把不再使用的对象，从内存中清空，给后面新创建的对象腾出来点儿地方。 清理掉了不再使用的对象之后，那么也会将存活下来的对象（还要继续使用的），放入之前空闲的那一个survivor区域中。这里可能会出现一个问题。默认eden、survior1和survivor2的内存占比是8:1:1。问题是，如果存活下来的对象是1.5，一个survivor区域放不下。此时就可能通过JVM的担保机制（不同JVM版本可能对应的行为），将多余的对象，直接放入老年代了。 如果你的JVM内存不够大的话，可能导致频繁的年轻代内存满溢，频繁的进行minor gc。频繁的minor gc会导致短时间内，有些存活的对象，多次垃圾回收都没有回收掉。会导致这种短声明周期（其实不一定是要长期使用的）对象，年龄过大，垃圾回收次数太多还没有回收到，跑到老年代。 老年代中，可能会因为内存不足，囤积一大堆，短生命周期的，本来应该在年轻代中的，可能马上就要被回收掉的对象。此时，可能导致老年代频繁满溢。频繁进行full gc（全局/全面垃圾回收）。full gc就会去回收老年代中的对象。full gc由于这个算法的设计，是针对的是，老年代中的对象数量很少，满溢进行full gc的频率应该很少，因此采取了不太复杂，但是耗费性能和时间的垃圾回收算法。full gc很慢。 full gc / minor gc，无论是快，还是慢，都会导致jvm的工作线程停止工作，stop the world。简而言之，就是说，gc的时候，spark停止工作了。等着垃圾回收结束。 内存不充足的时候，问题：1、频繁minor gc，也会导致频繁spark停止工作2、老年代囤积大量活跃对象（短生命周期的对象），导致频繁full gc，full gc时间很长，短则数十秒，长则数分钟，甚至数小时。可能导致spark长时间停止工作。3、严重影响咱们的spark的性能和运行的速度。 降低cache操作的内存占比JVM调优的第一个点：降低cache操作的内存占比 spark中，堆内存又被划分成了两块儿，一块儿是专门用来给RDD的cache、persist操作进行RDD数据缓存用的；另外一块儿，就是我们刚才所说的，用来给spark算子函数的运行使用的，存放函数中自己创建的对象。 默认情况下，给RDD cache操作的内存占比，是0.6，60%的内存都给了cache操作了。但是问题是，如果某些情况下，cache不是那么的紧张，问题在于task算子函数中创建的对象过多，然后内存又不太大，导致了频繁的minor gc，甚至频繁full gc，导致spark频繁的停止工作。性能影响会很大。 针对上述这种情况，大家可以在之前我们讲过的那个spark ui。yarn去运行的话，那么就通过yarn的界面，去查看你的spark作业的运行统计，很简单，大家一层一层点击进去就好。可以看到每个stage的运行情况，包括每个task的运行时间、gc时间等等。如果发现gc太频繁，时间太长。此时就可以适当调节这个比例。 降低cache操作的内存占比，大不了用persist操作，选择将一部分缓存的RDD数据写入磁盘，或者序列化方式，配合Kryo序列化类，减少RDD缓存的内存占用；降低cache操作内存占比；对应的，算子函数的内存占比就提升了。这个时候，可能，就可以减少minor gc的频率，同时减少full gc的频率。对性能的提升是有一定的帮助的。 一句话，让task执行算子函数时，有更多的内存可以使用。 spark.storage.memoryFraction，0.6 -&gt; 0.5 -&gt; 0.4 -&gt; 0.2 一个一个去调节,然后去看日志,确定最终的参数 SparkConf.set(“spark.storage.memoryFraction”, “0.5”)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十八之调节数据本地化等待时长","date":"2017-04-20T08:08:17.098Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十八之调节数据本地化等待时长/","text":"数据本地化的几种情况 Spark在Driver上，对Application的每一个stage的task，进行分配之前，都会计算出每个task要计算的是哪个分片数据，RDD的某个partition；Spark的task分配算法，优先，会希望每个task正好分配到它要计算的数据所在的节点，这样的话，就不用在网络间传输数据； 但是呢，通常来说，有时，事与愿违，可能task没有机会分配到它的数据所在的节点，为什么呢，可能那个节点的计算资源和计算能力都满了；所以呢，这种时候，通常来说，Spark会等待一段时间，默认情况下是3s钟（不是绝对的，还有很多种情况，对不同的本地化级别，都会去等待），到最后，实在是等待不了了，就会选择一个比较差的本地化级别，比如说，将task分配到靠它要计算的数据所在节点，比较近的一个节点，然后进行计算。 但是对于第二种情况，通常来说，肯定是要发生数据传输，task会通过其所在节点的BlockManager来获取数据，BlockManager发现自己本地没有数据，会通过一个getRemote()方法，通过TransferService（网络数据传输组件）从数据所在节点的BlockManager中，获取数据，通过网络传输回task所在节点。 对于我们来说，当然不希望是类似于第二种情况的了。最好的，当然是task和数据在一个节点上，直接从本地executor的BlockManager中获取数据，纯内存，或者带一点磁盘IO；如果要通过网络传输数据的话，那么实在是，性能肯定会下降的，大量网络传输，以及磁盘IO，都是性能的杀手。 什么时候要调节这个参数观察日志，spark作业的运行日志，推荐大家在测试的时候，先用client模式，在本地就直接可以看到比较全的日志。日志里面会显示，1starting task。。。，PROCESS LOCAL、NODE LOCAL 观察大部分task的数据本地化级别 如果大多都是PROCESS_LOCAL，那就不用调节了,如果是发现，好多的级别都是NODE_LOCAL、ANY，那么最好就去调节一下数据本地化的等待时长,调节完，应该是要反复调节，每次调节完以后，再来运行，观察日志看看大部分的task的本地化级别有没有提升；看看，整个spark作业的运行时间有没有缩短 你别本末倒置，本地化级别倒是提升了，但是因为大量的等待时长，spark作业的运行时间反而增加了，那就还是不要调节了 调节本地化等待时长123456789spark.locality.wait，默认是3s；6s，10s默认情况下，下面3个的等待时长，都是跟上面那个是一样的，都是3sspark.locality.wait.processspark.locality.wait.nodespark.locality.wait.racknew SparkConf() .set(&quot;spark.locality.wait&quot;, &quot;10&quot;)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十七之使用fastutil优化数据格式","date":"2017-04-20T07:39:08.597Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十七之使用fastutil优化数据格式/","text":"fastutil介绍fastutil是扩展了Java标准集合框架（Map、List、Set；HashMap、ArrayList、HashSet）的类库，提供了特殊类型的map、set、list和queue； fastutil能够提供更小的内存占用，更快的存取速度；我们使用fastutil提供的集合类，来替代自己平时使用的JDK的原生的Map、List、Set，好处在于，fastutil集合类，可以减小内存的占用，并且在进行集合的遍历、根据索引（或者key）获取元素的值和设置元素的值的时候，提供更快的存取速度； fastutil也提供了64位的array、set和list，以及高性能快速的，以及实用的IO类，来处理二进制和文本类型的文件； fastutil最新版本要求Java 7以及以上版本； fastutil的每一种集合类型，都实现了对应的Java中的标准接口（比如fastutil的map，实现了Java的Map接口），因此可以直接放入已有系统的任何代码中。fastutil还提供了一些JDK标准类库中没有的额外功能（比如双向迭代器）。 fastutil除了对象和原始类型为元素的集合，fastutil也提供引用类型的支持，但是对引用类型是使用等于号（=）进行比较的，而不是equals()方法。 fastutil尽量提供了在任何场景下都是速度最快的集合类库。 Spark中应用fastutil的场景1、如果算子函数使用了外部变量第一，你可以使用Broadcast广播变量优化；第二，可以使用Kryo序列化类库，提升序列化性能和效率；第三，如果外部变量是某种比较大的集合，那么可以考虑使用fastutil改写外部变量，首先从源头上就减少内存的占用，通过广播变量进一步减少内存占用，再通过Kryo序列化类库进一步减少内存占用。 2、在你的算子函数里，也就是task要执行的计算逻辑里面，如果有逻辑中，出现，要创建比较大的Map、List等集合，可能会占用较大的内存空间，而且可能涉及到消耗性能的遍历、存取等集合操作；那么此时，可以考虑将这些集合类型使用fastutil类库重写，使用了fastutil集合类以后，就可以在一定程度上，减少task创建出来的集合类型的内存占用。避免executor内存频繁占满，频繁唤起GC，导致性能下降。 关于fastutil调优的说明fastutil其实没有你想象中的那么强大，也不会跟官网上说的效果那么一鸣惊人。广播变量、Kryo序列化类库、fastutil，都是之前所说的，对于性能来说，类似于一种调味品，烤鸡，本来就很好吃了，然后加了一点特质的孜然麻辣粉调料，就更加好吃了一点。分配资源、并行度、RDD架构与持久化，这三个就是烤鸡；broadcast、kryo、fastutil，类似于调料。 比如说，你的spark作业，经过之前一些调优以后，大概30分钟运行完，现在加上broadcast、kryo、fastutil，也许就是优化到29分钟运行完、或者更好一点，也许就是28分钟、25分钟。 shuffle调优，15分钟；groupByKey用reduceByKey改写，执行本地聚合，也许10分钟；跟公司申请更多的资源，比如资源更大的YARN队列，1分钟。 fastutil的使用第一步：在pom.xml中引用fastutil的包12345&lt;dependency&gt; &lt;groupId&gt;fastutil&lt;/groupId&gt; &lt;artifactId&gt;fastutil&lt;/artifactId&gt; &lt;version&gt;5.0.9&lt;/version&gt;&lt;/dependency&gt; 速度比较慢，可能是从国外的网去拉取jar包，可能要等待5分钟，甚至几十分钟，不等 List =&gt; IntList 基本都是类似于IntList的格式，前缀就是集合的元素类型；特殊的就是Map，Int2IntMap，代表了key-value映射的元素类型。除此之外，刚才也看到了，还支持object、reference。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十六之使用Kryo序列化","date":"2017-04-20T07:06:00.038Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十六之使用Kryo序列化/","text":"Kyro的优点默认情况下，Spark内部是使用Java的序列化机制，ObjectOutputStream / ObjectInputStream，对象输入输出流机制，来进行序列化 这种默认序列化机制的好处在于，处理起来比较方便；也不需要我们手动去做什么事情，只是，你在算子里面使用的变量，必须是实现Serializable接口的，可序列化即可。 但是缺点在于，默认的序列化机制的效率不高，序列化的速度比较慢；序列化以后的数据，占用的内存空间相对还是比较大。 可以手动进行序列化格式的优化 Spark支持使用Kryo序列化机制。Kryo序列化机制，比默认的Java序列化机制，速度要快，序列化后的数据要更小，大概是Java序列化机制的1/10。 所以Kryo序列化优化以后，可以让网络传输的数据变少；在集群中耗费的内存资源大大减少。 使用Kyro的几个地方Kryo序列化机制，一旦启用以后，会生效的几个地方： 1、算子函数中使用到的外部变量2、持久化RDD时进行序列化，StorageLevel.MEMORY_ONLY_SER3、shuffle 1、算子函数中使用到的外部变量，使用Kryo以后：优化网络传输的性能，可以优化集群中内存的占用和消耗2、持久化RDD，优化内存的占用和消耗；持久化RDD占用的内存越少，task执行的时候，创建的对象，就不至于频繁的占满内存，频繁发生GC。3、shuffle：可以优化网络传输的性能 Kryo的使用首先第一步，在SparkConf中设置一个属性，spark.serializer，org.apache.spark.serializer.KryoSerializer类； 1SparkConf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) Kryo之所以没有被作为默认的序列化类库的原因，就要出现了：主要是因为Kryo要求，如果要达到它的最佳性能的话，那么就一定要注册你自定义的类（比如，你的算子函数中使用到了外部自定义类型的对象变量，这时，就要求必须注册你的类，否则Kryo达不到最佳性能）。 第二步，注册你使用到的，需要通过Kryo序列化的，一些自定义类，SparkConf.registerKryoClasses() 项目中的使用：1234#下面是java方法.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;).registerKryoClasses(new Class[]&#123;CategorySortKey.class&#125;)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十五之广播大变量","date":"2017-04-20T04:01:11.342Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十五之广播大变量/","text":"大变量产生的影响这种默认的，task执行的算子中，使用了外部的变量，每个task都会获取一份变量的副本，有什么缺点呢？在什么情况下，会出现性能上的恶劣的影响呢？ map，本身是不小，存放数据的一个单位是Entry，还有可能会用链表的格式的来存放Entry链条。所以map是比较消耗内存的数据格式。 比如，map是1M。总共，你前面调优都调的特好，资源给的到位，配合着资源，并行度调节的绝对到位，1000个task。大量task的确都在并行运行。 1.这些task里面都用到了占用1M内存的map，那么首先，map会拷贝1000份副本，通过网络传输到各个task中去，给task使用。总计有1G的数据，会通过网络传输。网络传输的开销，不容乐观啊！！！网络传输，也许就会消耗掉你的spark作业运行的总时间的一小部分。 2.map副本，传输到了各个task上之后，是要占用内存的。1个map的确不大，1M；1000个map分布在你的集群中，一下子就耗费掉1G的内存。对性能会有什么影响呢？ 3.不必要的内存的消耗和占用，就导致了，你在进行RDD持久化到内存，也许就没法完全在内存中放下；就只能写入磁盘，最后导致后续的操作在磁盘IO上消耗性能； 4.你的task在创建对象的时候，也许会发现堆内存放不下所有对象，也许就会导致频繁的垃圾回收器的回收，GC。GC的时候，一定是会导致工作线程停止，也就是导致Spark暂停工作那么一点时间。频繁GC的话，对Spark作业的运行的速度会有相当可观的影响。 广播变量的使用如果说，task使用大变量（1m~100m），明知道会导致性能出现恶劣的影响。那么我们怎么来解决呢？ 广播，Broadcast，将大变量广播出去。而不是直接使用。 广播变量，初始的时候，就在Drvier上有一份副本。 task在运行的时候，想要使用广播变量中的数据，此时首先会在自己本地的Executor对应的BlockManager中，尝试获取变量副本；如果本地没有，那么就从Driver远程拉取变量副本，并保存在本地的BlockManager中；此后这个executor上的task，都会直接使用本地的BlockManager中的副本。 executor的BlockManager除了从driver上拉取，也可能从其他节点的BlockManager上拉取变量副本，距离越近越好。 举例来说，（虽然是举例，但是基本都是用我们实际在企业中用的生产环境中的配置和经验来说明的）。50个executor，1000个task。一个map，10M。 默认情况下，1000个task，1000份副本。10G的数据，网络传输，在集群中，耗费10G的内存资源。 如果使用了广播变量。50个execurtor，50个副本。500M的数据，网络传输，而且不一定都是从Driver传输到每个节点，还可能是就近从最近的节点的executor的bockmanager上拉取变量副本，网络传输速度大大增加；500M的内存消耗。 10000M，500M，20倍。20倍~以上的网络传输性能消耗的降低；20倍的内存消耗的减少。 对性能的提升和影响，还是很客观的。 虽然说，不一定会对性能产生决定性的作用。比如运行30分钟的spark作业，可能做了广播变量以后，速度快了2分钟，或者5分钟。但是一点一滴的调优，积少成多。最后还是会有效果的。 没有经过任何调优手段的spark作业，16个小时；三板斧下来，就可以到5个小时；然后非常重要的一个调优，影响特别大，shuffle调优，2~3个小时；应用了10个以上的性能调优的技术点，JVM+广播，30分钟。16小时~30分钟。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十四之重构RDD以及RDD持久化","date":"2017-04-20T03:05:51.420Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十四之重构RDD以及RDD持久化/","text":"RDD需要重构的场景 第一，RDD架构重构与优化 尽量去复用RDD，差不多的RDD，可以抽取称为一个共同的RDD，供后面的RDD计算时，反复使用。 第二，对于要多次计算和使用的公共RDD，一定要进行持久化。 持久化，也就是说，将RDD的数据缓存到内存中/磁盘中，（BlockManager），以后无论对这个RDD做多少次计算，那么都是直接取这个RDD的持久化的数据，比如从内存中或者磁盘中，直接提取一份数据。 第三，持久化，是可以进行序列化的 如果正常将数据持久化在内存中，那么可能会导致内存的占用过大，这样的话，也许，会导致OOM内存溢出。 当纯内存无法支撑公共RDD数据完全存放的时候，就优先考虑，使用序列化的方式在纯内存中存储。将RDD的每个partition的数据，序列化成一个大的字节数组，就一个对象；序列化后，大大减少内存的空间占用。 序列化的方式，唯一的缺点就是，在获取数据的时候，需要反序列化。 如果序列化纯内存方式，还是导致OOM，内存溢出；就只能考虑磁盘的方式，内存+磁盘的普通方式（无序列化）。 内存+磁盘，序列化 第四，为了数据的高可靠性，而且内存充足，可以使用双副本机制，进行持久化 持久化的双副本机制，持久化后的一个副本，因为机器宕机了，副本丢了，就还是得重新计算一次；持久化的每个数据单元，存储一份副本，放在其他节点上面；从而进行容错；一个副本丢了，不用重新计算，还可以使用另外一份副本。 这种方式，仅仅针对你的内存资源极度充足","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十三之调节并行度","date":"2017-04-20T01:30:56.513Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十三之调节并行度/","text":"spark并行度指的是什么? 并行度:指的是在spark作业中,各个stage中的task数量,也就代表了spark作业在各个阶段(stage)的并行度,如果并行度过低,会怎样呢?假设,现在已经在spark-submit脚本里面,给我们的spark作业分配了足够多的资源,比如50个executor,每个executor有10G内存,每个executor有3个cpu core,基本已经达到了集群或者yarn队列的资源上限 task如果没有设置或者设置的很少,比如设置了100个task,那么资源中的50个executor,每个executor有3个cpu core,也就是说,你的Application任何一个stage运行的时候,都有总数是150个cpu core,可以并行运行,但是你现在,只有100个task,平均分配一下,每个executor分配到2个task,那么同时在运行的task只有100个,即每个executor只会并行运行2个task,每个executor剩下的一个cpu core就浪费掉了 你的资源虽然分配足够了,但是问题是,并行度没有与资源相匹配,导致你分配下去的资源都浪费掉了 打个比喻:如果我们将一件任务分配成100个小任务,然后分配给150个人去做,而每个人能够做1个任务,那么就会有50个人的人力浪费,不如我们将一件任务分成150个小任务或者跟多,这样就可以让更多的人参与进来,进而提高效率 所以合理的并行度的设置,应该是要设置的足够大,大到可以完全合理的利用你的集群资源,比如上面的例子,总共集群有150个cpu core，可以并行运行150个task。那么就应该将你的Application的并行度，至少设置成150，才能完全有效的利用你的集群资源，让150个task，并行执行；而且task增加到150个以后，即可以同时并行运行，还可以让每个task要处理的数据量变少；比如总共150G的数据要处理，如果是100个task，每个task计算1.5G的数据；现在增加到150个task，可以并行运行，而且每个task主要处理1G的数据就可以 很简单的道理，只要合理设置并行度，就可以完全充分利用你的集群计算资源，并且减少每个task要处理的数据量，最终，就是提升你的整个Spark作业的性能和运行速度 1、task数量，至少设置成与Spark application的总cpu core数量相同（最理想情况，比如总共150个cpu core，分配了150个task，一起运行，差不多同一时间运行完毕） 2、官方是推荐，task数量，设置成spark application总cpu core数量的2~3倍，比如150个cpu core，基本要设置task数量为300~500； 实际情况，与理想情况不同的，有些task会运行的快一点，比如50s就完了，有些task，可能会慢一点，要1分半才运行完，所以如果你的task数量，刚好设置的跟cpu core数量相同，可能还是会导致资源的浪费，因为，比如150个task，10个先运行完了，剩余140个还在运行，但是这个时候，有10个cpu core就空闲出来了，就导致了浪费。那如果task数量设置成cpu core总数的2~3倍，那么一个task运行完了以后，另一个task马上可以补上来，就尽量让cpu core不要空闲，同时也是尽量提升spark作业运行的效率和速度，提升性能 3、如何设置一个Spark Application的并行度？123spark.default.parallelism SparkConf conf = new SparkConf() .set(&quot;spark.default.parallelism&quot;, &quot;500&quot;)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十二之分配资源","date":"2017-04-20T00:50:33.488Z","path":"2017/04/20/bigdata/spark从入门到精通_笔记/spark性能优化十二之分配资源/","text":"1.分配哪些资源? executor,cpu per executor, memory per executor, drivere memory 2.在哪里分配这些资源?在实际的生产环境中,提交的spark作业,用的是spark-submit shell脚本,我们在里面调整对应的参数配置 1234567891011/usr/local/spark/bin/spark-submit \\--class cn.spark.test.WordCount \\#配置executor的数量--num-executor 3 \\#配置driver的内存(影响不大)--driver-memory 100m \\ #配置每个executor的内存大小--executor-memory 100m \\#配置每个executor的cpu core数量--executor-cores 3 \\/usr/local/SparkTest-xx.dependencies.jar \\ 3.调节到多大,算是最大呢?第一种,spark Standalone模式下,公司在集群上,搭建了一套spark集群,你心里应该清楚每台机器还能给你使用的大概还有多少内存和cpu core,那么设置的时候,就根据这个实际情况,去调节每个spark作业的资源分配,比如说你的每台机器能够给你使用4G内存,2个cpu core,供20台机器,那么平均每个executor就是4G内存,2个cpu core(根据每个机器上跑一个executor) 第二种,Yarn集群,yarn里面有一个资源队列的概念,此时就应该去查看你的spark作业要提交到的资源队列,大概有多少资源?比如此时的资源队列中有500G内存,100个cpu core,如果你想要跑50个executor,那么每个executor的资源是10G内存,2cpu core 调整参数的一个原则:你能使用的资源有多大,就尽量去调节到最大的大小(其中executor的数量,几十个到上百个不等,然后将每个executor的内存调节到最大,cpu core调节到最大) 4.为什么调节了资源以后,性能可以提升?","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之Structured Streaming","date":"2017-04-19T09:27:23.494Z","path":"2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Structured Streaming/","text":"Structured Streaming介绍 流式计算的现状大多数的流式计算引擎（比如storm、spark streaming等）都仅仅关注流数据的计算方面：比如使用一个map函数对一个流中每条数据都进行转换，或者是用reduce函数对一批数据进行聚合。但是，实际上在大部分的流式计算应用中，远远不只是需要一个流式计算引擎那么简单。相反的，流式计算仅仅在流式应用中占据一个部分而已。因此现在出现了一个新的名词，叫做持续计算/应用，continuous application。比如以下一些持续应用的例子： 1、更新需要以服务形式实时提供出去的数据：例如，我们可能需要更新一份数据，然后其他用户会通过web应用来实时查询这些数据。这种情况下，一个技术难题就是实时计算应用如何与实时数据服务进行交互，比如说，当实时计算应用在更新数据的时候，如果用户通过实时数据服务来进行查询，此时该如何处理？因此为了处理这种场景下的技术难题，就必须以一个完整的持续计算应用的方式来构建整个系统，而不是站在实时计算的角度，仅仅考虑实时更新数据。2、实时ETL（Extract、Transform和Load）：实时计算领域一个常见的应用就是，将一个存储系统中的数据转换后迁移至另外一个存储系统。例如说，将JSON格式的日志数据迁移到Hive表中。这种场景下的技术难题就在于，如何与两边的存储系统进行交互，从而保证数据不会丢失，同时也不会发生重复。这种协调逻辑是非常复杂的。3、为一个已经存在的批量计算作业开发一个对应的实时计算作业：这个场景的技术难题在于，大多数的流式计算引擎都无法保证说，它们计算出的结果是与离线计算结果相匹配的。例如说，有些企业会通过实时计算应用来构建实时更新的dashboard，然后通过批量计算应用来构建每天的数据报表，此时很多用户就会发现并且抱怨，离线报表与实时dashboard的指标是不一致的。4、在线机器学习：这类持续计算应用，通常都包含了大型的静态数据集以及批处理作业，还有实时数据流以及实时预测服务等各个组件。 以上这些例子就表明了在一个大型的流式计算应用中，流式计算本身其实只是占据了一个部分而已，其他部分还包括了数据服务、存储以及批处理作业。但是目前的现状是，几乎所有的流式计算引擎都仅仅是关注自己的那一小部分而已，仅仅是做流式计算处理。这就使得开发人员需要去处理复杂的流式计算应用与外部存储系统之间的交互，比如说管理事务，同时保证他们的流式计算结果与离线批处理计算结果保持一致。这就是目前流式计算领域急需要解决的难题与现状。 持续计算应用可以定义为，对数据进行实时处理的整套应用系统。spark社区希望能够让开发人员仅仅使用一套api，就可以完整持续计算应用中各个部分涉及的任务和操作，而这各个部分的任务和操作目前都是通过分离的单个系统来完成的，比如说实时数据查询服务，以及与批处理作业的交互等。举例来说，未来对于解决这些问题的一些设想如下： 1、更新那些需要被实时提供服务的数据：开发人员可以开发一个spark应用，来同时完成更新实时数据，以及提供实时数据查询服务，可能是通过jdbc相关接口来实现。也可以通过内置的api来实现事务性的、批量的数据更新，对一些诸如mysql、redis等存储系统。2、实时ETL：开发人员仅仅需要如同批处理作业一样，开发一样的数据转换操作，然后spark就可以自动完成针对存储系统的操作，并且保证数据的一次且仅一次的强一致性语义。3、为一个批处理作业开发一个实时版本：spark可以保证实时处理作业与批处理作业的结果一定是一致的。4、在线机器学习：机器学习的api将会同时支持实时训练、定期批量训练、以及实时预测服务。 Structured StreamingSpark 2.0中，引入的structured streaming，就是为了实现上述所说的continuous application，也就是持续计算的。首先，structured streaming是一种比spark更高阶的api，主要是基于spark的批处理中的高阶api，比如dataset/dataframe。此外，structured streaming也提供很多其他流式计算应用所无法提供的功能： 1、保证与批处理作业的强一致性：开发人员可以通过dataset/dataframe api以开发批处理作业的方式来开发流式处理作业，进而structured streaming可以以增量的方式来运行这些计算操作。在任何时刻，流式处理作业的计算结果，都与处理同一份batch数据的批处理作业的计算结果，是完全一致的。而大多数的流式计算引擎，比如storm、kafka stream、flink等，是无法提供这种保证的。2、与存储系统进行事务性的整合：structured streaming在设计时就考虑到了，要能够基于存储系统保证数据被处理一次且仅一次，同时能够以事务的方式来操作存储系统，这样的话，对外提供服务的实时数据才能在任何时刻都保持一致性。目前spark 2.0版本的structured streaming，仅仅支持hdfs这一种外部存储，在未来的版本中，会加入更多的外部存储的支持。事务性的更新是流式计算开发人员的一大痛点，其他的流式计算引擎都需要我们手动来实现，而structured streaming希望在内核中自动来实现。3、与spark的其他部分进行无缝整合：structured steaming在未来将支持基于spark sql和jdbc来对streaming state进行实时查询，同时提供与mllib进行整合。spark 2.0仅仅开始做这些整合的工作，在未来的版本中会逐渐完善这些整合。 除了这些独一无二的特性以外，structured streaming还会提供其他feature来简化流式应用的开发，例如对event time的支持，从而可以自动处理延迟到达的数据，以及对滑动窗口和会话的更多的支持。目前structured streaming还停留在beta阶段，因此官方声明，仅供用户学习、实验和测试。 Structured Streaming的未来spark官方对structured streaming未来的计划是非常有野心的：希望spark的所有组件（core、sql、dataset、mllib等）都能够通过structured steaming，以增量的方式来运行，进而支持更丰富的实时计算操作。structured streaming会设计为让其计算结果与批处理计算结果是强一致的。大数据用户的一个非常大的痛点，就是需要一个完全统一的编程接口。例如说，之前用户进行大数据开发时，需要整合使用多种计算引擎，比如mapreduce来进行etl，hive来执行sql查询，giraph来进行图计算，storm来进行实时计算，等等。而spark则可以完全统一这些操作。此外，structured streaming也希望能够完全涵盖一个持续计算应用中的方方面面。 Structured Streaming与其他流式计算应用的对比 wordcount入门案例structured streaming是一种可伸缩的、容错的、基于Spark SQL引擎的流式计算引擎。你可以使用，与针对静态数据的批处理计算操作一样的方式来编写流式计算操作。随着数据不断地到达，Spark SQL引擎会以一种增量的方式来执行这些操作，并且持续更新结算结果。可以使用java、scala等编程语言，以及dataset/dataframe api来编写计算操作，执行数据流的聚合、基于event的滑动窗口、流式数据与离线数据的join等操作。所有这些操作都与Spark SQL使用一套引擎来执行。此外，structured streaming会通过checkpoint和预写日志等机制来实现一次且仅一次的语义。简单来说，对于开发人员来说，根本不用去考虑是流式计算，还是批处理，只要使用同样的方式来编写计算操作即可，structured streaming在底层会自动去实现快速、可伸缩、容错、一次且仅一次语义。 123456789101112131415161718192021222324252627282930import org.apache.spark.sql.SparkSessionobject StructuredNetworkWordCount &#123; def main(args: Array[String]) &#123; val spark = SparkSession .builder() .appName(&quot;StructuredNetworkWordCount&quot;) .getOrCreate() import spark.implicits._ val lines = spark.readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, &quot;9999&quot;) .load() val words = lines.as[String].flatMap(_.split(&quot; &quot;)) val wordCounts = words.groupBy(&quot;value&quot;).count() val query = wordCounts.writeStream .outputMode(&quot;complete&quot;) .format(&quot;console&quot;) .start() query.awaitTermination() &#125; &#125; 测试123yum install -y ncnc -lk 9999 编程模型编程模型structured streaming的核心理念，就是将数据流抽象成一张表，而源源不断过来的数据是持续地添加到这个表中的。这就产生了一种全新的流式计算模型，与离线计算模型是很类似的。你可以使用与在一个静态表中执行离线查询相同的方式来编写流式查询。spark会采用一种增量执行的方式来对表中源源不断的数据进行查询。我们可以将输入数据流想象成是一张input table。数据流中每条新到达的数据，都可以想象成是一条添加到表中的新数据。画图讲解。 针对输入数据执行的查询，会产生一张result table。每个trigger interval，比如说1秒钟，添加到input table中的新数据行，都会被增量地执行我们定义的查询操作，产生的结果会更新到结果表中。当结果表被更新的时候，我们可能会希望将结果表中变化的行写入一个外部存储中。画图讲解。 我们可以定义每次结果表中的数据更新时，以何种方式，将哪些数据写入外部存储。我们有多种模式的output：complete mode，被更新后的整个结果表中的数据，都会被写入外部存储。具体如何写入，是根据不同的外部存储自身来决定的。append mode，只有最近一次trigger之后，新增加到result table中的数据，会被写入外部存储。只有当我们确定，result table中已有的数据是肯定不会被改变时，才应该使用append mode。update mode，只有最近一次trigger之后，result table中被更新的数据，包括增加的和修改的，会被写入外部存储中。spark 2.0中还不支持这种mode。这种mode和complete mode不同，没有改变的数据是不会写入外部存储的。 我们可以以上一讲的wordcount例子作为背景来理解，lines dataframe是一个input table，而wordcounts dataframe就是一个result table。当应用启动后，spark会周期性地check socket输入源中是否有新数据到达。如果有新数据到达，那么spark会将之前的计算结果与新到达的数据整合起来，以增量的方式来运行我们定义的计算操作，进而计算出最新的单词计数结果。 这种模型跟其他很多流式计算引擎都不同。大多数流式计算引擎都需要开发人员自己来维护新数据与历史数据的整合并进行聚合操作。然后我们就需要自己去考虑和实现容错机制、数据一致性的语义等。然而在structured streaming的这种模式下，spark会负责将新到达的数据与历史数据进行整合，并完成正确的计算操作，同时更新result table，不需要我们去考虑这些事情。画图讲解。 event-time和late-data processevent-time指的是嵌入在数据自身内部的一个时间。在很多流式计算应用中，我们可能都需要根据event-time来进行处理。例如，可能我们需要获取某个设备每分钟产生的事件的数量，那么我们就需要使用事件产生时的时间，而不是spark接受到这条数据的时间。设备产生的每个事件都是input table中的一行数据，而event-time就是这行数据的一个字段。这就可以支持我们进行基于时间窗口的聚合操作（例如每分钟的事件数量），只要针对input table中的event-time字段进行分组和聚合即可。每个时间窗口就是一个分组，而每一行都可以落入不同行的分组内。因此，类似这样的基于时间窗口的分组聚合操作，既可以被定义在一份静态数据上，也可以被定义在一个实时数据流上。 此外，这种模型也天然支持延迟到达的数据，late-data。spark会负责更新result table，因此它有决定的控制权来针对延迟到达的数据进行聚合结果的重新计算。虽然目前在spark 2.0中还没有实现这个feature，但是未来会基于event-time watermark（水印）来实现这个late-data processing的feature。 容错语义structured streaming的核心设计理念和目标之一，就是支持一次且仅一次的语义。为了实现这个目标，structured streaming设计将source、sink和execution engine来追踪计算处理的进度，这样就可以在任何一个步骤出现失败时自动重试。每个streaming source都被设计成支持offset，进而可以让spark来追踪读取的位置。spark基于checkpoint和wal来持久化保存每个trigger interval内处理的offset的范围。sink被设计成可以支持在多次计算处理时保持幂等性，就是说，用同样的一批数据，无论多少次去更新sink，都会保持一致和相同的状态。这样的话，综合利用基于offset的source，基于checkpoint和wal的execution engine，以及基于幂等性的sink，可以支持完整的一次且仅一次的语义。 总结:1.需要跟历史数据整合起来进行聚合的操作,启动完成2.基于event-time自定进行延迟到达数据的聚合结果的纠正计算3.自动完成一次且一次的语义 创建流式的dataset和dataframe流式dataframe可以通过DataStreamReader接口来创建，DataStreamReader对象是通过SparkSession的readStream()方法返回的。与创建静态dataframe的read()方法类似，我们可以指定数据源的一些配置信息，比如data format、schema、option等。spark 2.0中初步提供了一些内置的source支持。 file source：以数据流的方式读取一个目录中的文件。支持text、csv、json、parquet等文件类型。文件必须是被移动到目录中的，比如用mv命令。 socket source：从socket连接中读取文本内容。driver是负责监听请求的server socket。socket source只能被用来进行测试。 12345678910111213141516val socketDF = spark .readStream .format(&quot;socket&quot;) .option(&quot;host&quot;, &quot;localhost&quot;) .option(&quot;port&quot;, 9999) .load()socketDF.isStreaming socketDF.printSchema val userSchema = new StructType().add(&quot;name&quot;, &quot;string&quot;).add(&quot;age&quot;, &quot;integer&quot;)val csvDF = spark .readStream .option(&quot;sep&quot;, &quot;;&quot;) .schema(userSchema) .csv(&quot;/path/to/directory&quot;) 上面的例子都是产生untyped类型的dataframe，这就意味着在编译时是无法检查其schema的，只有在计算被提交并运行时才会进行检查。一些操作，比如map、flatMap等，需要在编译时就知道具体的类型。为了使用一些typed类型的操作，我们可以将dataframe转换为typed类型的dataset，比如df.as[String]。 对流式的dataset和dataframe执行计算操作基础操作：选择、映射、聚合1234567891011121314151617我们可以对流式dataset/dataframe执行所有类型的操作，包括untyped操作，SQL类操作，typed操作。case class DeviceData(device: String, type: String, signal: Double, time: DateTime)val df: DataFrame = ... // streaming DataFrame with IOT device data with schema &#123; device: string, type: string, signal: double, time: string &#125;val ds: Dataset[DeviceData] = df.as[DeviceData] // streaming Dataset with IOT device data// Select the devices which have signal more than 10df.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;) // using untyped APIs ds.filter(_.signal &gt; 10).map(_.device) // using typed APIs// Running count of the number of updates for each device typedf.groupBy(&quot;type&quot;).count() // using untyped API// Running average signal for each device typeImport org.apache.spark.sql.expressions.scalalang.typed._ds.groupByKey(_.type).agg(typed.avg(_.signal)) // using typed API 滑动窗口：基于event-time12345678910import spark.implicits._val words = ... // streaming DataFrame of schema &#123; timestamp: Timestamp, word: String &#125;// Group the data by window and word and compute the count of each groupval windowedCounts = words.groupBy( //使用timestamp作为event-time,每隔5分钟去统计最近10分钟内的滑动窗口 window($&quot;timestamp&quot;, &quot;10 minutes&quot;, &quot;5 minutes&quot;), $&quot;word&quot;).count() 如果有延迟到达的数据,如下有一条延迟10min到达的数据 join操作1234567structured streaming，支持将一个流式dataset与一个静态dataset进行join。val staticDf = spark.read. ...val streamingDf = spark.readStream. ... streamingDf.join(staticDf, “type”) // inner equi-join with a static DFstreamingDf.join(staticDf, “type”, “right_join”) // right outer join with a static DF 不支持的操作123456789101112131415161718192021222324252627//不允许对一个dataframe连续进行聚合操作streaming dataframe的chain aggregation//不允许尽心limit和take操作limit and take//不允许尽心distinctdistinct//sort仅在聚合过后，同时使用complete output mode时可用sortstreaming dataframe和static dataframe的outer join full outer join是不支持的 streaming dataframe在左侧时，left outer join是不支持的 streaming dataframe在右侧时，right outer join是不支持的两个streaming dataframe的join是不支持的//不能对一个dataframe直接进行count,要先进行分组之后在进行countcount() -&gt; groupBy().count()//不能对一个dataframe直接进行foreach,要用df.writeStream.foreach()foreach() -&gt; df.writeStream.foreach()//不能对一个dataframe直接进行show,要用 console output sinkshow() -&gt; console output sink output mode、sink以及foreach sink详解output操作定义好了各种计算操作之后，就需要启动这个应用。此时就需要使用DataStreamWriter，通过spark.writeStream()方法返回。此时需要指定以下一些信息： output sink的一些细节：数据格式、位置等。 output mode：以哪种方式将result table的数据写入sink。 query name：指定查询的标识。 trigger interval：如果不指定，那么默认就会尽可能快速地处理数据，只要之前的数据处理完，就会立即处理下一条数据。如果上一个数据还没处理完，而这一个trigger也错过了，那么会一起放入下一个trigger再处理。 checkpoint地址：对于某些sink，可以做到一次且仅一次的语义，此时需要指定一个目录，进而可以将一些元信息写入其中。一般会是类似hdfs上的容错目录。 ## output mode 目前仅仅支持两种output modeappend mode：仅适用于不包含聚合操作的查询。complete mode：仅适用于包含聚合操作的查询。 output sink目前有一些内置支持的sink file sink：在spark 2.0中，仅仅支持parquet文件，以及append模式 foreach sink console sink：仅供调试 memory sink：仅供调试 12345678910111213141516171819202122232425262728val noAggDF = deviceDataDf.select(&quot;device&quot;).where(&quot;signal &gt; 10&quot;) noAggDF .writeStream .format(&quot;console&quot;) .start()noAggDF .writeStream .parquet(&quot;path/to/destination/directory&quot;) .start() val aggDF = df.groupBy(“device”).count()aggDF .writeStream .outputMode(&quot;complete&quot;) .format(&quot;console&quot;) .start()aggDF .writeStream .queryName(&quot;aggregates&quot;) // this query name will be the table name .outputMode(&quot;complete&quot;) .format(&quot;memory&quot;) .start()spark.sql(&quot;select * from aggregates&quot;).show() // interactively query in-memory table foreach sink详解12345678910111213使用foreach sink时，我们需要自定义ForeachWriter，并且自定义处理每条数据的业务逻辑。每次trigger发生后，根据output mode需要写入sink的数据，就会传递给ForeachWriter来进行处理。使用如下方式来定义ForeachWriter：datasetOfString.write.foreach(new ForeachWriter[String] &#123; def open(partitionId: Long, version: Long): Boolean = &#123; // open connection &#125; def process(record: String) = &#123; // write string to connection &#125; def close(errorOrNull: Throwable): Unit = &#123; // close the connection &#125;&#125;) 需要有如下一些注意点： ForeachWriter必须支持序列化，因为该对象会被序列化后发送到executor上去执行。 open、process和close这三个方法都会给executor调用。 ForeachWriter所有的初始化方法，必须创建数据库连接，开启一个事务，打开一个IO流等等，都必须在open方法中完成。必须注意，如果在ForeachWriter的构造函数中进行初始化，那么这些操作都是在driver上发生的。 open中有两个参数，version和partition，可以唯一标识一批需要处理的数据。每次发生一次trigger，version就会自增长一次。partition是要处理的结果数据的分区号。因为output操作是分布式执行的，会分布在多个executor上并行执行。 open可以使用version和partition来决定，是否要处理这一批数据。此时可以选择返回true或false。如果返回false，那么process不会被调用。举个例子来说，有些partition的数据可能已经被持久化了，而另外一些partiton的处理操作由于失败被重试，此时之前已经被持久化的数据可以不再次进行持久化，避免重复计算。 close方法中，需要处理一些异常，以及一些资源的释放。 管理streaming query123456789101112131415val query = df.writeStream.format(&quot;console&quot;).start() // get the query objectquery.id // get the unique identifier of the running queryquery.name // get the name of the auto-generated or user-specified namequery.explain() // print detailed explanations of the queryquery.stop() // stop the query query.awaitTermination() // block until query is terminated, with stop() or with errorquery.exception() // the exception if the query has been terminated with errorquery.sourceStatus() // progress information about data has been read from the input sourcesquery.sinkStatus() // progress information about data written to the output sinkval spark: SparkSession = ...spark.streams.active // get the list of currently active streaming queriesspark.streams.get(id) // get a query object by its unique idspark.streams.awaitAnyTermination() // block until any one of them terminates 基于checkpoint的容错机制如果实时计算作业遇到了某个错误挂掉了，那么我们可以配置容错机制让它自动重启，同时继续之前的进度运行下去。这是通过checkpoint和wal机制完成的。可以给query配置一个checkpoint location，接着query会将所有的元信息（比如每个trigger消费的offset范围、至今为止的聚合结果数据），写入checkpoint目录。 123456aggDF .writeStream .outputMode(&quot;complete&quot;) .option(“checkpointLocation”, “path/to/HDFS/dir”) .format(&quot;memory&quot;) .start()","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之Dataset","date":"2017-04-19T02:23:11.944Z","path":"2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Dataset/","text":"DataFrame的基本操作实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#employee.json&#123;&quot;name&quot;: &quot;Leo&quot;, &quot;age&quot;: 25, &quot;depId&quot;: 1, &quot;gender&quot;: &quot;male&quot;, &quot;salary&quot;: 20000&#125;&#123;&quot;name&quot;: &quot;Marry&quot;, &quot;age&quot;: 30, &quot;depId&quot;: 2, &quot;gender&quot;: &quot;female&quot;, &quot;salary&quot;: 25000&#125;#department.json&#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;Technical Department&quot;&#125;&#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;Financial Department&quot;&#125;&#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;HR Department&quot;&#125;/*需求:1.只统计年龄在20岁以上的员工2.根据部门和员工性别进行分组,统计出每个部门分性别的平均薪资和年龄*/ // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ //导入spark sql的functions import org.apache.spark.sql.functions._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) //进行计算操作 employee //首先对employee进行过滤,只统计20岁以上的员工 .filter($&quot;age&quot; &gt; 20) //需要跟department数据进行join,注意:untyped join,两个表的字段的连接条件,需要使用三个等号 .join(department, $&quot;depId&quot; === $&quot;id&quot;) //根据部门名称和员工性别进行分组 .groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;)) //执行聚合函数 .agg(avg(employee(&quot;salary&quot;)), avg(employee(&quot;age&quot;))) //最后将结构显示出来 .show/*+--------------------+------+-----------+--------+| name|gender|avg(salary)|avg(age)|+--------------------+------+-----------+--------+| HR Department|female| 21000.0| 21.0||Technical Department| male| 17500.0| 30.0||Financial Department|female| 26500.0| 30.0|| HR Department| male| 18000.0| 42.0|+--------------------+------+-----------+--------+ *//*总结:1.DataFrame==dataset[Row]2.DataFrame的类型是Row,所以untyped类型,弱类型3.dataset的类型通常是我们自定义的case class ,所以是type类型,强类型4.dataset开发,与rdd开发有很多的共同点 dataset API也分成transformation和action,transformation是lazy */ dataset的action操作(collect,count,foreach,reduce)1234567891011121314151617181920212223242526272829303132333435// 构造SparkSession,基于builderval spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate()//导入spark的隐式转换import spark.implicits._//首先将两份数据文件加载进行,形成两个DataFrameval employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;)//collect:将存储在集群上的分布式数据集(比如dataset)中的所有数据获取到driver端employee.collect.foreach(print)//[25,1,male,Leo,20000][30,2,female,Marry,25000][35,1,male,Jack,15000]//count:对dataset中的记录进行统计个数println(employee.count)//first:获取数据集中的第一条数据println(employee.first)//foreach:遍历数据集中的每一条数据,对数据进行操作,这个跟collect不同,//collect是将数据获取到driver端进行操作,而foreach是将计算操作推到集群上去分布式的执行//foreach(println(_))这种操作,最终的结果是打印在集群中的各个节点上的employee.foreach(println(_))//reduce:对数据集中的所有的数据进行规约的操作employee.map(employee=&gt;1).reduce(_ + _)//take:从数据集中获取指定条数employee.take(3).foreach(println(_)) 基础操作(持久化,临时视图,执行计划,ds/df互转换,写数据)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103// 构造SparkSession,基于builderval spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate()//导入spark的隐式转换import spark.implicits._//首先将两份数据文件加载进行,形成两个DataFrameval employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;)//如果要对一个dataset重复计算两次的话,那么建议先对这个dataset进行持久化在进行操作,避免重复计算employee.cacheprintln(employee.count)println(employee.count)/*17/04/19 15:25:43 INFO DAGScheduler: Job 1 finished: count at TopNBasic.scala:35, took 0.733971 s17/04/19 15:25:44 INFO CodeGenerator: Code generated in 18.156697 ms7// ....17/04/19 15:25:44 INFO DAGScheduler: ResultStage 4 (count at TopNBasic.scala:36) finished in 0.015 s17/04/19 15:25:44 INFO DAGScheduler: Job 2 finished: count at TopNBasic.scala:36, took 0.118242 s7 *///创建临时视图,主要为了可以直接对数据执行sql语句employee.createTempView(&quot;employee&quot;)spark.sql(&quot;select * from employee where age&gt;30&quot;).show/*+---+-----+------+----+------+|age|depId|gender|name|salary|+---+-----+------+----+------+| 35| 1| male|Jack| 15000|| 42| 3| male| Tom| 18000|+---+-----+------+----+------+ *///获取spark sql的执行计划spark.sql(&quot;select * from employee where age&gt;30&quot;).explain/*== Physical Plan ==*Filter (isnotnull(age#0L) &amp;&amp; (age#0L &gt; 30))+- InMemoryTableScan [age#0L, depId#1L, gender#2, name#3, salary#4L], [isnotnull(age#0L), (age#0L &gt; 30)] +- InMemoryRelation [age#0L, depId#1L, gender#2, name#3, salary#4L], true, 10000, StorageLevel(disk, memory, deserialized, 1 replicas) +- *FileScan json [age#0L,depId#1L,gender#2,name#3,salary#4L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/C:/Users/Administrator/Desktop/employee.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;age:bigint,depId:bigint,gender:string,name:string,salary:bigint&gt;//我们知道带*号的都是自动化生成的(whole-stage-code-generation) *///查看schemeemployee.printSchema/*root |-- age: long (nullable = true) |-- depId: long (nullable = true) |-- gender: string (nullable = true) |-- name: string (nullable = true) |-- salary: long (nullable = true) *///写数据val employeeWithAgeDF = spark.sql(&quot;select * from employee where age&gt;30&quot;)employeeWithAgeDF.write.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employeeWithAge.txt&quot;)/*在C:\\Users\\Administrator\\Desktop\\employeeWithAge.txt目录下._SUCCESS.crc.part-00000-d683f074-b191-49e4-9725-c92002f25c9f.json.crc_SUCCESSpart-00000-d683f074-b191-49e4-9725-c92002f25c9f.json *///DataFrame转换成dataset//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)val employeeDS = employee.as[Employee]employeeDS.show()employeeDS.printSchema()/*+---+-----+------+------+------+|age|depId|gender| name|salary|+---+-----+------+------+------+| 25| 1| male| Leo| 20000|| 30| 2|female| Marry| 25000|| 35| 1| male| Jack| 15000|| 42| 3| male| Tom| 18000|| 21| 3|female|Kattie| 21000|| 30| 2|female| Jen| 28000|| 19| 2|female| Jen| 8000|+---+-----+------+------+------+root |-- age: long (nullable = true) |-- depId: long (nullable = true) |-- gender: string (nullable = true) |-- name: string (nullable = true) |-- salary: long (nullable = true) *///dataset转成DataFrameemployeeDS.toDF type操作(coalesce,reparation)123456789101112131415161718192021222324252627// 构造SparkSession,基于builderval spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate()//导入spark的隐式转换import spark.implicits._//首先将两份数据文件加载进行,形成两个DataFrameval employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;)val employeeDS = employee.as[Employee]println(employeeDS.rdd.partitions.size)//coalesce和reparation操作,都是用来重新定义分区的//区别在于:coalesce只能用于减少分区数据量,而且可以选择不发生shuffle//reparation可以增加分区,也可以减少分区,必须会发生shuffle,相当于进行了一次重分区操作val employeeDSRepartitioned = employeeDS.repartition(7)println(employeeDSRepartitioned.rdd.partitions.size)val employeeDSCoalesced = employeeDS.coalesce(3)println(employeeDSRepartitioned.rdd.partitions.size)employeeDSCoalesced.show() typed操作(distinct和dropDuplicates)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748 // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long) //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val employeeDS = employee.as[Employee] //distinct和dropDuplicates都是用来去重的 //区别在于: // distinct:是根据每一条数据进行完整内容的对比和去重 // dropDuplicates可以根据指定的字段进行去重 employeeDS.distinct.show employeeDS.dropDuplicates(Seq(&quot;name&quot;)).show/*+---+-----+------+------+------+|age|depId|gender| name|salary|+---+-----+------+------+------+| 30| 2|female| Marry| 25000|| 21| 3|female|Kattie| 21000|| 42| 3| male| Tom| 18000|| 35| 1| male| Jack| 15000|| 30| 2|female| Jen| 28000|| 19| 2|female| Jen| 8000|| 25| 1| male| Leo| 20000|+---+-----+------+------+------++---+-----+------+------+------+|age|depId|gender| name|salary|+---+-----+------+------+------+| 35| 1| male| Jack| 15000|| 42| 3| male| Tom| 18000|| 30| 2|female| Jen| 28000|| 30| 2|female| Marry| 25000|| 21| 3|female|Kattie| 21000|| 25| 1| male| Leo| 20000|+---+-----+------+------+------+*/ typed操作(except,filter,interset)123456789101112131415161718192021222324252627 // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._//case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long) //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val employee2 = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee2.json&quot;) val employeeDS = employee.as[Employee] val employeeDS2 = employee2.as[Employee] //except:获取当前dataset中有,但是另外一个dataset中没有的元素 //filter:根据我们自己的逻辑,如果返回true,那么就保留该元素,否则就过滤掉 //intersect:求两个数据集的交集 employeeDS.except(employeeDS2).show employeeDS.filter(employee =&gt; employee.age &gt; 20) employeeDS.intersect(employeeDS2) typed(map,flatMap,mapPartitions)12345678910111213141516171819202122232425262728293031323334353637383940414243case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)case class Department(id:Long, name:String) // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) val employeeDS = employee.as[Employee] val departmentDS = department.as[Department] //map:将数据集中的每条数据都做一个映射,返回一条新数据 //flatMap:数据集中的每条数据都可以返回多条数据,然后进行压平 //mapPartitions:一次性对一个partition中的数据进行处理 employeeDS.map(employee =&gt; (employee.name, employee.age, employee.salary+1000)) departmentDS.flatMap&#123; department=&gt; Seq(Department(department.id+1, department.name+&quot;_1&quot;), Department(department.id+2, department.name+&quot;_2&quot;)) &#125; employeeDS.mapPartitions&#123; employeeIter=&gt; val result = mutable.ArrayBuffer[(String,Long)]() while(employeeIter.hasNext)&#123; var emp = employeeIter.next result += ((emp.name, emp.salary+1000)) &#125; //需要将集合转成迭代器返回 result.toIterator &#125; typed(joinwith,sort)12345678910111213141516171819202122232425case class Employee(name: String, age:Long, depId:Long, gender:String, salary:Long)case class Department(id:Long, name:String) // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) val employeeDS = employee.as[Employee] val departmentDS = department.as[Department] employeeDS.joinWith(departmentDS, $&quot;depId&quot;===$&quot;id&quot;).show employeeDS.sort($&quot;salary&quot;.desc).show typed(randomSplit,sample)123456789101112131415161718192021222324252627randomSplit:将一个数据集随机切分成几份数据集sample:按照指定的比例随机抽取出来一些数据 // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) val employeeDS = employee.as[Employee] val departmentDS = department.as[Department] //切成3分,每份的权重为2,10,20 employeeDS.randomSplit(Array(2,10,20)) //随机抽取数据总量的0.3的比率 employeeDS.sample(false, 0.3).show untyped操作(select,where,groupBy,agg,col,join)1234567891011121314151617181920212223242526272829303132untyped操作:实际上就涵盖了普通sql语法的全部了 // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ import org.apache.spark.sql.functions._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) employee .where(&quot;age &gt; 20&quot;) .join(department, $&quot;depId&quot; === $&quot;id&quot;) .groupBy(department(&quot;name&quot;), employee(&quot;gender&quot;)) //要使用avg函数,需要导入org.apache.spark.sql.functions._ .agg(avg(employee(&quot;salary&quot;))) .show employee .select($&quot;name&quot;, $&quot;depId&quot;, $&quot;salary&quot;) .where(&quot;age&gt;30&quot;) .show 聚合函数(avg,sum,max,min,count,countDistinct)1234567891011121314151617181920212223242526272829303132 // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ import org.apache.spark.sql.functions._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) employee .join(department, $&quot;depId&quot;===$&quot;id&quot;) .groupBy(department(&quot;name&quot;)) //要使用avg函数,需要导入org.apache.spark.sql.functions._ .agg(avg(employee(&quot;salary&quot;)), sum(employee(&quot;salary&quot;)), max(employee(&quot;salary&quot;)), min(employee(&quot;salary&quot;)), count(employee(&quot;name&quot;)), countDistinct(employee(&quot;name&quot;))) .show/*+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+| name| avg(salary)|sum(salary)|max(salary)|min(salary)|count(name)|count(DISTINCT name)|+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+|Technical Department| 17500.0| 35000| 20000| 15000| 2| 2|| HR Department| 19500.0| 39000| 21000| 18000| 2| 2||Financial Department|20333.333333333332| 61000| 28000| 8000| 3| 2|+--------------------+------------------+-----------+-----------+-----------+-----------+--------------------+ */ 聚合函数(collect_list, collect_set)1234567891011121314151617181920212223242526272829303132333435363738394041collect_list:就是将一个分组内,指定字段的值都收集到一起变成一个数组,不去重collect_set:同上,唯一的区别是,会去重常用于行转列例如:depId=1,employe=leodepId=1,employe=jackdepId=1,employe=[leo,jack] // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ import org.apache.spark.sql.functions._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) employee .groupBy(employee(&quot;depId&quot;)) .agg(collect_list(employee(&quot;name&quot;)), collect_set(employee(&quot;name&quot;))) .show /* +-----+------------------+-----------------+ |depId|collect_list(name)|collect_set(name)| +-----+------------------+-----------------+ | 1| [Leo, Jack]| [Jack, Leo]| | 3| [Tom, Kattie]| [Tom, Kattie]| | 2| [Marry, Jen, Jen]| [Marry, Jen]| +-----+------------------+-----------------+ */ 其他常用的函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/*日期函数:current_date, current_timestamp数学函数:round随机函数:rand字符串函数:concat, concat_ws自定义函数udf和自定义聚合函数udaf需要的时候去查看官网的APIhttp://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$*/ // 构造SparkSession,基于builder val spark = SparkSession .builder() .appName(&quot;DepartmentAvgSalaryAndAgeStat&quot;) .master(&quot;local&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate() //导入spark的隐式转换 import spark.implicits._ import org.apache.spark.sql.functions._ //首先将两份数据文件加载进行,形成两个DataFrame val employee = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\employee.json&quot;) val department = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\department.json&quot;) employee .select(employee(&quot;name&quot;), current_date, current_timestamp, rand, round(employee(&quot;salary&quot;),2), concat(employee(&quot;gender&quot;), employee(&quot;age&quot;)), concat_ws(&quot;|&quot;,employee(&quot;gender&quot;), employee(&quot;age&quot;)) ).show/*+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+| name|current_date()| current_timestamp()|rand(9062034925792708500)|round(salary, 2)|concat(gender, age)|concat_ws(|, gender, age)|+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+| Leo| 2017-04-19|2017-04-19 17:25:...| 0.8536958083571142| 20000| male25| male|25|| Marry| 2017-04-19|2017-04-19 17:25:...| 0.10866516208665833| 25000| female30| female|30|| Jack| 2017-04-19|2017-04-19 17:25:...| 0.6128816303412895| 15000| male35| male|35|| Tom| 2017-04-19|2017-04-19 17:25:...| 0.9614274109004534| 18000| male42| male|42||Kattie| 2017-04-19|2017-04-19 17:25:...| 0.10936129046706444| 21000| female21| female|21|| Jen| 2017-04-19|2017-04-19 17:25:...| 0.25947595067767937| 28000| female30| female|30|| Jen| 2017-04-19|2017-04-19 17:25:...| 0.12866036956519833| 8000| female19| female|19|+------+--------------+--------------------+-------------------------+----------------+-------------------+-------------------------+","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之SparkSession,DataFrame,Dataset","date":"2017-04-19T00:41:15.576Z","path":"2017/04/19/bigdata/spark从入门到精通_笔记/Spark2.0新特性之SparkSession,DataFrame,Dataset/","text":"Spark SQL介绍Spark SQL是Spark的一个模块，主要用于处理结构化的数据。与基础的Spark RDD API不同的是，Spark SQL的接口会向提供更多的信息，包括数据结构以及要执行的计算操作等。在Spark SQL内部，会使用这些信息执行一些额外的优化。使用Spark SQL有两种方式，包括SQL语句以及Dataset API。但是在计算的时候，无论你是用哪种接口去进行计算，它们使用的底层执行引擎是完全一模一样的。这种底层执行机制的统一，就意味着我们可以在不同的方式之间任意来回切换，只要我们可以灵活地运用不同的方式来最自然地表达我们要执行的计算操作就可以了。 Spark SQL的一个主要的功能就是执行SQL查询语句。Spark 2.0开始，最大的一个改变，就是支持了SQL 2003标准语法，还有就是支持子查询。Spark SQL也可以用来从Hive中查询数据。当我们使用某种编程语言(scala,java)开发的Spark作业来执行SQL时，返回的结果是Dataframe/Dataset类型的。当然，我们也可以通过Spark SQL的shell命令行工具，或者是JDBC/ODBC接口来访问。 Dataframe/Dataset介绍Dataset是一个分布式的数据集。Dataset是Spark 1.6开始新引入的一个接口，它结合了RDD API的很多优点（包括强类型，支持lambda表达式等），以及Spark SQL的优点（优化后的执行引擎）。Dataset可以通过JVM对象来构造，然后通过transformation类算子（map，flatMap，filter等）来进行操作。Scala和Java的API中支持Dataset，但是Python不支持Dataset API。不过因为Python语言本身的天然动态特性，Dataset API的不少feature本身就已经具备了（比如可以通过row.columnName来直接获取某一行的某个字段）。R语言的情况跟Python也很类似。 Dataframe就是按列组织的Dataset。在逻辑概念上，可以大概认为Dataframe等同于关系型数据库中的表，或者是Python/R语言中的data frame，但是在底层做了大量的优化。Dataframe可以通过很多方式来构造：比如结构化的数据文件，Hive表，数据库，已有的RDD。Scala，Java，Python，R等语言都支持Dataframe。在Scala API中，Dataframe就是Dataset[Row]的类型别名。在Java中，需要使用Dataset来代表一个Dataframe。 SparkSession从Spark 2.0开始，一个最大的改变就是，Spark SQL的统一入口就是SparkSession，SQLContext和HiveContext未来会被淘汰。可以通过SparkSession.builder()来创建一个SparkSession，如下代码所示。SparkSession内置就支持Hive，包括使用HiveQL语句查询Hive中的数据，使用Hive的UDF函数，以及从Hive表中读取数据等。 1234567891011// 构造SparkSession,基于builderval spark = SparkSession .builder() .appName(&quot;SparkSQLDemo&quot;) .master(&quot;local&quot;) // spark.sql.warehouse.dir必须设置,这是spark sql的元数据仓库目录 .config(&quot;spark.sql.warehouse.dir&quot;,&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\spark-warehouse&quot;) .getOrCreate()// 导入隐式转换import spark.implicits._ Dataframe：untyped操作有了SparkSession之后，就可以通过已有的RDD，Hive表，或者其他数据源来创建Dataframe，比如说通过json文件来创建。Dataframe提供了一种domain-specific language来进行结构化数据的操作，这种操作也被称之为untyped操作，与之相反的是基于强类型的typed操作。12345678910111213141516171819// 读取json文件,构造一个untyped弱类型的DataFrameval df = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\people.json&quot;)/*&#123;&quot;name&quot;:&quot;Michael&quot;&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;age&quot;:30&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;age&quot;:19&#125; */// 打印数据df.show// 打印元数据df.printSchema// select操作,典型的弱类型,untyped操作df.select(&quot;name&quot;).show//scala的表达式的语法,要用$作为前缀df.select($&quot;name&quot;, $&quot;age&quot;+1).show//filter操作+表达式df.filter($&quot;age&quot;&gt;21).show//分组之后求countdf.groupBy(&quot;age&quot;).count.show 运行SQL查询SparkSession的sql()函数允许我们执行SQL语句，得到的结果是一个Dataframe。123456789101112131415//基于DataFrame创建临时视图df.createTempView(&quot;people&quot;)// 使用SparkSession的sql()函数就可以进行sql操作val sqlDF = spark.sql(&quot;select * from people&quot;)sqlDF.show/*+----+-------+| age| name|+----+-------+|null|Michael|| 30| Andy|| 19| Justin|+----+-------+ */ Dataset：typed操作Dataset与RDD比较类似，但是非常重要的一点不同是，RDD的序列化机制是基于Java序列化机制或者是Kryo的，而Dataset的序列化机制基于一种特殊的Encoder，来将对象进行高效序列化，以进行高性能处理或者是通过网络进行传输。Dataset除了Encoder，也同时支持Java序列化机制，但是encoder的特点在于动态的代码生成，同时提供一种特殊的数据格式，来让spark不将对象进行反序列化，即可直接基于二进制数据执行一些常见的操作，比如filter、sort、hash等。1234567891011121314151617181920212223242526272829303132333435363738394041case class Person(name: String, age: Long) //直接基于jvm object来构造dataset val caseClassDS = Seq(Person(&quot;leo&quot;, 25)).toDS() caseClassDS.show /* +----+---+ |name|age| +----+---+ | leo| 25| +----+---+ */ //基于原始数据类型构造dataset val primitiveDS = Seq(1, 2, 3).toDS primitiveDS.map(_ + 1).show /* +-----+ |value| +-----+ | 2| | 3| | 4| +-----+ */ //基于已有的结构化文件,来构造dataset // spark.read.json首先获取到的是一个DataFrame,然后使用as[People]之后,将DataFrame转换成dataset val peopleDS = spark.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\people.json&quot;).as[Person] peopleDS.show /* +----+-------+ | age| name| +----+-------+ |null|Michael| | 30| Andy| | 19| Justin| +----+-------+ */ Hive操作在Spark 2.0中，是支持读写hive中存储的数据的。但是，因为hive有较多的依赖，所以默认情况下，这些依赖没有包含在spark的发布包中。如果hive依赖可以在classpath路径中，那么spark会自动加载这些依赖。这些hive依赖必须在所有的worker node上都放一份，因为worker node上运行的作业都需要使用hive依赖的序列化与反序列化包来访问hive中的数据。 只要将hive-site.xml、hdfs-site.xml和core-site.xml都放入spark/conf目录下即可。 如果要操作Hive，那么构建SparkSession的时候，就必须启用Hive支持，包括连接到hive的元数据库，支持使用hive序列化与反序列化包，以及支持hive udf函数。如果我们没有安装hive，也是可以启用hive支持的。如果我们没有放置hive-site.xml到spark/conf目录下，SparkSession就会自动在当前目录创建元数据库，同时创建一个spark.sql.warehouse.dir参数设置的目录，该参数的值默认是当前目录下的spark-warehouse目录。在spark 2.0中，hive.metastore.warehouse.dir属性已经过时了，现在使用 spark.sql.warehouse.dir属性来指定hive元数据库的位置。 Hive操作123456789101112131415161718192021222324252627282930case class Record(key: Int, value: String)val warehouseLocation = &quot;file:$&#123;system:user.dir&#125;/spark-warehouse&quot;val spark = SparkSession .builder() .appName(&quot;Spark Hive Example&quot;) .config(&quot;spark.sql.warehouse.dir&quot;, warehouseLocation) .enableHiveSupport() .getOrCreate()import spark.implicits._// 创建hive表spark.sql(&quot;create table if not exists src(key INT, value STRING&quot;)// 加载数据spark.sql(&quot;load data local inpath &apos;C:\\\\Users\\\\Administrator\\\\Desktop\\\\kv1.txt&apos; into table src&quot;)spark.sql(&quot;select * from src&quot;).showspark.sql(&quot;select count(*) from src&quot;).showval hiveSqlDF = spark.sql(&quot;select key,value from src where key &lt; 10 order by key&quot;)// 将DataFrame转成datasetval hiveSqlDS = hiveSqlDF.map&#123; case Row(key:Int, value:String) =&gt; s&quot;KEY: $key, VALUE: $value&quot;&#125;hiveSqlDS.showval recordDF = spark.createDataFrame((1 to 100).map(i =&gt; Record(i,s&quot;val_$i&quot;)))recordDF.createOrReplaceTempView(&quot;records&quot;)spark.sql(&quot;select * from src join records on src.key=records.key&quot;).show Hive 1.2.1安装spark 2.0，默认是跟hive 1.2.1进行整合的，所以之前我们安装的是hive 0.13.1是不Ok的，实际跑的时候会出现hive 0.13支持的一些操作，spark 2.0会用自己内置的hive 1.2.1 lib去操作和访问我们的hive 0.13（包括metastore service），出现版本不一致的问题 1、将/usr/local/hive删除2、将apache-hive-1.2.1-bin.tar.gz使用WinSCP上传到spark1的/usr/local目录下。3、解压缩hive安装包：tar -zxvf apache-hive-1.2.1-bin.tar.gz。4、重命名hive目录：mv apache-hive-1.2.1-bin hive5、cp /usr/share/java/mysql-connector-java-5.1.17.jar /usr/local/hive/lib 12345678910111213141516171819202122mv hive-default.xml.template hive-site.xmlvi hive-site.xml&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://spark2upgrade01:3306/hive_metadata?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://spark2upgrade01:9083&lt;/value&gt;&lt;/property&gt; 把hive-site.xml中所有${system:java.io.tmpdir}全部替换为/usr/local/hive/iotmp把hive-site.xml中所有${system:user.name}全部替换为root jline报错解决12rm -rf /usr/local/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jarcp /usr/local/hive/lib/jline-2.12.jar /usr/local/hadoop/share/hadoop/yarn/lib hive-env.sh12345mv hive-env.sh.template hive-env.shvi /usr/local/hive/bin/hive-config.shexport JAVA_HOME=/usr/java/latestexport HIVE_HOME=/usr/local/hiveexport HADOOP_HOME=/usr/local/hadoop 1、将hive-site.xml放置到spark的conf目录下2、启动hive metastore service1hive --service metastore &amp; 1、创建一份文件，students.txt，每行是一个学生的信息2、CREATE TABLE students(name string, age int, score double)3、LOAD DATA LOCAL INPATH ‘/usr/local/test_data/students.txt’ INTO TABLE students4、spark-shell –master spark://spark2upgrade01:7077 –driver-memory 500m –executor-memory 500m5、在spark-shell中，运行针对hive的sql语句spark.sql(“select * from students”).show();spark.sql(“select name from students where score&gt;=90;”).show();spark.sql(“select name from students where age&lt;=15;”).show();6、观察是否有正确的结果7、在spark web ui中检查是否有运行的作业记录","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之与1.x的对比","date":"2017-04-18T07:54:47.353Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之与1.x的对比/","text":"Spark 2.x与1.x对比Spark 1.x：Spark Core（RDD）、Spark SQL（SQL+Dataframe+Dataset）、Spark Streaming、Spark MLlib、Spark Graphx Spark 2.x：Spark Core（RDD）、Spark SQL（ANSI-SQL(标准化sql)+Subquery(子查询)+Dataframe/Dataset）、Spark Streaming、Structured Streaming、Spark MLlib（Dataframe/Dataset）、Spark Graphx、Second Generation Tungsten Engine（Whole-stage code generation+Vectorization） 这里首先给大家理清楚一个前提：Spark 1.x到Spark 2.x，完全是一脉相承的关系，即，Spark 2.x基本上是基于Spark 1.x进行了更多的功能和模块的扩展，以及底层性能的改良。绝对不是说，Spark 2.x彻底淘汰和替代了Spark 1.x中的组件。而且实际上，对于Spark 1.x中90%以上的东西，Spark 2.x几乎都完全保留了支持和延续，并没有做任何改变。这是大家必须要了解的一件事情。 下面我们就对Spark 2.x中的每个组件都进行分析，告诉大家这些组件的基本原理，以及其适用和不适用的场景。避免大家对Spark 1.x和Spark 2.x有错误的认知。 Spark 2.x各组件分析Spark Core（RDD）从Spark诞生之日开始，RDD就是Spark最主要的编程接口，重要程度类似于Hadoop中的MapReduce。RDD，简单来说，就是一个不可变的分布式数据集，被分为多个partition从而在一个集群上分布式地存储。我们可以使用RDD提供的各种transformation和action算子，对RDD执行分布式的计算操作。 可能很多人会问，Spark 2.0开始，包括Structured Streaming、Spark MLlib、Spark SQL底层都开始基于Dataframe/Dataset来作为基础计算引擎，那么Spark Core/RDD是不是就要被淘汰了？ 回答是：错误！ Spark官方社区对于这个问题也是这个态度，Spark Core绝对不会被淘汰掉。因为Spark Core/RDD作为一种low-level的API有它的较为底层的应用场景，虽然后续这种场景会越来越少，Dataframe/Dataset API会逐渐替代原先Spark Core的一些场景，但是不可否认的是，这种场景还是存在的。此外，Dataframe/Dataset实际上底层也是基于Spark Core/RDD构建的。所以说，Spark Core/RDD是Spark生态中，不可替代的基础API和引擎，其他所有的组件几乎都是构建在它之上。未来它不会被淘汰，只是应用场景会减少而已。 Spark 2.x中，在离线批处理计算中，编程API，除了RDD以外，还增强了Dataframe/Dataset API。那么，我们到底什么时候应该使用Spark Core/RDD来进行编程呢？实际上，RDD和Dataset最大的不同在于，RDD是底层的API和内核，Dataset实际上基于底层的引擎构建的high-level的计算引擎。 1、如果我们需要对数据集进行非常底层的掌控和操作，比如说，手动管理RDD的分区，或者根据RDD的运行逻辑来结合各种参数和编程来进行较为底层的调优。因为实际上Dataframe/Dataset底层会基于whole-stage code generation技术自动生成很多代码，那么就意味着，当我们在进行线上报错的troubleshooting以及性能调优时，对Spark的掌控能力就会降低。而使用Spark Core/RDD，因为其运行完全遵循其源码，因此我们完全可以在透彻阅读Spark Core源码的基础之上，对其进行troubleshooting和底层调优。（最重要的一点）2、我们要处理的数据是非结构化的，比如说多媒体数据，或者是普通文本数据。3、我们想要使用过程式编程风格来处理数据，而不想使用domain-specific language的编程风格来处理数据。4、我们不关心数据的schema，即元数据。5、我们不需要Dataframe/Dataset底层基于的第二代tungsten引擎提供的whole-stage code generation等性能优化技术。 Spark SQL（ANSI-SQL+Subquery）Spark 2.x中的Spark SQL，提供了标准化SQL的支持，以及子查询的支持，大幅度提升了Spark在SQL领域的应用场景。而且本身在大数据领域中，SQL就是一个最广泛使用的用户入口，据不完全统计，做大数据的公司里，90%的应用场景都是基于SQL的。最典型的例子就是Hadoop，几乎用Hadoop的公司，90%都是基于Hive进行各种大数据的统计和分析。剩下10%是实时计算、机器学习、图计算。之所以有这种现象，主要就是因为SQL简单、易学、易用、直观。无论是研发人员，还是产品经理，还是运营人员，还是其他的人，都能在几天之内入门和学会SQL的使用，然后就可以基于大数据SQL引擎（比如Hive）基于企业积累的海量数据，根据自己的需求进行各种统计和分析。 此外，据Spark官方社区所说，Spark 2.x一方面对SQL的支持做了大幅度的增强，另一方面，也通过优化了底层的计算引擎（第二代tungsten引擎，whole-stage code generation等），提升了SQL的执行性能以及稳定性。 所以在Spark 2.x中，一方面，开始鼓励大家多使用Spark SQL的SQL支持，采用Spark SQL来编写SQL进行最常见的大数据统计分析。比如可以尝试将Hive中的运行的一些SQL语句慢慢迁移到Spark SQL上来。另外一方面，也提醒大家，一般一个新的大版本，都是不太稳定的，因此Spark SQL虽然在功能、性能和稳定性上做了很多的增强，但是难免还是会有很多的坑。因此建议大家在做Hive/RDBMS（比如Oracle）到Spark SQL的迁移时，要小心谨慎，一点点迁移，同时做好踩坑的准备。 Spark SQL（Dataframe/Dataset）就像RDD一样，Dataframe也代表一个不可变的分布式数据集。与RDD不同的一点是，Dataframe引入了schema的概念，支持以复杂的类型作为元素类型，同时指定schema，比如Row。因此Dataframe更像是传统关系型数据库中的表的概念。为了提升开发人员对大数据的处理能力，Dataframe除了提供schema的引入，还基于Schema提供了很多RDD所不具备的high-level API，以及一些domain-specific language（特定领域编程语言）。但是在Spark 2.0中，Dataframe和Dataset合并了，Dataframe已经不是一个单独的概念了，目前仅仅只是Dataset[Row]的一个类型别名而已，你可以理解为Dataframe就是Dataset。 从Spark 2.0开始，Dataset有两种表现形式：typed API和untyped API。我们可以认为，Dataframe就是Dataset[Row]的别名，Row就是一个untyped类型的对象，因为Row是类似于数据库中的一行，我们只知道里面有哪些列，但是有些列即使不存在，我们也可以这对这些不存在的列进行操作。因此其被定义为untyped，就是弱类型。 而Dataset[T]本身，是一种typed类型的API，其中的Object通常都是我们自己自定义的typed类型的对象，因为对象是我们自己定义的，所以包括字段命名以及字段类型都是强类型的。目前Scala支持Dataset和Dataframe两种类型，Java仅仅支持Dataset类型，Python和R因为不具备compile-time type-safety特性，因此仅仅支持Dataframe。 Dataset API有哪些优点呢？ 1、静态类型以及运行时的类型安全性 SQL语言具有最不严格的限制，而Dataset具有最严格的限制。SQL语言在只有在运行时才能发现一些错误，比如类型错误，但是由于Dataframe/Dataset目前都是要求类型指定的（静态类型），因此在编译时就可以发现类型错误，并提供运行时的类型安全。比如说，如果我们调用了一个不属于Dataframe的API，编译时就会报错。但是如果你使用了一个不存在的列，那么也只能到运行时才能发现了。而最严格的就是Dataset了，因为Dataset是完全基于typed API来设计的，类型都是严格而且强类型的，因此如果你使用了错误的类型，或者对不存在的列进行了操作，都能在编译时就发现。 2、将半结构化的数据转换为typed自定义类型 举例来说，如果我们现在有一份包含了学校中所有学生的信息，是以JSON字符串格式定义的，比如：{“name”: “leo”, “age”, 19, “classNo”: 1}。我们可以自己定义一个类型，比如case class Student(name: String, age: Integer, classNo: Integer)。接着我们就可以加载指定的json文件，并将其转换为typed类型的Dataset[Student]，比如val ds = spark.read.json(“students.json”).as[Student]。 在这里，Spark会执行三个操作：1、Spark首先会读取json文件，并且自动推断其schema，然后根据schema创建一个Dataframe。2、在这里，会创建一个Dataframe=Dataset[Row]，使用Row来存放你的数据，因为此时还不知道具体确切的类型,此时还是弱类型。3、接着将Dataframe转换为Dataset[Student]，因为此时已经知道具体的类型是Student了。 这样，我们就可以将半结构化的数据，转换为自定义的typed结构化强类型数据集。并基于此，得到之前说的编译时和运行时的类型安全保障。 3、API的易用性 Dataframe/Dataset引入了很多的high-level API，并提供了domain-specific language风格的编程接口。这样的话，大部分的计算操作，都可以通过Dataset的high-level API来完成。通过typed类型的Dataset，我们可以轻松地执行agg、select、sum、avg、map、filter、groupBy等操作。使用domain-specific language也能够轻松地实现很多计算操作，比如类似RDD算子风格的map()、filter()等。 4、性能 除了上述的优点，Dataframe/Dataset在性能上也有很大的提升。首先，Dataframe/Dataset是构建在Spark SQL引擎之上的，它会根据你执行的操作，使用Spark SQL引擎的Catalyst来生成优化后的逻辑执行计划和物理执行计划，可以大幅度节省内存或磁盘的空间占用的开销（相对于RDD来说，Dataframe/Dataset的空间开销仅为1/3~1/4），也能提升计算的性能。其次，Spark 2.x还引入第二代Tungsten引擎，底层还会使用whole-stage code generation、vectorization等技术来优化性能。 什么时候应该使用Dataframe/Dataset，而不是RDD呢？ 1、如果需要更加丰富的计算语义，high-level的抽象语义，以及domain-specific API。2、如果计算逻辑需要high-level的expression、filter、map、aggregation、average、sum、SQL、列式存储、lambda表达式等语义，来处理半结构化，或结构化的数据。3、如果需要高度的编译时以及运行时的类型安全保障。4、如果想要通过Spark SQL的Catalyst和Spark 2.x的第二代Tungsten引擎来提升性能。5、如果想要通过统一的API来进行离线、流式、机器学习等计算操作。6、如果是R或Python的用户，那么只能使用Dataframe。 最后，实际上，Spark官方社区对RDD和Dataframe/Dataset的建议时，按照各自的特点，根据的需求场景，来灵活的选择最合适的引擎。甚至说，在一个Spark应用中，也可以将两者结合起来一起使用。 实际上建议优先使用Dateset,如果在Dataset使用的过程中出现了搞不定的错误,那么建议使用RDD Spark Streaming&amp;Structured StreamingSpark Streaming是老牌的Spark流式计算引擎，底层基于RDD计算引擎。除了类似RDD风格的计算API以外，也提供了更多的流式计算语义，比如window、updateStateByKey、transform等。同时对于流式计算中重要的数据一致性、容错性等也有一定的支持。 但是Spark 2.x中也推出了全新的基于Dataframe/Dataset的Structured Streaming流式计算引擎。相较于Spark Streaming来说，其最大的不同之处在于，采用了全新的逻辑模型，提出了real-time incremental table(实时增长表)的概念，更加统一了流式计算和离线计算的概念，减轻了用户开发的负担。同时还提供了（可能在未来提供）高度封装的特性，比如双流的全量join、与离线数据进行join的语义支持、内置的自动化容错机制、内置的自动化的一次且仅一次的强一致性语义、time-based processing、延迟数据达到的自动处理、与第三方外部存储进行整合的sink概念，等等高级特性。大幅度降低了流式计算应用的开发成本。 这里要提的一句是，首先，目前暂时建议使用Spark Streaming，因为Spark Streaming基于RDD，而且经过过个版本的考验，已经趋向于稳定。对于Structured Streaming来说，一定要强调，在Spark 2.0版本刚推出的时候，千万别在生产环境使用，因为目前官方定义为beta版，就是测试版，里面可能有很多的bug和问题，而且上述的各种功能还不完全，很多功能还没有。因此Structured Streaming的设计理念虽然非常好，但是个人建议在后续的版本中再考虑使用。目前可以保持关注和学习，并做一些实验即可。 Spark MLlib&amp;GraphXSpark MLlib未来将主要基于Dataframe/Dataset API来开发。而且还会提供更多的机器学习算法。因此可以主要考虑使用其spark.ml包下的API即可。 Spark GraphX，目前发展较为缓慢，如果有图计算相关的应用，可以考虑使用。 Spark 2.x学习建议纵观之前讲的内容，Spark 2.0本次，其实主要就是提升了底层的性能，搭载了第二代Tungsten引擎；同时大幅度调整和增强了ANSI-SQL支持和Dataframe/Dataset API，并将该API作为Spark未来重点发展的发现；此外，为了提供更好的流式计算解决方案，发布了一个测试版的Structured Streaming模块。 而且之前也讲解了Spark 1.x和Spark 2.x中的每一个模块。大家可以明确看到： 第一，Spark 1.x没有任何一个组件是被淘汰的；第二，Spark这次重点改造的是Tungsten Engine、Dataframe/Dataset以及Structured Streaming，对于之前Spark 1.x课程中讲解的Spark Core、Spark SQL以及Spark Streaming，包括Spark Core的性能调优和源码剖析，集群运维管理，几乎没有做太多的调整；第三，Spark Core、Spark SQL、Spark Streaming、Dataframe/Dataset、Structured Streaming、Spark MLlib和GraphX，每个组件目前都有其特点和用途，任何一个不是积累和过时的技术；第五，Spark 2.0的新东西中，ANSI-SQL和Dataframe/Dataset API是可以重点尝试使用的，但是Structured Streaming还停留在实验阶段，完全不能应用到生产项目中。因此目前流式计算主要还是使用Spark Streaming。个人预计，至少要在2017年春节过后，Structured Streaming才有可能进入稳定状态，可以尝试使用。 首先，对于课程之前讲解的Spark 1.x的所有知识，目前以及之后可预见的时间范围内，都是一直有价值的，都是需要学习的。无论是Spark Core（RDD）编程，作为整个Spark生态的基石（包括Dataframe/Dataset），以及掌握Spark底层的知识；还是Spark SQL的开发，或者是Spark Streaming的开发；还有它们的性能调优、Spark Core源码剖析；以及管理运维，这些知识都没有过时，都是价值的，大家都必须认真、仔细的学习，绝对不能轻浮冒进，直接就简单学学Dataframe/Dataset，Structured Streaming，就以为自己掌握了Spark 2.x了，那是绝对错误的！ 本次课程升级，主要分为三个阶段，第一个阶段就是Spark 2.x的新特性介绍，主要包括了新特性概览、发展方向、核心原理以及与1.x的对比分析、学习建议以及使用建议；第二个阶段就是Dataset的开发详解；第三个阶段就是Structured Streaming开发详解。因此在透彻掌握Spark 1.x的基础之上，再来学习Spark 2.x效果更佳。其中最重要的，是要掌握Spark第二代Tungsten引擎的性能提升原理、Spark ANSI-SQL和子查询的支持、Dataset的开发以及使用、Structured Streaming的开发以及使用。 在透彻学习了Spark 1.x和Spark 2.x的知识体系之后，对于Spark的使用，建议如下 1、建议开始大量尝试使用Spark SQL的标准化SQL支持以及子查询支持的特性，大部分的大数据统计分析应用，采用Spark SQL来实现。2、其次，对于一些无法通过SQL来实现的复杂逻辑，比如一些算法的实施，或者一些跟DB、缓存打交道的大数据计算应用，建议采用Dataframe/Dataset API来实施。3、接着，对于一些深刻理解课程中讲解的Spark Core/RDD，以及内核源码的高阶同学，如果遇到了因为Spark SQL和Dataframe/Dataset导致的线上的莫名其妙的报错，始终无法解决，或者是觉得有些性能，通过第二代Tungsten引擎也无法很好的调优，需要自己手工通过RDD控制底层的分区以及各种参数来进行调优，那么建议使用Spark Core/RDD来重写SQL类应用。4、对于流式计算应用，建议目前还是使用Spark Streaming，因为其稳定；Structured Streaming目前是beta版本，很不稳定，因此目前建议仅仅是学习和实验即可。个人预计和建议，估计至少要到2017年春节后，Structured Streaming才可能具备部署生产环境的能力。5、对于机器学习应用，建议使用spark.ml包下的机器学习API，因为其基于Dataframe/Dataset API实现，性能更好，而且未来是社区重点发展方向","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之whole-stage code generation技术和vectorization技术","date":"2017-04-18T07:33:29.755Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之whole-stage code generation技术和vectorization技术/","text":"Whole-stage code generation之前讲解了手工编写的代码的性能，为什么比Volcano Iterator Model要好。所以如果要对Spark进行性能优化，一个思路就是在运行时动态生成代码，以避免使用Volcano模型，转而使用性能更高的代码方式。要实现上述目的，就引出了Spark第二代Tungsten引擎的新技术，whole-stage code generation。通过该技术，SQL语句编译后的operator-treee中，每个operator执行时就不是自己来执行逻辑了，而是通过whole-stage code generation技术，动态生成代码，生成的代码中会尽量将所有的操作打包到一个函数中，然后再执行动态生成的代码。 就以上一讲的SQL语句来作为示例，Spark会自动生成以下代码。如果只是一个简单的查询，那么Spark会尽可能就生成一个stage，并且将所有操作打包到一起。但是如果是复杂的操作，就可能会生成多个stage。 Spark提供了explain()方法来查看一个SQL的执行计划，而且这里面是可以看到通过whole-stage code generation生成的代码的执行计划的。如果看到一个步骤前面有个*符号，那么就代表这个步骤是通过该技术自动生成的。在这个例子中，Range、Filter和Aggregation都是自动生成的，Exchange不是自动生成的，因为这是一个网络传输数据的过程。 很多用户会疑惑，从Spark 1.1版本开始，就一直听说有code generation类的feature引入，这跟spark 2.0中的这个技术有什么不同呢。实际上在spark 1.x版本中，code generation技术仅仅被使用在了expression evoluation方面（比如a + 1），即表达式求值，还有极其少数几个算子上（比如filter等）。而spark 2.0中的whole-stage code generation技术是应用在整个spark运行流程上的。 Vectorization(向量化)对于很多查询操作，whole-stage code generation技术都可以很好地优化其性能。但是有一些特殊的操作，却无法很好的使用该技术，比如说比较复杂一些操作，如parquet文件扫描、csv文件解析等，或者是跟其他第三方技术进行整合。 如果要在上述场景提升性能，spark引入了另外一种技术，称作“vectorization”，即向量化。向量化的意思就是避免每次仅仅处理一条数据，相反，将多条数据通过面向列的方式来组织成一个一个的batch，然后对一个batch中的数据来迭代处理。每次next()函数调用都返回一个batch的数据，这样可以减少virtual function dispatch的开销。同时通过循环的方式来处理，也可以使用编译器和CPU的loop unrolling等优化特性。 这种向量化的技术，可以使用到之前说的3个点中的2个点。即，减少virtual function dispatch，以及进行loop unrolling优化。但是还是需要通过内存缓冲来读写中间数据的(无法使用cpu register读取数据)。所以，仅仅当实在无法使用whole-stage code generation时，才会使用vectorization技术。有人做了一个parquet文件读取的实验，采用普通方式以及向量化方式，性能也能够达到一个数量级的提升： 上述的whole-stage code generation技术，能否保证将spark 2.x的性能比spark 1.x来说提升10倍以上呢？这是无法完全保证的。虽然说目前的spark架构已经搭载了目前世界上最先进的性能优化技术，但是并不是所有的操作都可以大幅度提升性能的。简单来说，CPU密集型的操作，可以通过这些新技术得到性能的大幅度提升，但是很多IO密集型的操作，比如shuffle过程的读写磁盘，是无法通过该技术提升性能的。在未来，spark会花费更多的精力在优化IO密集型的操作的性能上。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之spark1.x的Volcano Iterator Model(火山迭代器模型)","date":"2017-04-18T07:02:38.709Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之Volcano Iterator Model(火山迭代器模型)/","text":"Volcano Iterator Model深入剖析Spark 2.x的第二代tungsten引擎原理之前，先看一下当前的Spark的工作原理。我们可以通过一个SQL来举例，这个SQL扫描了单个表，然后对属性等于指定值的记录进行汇总计数。SQL语句如下：select count(*) from store_sales where ss_item_sk=1000。 要执行这个查询，Spark 1.x会使用一种最流行、最经典的查询求值策略，该策略主要基于Volcano Iterator Model。在这种模型中，一个查询会包含多个operator，每个operator都会实现一个接口，提供一个next()方法，该方法返回operator tree中的下一个operator。 scan:是去全表扫描filter:是过滤表中的记录Project:是去表的对应的字段Aggregate:是聚合操作,这里对应的是count(*) 举例来说，上面那个查询中的filter operator的代码大致如下所示： 让每一个operator都实现一个iterator接口，可以让查询引擎优雅的组装任意operator在一起。而不需要查询引擎去考虑每个operator具体的一些处理逻辑，比如数据类型等。 Vocano Iterator Model也因此成为了数据库SQL执行引擎领域内内的20年中最流行的一种标准。而且Spark SQL最初的SQL执行引擎也是基于这个思想来实现的。 对于上面的那个查询，如果我们通过java来手工编写一段代码实现那个功能，代码大致如下所示： 上面这段代码是专门为实现这个指定的功能编写的，因此不具备良好的组装性以及扩展性。那么Volcano Iterator Model与这段手写代码的性能对比是怎么样的呢？一边是20年中最流行的一种SQL引擎思想，另一种是一段近乎小白编写的简单代码。有人对这两种方式的性能做了一个实验和对比： 我们可以清晰地看到，手写的代码的性能比Volcano Iterator Model高了一整个数量级，而这其中的原因包含以下几点： 1、避免了virtual function dispatch：在Volcano Iterator Model中，至少需要调用一次next()函数来获取下一个operator。这些函数调用在操作系统层面，会被编译为virtual function dispatch。而手写代码中，没有任何的函数调用逻辑。虽然说，现代的编译器已经对虚函数调用进行了大量的优化，但是该操作还是会执行多个CPU指令，并且执行速度较慢，尤其是当需要成百上千次地执行虚函数调用时。 2、通过CPU Register存取中间数据，而不是内存缓冲：在Volcano Iterator Model中，每次一个operator将数据交给下一个operator，都需要将数据写入内存缓冲中。然而在手写代码中，JVM JIT编译器会将这些数据写入CPU Register。CPU从内存缓冲种读写数据的性能比直接从CPU Register中读写数据，要低了一个数量级。 3、Loop Unrolling和SIMD：现代的编译器和CPU在编译和执行简单的for循环时，性能非常地高。编译器通常可以自动对for循环进行unrolling(展开)，并且还会生成SIMD指令以在每次CPU指令执行时处理多条数据。CPU也包含一些特性，比如pipelining，prefetching，指令reordering，可以让for循环的执行性能更高。然而这些优化特性都无法在复杂的函数调用场景中施展，比如Volcano Iterator Model。 loop unrolling解释(for循环展开)（小白的方式）1234567for(int i = 0; i &lt; 10; i++) &#123; System.out.println(i) &#125;//会将上述的代码直接编译成下面的形式(for循环展开):System.out.println(1)System.out.println(2)System.out.println(3)...... 手写代码的好处就在于，它是专门为实现这个功能而编写的，代码简单，因此可以吸收上述所有优点，包括避免虚函数调用，将中间数据保存在CPU寄存器中，而且还可以被底层硬件进行for循环的自动优化。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之Structured Streaming介绍","date":"2017-04-18T06:49:51.634Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之智能化/","text":"Spark Streaming应该说是将离线计算操作和流式计算操作统一起来的大数据计算框架之一(就是DStream和rdd的很多操作都是类似的)。从Spark 0.7开始引入的Spark Streaming，为开发人员提供了很多有用的特性：一次且仅一次的语义支持、容错性、强一致性保证、高吞吐量。 但是实际上在真正工业界的流式计算项目中，并不仅仅只是需要一个流式计算引擎。这些项目实际上需要深度地使用批处理计算以及流式处理技术，与外部存储系统进行整合，还有应对业务逻辑变更的能力。因此，企业实际上不仅仅只是需要一个流式计算引擎，他们需要的是一个全栈式的技术，让他们能够开发end-to-end的持续计算应用（continuous application）。 Spark 2.0为了解决上述流式计算的痛点和需求，开发了新的模块——Structured Streaming。 Structured Streaming提供了与批处理计算类似的API。要开发一个流式计算应用，开发人员只要使用Dataframe/Dataset API编写与批处理计算一样的代码即可，Structured Streaming会自动将这些类似批处理的计算代码增量式地应用到持续不断进入的新数据上。这样，开发人员就不需要花太多时间考虑状态管理、容错、与离线计算的同步等问题。Structured Streaming可以保证，针对相同的数据，始终与离线计算产出完全一样的计算结果。 Structured Streaming还提供了与存储系统的事务整合。它会进行自动的容错管理以及数据一致性的管理，如果开发人员要写一个应用程序来更新数据库，进而提供一些实时数据服务，与静态数据进行join，或者是在多个存储系统之间移动数据，那么Structured Streaming可以让这些事情更加简单。 Structured Streaming与Spark其余的组件都能够进行完美的整合。比如可以通过Spark SQL对实时数据进行统计分析，与静态数据进行join，还有其他的使用dataframe/dataset的组件，这样就可以让开发人员构建完整的流式计算引用，而不仅仅只是一个流式计算引擎而已。在未来，Spark会将Structured Streaming与Spark MLlib的整合做的更好。 Spark 2.0搭载了一个beta版本的Structured Streaming，目前是作为Dataframe/Dataset的一个小的附加组件。主要是让Spark用户可以先尝试使用一下Structured Streaming，比如做一些实验和测试。Structured Streaming的一些关键特性，比如基于时间的处理，延迟数据的处理，交互式的查询，以及与非流式的数据源和存储进行整合，可能会基于未来的版本来实现。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之高性能","date":"2017-04-18T06:33:59.160Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之高性能/","text":"在一个2015年的spark调查中显示，91%的spark用户是因为spark的高性能才选择使用它的。所以spark的性能优化也就是社区的一个重要的努力方向了。spark 1.x相较于hadoop mapreduce来说，速度已经快了数倍了，但是spark 2.x中，还能不能相较于spark 1.x来说，速度再提升10倍呢？ 带着这个疑问，我们可以重新思考一下spark的物理执行机制。对于一个现代的大数据处理引擎来说，CPU的大部分时间都浪费在了一些无用的工作上，比如说virtual function call，或者从CPU缓冲区中读写数据。现代的编译器为了减少cpu浪费在上述工作的时间，付出了大量的努力。 Spark 2.0的一个重大的特点就是搭载了最新的第二代tungsten引擎。第二代tungsten引擎吸取了现代编译器以及并行数据库的一些重要的思想，并且应用在了spark的运行机制中。其中一个核心的思想，就是在运行时动态地生成代码，在这些自动动态生成的代码中，可以将所有的操作都打包到一个函数中，这样就可以避免多次virtual function call，而且还可以通过cpu register来读写中间数据，而不是通过cpu cache来读写数据。上述技术整体被称作“whole-stage code generation”，中文也可以叫“全流程代码生成”。 之前有人做过测试，用单个cpu core来处理一行数据，对比了spark 1.6和spark 2.0的性能。spark 2.0搭载的是whole-stage code generation技术，spark 1.6搭载的是第一代tungsten引擎的expression code generation技术。测试结果显示，spark 2.0的性能相较于spark 1.6得到了一个数量级的提升。 除了刚才那个简单的测试以外，还有人使用完整的99个SQL基准测试来测试过spark 1.6和spark 2.0的性能。测试结果同样显示，spark 2.0的性能比spark 1.6来说，提升了一个数量级。 spark 2.0中，除了whole-stage code generation技术以外，还使用了其他一些新技术来提升性能。比如说对Spark SQL的catalyst查询优化器做了一些性能优化，来提升对一些常见查询的优化效率，比如null值处理等。再比如说，通过vectarization技术将parquet文件扫描的吞吐量提升了3倍以上。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之标准化的SQL支持以及更合理的API","date":"2017-04-18T06:21:43.713Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之标准化的SQL支持以及更合理的API/","text":"标准化SQL支持以及更合理的APISpark最引以为豪的几个特点就是简单、直观、表达性好。Spark 2.0为了继续加强这几个特点，做了两件事情：1、提供标准化的SQL支持；2、统一了Dataframe和Dataset两套API。 在标准化SQL支持方面，引入了新的ANSI-SQL解析器，提供标准化SQL的解析功能，而且还提供了子查询的支持。Spark现在可以运行完整的99个TPC-DS查询，这就要求Spark包含大多数SQL 2003标准的特性。这么做的好处在于，SQL一直是大数据应用领域的一个最广泛接受的标准，比如说Hadoop，做大数据的企业90%的时间都在用Hive，写SQL做各种大数据的统计和分析。因此Spark SQL提升对SQL的支持，可以大幅度减少用户将应用从其他技术（比如Oracle、Hive等）迁移过来的成本。 统一Dataframe和Dataset API从Spark 2.0开始，Dataframe就只是Dataset[Row]的一个别名，不再是一个单独的类了。无论是typed方法（map、filter、groupByKey等）还是untyped方法（select、groupBy等），都通过Dataset来提供。而且Dataset API将成为Spark的新一代流式计算框架——structured streaming的底层计算引擎。但是由于Python和R这两个语言都不具备compile-time type-safety的特性，所以就没有引入Dataset API，所以这两种语言中的主要编程接口还是Dataframe。 SparkSessionSparkSession是新的Spark上下文以及入口，用于合并SQLContext和HiveContext，并替代它们。因为以前提供了SQLContext和HiveContext两种上下文入口，因此用户有时会有些迷惑，到底该使用哪个接口。现在好了，只需要使用一个统一的SparkSession即可。但是为了向后兼容性，SQLContext和HiveContext还是保留下来了。 新版本Accumulator APISpark 2.0提供了新版本的Accumulator，提供了各种方便的方法，比如说直接通过一个方法的调用，就可以创建各种primitive data type（原始数据类型，int、long、double）的Accumulator。并且在spark web ui上也支持查看spark application的accumulator，性能也得到了提升。老的Accumulator API还保留着，主要是为了向后兼容性。 基于Dataframe/Dataset的Spark MLlibSpark 2.0中，spark.ml包下的机器学习API，主要是基于Dataframe/Dataset来实现的，未来将会成为主要发展的API接口。原先老的基于RDD的spark.mllib包的机器学习API还会保留着，为了向后兼容性，但是未来主要会基于spark.ml包下的接口来进行开发。而且用户使用基于Dataframe/Dataset的新API，还能够对算法模型和pipeline进行持久化保存以及加载。 SparkR中的分布式机器学习算法以及UDF函数Spark 2.0中，为SparkR提供了分布式的机器学习算法，包括经典的Generalized Linear Model，朴素贝叶斯，Survival Regression，K-means等。此外SparkR还支持用户自定义的函数，即UDF。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark2.0新特性之介绍","date":"2017-04-18T05:45:01.892Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark2.0新特性之介绍/","text":"spark core和spark sql的API dataframe与dataset统一，dataframe只是dataset[Row]的类型别名 SparkSession：统一SQLContext和HiveContext，新的上下文入口 为SparkSession开发的一种新的流式调用的configuration api accumulator功能增强：便捷api、web ui支持、性能更高 dataset的增强聚合api spark core和spark sql的sql 支持sql 2003标准 支持ansi-sql和hive ql的sql parser 支持ddl命令 支持子查询：in/not in、exists/not exists Spark Core&amp;Spark SQL性能 通过whole-stage code generation技术将spark sql和dataset的性能提升2~10倍 通过vectorization技术提升parquet文件的扫描吞吐量 提升orc文件的读写性能 提升catalyst查询优化器的性能 通过native实现方式提升窗口函数的性能 对某些数据源进行自动文件合并 Spark MLlib spark mllib未来将主要基于dataset api来实现，基于rdd的api转为维护阶段 基于dataframe的api，支持持久化保存和加载模型和pipeline 基于dataframe的api，支持更多算法，包括二分kmeans、高斯混合、maxabsscaler等 spark R支持mllib算法，包括线性回归、朴素贝叶斯、kmeans、多元回归等 pyspark支持更多mllib算法，包括LDA、高斯混合、泛化线性回顾等 基于dataframe的api，向量和矩阵使用性能更高的序列化机制 Spark Streaming 发布测试版的structured streaming 基于spark sql和catalyst引擎构建 支持使用dataframe风格的api进行流式计算操作 catalyst引擎能够对执行计划进行优化 基于dstream的api支持kafka 0.10版本 依赖管理、打包和操作 不再需要在生产环境部署时打包fat jar，可以使用provided风格 完全移除了对akka的依赖 mesos粗粒度模式下，支持启动多个executor 支持kryo 3.0版本 使用scala 2.11替代了scala 2.10 移除的功能 bagel模块 对hadoop 2.1以及之前版本的支持 闭包序列化配置的支持 HTTPBroadcast支持 基于TTL模式的元数据清理支持 半私有的org.apache.spark.Logging的使用支持 SparkContext.metricsSystem API 与tachyon的面向block的整合支持 spark 1.x中标识为过期的所有api python dataframe中返回rdd的方法 使用很少的streaming数据源支持：twitter、akka、MQTT、ZeroMQ hash-based shuffle manager不支持了 standalone master的历史数据支持功能 dataframe不再是一个类，而是dataset[Row]的类型别名 变化的机制 要求基于scala 2.11版本进行开发，而不是scala 2.10版本 SQL中的浮点类型，使用decimal类型来表示，而不是double类型 kryo版本升级到了3.0 java的flatMap和mapPartitions方法，从iterable类型转变为iterator类型 java的countByKey返回类型，而不是类型 写parquet文件时，summary文件默认不会写了，需要开启参数来启用 spark mllib中，基于dataframe的api完全依赖于自己，不再依赖mllib包 过期的API mesos的细粒度模式不支持了 java 7支持标识为过期，可能2.x未来版本会移除支持 python 2.6的支持不支持了","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之作业资源调度","date":"2017-04-18T00:29:59.402Z","path":"2017/04/18/bigdata/spark从入门到精通_笔记/Spark运维管理之作业资源调度/","text":"静态资源分配原理spark提供了许多功能用来在集群中同时调度多个作业。首先，回想一下，每个spark作业都会运行自己独立的一批executor进程，此时集群管理器会为我们提供同时调度多个作业的功能。第二，在每个spark作业内部，多个job也可以并行执行，比如说spark-shell就是一个spark application，但是随着我们输入scala rdd action类代码，就会触发多个job，多个job是可以并行执行的。为这种情况，spark也提供了不同的调度器来在一个application内部调度多个job。 我们先来看一下多个作业的同时调度 静态资源分配 当一个spark application运行在集群中时，会获取一批独立的executor进程专门为自己服务，比如运行task和存储数据。如果多个用户同时在使用一个集群，并且同时提交多个作业，那么根据cluster manager的不同，有几种不同的方式来管理作业间的资源分配。 最简单的一种方式，是所有cluster manager都提供的，也就是静态资源分配。在这种方式下，每个作业都会被给予一个它能使用的最大资源量的限额，并且可以在运行期间持有这些资源。这是spark standalone集群和YARN集群使用的默认方式。 Standalone集群: 默认情况下，提交到standalone集群上的多个作业，会通过FIFO的方式来运行，每个作业都会尝试获取所有的资源。可以限制每个作业能够使用的cpu core的最大数量（spark.cores.max），或者设置每个作业的默认cpu core使用量（spark.deploy.defaultCores）。最后，除了控制cpu core之外，每个作业的spark.executor.memory也用来控制它的最大内存的使用。 YARN集群管理器: –num-executors属性用来配置作业可以在集群中分配到多少个executor，–executor-memory和–executor-cores可以控制每个executor能够使用的资源。 要注意的是，没有一种cluster manager可以提供多个作业间的内存共享功能。如果你想要通过这种方式来在多个作业间共享数据，我们建议就运行一个spark作业，但是可以接收网络请求，并对相同RDD的进行计算操作。在未来的版本中，内存存储系统，比如Tachyon会提供其他的方式来共享RDD数据。 动态资源分配原理spark 1.2开始，引入了一种根据作业负载动态分配集群资源给你的多个作业的功能。这意味着你的作业在申请到了资源之后，可以在使用完之后将资源还给cluster manager，而且可以在之后有需要的时候再次申请这些资源。这个功能对于多个作业在集群中共享资源是非常有用的。如果部分资源被分配给了一个作业，然后出现了空闲，那么可以还给cluster manager的资源池中，并且被其他作业使用(如果是今天资源分配,那么属于该作业的空闲资源,要等到作业完成之后才会释放,而不会立即释放空闲的资源)。在spark中，动态资源分配在executor粒度上被实现，可以通过spark.dynamicAllocation.enabled来启用。 资源分配策略以一个较高的角度来说，当executor不再被使用的时候，spark就应该释放这些executor，并且在需要的时候再次获取这些executor。因为没有一个绝对的方法去预测一个未来可能会运行一个task的executor应该被移除掉，或者一个新的executor应该别加入，我们需要一系列的探索式算法来决定什么应该移除和申请executor。 申请策略一个启用了动态资源分配的spark作业会在它有pending住的task等待被调度时，申请额外的executor。这个条件必要地暗示了，已经存在的executor是不足以同时运行所有的task的，这些task已经提交了，但是没有完成。 driver会轮询式地申请executor。当在一定时间内（spark.dynamicAllocation.schedulerBacklogTimeout）有pending的task时，就会触发真正的executor申请，然后每隔一定时间后（spark.dynamicAllocation.sustainedSchedulerBacklogTimeout），如果又有pending的task了，则再次触发申请操作。此外，每一轮申请到的executor数量都会比上一轮要增加。举例来说，一个作业需要增加一个executor在第一轮申请时，那么在后续的一轮中会申请2个、4个、8个executor。 每轮增加executor数量的原因主要有两方面。第一，一个作业应该在开始谨慎地申请以防它只需要一点点executor就足够了。第二，作业应该会随着时间的推移逐渐增加它的资源使用量，以防突然大量executor被增加进来。 移除策略移除一个executor的策略比较简单。一个spark作业会在它的executor出现了空闲超过一定时间后（spark.dynamicAllocation.executorIdleTimeout），被移除掉。要注意，在大多数环境下，这个条件都是跟申请条件互斥的，因为如果有task被pending住的话，executor是不该是空闲的。 executor如何优雅地被释放掉在使用动态分配之前，executor无论是发生了故障失败，还是关联的application退出了，都还是存在的。在所有场景中，executor关联的所有状态都不再被需要，并且可以被安全地抛弃。使用动态分配之后，executor移除之后，作业还是存在的。如果作业尝试获取executor写的中间状态数据，就需要去重新计算哪些数据。因此，spark需要一种机制来优雅地卸载executor，在移除它之前要保护它的状态。 解决方案就是使用一个外部的shuffle服务来保存每个executor的中间写状态，这也是spark 1.2引入的特性。这个服务是一个长时间运行的进程，集群的每个节点上都会运行一个,为你的spark作业和executor服务。如果服务被启用了，那么spark executor会在shuffle write和read时，将数据写入该服务，并从该服务获取数据。这意味着所有executor写的shuffle数据都可以在executor声明周期之外继续使用。 除了写shuffle文件，executor也会在内存或磁盘中持久化数据。当一个executor被移除掉时，所有缓存的数据都会消失。目前还没有有效的方案。在未来的版本中，缓存的数据可能会通过堆外存储来进行保存，就像external shuffle service保存shuffle write文件一样。 Standalone模式下使用动态资源分配1234567#启用external shuffle service--conf spark.shuffle.service.enabled=true \\#启用动态资源分配--conf spark.dynamicAllocation.enabled=true \\#指定external shuffle service的端口号--conf spark.shuffle.service.port=7337 \\ 1、启动external shuffle service1./sbin/.start-shuffle-service.sh 2、启动spark-shell，启用动态资源分配 1spark-shell --master spark://192.168.0.103:7077 --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.port=7337 jps可以观察到ExternalShuffleService,并且在spark-shell开始的时候,会启动一个executor进程(CoarseGrainedExecutorBackend) 3、过60s，发现打印日志，说executor被removed，executor进程也没了,jps再次查看 而且通过观察spark-shell的日志可以看到 4、然后动手写一个wordcount程序，最后提交job的时候，会动态申请一个新的executor，出来一个新的executor进程 1234scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 新的executor进程会从external shuffle service进程中读取shuffle write的数据 5、然后整个作业执行完毕，证明external shuffle service+动态资源分配，流程可以走通 6、再等60s，executor又被释放掉 yarn模式下使用动态资源分配先停止之前为standalone集群启动的shuffle service1./sbin/stop-shuffle-service.sh 配置动态资源分配功能使用的所有配置，都是以spark.dynamicAllocation作为前缀的。要启用这个功能，你的作业必须将spark.dynamicAllocation.enabled设置为true。其他相关的配置之后会详细说明。 此外，你的作业必须有一个外部shuffle服务（external shuffle service）。这个服务的目的是去保存executor的shuffle write文件，从而让executor可以被安全地移除。要启用这个服务，可以将spark.shuffle.service.enabled设置为true。在YARN中，这个外部shuffle service是由org.apache.spark.yarn.network.YarnShuffleService实现的，在每个NodeManager中都会运行。要启用这个服务，需要使用以下步骤： 1、首先配置好yarn的shuffle service，然后重启集群1.1.使用预编译好的spark版本。1.2.定位到spark--yarn-shuffle.jar。这个应该在$SPARK_HOME/lib目录下。1.3.将上面的jar加入到所有NodeManager的classpath中(即在每个NodeManager中都拷贝一份)。 1.4.在yarn-site.xml中，将yarn.nodemanager.aux-services设置为spark_shuffle，将yarn.nodemanager.aux-services.spark_shuffle.class设置为org.apache.spark.network.yarn.YarnShuffleService 1.5.重启所有hadoop集群1start-all.sh 查看进程 在浏览器查看确定:HDFS和yarn是启动OK的HDFS:192.168.0.103:50070yarn:192.168.0.103:8088 2、接着呢，启动spark shell，并启用动态资源分配，但是这里跟standalone不一样，上来不会立刻申请executor1234567spark-shell --master spark://192.168.0.103:7077 --conf spark.shuffle.service.enabled=true --conf spark.dynamicAllocation.enabled=true --conf spark.shuffle.service.port=7337scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 3、接着执行wordcount，会尝试动态申请executor，并且申请到后，执行job，在spark web ui上，有两个executor4、过了一会儿，60s过后，executor由于空闲，所以自动被释放掉了，在看spark web ui，没有executor了 多个job资源调度原理在一个spark作业内部，多个并行的job是可以同时运行的。对于job，就是一个spark action操作触发的计算单元。spark的调度器是完全线程安全的，而且支持一个spark application来服务多个网络请求，以及并发执行多个job。 默认情况下，spark的调度会使用FIFO的方式来调度多个job。每个job都会被划分为多个stage，而且第一个job会对所有可用的资源获取优先使用权，并且让它的stage的task去运行，然后第二个job再获取资源的使用权，以此类推。如果队列头部的job不需要使用整个集群资源，之后的job可以立即运行，但是如果队列头部的job使用了集群几乎所有的资源，那么之后的job的运行会被推迟。 从spark 0.8开始，我们是可以在多个job之间配置公平的调度器的。在公平的资源共享策略下，spark会将多个job的task使用一种轮询的方式来分配资源和执行，所以所有的job都有一个基本公平的机会去使用集群的资源。这就意味着，即使运行时间很长的job先提交并在运行了，之后提交的运行时间较短的job，也同样可以立即获取到资源并且运行，而不会等待运行时间很长的job结束之后才能获取到资源。这种模式对于多个并发的job是最好的一种调度方式。 fair Scheduler使用详解默认采用的是FIFO,要启用Fair Scheduler，只要简单地将spark.scheduler.mode属性设置为FAIR即可1234567val conf = new SparkConf().setMaster(...).setAppName(...)conf.set(&quot;spark.scheduler.mode&quot;, &quot;FAIR&quot;)val sc = new SparkContext(conf)或者--conf spark.scheduler.mode=FAIR fair scheduler也支持将job分成多个组并放入多个池中，以及为每个池设置不同的调度优先级。这个feature对于将重要的和不重要的job隔离运行的情况非常有用，可以为重要的job分配一个池，并给予更高的优先级; 为不重要的job分配另一个池，并给予较低的优先级。 默认情况下，新提交的job会进入一个默认池，但是job的池是可以通过spark.scheduler.pool属性来设置的。 如果你的spark application是作为一个服务启动的，SparkContext 7*24小时长时间存在，然后服务每次接收到一个请求，就用一个子线程去服务它在子线程内部，去执行一系列的RDD算子以及代码来触发job的执行在子线程内部，可以调用SparkContext.setLocalProperty(“spark.scheduler.pool”, “pool1”) 在设置这个属性之后，所有在这个线程中提交的job都会进入这个池中。同样也可以通过将该属性设置为null来清空池子。 池的默认行为 默认情况下，每个池子都会对集群资源有相同的优先使用权，但是在每个池内，job会使用FIFO的模式来执行。举例来说，如果要为每个用户创建一个池，这就意味着每个用户都会获得集群的公平使用权，但是每个用户自己的job会按照顺序来执行。 配置池的属性 可以通过配置文件来修改池的属性。每个池都支持以下三个属性: 1、schedulingMode: 可以是FIFO或FAIR，来控制池中的jobs是否要排队，或者是共享池中的资源2、weight: 控制每个池子对集群资源使用的权重。默认情况下，所有池子的权重都是1.如果指定了一个池子的权重为2。举例来说，它就会获取其他池子两倍的资源使用权。设置一个很高的权重值，比如1000，也会很有影响，基本上该池子的task会在其他所有池子的task之前运行。3、minShare: 除了权重之外，每个池子还能被给予一个最小的资源使用量(cpu core)。 池子的配置是通过xml文件来配置的，在spark/conf的fairscheduler.xml中配置我们自己去设置这个文件的路径，conf.set(“spark.scheduler.allocation.file”, “/path/to/file”) 文件内容大致如下所示12345678910111213&lt;?xml version=&quot;1.0&quot;?&gt;&lt;allocations&gt; &lt;pool name=&quot;production&quot;&gt; &lt;schedulingMode&gt;FAIR&lt;/schedulingMode&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;minShare&gt;2&lt;/minShare&gt; &lt;/pool&gt; &lt;pool name=&quot;test&quot;&gt; &lt;schedulingMode&gt;FIFO&lt;/schedulingMode&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;minShare&gt;3&lt;/minShare&gt; &lt;/pool&gt;&lt;/allocations&gt;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之spark Metrics系统以及自定义Metrics Sink","date":"2017-04-17T13:57:08.613Z","path":"2017/04/17/bigdata/spark从入门到精通_笔记/Spark运维管理之spark Metrics系统以及自定义Metrics Sink/","text":"Spark有一套可配置的metrics系统，是基于Coda Hale Metrics类库实现的。该metrics系统允许用户将Spark的metrics统计指标上报到多种目标源（sink）中，包括http，jmx和csv文件。这个metrics系统是通过一个配置文件进行配置的，在$SPARK_HOME目录的conf目录下，用一个metrics.properties文件来配置。可以通过在spark-defaults.conf中配置spark.metrics.conf属性来配置自定义的文件路径。spark metrics依据不同的spark组件划分为了不同的实例。在每一个实例中，你都可以配置一系列的sink来指定该实例的metrics要上报到哪里去。 以下实例是目前被支持的 master: spark standalone master进程,也就是说master进程的统计信息上报到哪applications: master中的组件，可以上报所有application的metricsworker: spark standalone worker进程executor: spark executor进程driver: spark driver进程 每个实例都可以上报metrics到0个或多个sink中。sink被包含在了org.apache.spark.metrics.sink包下。 ConsoleSink: 日志metrics，打印到控制台CSVSink: 以固定的频率将metrics数据导出到CSV文件中JmxSink: 注册metrics到JMX console中MetricsServlet: 在Spark UI中添加一个servlet来通过JSON数据提供metrics数据（之前的REST API就是通过该方式进行的）Slf4jSink: 以日志的形式发送metrics到slf4j GraphiteSink: 发送metrics到Graphite节点GangliaSink: 发送metrics到Ganglia节点。Spark也支持Ganglia sink，但是没有包含在默认的打包内，因为有版权的问题。要安装GangliaSink，就需要自己编译一个spark。要注意，必须要提供必要的授权信息。 metrics系统的意义 1、metrics只能在spark web ui上看到，或者是history server上看历史作业的web ui。2、如果你希望将metrics数据，结构化处理以后导入到，比如mysql里面，然后进行一个存储，开发一个系统对外开放3、spark集群运行分析系统4、自定义metrics sink，将所有的metrics全部写入外部的你指定的存储文件中，然后定时导入到你的mysql中 实验: 自定义metrics sink 1、停止集群1./sbin/stop-all.sh 2、配置spark.metrics.conf文件，启用CSVSink123456789101112131415161718cd confcp metrics.properties.template metrics.propertiesvim metrics.properties# Enable CsvSink for all instances(启用所有的实例,包括master,applications,worker,executor,driver)*.sink.csv.class=org.apache.spark.metrics.sink.CsvSink# Polling period for CsvSink(推送间隔为1min)*.sink.csv.period=1*.sink.csv.unit=minutes# Polling directory for CsvSink(推送的目录)*.sink.csv.directory=/usr/local/spark-metrics#需要创建一个目录mkdir /usr/local/spark-metrics 3、重启集群1./sbin/start-all.sh 4、运行一个作业，查看指定目录下的csv文件 使用spark-shell去模拟一个作业 123456spark-shell --master spark://192.168.0.103:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之curl+REST API作业监控","date":"2017-04-17T13:06:13.318Z","path":"2017/04/17/bigdata/spark从入门到精通_笔记/Spark运维管理之curl+REST API作业监控/","text":"除了查看web ui上的统计来监控作业，还可以通过Spark提供的REST API来获取作业信息，并进行作业监控。REST API就给我们自己开发Spark的一些监控系统或平台提供了可能。REST API是通过http协议发送的，并给我们返回JSON格式的数据。因此无论你是用java，还是python，亦或是php，都可以获取Spark的监控信息。 运行中的作业以及history server中的历史作业，都可以获取到信息 1、如果是要获取运行中的作业的信息，可以通过http://host:4040/api/v1/...的方式来获取2、如果是要获取历史作业的信息，可以通过http://host:18080/api/v1/...的方式来获取 比如说，http://192.168.0.103:18080/api/v1/applications，就可以获取到所有历史作业的基本信息 以下是所有API的说明12345678910111213/applications 获取作业列表/applications/[app-id]/jobs 指定作业的job列表/applications/[app-id]/jobs/[job-id] 指定job的信息/applications/[app-id]/stages 指定作业的stage列表/applications/[app-id]/stages/[stage-id] 指定stage的所有attempt列表/applications/[app-id]/stages/[stage-id]/[stage-attempt-id] 指定stage attempt的信息/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskSummary 指定stage attempt所有task的metrics统计信息/applications/[app-id]/stages/[stage-id]/[stage-attempt-id]/taskList 指定stage attempt的task列表/applications/[app-id]/executors 指定作业的executor列表/applications/[app-id]/storage/rdd 指定作业的持久化rdd列表/applications/[app-id]/storage/rdd/[rdd-id] 指定持久化rdd的信息/applications/[app-id]/logs 下载指定作业的所有日志的压缩包/applications/[app-id]/[attempt-id]/logs 下载指定作业的某次attempt的所有日志的压缩包 当作业运行在yarn中时，每个作业都可能会尝试多次运行，所以上述的所有[app-id]都必须替换为[app-id]/[attempt-id] 这些API都非常便于让我们去基于它们开发各种监控系统或应用。特别是，spark保证以下几点: 1、API永远不会因为版本的变更而更改2、JSON中的字段用于不会被移除3、新的API接口可能会被增加4、已有API接口中可能会增加新的字段5、API的新版本可能会作为新接口被添加进来。新版本的接口不要求向后兼容。6、API版本可能会被删除掉，但是肯定是在一个相关的新API版本发布之后。 要注意的是，当查看运行中作业的UI时，applications/[app-id]还是需要提供的，尽管此时在那个4040端口上可能只有一个作业在运行。比如说，要查看正在运行的作业的job列表，可能需要使用以下API: http://host:4040/api/v1/applications/[app-id]/jobs这主要是为了尽可能地复用API接口 实验 1、安装curl工具，来发送http请求: yum install -y curl2、试一试以上的几个API，去获取历史作业的信息 standalone模式和yarn模式运行中的作业和历史作业的获取相同,也就是将http请求的端口换成作业的4040端口 123456spark-shell --master spark://192.168.0.103:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之查看Web UI进行作业监控","date":"2017-04-17T07:54:54.518Z","path":"2017/04/17/bigdata/spark从入门到精通_笔记/Spark运维管理之查看Web UI/","text":"对于Spark作业的监控，Spark给我们提供了很多种方式：Spark Web UI，Spark History Web UI，RESTFUL API以及Metrics。 Spark Web UI每提交一个Spark作业，并且启动SparkContext之后，都会启动一个对应的Spark Web UI服务。默认情况下Spark Web UI的访问地址是driver进程所在节点的4040端口。在Spark Web UI上会展示作业相关的详细信息，非常有用，是Spark作业监控的最主要的手段。 Spark Web UI包括了以下信息： 1、stage和task列表2、RDD大小以及内存使用的概览3、环境信息4、作业对应的executor的信息 可以通过在浏览器中访问http://:4040地址，来进入Spark Web UI界面。如果多个driver在一个机器上运行，它们会自动绑定到不同的端口上。默认从4040端口开始，如果发现已经被绑定了，那么会选择4041、4042等端口，以此类推。 要注意的是，这些信息默认情况下仅仅在作业运行期间有效并且可以看到。一旦作业完毕，那么driver进程以及对应的web ui服务也会停止，我们就无法看到已经完成的作业的信息了。如果要在作业完成之后，也可以看到其Spark Web UI以及详细信息，那么就需要启用Spark的History Server。 监控实验 1、通过spark-shell以standalone模式执行一个wordcount作业，通过直接访问4040端口以及从8080端口(这是spark集群的web ui,然后从集群的web ui进入到作业的web ui)两种方式进入web ui。1spark-shell --master spark://192.168.0.103:7077 直接访问4040端口,查看的是spark作业的web ui 通过spark集群的web ui查看 2、在作业运行完毕之后，再尝试看看作业的Web UI。在spark-shell中执行下面的wordcount1234scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 查看一个作业的所有的job 查看一个job的详情 查看一个stage的详情 如果,我们进行了cache,那么可以看到Storage的详情 12345scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.cachescala&gt;counts.collect 查看作业的executor1234scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.foreach(println) 3、通过spark-shell以yarn模式执行一个wordcount作业，并重复上述过程。 123456spark-shell --master yarn-clientscala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 那么此时的集群管理器的web ui是通过yarn来查看的http://resourceManager_host:8088 standalone模式下查看历史作业的web ui默认情况下，一个作业运行完成之后，就再也无法看到其web ui以及执行信息了，在生产环境中，这对调试以及故障定位有影响。 如果要在作业执行完之后，还能看到其web ui，那么必须将作业的spark.eventLog.enabled属性设置为true，这个属性会告诉spark去记录该作业的所有要在web ui上展示的事件以及信息。 如果spark记录下了一个作业生命周期内的所有事件，那么就会在该作业执行完成之后，我们进入其web ui时，自动用记录的数据重新绘制作业的web ui。 有3个属性我们可以设置 1、spark.eventLog.enabled，必须设置为true2、spark.eventLog.dir，默认是/tmp/spark-events，建议自己手动调整为其他目录，比如/usr/local/spark-event或是hdfs目录，必须手动创建3、spark.eventLog.compress ，是否压缩数据，默认为false，建议可以开启压缩以减少磁盘空间占用 这些属性可以在提交一个作业的时候设置如果想要对所有作业都启用该机制，那么可以在spark-defaults.conf文件中配置这三个属性 实验 1、先看看之前的已经执行完成的作业，是否可以进入spark web ui界面 2、关闭现有的master和worker进程1./sbin/stop-all.sh 3、修改spark-defaults.conf文件，配置上述三个属性，启用standalone模式下的作业历史信息记录，手动创建hdfs目录 12345678#vim spark-defaults.confspark.eventLog.enabled truespark.eventLog.dir hdfs://192.168.0.103:9000/spark-eventsspark.eventLog.compress true#手动创建HDFS目录hdfs dfs -mkdir /spark-events 4、重新启动spark集群1./sbin/start-all.sh 5、使用spark-shell提交一个作业，然后再次尝试进入spark web ui界面 123456spark-shell --master spark://192.168.0.103:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 可以看到在HDFS的目录下有对应的文件生成,这些文件就是记录spark历史作业的文件 注意：如果要让spark完成作业的事件记录，那么必须最后以sc.stop()结尾。 启动HistoryServer查看历史作业的web ui1、停止集群1./sbin/stop-all.sh 2、配置spark-env.sh和spark-defaults.conf 123456789101112#vim spark-defaults.confspark.eventLog.enabled truespark.eventLog.dir hdfs://192.168.0.103:9000/spark-eventsspark.eventLog.compress true#vim spark-env.shexport SPARK_HISTORY_OPTS=&quot;-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=50 -Dspark.history.fs.logDirectory=hdfs://192.168.0.103:9000/spark-events&quot;#history Server的端口是:18080#retainedApplications保留50个的application的历史 注意: 务必预先创建好hdfs://192.168.0.103:9000/spark-events目录 spark.eventLog.dir与spark.history.fs.logDirectory指向的必须是同一个目录,因为spark.eventLog.dir会指定作业事件记录在哪里，spark.history.fs.logDirectory会指定从哪个目录中去读取作业数据 3、重启集群1./sbin/start-all.sh 4、启动history server 1./sbin/start-history-server.sh 访问地址: 192.168.0.103:18080 5、运行spark-shell，在standalone模式下和yarn模式下，分别执行一个作业 6、通过192.168.80.103:18080的HistoryServer UI可以看到所有运行后的作业信息 standalone模式下 123456spark-shell --master spark://192.168.0.103:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect yarn模式下 123456spark-shell --master yarn-clientscala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之基于文件系统实现HA高可用以及手动主备切换","date":"2017-04-17T07:32:55.350Z","path":"2017/04/17/bigdata/spark从入门到精通_笔记/Spark运维管理之基于文件系统实现HA高可用以及手动主备切换/","text":"概述zookeeper是实现生产级别的高可用性的最佳方式，但是如果你就是想要在master进程挂掉的时候，手动去重启它，而不是依靠zookeeper实现自动主备切换，那么可以使用FILESYSTEM模式。当应用程序和worker都注册到master之后，master就会将它们的信息写入指定的文件系统目录中，以便于当master重启的时候可以从文件系统中恢复注册的应用程序和worker状态。 配置要启用这种恢复模式，需要在spark-env.sh中设置SPARK_DAEMON_JAVA_OPTS12spark.deploy.recoveryMode 设置为FILESYSTEM来启用单点恢复（默认值为NONE）spark.deploy.recoveryDirectory spark在哪个文件系统目录内存储状态信息，必须是master可以访问的目录 细节1、这个解决方案可以与进程监控或管理器（比如monit）结合使用，或者就仅仅是启用手动重启恢复机制即可。2、文件系统恢复比不做任何恢复机制肯定是要好的，这个模式更加适合于开发和测试环境，而不是生产环境。此外，通过stop-master.sh脚本杀掉一个master进程是不会清理它的恢复状态的，所以当你重启一个新的master进程时，它会进入恢复模式。这会增加你的恢复时间至少1分钟，因为它需要等待之前所有已经注册的worker等节点先timeout。3、这种方式没有得到官方的支持，也可以使用一个NFS目录作为恢复目录。如果原先的master节点完全死掉了，你可以在其他节点上启动一个master进程，它会正确地恢复之前所有注册的worker和应用程序。之后的应用程序可以找到新的master，然后注册。 实验1、关闭两台机器上的master和worker2、修改192.168.0.103机器上的spark-env.shexport SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=FILESYSTEM -Dspark.deploy.recoveryDirectory=/usr/local/spark_recovery”3、在192.168.0.103上启动spark集群这样会生成一个我们配置的目录 4、在spark-shell中进行wordcount计数，到一半，有一个running application这里使用spark-shell去模拟提交应用程序123456spark-shell --master spark://192.168.0.103:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 查看web ui 在指定的目录下会生成我们提交应用的文件 5、杀掉192.168.0.103上的master进程1./sbin/stop-master.sh 6、重启192.168.0.103上的master进程1./sbin/start-master.sh 因为我们配置的目录文件的存在,再次启动的时候,会根据文件重新运行原来的应用 我们观察一下重启时的日志 再看一下配置的目录文件下application的文件是否存在 7、观察web ui上，是否恢复了worker以及原先正在运行的application","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark运维管理之基于zookeeper实现HA以及自动主备切换","date":"2017-04-17T06:37:38.917Z","path":"2017/04/17/bigdata/spark从入门到精通_笔记/Spark运维管理之基于zookeeper实现HA以及自动主备切换/","text":"默认情况下，standalone cluster manager对于worker节点的失败是具有容错性的（迄今为止，Spark自身而言对于丢失部分计算工作是有容错性的，它会将丢失的计算工作迁移到其他worker节点上执行）。然而，调度器是依托于master进程来做出调度决策的，这就会造成单点故障：如果master挂掉了，就没法提交新的应用程序了。为了解决这个问题，spark提供了两种高可用性方案，分别是基于zookeeper的HA方案以及基于文件系统的HA方案。 基于zookeeper的HA方案 概述使用zookeeper来提供leader选举以及一些状态存储，你可以在集群中启动多个master进程，让它们连接到zookeeper实例。其中一个master进程会被选举为leader，其他的master会被指定为standby模式。如果当前的leader master进程挂掉了，其他的standby master会被选举，从而恢复旧master的状态。并且恢复作业调度。整个恢复过程（从leader master挂掉开始计算）大概会花费1~2分钟。要注意的是，这只会推迟调度新的应用程序，master挂掉之前就运行的应用程序是不被影响的。 配置如果要启用这个恢复模式，需要在spark-env.sh文件中，设置SPARK_DAEMON_JAVA_OPTS选项：123spark.deploy.recoveryMode 设置为ZOOKEEPER来启用standby master恢复模式（默认为NONE）spark.deploy.zookeeper.url zookeeper集群url（举例来说，192.168.0.103:2181,192.168.0.104:2181）spark.deploy.zookeeper.dir zookeeper中用来存储恢复状态的目录（默认是/spark） 备注：如果在集群中启动了多个master节点，但是没有正确配置master去使用zookeeper，master在挂掉进行恢复时是会失败的，因为没法发现其他master，并且都会认为自己是leader。这会导致集群的状态不是健康的，因为所有master都会自顾自地去调度。 细节在启动一个zookeeper集群之后，启用高可用性是很直接的。简单地在多个节点上启动多个master进程，并且给它们相同的zookeeper配置（zookeeper url和目录）。master就可以被动态加入master集群，并可以在任何时间被移除掉。 为了调度新的应用程序或者向集群中添加worker节点，它们需要知道当前leader master的ip地址。这可以通过传递一个master列表来完成。举例来说，我们可以将我们的SparkContext连接的地址指向spark://host1:port1,host2:port2。这就会导致你的SparkContext尝试去注册所有的master，如果host1挂掉了，那么配置还是正确的，因为会找到新的leader master，也就是host2。 对于注册一个master和普通的操作，这是一个重要的区别。当一个应用程序启动的时候，或者worker需要被找到并且注册到当前的leader master的时候。一旦它成功注册了，就被保存在zookeeper中了。如果故障发生了，new leader master会去联系所有的之前注册过的应用程序和worker，并且通知它们master的改变。这样的话，它们甚至在启动的时候都不需要知道new master的存在。 正是由于这个属性，new master可以在任何时间被创建，并且我们唯一需要担心的一件事情就是新的应用程序和worker可以找到并且注册到master。一旦注册上去之后，我们就不用担心它了。 实验1、将192.168.0.103机器上的spark集群先停止1./sbin/stop-all.sh 2、修改机器上的spark-env.sh文件，在其中加入上述三个属性1export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=192.168.0.103:2181,192.168.0.104:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 3、启动集群,这样就启动了一个master和一个worker在192.168.0.103上直接用启动集群：1./sbin/start-all.sh 此时去zookeeper中看是否有个”/spark”目录生成1234#连接zkzkCli.sh#执行ls命令ls / 4、在192.168.0.104上部署spark安装包，并启动一个master进程 4.1.安装scala 2.11.4123456781、将课程提供的scala-2.11.4.tgz使用WinSCP拷贝到/usr/local/src目录下。2、对scala-2.11.4.tgz进行解压缩：tar -zxvf scala-2.11.4.tgz3、对scala目录进行重命名：mv scala-2.11.4 scala4、配置scala相关的环境变量vi ~/.bashrcexport SCALA_HOME=/usr/local/scalaexport PATH=$SCALA_HOME/binsource ~/.bashrc 查看scala是否安装成功：scala -version 4.2.安装spark客户端 1、将spark-1.5.1-bin-hadoop2.4.tgz使用WinSCP上传到/usr/local/src目录下。2、解压缩spark包：tar -zxvf spark-1.5.1-bin-hadoop2.4.tgz。3、重命名spark目录：mv spark-1.5.1-bin-hadoop2.4 spark4、修改spark环境变量123456vi ~/.bashrcexport SPARK_HOME=/usr/local/sparkexport PATH=$SPARK_HOME/binexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/libsource ~/.bashrc 修改spark-env.sh文件 1、cd /usr/local/spark/conf2、cp spark-env.sh.template spark-env.sh3、vi spark-env.sh1234567export JAVA_HOME=/usr/java/latestexport SCALA_HOME=/usr/local/scalaexport HADOOP_HOME=/usr/local/hadoopexport HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoopexport SPARK_MASTER_IP=192.168.0.104export SPARK_DAEMON_MEMORY=100mexport SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=192.168.0.103:2181,192.168.0.104:2181 -Dspark.deploy.zookeeper.dir=/spark&quot; 4.3.在192.168.0.104上单独启动一个standby master进程：1./sbin/start-master.sh 查看启动的两个master的web UI情况 4、提交应用程序在我们编写的程序中将master地址修改为192.168.0.103:7077,192.168.0.103:7078 这里使用spark-shell去模拟提交应用程序123456spark-shell --master spark://192.168.0.103:7077,192.168.0.104:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 此时在web ui上查看(alive master) 5、杀掉原先的leader master 12345#jps查看192.168.0.103机器上的master的pidjps#杀掉master进程kill -9 xxxx#再次jps,看是否杀掉 此时查看103的web ui,发现访问不了 此时查看104的web ui上查看(standby master) 5.等到standby master接管集群再次提交应用程序这里使用spark-shell去模拟提交应用程序1234567spark-shell --master spark://192.168.0.103:7077,192.168.0.104:7077//此时查看启动日志的时候,会看到去连接192.168.0.103:7077会报错,然后会再去连接192.168.0.104:7077scala&gt;val lines = sc.textFile(&quot;hdfs://192.168.0.103:9000/test/hello.txt&quot;)scala&gt;val words = lines.flatMap(_.split(&quot; &quot;)).map((_,1))scala&gt;val counts = words.reduceByKey(_+_)scala&gt;counts.collect 查看104的web ui 6、再次手动启动原来的leader master（死掉） 在103的机器上启动master1./sbin/start-master.sh 查看103的web ui","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之新闻网站关键指标实时统计","date":"2017-04-16T10:33:08.930Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之新闻网站关键指标实时统计/","text":"构造模拟的数据(kafka的生产者)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133package cn.spark.study.streaming.upgrade.news;import java.text.SimpleDateFormat;import java.util.Date;import java.util.Properties;import java.util.Random;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;/** * 访问日志Kafka Producer * @author Administrator * */public class AccessProducer extends Thread &#123; private static SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); private static Random random = new Random(); private static String[] sections = new String[] &#123;&quot;country&quot;, &quot;international&quot;, &quot;sport&quot;, &quot;entertainment&quot;, &quot;movie&quot;, &quot;carton&quot;, &quot;tv-show&quot;, &quot;technology&quot;, &quot;internet&quot;, &quot;car&quot;&#125;; private static int[] arr = new int[]&#123;1, 2, 3, 4, 5, 6, 7, 8, 9, 10&#125;; private static String date; private Producer&lt;Integer, String&gt; producer; private String topic; public AccessProducer(String topic) &#123; this.topic = topic; producer = new Producer&lt;Integer, String&gt;(createProducerConfig()); date = sdf.format(new Date()); &#125; private ProducerConfig createProducerConfig() &#123; Properties props = new Properties(); props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); props.put(&quot;metadata.broker.list&quot;, &quot;192.168.0.103:9092,192.168.0.104:9092&quot;); return new ProducerConfig(props); &#125; public void run() &#123; int counter = 0; while(true) &#123; for(int i = 0; i &lt; 100; i++) &#123; String log = null; if(arr[random.nextInt(10)] == 1) &#123; log = getRegisterLog(); &#125; else &#123; log = getAccessLog(); &#125; producer.send(new KeyedMessage&lt;Integer, String&gt;(topic, log)); counter++; if(counter == 100) &#123; counter = 0; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; &#125; private static String getAccessLog() &#123; StringBuffer buffer = new StringBuffer(&quot;&quot;); // 生成时间戳 long timestamp = new Date().getTime(); // 生成随机userid（默认1000注册用户，每天1/10的访客是未注册用户） Long userid = 0L; int newOldUser = arr[random.nextInt(10)]; if(newOldUser == 1) &#123; userid = null; &#125; else &#123; userid = (long) random.nextInt(1000); &#125; // 生成随机pageid（总共1k个页面） Long pageid = (long) random.nextInt(1000); // 生成随机版块（总共10个版块） String section = sections[random.nextInt(10)]; // 生成固定的行为，view String action = &quot;view&quot;; return buffer.append(date).append(&quot; &quot;) .append(timestamp).append(&quot; &quot;) .append(userid).append(&quot; &quot;) .append(pageid).append(&quot; &quot;) .append(section).append(&quot; &quot;) .append(action).toString(); &#125; private static String getRegisterLog() &#123; StringBuffer buffer = new StringBuffer(&quot;&quot;); // 生成时间戳 long timestamp = new Date().getTime(); // 新用户都是userid为null Long userid = null; // 生成随机pageid，都是null Long pageid = null; // 生成随机版块，都是null String section = null; // 生成固定的行为，view String action = &quot;register&quot;; return buffer.append(date).append(&quot; &quot;) .append(timestamp).append(&quot; &quot;) .append(userid).append(&quot; &quot;) .append(pageid).append(&quot; &quot;) .append(section).append(&quot; &quot;) .append(action).toString(); &#125; public static void main(String[] args) &#123; AccessProducer producer = new AccessProducer(&quot;news-access&quot;); producer.start(); &#125; &#125; 创建topic,进行”消费”测试12kafka-topics.sh --zookeeper 192.168.0.103:2181,192.168.0.104:2181 --topic news-access --replication-factor 1 --partitions 1 --createkafka-console-consumer.sh --zookeeper 192.168.0.103:2181,192.168.0.104:2181 --topic news-access --from-beginning 消费到的数据的格式为:122016-02-22 1442343424234234 125 115 car view2016-02-22 1442343424234235 null null null register 总体的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346package cn.spark.study.streaming.upgrade.news;import java.util.HashMap;import java.util.HashSet;import java.util.Map;import java.util.Set;import kafka.serializer.StringDecoder;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.function.Function;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaPairInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.kafka.KafkaUtils;import scala.Tuple2;/** * 新闻网站关键指标实时统计Spark应用程序 * @author Administrator * */public class NewsRealtimeStatSpark &#123; public static void main(String[] args) throws Exception &#123; // 创建Spark上下文 SparkConf conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;NewsRealtimeStatSpark&quot;); JavaStreamingContext jssc = new JavaStreamingContext( conf, Durations.seconds(5)); // 创建输入DStream Map&lt;String, String&gt; kafkaParams = new HashMap&lt;String, String&gt;(); kafkaParams.put(&quot;metadata.broker.list&quot;, &quot;192.168.0.103:9092,192.168.0.104:9092&quot;); Set&lt;String&gt; topics = new HashSet&lt;String&gt;(); topics.add(&quot;news-access&quot;); JavaPairInputDStream&lt;String, String&gt; lines = KafkaUtils.createDirectStream( jssc, String.class, String.class, StringDecoder.class, StringDecoder.class, kafkaParams, topics); // 过滤出访问日志 JavaPairDStream&lt;String, String&gt; accessDStream = lines.filter( new Function&lt;Tuple2&lt;String,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); String action = logSplited[5]; if(&quot;view&quot;.equals(action)) &#123; return true; &#125; else &#123; return false; &#125; &#125; &#125;); // 统计第一个指标：实时页面pv(页面pv的实时统计,每10秒内各个页面被访问的pv) calculatePagePv(accessDStream); // 统计第二个指标：实时页面uv calculatePageUv(accessDStream); // 统计第三个指标：实时注册用户数 calculateRegisterCount(lines); // 统计第四个指标：实时用户跳出数:注册用户只是访问了一个页面就退出了,那么就属于跳出用户 calculateUserJumpCount(accessDStream); // 统计第五个指标：实时版块pv calcualteSectionPv(accessDStream); jssc.start(); jssc.awaitTermination(); jssc.close(); &#125; /** * 计算页面pv * @param accessDStream */ private static void calculatePagePv(JavaPairDStream&lt;String, String&gt; accessDStream) &#123; JavaPairDStream&lt;Long, Long&gt; pageidDStream = accessDStream.mapToPair( new PairFunction&lt;Tuple2&lt;String,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); Long pageid = Long.valueOf(logSplited[3]); return new Tuple2&lt;Long, Long&gt;(pageid, 1L); &#125; &#125;); JavaPairDStream&lt;Long, Long&gt; pagePvDStream = pageidDStream.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); pagePvDStream.print(); // 在计算出每10秒钟的页面pv之后，其实在真实项目中，应该持久化 // 到mysql，或redis中，对每个页面的pv进行累加(或者取原来的值,然后加上此时的pv,再生成一条新的记录,那么就可以做成pv随时间变化的曲线图) // javaee系统，就可以从mysql或redis中，读取page pv实时变化的数据，以及曲线图 &#125; /** * 计算页面uv * @param &lt;U&gt; * @param accessDStream */ private static &lt;U&gt; void calculatePageUv(JavaPairDStream&lt;String, String&gt; accessDStream) &#123; JavaDStream&lt;String&gt; pageidUseridDStream = accessDStream.map( new Function&lt;Tuple2&lt;String,String&gt;, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public String call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); Long pageid = Long.valueOf(logSplited[3]); Long userid = Long.valueOf(&quot;null&quot;.equalsIgnoreCase(logSplited[2]) ? &quot;-1&quot; : logSplited[2]); return pageid + &quot;_&quot; + userid; &#125; &#125;); // 使用transform操作,对DStream中的每个rdd进行distinct操作 JavaDStream&lt;String&gt; distinctPageidUseridDStream = pageidUseridDStream.transform( new Function&lt;JavaRDD&lt;String&gt;, JavaRDD&lt;String&gt;&gt;() &#123; private static final long serialVersionUID = 1L; @Override public JavaRDD&lt;String&gt; call(JavaRDD&lt;String&gt; rdd) throws Exception &#123; return rdd.distinct(); &#125; &#125;); JavaPairDStream&lt;Long, Long&gt; pageidDStream = distinctPageidUseridDStream.mapToPair( new PairFunction&lt;String, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(String str) throws Exception &#123; String[] splited = str.split(&quot;_&quot;); Long pageid = Long.valueOf(splited[0]); return new Tuple2&lt;Long, Long&gt;(pageid, 1L); &#125; &#125;); JavaPairDStream&lt;Long, Long&gt; pageUvDStream = pageidDStream.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); pageUvDStream.print(); &#125; /** * 计算实时注册用户数 * @param lines */ private static void calculateRegisterCount(JavaPairInputDStream&lt;String, String&gt; lines) &#123; JavaPairDStream&lt;String, String&gt; registerDStream = lines.filter( new Function&lt;Tuple2&lt;String,String&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); String action = logSplited[5]; if(&quot;register&quot;.equals(action)) &#123; return true; &#125; else &#123; return false; &#125; &#125; &#125;); JavaDStream&lt;Long&gt; registerCountDStream = registerDStream.count(); registerCountDStream.print(); // 每次统计完一个最近10秒的数据之后，不是打印出来 // 去存储（mysql、redis、hbase），选用哪一种主要看你的公司提供的环境，以及你的看实时报表的用户以及并发数量，包括你的数据量 // 如果是一般的展示效果，就选用mysql就可以 // 如果是需要超高并发的展示，比如QPS 1w来看实时报表，那么建议用redis、memcached // 如果是数据量特别大，建议用hbase // 每次从存储中，查询注册数量，最近一次插入的记录，比如上一次是10秒前 // 然后将当前记录与上一次的记录累加，然后往存储中插入一条新记录，就是最新的一条数据 // 然后javaee系统在展示的时候，可以比如查看最近半小时内的注册用户数量变化的曲线图 // 查看一周内，每天的注册用户数量的变化曲线图（每天就取最后一条数据，就是每天的最终数据） &#125; /** * 计算用户跳出数量 * @param accessDStream */ private static void calculateUserJumpCount(JavaPairDStream&lt;String, String&gt; accessDStream) &#123; JavaPairDStream&lt;Long, Long&gt; useridDStream = accessDStream.mapToPair( new PairFunction&lt;Tuple2&lt;String,String&gt;, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;Long, Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); Long userid = Long.valueOf(&quot;null&quot;.equalsIgnoreCase(logSplited[2]) ? &quot;-1&quot; : logSplited[2]); return new Tuple2&lt;Long, Long&gt;(userid, 1L); &#125; &#125;); JavaPairDStream&lt;Long, Long&gt; useridCountDStream = useridDStream.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); JavaPairDStream&lt;Long, Long&gt; jumpUserDStream = useridCountDStream.filter( new Function&lt;Tuple2&lt;Long,Long&gt;, Boolean&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Boolean call(Tuple2&lt;Long, Long&gt; tuple) throws Exception &#123; if(tuple._2 == 1) &#123; return true; &#125; else &#123; return false; &#125; &#125; &#125;); JavaDStream&lt;Long&gt; jumpUserCountDStream = jumpUserDStream.count(); jumpUserCountDStream.print(); &#125; /** * 版块实时pv * @param accessDStream */ private static void calcualteSectionPv(JavaPairDStream&lt;String, String&gt; accessDStream) &#123; JavaPairDStream&lt;String, Long&gt; sectionDStream = accessDStream.mapToPair( new PairFunction&lt;Tuple2&lt;String,String&gt;, String, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Long&gt; call(Tuple2&lt;String, String&gt; tuple) throws Exception &#123; String log = tuple._2; String[] logSplited = log.split(&quot; &quot;); String section = logSplited[4]; return new Tuple2&lt;String, Long&gt;(section, 1L); &#125; &#125;); JavaPairDStream&lt;String, Long&gt; sectionPvDStream = sectionDStream.reduceByKey( new Function2&lt;Long, Long, Long&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Long call(Long v1, Long v2) throws Exception &#123; return v1 + v2; &#125; &#125;); sectionPvDStream.print(); &#125; &#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之自定义Receiver","date":"2017-04-16T10:04:24.116Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之自定义Receiver/","text":"spark streaming可以从任何数据源来接收数据，哪怕是除了它内置支持的数据源以外的其他数据源（比如flume、kafka、socket等）。如果我们想要从spark streaming没有内置支持的数据源中接收实时数据，那么我们需要自己实现一个receiver。 实现一个自定义的receiver一个自定义的receiver必须实现以下两个方法：onStart()、onStop()。onStart()和onStop()方法必须不能阻塞数据，一般来说，onStart()方法会启动负责接收数据的线程，onStop()方法会确保之前启动的线程都已经停止了。负责接收数据的线程可以调用isStopped()方法来检查它们是否应该停止接收数据。 一旦数据被接收了，就可以调用store(data)方法，数据就可以被存储在Spark内部。有一系列的store()重载方法供我们调用，来将数据每次一条进行存储，或是每次存储一个集合或序列化的数据。 接收线程中的任何异常都应该被捕获以及正确处理，从而避免receiver的静默失败。restart()方法会通过异步地调用onStop()和onStart()方法来重启receiver。stop()方法会调用onStop()方法来停止receiver。reportError()方法会汇报一个错误消息给driver，但是不停止或重启receiver。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class JavaCustomReceiver extends Receiver&lt;String&gt; &#123; String host = null; int port = -1; public JavaCustomReceiver(String host_ , int port_) &#123; super(StorageLevel.MEMORY_AND_DISK_2()); host = host_; port = port_; &#125; public void onStart() &#123; // Start the thread that receives data over a connection new Thread() &#123; @Override public void run() &#123; receive(); &#125; &#125;.start(); &#125; public void onStop() &#123; &#125; /** Create a socket connection and receive data until receiver is stopped */ private void receive() &#123; Socket socket = null; String userInput = null; try &#123; // connect to the server 从sock中拉取数据 socket = new Socket(host, port); BufferedReader reader = new BufferedReader(new InputStreamReader(socket.getInputStream())); // Until stopped or connection broken continue reading while (!isStopped() &amp;&amp; (userInput = reader.readLine()) != null) &#123; System.out.println(&quot;Received data &apos;&quot; + userInput + &quot;&apos;&quot;); store(userInput); &#125; reader.close(); socket.close(); // Restart in an attempt to connect again when server is active again restart(&quot;Trying to connect again&quot;); &#125; catch(ConnectException ce) &#123; // restart if could not connect to server restart(&quot;Could not connect&quot;, ce); &#125; catch(Throwable t) &#123; // restart if there is any other error restart(&quot;Error receiving data&quot;, t); &#125; &#125;&#125; 在spark streaming中使用自定义的receiverJavaDStream lines = ssc.receiverStream(new JavaCustomReceiver(host, port)); 安装网络服务工具12345rpm -ihv netcat-0.7.1-1.i386.rpmnc -lk 9999//通过nc去构造一些数据","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之flume","date":"2017-04-16T08:50:08.714Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之flume/","text":"flume的安装参见详细的文档 接收flume实时数据流之push方式Flume被设计为可以在agent之间推送数据(数据从一个agent到另外一个agent)，而不一定是从agent将数据传输到sink中。在这种方式下，Spark Streaming需要启动一个作为Avro Agent的Receiver，来让flume可以推送数据过来。下面是我们的整合步骤： 前提需要选择一台机器：1、Spark Streaming与Flume都可以在这台机器上启动，Spark的其中一个Worker必须运行在这台机器上面2、Flume可以将数据推送到这台机器上的某个端口 由于flume的push模型，Spark Streaming必须先启动起来，Receiver要被调度起来并且监听本地某个端口，来让flume推送数据。 配置flume接收来自目录的变化,然后将变化的文件push到spark streaming中,在flume-conf.properties文件中，配置flume的sink是将数据推送到其他的agent中1234agent1.sinks.sink1.type = avroagent1.sinks.sink1.channel = channel1agent1.sinks.sink1.hostname = 192.168.0.103agent1.sinks.sink1.port = 8888 整个配置文件如下: 配置spark streaming在我们的spark工程的pom.xml中加入spark streaming整合flume的依赖123groupId = org.apache.sparkartifactId = spark-streaming-flume_2.10version = 1.5.0 在代码中使用整合flume的方式创建输入DStream1234import org.apache.spark.streaming.flume.*;JavaReceiverInputDStream&lt;SparkFlumeEvent&gt; flumeStream = FlumeUtils.createStream(streamingContext, [chosen machine&apos;s hostname], [chosen port]); 这里有一点需要注意的是，这里监听的hostname，必须与cluster manager（比如Standalone Master、YARN ResourceManager）是同一台机器，这样cluster manager才能匹配到正确的机器，并将receiver调度在正确的机器上运行。 部署spark streaming应用12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package cn.spark.study.streaming;import java.util.Arrays;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.flume.FlumeUtils;import org.apache.spark.streaming.flume.SparkFlumeEvent;import scala.Tuple2;/** * 基于Flume Push方式的实时wordcount程序 * @author Administrator * */public class FlumePushWordCount &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;FlumePushWordCount&quot;); JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(5)); JavaReceiverInputDStream&lt;SparkFlumeEvent&gt; lines = FlumeUtils.createStream(jssc, &quot;192.168.0.103&quot;, 8888); JavaDStream&lt;String&gt; words = lines.flatMap( new FlatMapFunction&lt;SparkFlumeEvent, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(SparkFlumeEvent event) throws Exception &#123; String line = new String(event.event().getBody().array()); return Arrays.asList(line.split(&quot; &quot;)); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair( new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey( new Function2&lt;Integer, Integer, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); wordCounts.print(); jssc.start(); jssc.awaitTermination(); jssc.close(); &#125; &#125; 打包工程为一个jar包，使用spark-submit来提交作业 启动flume agentflume-ng agent -n agent1 -c conf -f /usr/local/flume/conf/flume-conf.properties -Dflume.root.logger=DEBUG,console flume的使用场景什么时候我们应该用Spark Streaming整合Kafka去用，做实时计算？什么使用应该整合flume？ 看你的实时数据流的产出频率1、如果你的实时数据流产出特别频繁，比如说一秒钟10w条，那就必须是kafka，分布式的消息缓存中间件，可以承受超高并发2、如果你的实时数据流产出频率不固定，比如有的时候是1秒10w，有的时候是1个小时才10w，可以选择将数据用nginx日志来表示，每隔一段时间将日志文件放到flume监控的目录中，然后呢，spark streaming来计算 接收flume实时数据流之poll方式除了让flume将数据推送到spark streaming，还有一种方式，可以运行一个自定义的flume sink1、Flume推送数据到sink中，然后数据缓存在sink中2、spark streaming用一个可靠的flume receiver以及事务机制从sink中拉取数据 前提条件1、选择一台可以在flume agent中运行自定义sink的机器2、将flume的数据管道流配置为将数据传送到那个sink中3、spark streaming所在的机器可以从那个sink中拉取数据 配置flume1、加入sink jars，将以下jar加入flume的classpath中1234567891011groupId = org.apache.sparkartifactId = spark-streaming-flume-sink_2.10version = 1.5.1groupId = org.scala-langartifactId = scala-libraryversion = 2.10.4groupId = org.apache.commonsartifactId = commons-lang3version = 3.3.2 2、修改配置文件1234agent1.sinks.sink1.type = org.apache.spark.streaming.flume.sink.SparkSinkagent1.sinks.sink1.hostname = 192.168.0.103agent1.sinks.sink1.port = 8888agent1.sinks.sink1.channel = channel1 配置spark streaming1234import org.apache.spark.streaming.flume.*;JavaReceiverInputDStream&lt;SparkFlumeEvent&gt;flumeStream = FlumeUtils.createPollingStream(streamingContext, [sink machine hostname], [sink port]); 代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package cn.spark.study.streaming;import java.util.Arrays;import org.apache.spark.SparkConf;import org.apache.spark.api.java.function.FlatMapFunction;import org.apache.spark.api.java.function.Function2;import org.apache.spark.api.java.function.PairFunction;import org.apache.spark.streaming.Durations;import org.apache.spark.streaming.api.java.JavaDStream;import org.apache.spark.streaming.api.java.JavaPairDStream;import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;import org.apache.spark.streaming.api.java.JavaStreamingContext;import org.apache.spark.streaming.flume.FlumeUtils;import org.apache.spark.streaming.flume.SparkFlumeEvent;import scala.Tuple2;/** * 基于Flume Poll方式的实时wordcount程序 * @author Administrator * */public class FlumePollWordCount &#123; public static void main(String[] args) &#123; SparkConf conf = new SparkConf() .setMaster(&quot;local[2]&quot;) .setAppName(&quot;FlumePollWordCount&quot;); JavaStreamingContext jssc = new JavaStreamingContext(conf, Durations.seconds(5)); JavaReceiverInputDStream&lt;SparkFlumeEvent&gt; lines = FlumeUtils.createPollingStream(jssc, &quot;192.168.0.103&quot;, 8888); JavaDStream&lt;String&gt; words = lines.flatMap( new FlatMapFunction&lt;SparkFlumeEvent, String&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Iterable&lt;String&gt; call(SparkFlumeEvent event) throws Exception &#123; String line = new String(event.event().getBody().array()); return Arrays.asList(line.split(&quot; &quot;)); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; pairs = words.mapToPair( new PairFunction&lt;String, String, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Tuple2&lt;String, Integer&gt; call(String word) throws Exception &#123; return new Tuple2&lt;String, Integer&gt;(word, 1); &#125; &#125;); JavaPairDStream&lt;String, Integer&gt; wordCounts = pairs.reduceByKey( new Function2&lt;Integer, Integer, Integer&gt;() &#123; private static final long serialVersionUID = 1L; @Override public Integer call(Integer v1, Integer v2) throws Exception &#123; return v1 + v2; &#125; &#125;); wordCounts.print(); jssc.start(); jssc.awaitTermination(); jssc.close(); &#125; &#125; 一定要先启动flume，再去提交spark streaming 启动flume1flume-ng agent -n agent1 -c conf -f /usr/local/flume/conf/flume-conf.properties -Dflume.root.logger=DEBUG,console","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"数据仓库","date":"2017-04-16T04:47:25.920Z","path":"2017/04/16/数据仓库/数据仓库/","text":"数据仓库:Data Warehouse 简写DW,是企业索泽级别的决策制定过程,他处于分析性报告和决策支持目的而创建 数据仓库的特点 数据仓库的数据是面向主题的与传统数据库面向应用进行数据组织的特点相对应,数据仓库的数据是面向主题进行组织的,这个主题是一个抽象的概念,其实就是:老板关心哪个方面,然后在此建立主题 数据仓库的数据是集成的 数据仓库的数据是从原有的分散的数据库数据抽取出来的,数据仓库的数据不能从原有的数据库系统直接得到,因此数据在进入到数据仓库之前,需要进行etl的过程,然后再进入数据仓库,所要完成的工作有:1.要统一源数据中所有矛盾之处,如字段的同名异义,异名同义,单位不统一,字长不一致等(标准化的过程)2.进行数据综合和计算 数据仓库的数据是不可更新的数据仓库的数据主要是做决策分析的,只是做数据查询 数据仓库的数据是随时间不断变化的 数据仓库的发展历程 简单报表阶段生成的是一些简单的能够帮助领导进行决策所需要的汇总数据,这个阶段的大部分表现形式为数据库和前端报表工具 数据集市这个阶段,主要是根据某个页面部门的需要,在部门内部按照业务人员的需要,进行多维报表的数据展现,但是存在部门之间的数据冗余的情况 数据仓库主要是按照一定的数据模型,对整个企业的数据进行采集,整理,并且能够按照各个业务部门的需要,提供跨部门的,完全一致的业务报表数据,同时为决策提供支持 数据库与数据仓库的区别数据库软件:mysql,oracle等数据库:是一种逻辑的概念,由很多表组成,表是二维的数据仓库:从数据量来说,数据仓库要比数据库更庞大的多,数据仓库主要用于数据挖掘和数据分析,辅助领导做决策 数据库和数据仓库的区别讲的是OLTP和OLAP的区别操作型数据:联机事务处理OLTP,主要对数据进行CRUD分析型数据:联机分析处理","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"http://yoursite.com/tags/数据仓库/"}]},{"title":"第四章 类和对象","date":"2017-04-16T04:47:25.917Z","path":"2017/04/16/scala编程/第四章 类和对象/","text":"1.类 对象和方法1234567#类是对象的蓝图,一旦你定义了类,就可以用关键字new来创建对象class CheckSumAccumulator &#123;&#125;#使用类创建对象new CheckSumAccumulator 1234567891011121314类定义里,可以放置字段和方法,这些被笼统的称之为成员,字段:使用val或者是var定义,方法使用这些数据执行对象的运算工作方法:使用def定义,包含了可执行的代码,字段保留了对象的状态或数据当类被实例化的时候,运行时环境会预留一些内存来保留对象的状态映像------即变量的内容class CheckSumAccumulator &#123; var sum = 0&#125;//实例化两次val acc = new CheckSumAccumulatorval csa = new CheckSumAccumulator//内存里对象的状态映像看上去大概是这样的,如下图: 12由于在类CheckSumAccumulator里定义的字段sum是var,而不是val,因此之后可以重新赋给不同的Int值,如下acc.sum = 3 123需要注意的是:尽管acc对象是val,但是仍然可以修改对象中变量的值,但是我们不能修改对象的指向,#编译不过,因为acc是valacc = new CheckSumAccumulator private123456789class CheckSumAccumulator &#123; private var sum = 0&#125;val csa = new CheckSumAccumulatoracc.sum = 5 //编译不过,因为sum是私有的/*在scala里把成员公开的方法是不显示的指定任何访问修饰符,换句话说,在Java里要写上public的地方,在scala里只要什么都不要写就可以了,public是scala里默认的访问级别*/ 方法的参数123456789101112131415161718192021222324252627282930313233class CheckSumAccumulator &#123; private var sum = 0 def add(b: Byte): Unit =&#123; sum += b &#125; def checkSum():Unit = &#123; return -(sum &amp; 0xFF) + 1 &#125;&#125;/*传递给方法的任何参数都可以在方法内部使用,scala里方法参数的一个重要特征是他们都是val,不是var,如果你想在方法里面给参数赋值,结果是编译失败: def add(b: Byte): Unit =&#123; b = 1 //编译不过,因为b是val sum += b &#125;*//*checkSum方法最后的return语句是多余的可以去掉,如果没有发现任何显示的返回值,scala方法将返回方法中最后一次计算的到的结果方法的推荐风格是尽量避免使用返回语句,尤其是多条返回语句,代之以把每个方法当做是创建返回值的表达式,这种逻辑鼓励你分层简化方法,把较大的方法分解为多个更小的方法在这里checksum只要计算值,不用return,所以这个方法有另一种简写方式,假如某个方法仅计算单个表达式,则可以去掉花括号,如果表达式很短,甚至可以把它放在def的同一行里,这样改动之后,CheckSumAccumulator如下:*/class CheckSumAccumulator &#123; private var sum = 0 def add(b: Byte):Unit = sum += b def checkSum():Int = -(sum &amp; 0xFF) + 1&#125;/*如果去掉方法体前面的等号,那么方法的结果类型就必定是Unit,这种说法不论方法体里面包含什么都成立,因为scala编译器可以把任何类型转换为Unit,例如:如果方法的最后结果是?String,但结果类型被声明为Unit,那么String将变为Unit并丢弃原值*/ 2.分号推断123456789101112131415161718192021222324252627282930/*在scala里,语句末尾的分号通常是可选的,愿意可以加,若一行中仅有一个语句也可以不加,不过一行中包含多条语句时,分号则是必须的*/val s = &quot;Hello&quot;; println(s)/*输入跨越多行的语句时,多数情况下无需特别处理,scala将在正确的位置分割语句,如下的代码被当成跨四行的一条语句*/if (x&lt;2) println(&quot;to small&quot;)else println(&quot;ok&quot;)/*然而,偶尔scala也许并不如你所愿,把句子拆分成两部分*/x+ y //会被当做两个语句x 和 +y, 如果希望把它作为一个语句x + y ,可以把它放在括号里(x+ y)/*或者可以把+号放在行末,也正源于此,串接类似于+这样的中缀操作符的时候,scala通常的风格是把操作符放在行尾而不是行头*/x +y +z 1234567/*分号推断的规则除非以下情况的一种成立,否则行尾被认为是一个分号1.疑问行由一个不能合法作为语句结尾的字结束,如句点或中缀操作符2.下一行开始于不能作为语句开始的词,如下一行开始于+3.行结束语括号() 或方框[] 内部,因为这些符号不可能容纳多个语句*/ 3.singleton对象12345678910111213141516171819202122232425262728/*scala不能定义静态成员,取而代之的是单例对象,除了用object关键字替换了class关键字以外,单例对象的定义看上去与类定义一致*/class CheckSumAccumulator &#123; private var sum = 0 def add(b: Byte)&#123;sum += b&#125; def checkSum(): Int = -(sum &amp; 0xFF) + 1&#125;object CheckSumAccumulator&#123; import scala.collection.mutable.Map private val cache = Map[String, Int]() def calculate(s: String): Int = if(cache.contains(s)) cache(s) //包含,返回 else&#123; val acc = new CheckSumAccumulator for(c &lt;- s) acc.add(c.toByte) val cs = acc.checkSum() cache += (s-&gt;cs) //加入cache映射表,因为Map是mutable类型的,所以可以+= cs &#125;&#125;/*当单例对象与某个类共享一个名称时,他就被称为这个类的伴生对象,类和他的伴生对象必须定义在一个源文件里,类被称为是这个单例对象的伴生类,类和他的伴生对象可以互相访问其私有成员*/ 12345/*对于Java程序员来说,可以把单例对象当做是Java中可能会用到的静态方法工具类,也可以用类似的语法做方法调用,单例对象名,点,方法名,例如,可以调用CheckSumAccumulator单例对象的calculate方法,如下:*/CheckSumAccumulator.calculate(&quot;Every value is an object&quot;) 123/*单例对象不只是静态方法的工具类,他同样是头等的对象,因此单例对象的名字可以被看做是贴在对象上的&quot;名签&quot;*/ 1234/*类和单例对象间的差别是:单例对象不带参数,而类可以,因为单例对象不是用new关键字实例化的,所以没机会传递给它实例化参数,每个单例对象都被实现为虚构类的实例,并指向静态的变量,因此他们与Java静态类有着相同的初始化语义,特别要指出的是,单例对象在第一次被访问的时候才会被初始化不与伴生类共享名称的单例对象被称为独立对象,他可以用在很多地方,例如作为相关功能方法的工具类,或者定义scala应用的入口点*/ 4.scala程序123456789101112131415161718192021/*想要编写能够独立运行的scala程序,就必须创建有main(仅带一个参数Array[String],且结果类型为Unit)方法的单例对象,任何拥有合适签名的main方法的单例对象都可以用来作为程序的入口点*/object Summer&#123; def main(args: Array[String]): Unit = &#123; for (arg &lt;- args)&#123; import CheckSumAccumulator.calculate//定义CheckSumAccumulator对象中calculate方法的引用,它允许你在后面的文件里使用方法的简化名 println(arg + &quot;: &quot; + calculate(arg)) &#125; &#125;&#125;/*scala的每个源文件都隐含了对包Java.lang ,包scala,以及单例对象Predef的成员引用,包scala中的Predef对象包含了许多有用的方法,例如,scala源文件中写下的println语句,实际调用的是Predef的println(Predef.println转而调用Console.println,完成真正的工作),写下assert,实际是在调用Predef.assert*//*scala和Java之间有一点不同,Java需要你把公共类放在以这个类命名的源文件中,如类SpeedRacer要放在文件SpeedRacer.java里,scala对于源文件的命名没有硬性的规定,但是在通常情况下,如果不是脚本,推荐的风格是像Java里那样按照所包含的类名来命名文件,这样程序员就可以比较容易的根据文件名找到类*/ 编译scala文件1234$ scalac CheckSumAccumulator.scala Summer.scala//编译之后再运行$scala Summer of love 5.application特质12345678910111213import CheckSumAccumulator.calculateobject FallWinterSpringSummer extends Application&#123; for (seasion &lt;- List(&quot;fail&quot;, &quot;winter&quot;, &quot;apring&quot;))&#123; println(seasion + &quot;: &quot; + calculate(seasion)) &#125;&#125;/*首先在单例对象后面写上&quot;extends Application&quot; ,然后代之以main方法,你可以把想要执行的代码直接放在单例对象的花括号之间,如此而已,之后就可以正常的编译和运行了能这么做是因为Application声明了带有合适签名的main方法,并被你写的单例对象继承,使他可以像scala程序那样,好括号之间的代码被收集进了单例对象的主构造器*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十章 组合与继承","date":"2017-04-16T04:47:25.915Z","path":"2017/04/16/scala编程/第十章 组合与继承/","text":"&emsp;组合是指一个类持有另一个的引用,借助被引用的类完成任务,继承是超类/子类的关系 1.二维布局库123456789101112/*如下:Element是二维布局中的元素,该元素可以通过名为&quot;elem&quot;的工厂方法来通过传入的数据构造新的元素*/elem(s: String): Element//如上,你可以对名为Element的类型建模,可以对元素调用above或beside方法,并传入第二个元素来获取一个合并了前两个元素的新的元素val column1 = elem(&quot;hello&quot;) above elem(&quot;****&quot;)val column2 = elem(&quot;****&quot;) above elem(&quot;world&quot;)//该表达式的显示结果为:hello ******** world 2.抽象类123456789abstract class Element&#123; def contents: Array[String]&#125;/*contents被声明为没有实现的方法,他是Element类的抽象成员,具有抽象成员的类本省必须被声明为抽象的,要声明一个类是抽象的,那么只要在类的class之前加上abstract关键字即可抽象类是不能实例化的注意:Element类的contents方法并没带abstract修饰符,一个方法只要没有实现(即没有等号或方法体),他就是抽象的*/ 3.定义无参数方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*接下来,我们向Element添加显示宽度和高度的方法,height方法返回contents里的行数,width方法返回第一行的长度,如果元素没有行则返回零*/abstract class Element&#123; def contents: Array[String] def height: Int = contents.length def width: Int = if (height==0) 0 else contents(0).length&#125;/*注意:Element的三个方法都没有参数列表,甚至连空列表都没有,例如:不同于方法:*/def width():Int//以下的方法定义中不含括号:def width: Int/*scala的使用惯例是:只要方法中没有参数并且方法仅能通过读取所包含对象的属性方法区访问可变状态,就使用无参数方法,这个惯例支持统一访问原则,就是说客户端代码不应由属性是通过字段实现还是方法实现而受到影响,以下是将width和height作为字段而不是方法来实现,只要简单的将def修改成val,即可:*/abstract class Element&#123; def contents: Array[String] val height: Int = contents.length val width: Int = if (height==0) 0 else contents(0).length&#125;/*到底是使用字段还是方法,这取决于类的使用情况访问字段比调用方法略快,因为字段值在类初始化的时候被预计算,而方法调用在每次调用的时候都要计算使用字段需要为每个Element对象分配更多的内存空间*//*特别地,如果类的字段变为了访问方法,只要访问方法是纯的,即没有副作用且不依赖可变状态,那么Element类的客户端代码就不需要重写,客户端代码不应该关心到底用哪种方式实现的*///统一访问原则:str.length //可以说是调用了str的对象的一个字段(属性)str.length //也可以说是调用了str对象的无参数方法/*scala在遇到混合了无参数和空括号方法的情况时很自由,特别是,你可以用空括号方法重写无参数方法,反之亦然,你还可以在调用任何不带参数的方法时省略空的括号,如下:*/Array(2, 3, 4).toString&quot;abc&quot;.length/*原则上,scala的函数调用中可以省略所有的空括号,然而,在调用的方法超出其调用者对象的属性时(即:不能看做是调用者的属性的空参数函数),推荐仍然写一对空的括号,例如:如果方法执行了I/O,或写入了可重新赋值的变量(var),或读取不是调用者字段你的var,总之无论是直接还是非直接的使用可变对象,都应该添加空括号*/&quot;hello&quot;.length //没有副作用,所以无需()println() //最好别省略() , 因为他的副作用就是打印//如果你调用的函数执行了操作就使用括号,但如果仅提供了对某个属性的访问,那么省略括号 4.扩展类12345678910/*上面创建的抽象类,我们不能实例化,所以需要创建一个新的类,继承了上面的抽象类*/class ArrayElement(conts: Array[String]) extends Element &#123; def contents = conts&#125;/*extends子句有两个效果,使得ArrayElement类继承Element类的所有非私有的成员,并且让ArrayElement类型成为Element类型的子类型,因为ArrayElement扩展了Element,所以ArrayElement类被称为Element类的子类,反过来,Element是ArrayElement的超类如果你省略extends的子句,scala编译器将隐式的假设你的类扩展子scala.AnyRef,这与Java平台上的Java.lang.Object 相同 */ 1234567891011121314151617/*哪些超类的成员不会被继承:1.超类的私有成员不会被子类继承,2.超类中的成员若与子类中实现的成员具有相同的名称和参数则不会被子类继承,这种情况下被称为子类的成员重写了超类的成员,*//*如果子类的成员是具体的而超类的中是抽象的,我们可以具体的成员实现了抽象的成员*/val ae = new ArrayElement(Array(&quot;hello&quot;, &quot;world&quot;))ae.width //使用//子类型化:指子类的值可以在任何需要其超类的值的地方使用val e: Element = new ArrayElement(Array(&quot;hello&quot;, &quot;world&quot;))//其实类似于Java中的多态 5.重写方法和字段12345678910111213141516171819202122232425/*统一访问原则只是scala在对待字段和方法上比Java更统一的一个方面,另一个差异是scala里的字段和方法属于相同的命名空间,这让字段可以重写无参数方法,例如,你可以通过改变ArrayElement类中的contents的实现将其从一个方法变成一个字段,而无需修改类Element中的contents的抽象方法定义,如下:*/class ArrayElement(conts: Array[String]) extends Element &#123; val contents:Array[String] = conts&#125;//在scala里禁止在同一个类里用同样的名称定义字段和方法,尽管Java中可以允许你这样做,但是scala中将不能编译通过class WontCompile(conts: Array[String]) extends Element &#123; private var f = 0 //不能编译通过,因为字段和方法同名 def f = 1&#125;/*命名空间:Java准备了四个命名空间(分别是字段/方法/类型和包)scala仅有两个命名空间:值(字段,方法,包还有单例对象)类型(类和特质)scala把字段和方法放进同一个命名空间的理由很明确,因为这样你就可以实现使用val重写无参数方法这种你在Java里做不到的事情*/ 6.定义参数化字段12345678910111213141516171819202122232425262728class ArrayElement(conts: Array[String]) extends Element &#123; def contents = conts&#125;/*在上述代码中需要定义字段contents时需要将conts再重新赋值给contents,这样的做法有点不必要的累赘和重复,可以采用如下的定义:*/class ArrayElement(val contents: Array[String]) extends Element //注意contents参数的前缀是val,ArrayElement类现在拥有了一个可以从类外部访问的字段contents,字段使用参数值初始化//与下面的写法类似class ArrayElement(x123: Array[String]) extends Element&#123; val contents: Array[String] = x123&#125;/*类的参数同样也可以使用var做前缀,这种情况下相应的字段可以被重新赋值,这些参数化字段还可以添加如:private,protected,或override这类的修饰符*/class Tiger ( override val dangerous:Boolean, private var age:Int)extends Cat/*Tiger的定义是下面另一种类定义方式的简写,其中包含了重写成员dangerous和私有成员age*/class Tiger2(param1: Boolean, param2: Int) extends Cat&#123; override val dangerous = param1 private var age = param2&#125; 7.调用超类构造器12345//需求:现在想要创造由给定的单行字符串构成的布局元素class LineElement(s: String) extends ArrayElement(Array(s))&#123; override def width = s.length override def height = 1 //因为是单行,所以设置为1&#125; 8.使用override修饰符12345/*若子类成员重写了父类的具体成员则必须带有这个修饰符,若成员实现的是同名的抽象成员时,则这个修饰符是可选的,若成员并未重写或实现什么其他基类里的成员则禁用这个修饰符*/ 9.多态1234567891011/*在上面的例子中可以看到Element类型的变量可以指向ArrayElement类型的对象,这种现象叫做多态,Element对象可以有许多形式,目前为止,你已经看到了两种形式:ArrayElement,和LineElement,你可以定义新的Element子类创造Element的更多形式,例如:下面给出了如何定义拥有给定长度和高度并充满指定字符的新的Element*/class UniformElement( ch:Char, override val width: Int, override val height: Int) extends Element&#123; private val line = ch.toString * width def contents = Array.make(height, line)&#125; 10.定义final成员123456789101112131415161718/*有时你想要确保一个成员不被子类重写,在scala中可以和Java一样通过给成员添加final修饰符来实现*///如果不想子类重写父类的方法,可以在方法上添加finalclass ArrayElement extends Element&#123; final override def demo(): Unit =&#123; println(&quot;ArrayElement&apos;s implementation invoked&quot;) &#125;&#125;//这样如果ArrayElement的子类如果想要重写demo方法,是无法编译通过的//如果想要确保整个类都不会有子类,那么需要在类的声明上添加finalfinal class ArrayElement extends Element&#123; final override def demo(): Unit =&#123; println(&quot;ArrayElement&apos;s implementation invoked&quot;) &#125;&#125;//class LineElement extends ArrayElement 将编译不通过 11.使用组合与继承1234567891011121314/*组合和继承是利用其它现存类定义新类的两个方法,如果你追求的是根本上的代码重用,那么通常更推荐采用组合而不是继承,只有继承才受累于脆基类问题,因为你可能在改造超类时无意中破坏了子类*/class LineElement(s: String) extends Element&#123; val contents = Array(s) override def width = s.length override def height = 1&#125;/*在前一个版本中,LineElement与ArrayElement间是继承关系,LineElement从ArrayElement那里继承了contents,现在他与Array是组合关系,在它自己的contents字段中持有字符串数组的引用*/ 12.实现above,beside,和toString12345678910111213141516171819202122232425//above是将两个Element上下连接在一起def above(that: Element): Element = &#123; new ArrayElement(this.contents ++ that.contents) // ++操作符是连接两个数组,&#125;//beside将创造出一个新的元素,新元素的每一行都来自两个原始元素的相应行的串联,为简单起见,我们首先假设两个元素高度相同,这样就可以如下设计beside方法:def beside(that: Element):Element =&#123; val contents = new Array[String](this.contents.length) for (i &lt;- 0 until this.contents.length) contents(i) = this.contents(i) + that.contents(i) new ArrayElement(contents)&#125;//上面的代码是指令式的,如下的采用的是函数式new ArrayElement( for( (line1, line2) &lt;- this.contents zip that.contents ) yield line1+line2)//上述代码避免了显示的数组索引(使用索引可能有角标越界的情况出现),因此出错的机会更少//toString方法override def toString: String = contents.mkString(&quot;\\n&quot;)//注意:toString没有带空参数列表,这遵循了统一访问原则的建议,因为toString是一个不带任何参数的纯方法 13.定义工厂对象123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869/*工厂对象包含了构建其他对象的方法,客户端将使用这些工厂方法构造对象而不是直接使用new构造对象,这种方式的一个好处就是可以将对象的创建集中化并且隐藏对象实际代表的类的细节这种隐藏一方面可以让客户端更容易理解你的库,因为暴露的细节更少了,另一方面,你更多的机会可以在不破坏客户端代码的前提下改变库的实现*///工厂方法应该放在何处?//一种直接的方法是创建Element类的伴生对象并把它作为布局元素的工厂方法,使用这种方式,你唯一要暴露给客户端的就是Element的类/对象的组合,隐藏ArrayElement,LineElement,和UniformElement三个类的实现object Element&#123; def elem(contents:Array[String]):Element = new ArrayElement(contents) def elem(chr:Char, width: Int, height: Int ): Element = new UniformElement(chr,width,height) def elem(line: String): Element = new LineElement(line)&#125;/*为了直接调用工厂方法而不必使用单例对象的名臣Element做限定,我们将在源文件的开始引用Element.elem,那么在Element类的内部,只要使用elem就可以调用工厂方法*/import Element.elemabstract class Element&#123; def contents: Array[String] def height: Int = contents.length def width: Int = if (height==0) 0 else contents(0).length def above(that: Element): Element = &#123; elem(this.contents ++ that.contents) &#125; def beside(that: Element):Element =&#123; elem( for( (line1, line2) &lt;- this.contents zip that.contents ) yield line1+line2 ) &#125; override def toString: String = contents.mkString(&quot;\\n&quot;)&#125;/*有了上述的工厂方法之后,子类ArrayElement,LineElement,和UniformElement就可以是私有的,因为他们不再需要直接被客户访问,在scala中,你可以在类和单例对象的内部定义其他的类和单例对象,因此一种让Element的子类私有化的方式就是把他们放在Element单例对象中并在那里声明他们为私有的,如下:*/object Element&#123; private class ArrayElement(x123: Array[String]) extends Element&#123; val contents: Array[String] = x123 &#125; private class LineElement(s: String) extends Element&#123; val contents = Array(s) override def width = s.length override def height = 1 &#125; private class UniformElement( ch:Char, override val width: Int, override val height: Int ) extends Element&#123; private val line = ch.toString * width def contents = Array.make(height, line) &#125; def elem(contents:Array[String]):Element = new ArrayElement(contents) def elem(chr:Char, width: Int, height: Int ): Element = new UniformElement(chr,width,height) def elem(line: String): Element = new LineElement(line)&#125;","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十六章 使用列表","date":"2017-04-16T04:47:25.913Z","path":"2017/04/16/scala编程/第十六章 使用列表/","text":"1.列表字面量123456789101112val fruit = List(&quot;apples&quot;, &quot;oranges&quot;, &quot;pears&quot;)val nums = List(1,2,3,4)val diag3 = List( List(1,0,1), List(0,1,0), List(0,0,1) )val empty = List()/*列表与数组非常相似,不过有两点重要的差别,首先,列表时不可变的,也就是说,不能通过赋值改变列表的元素,其次列表具有递归的结构,而数组是连续的 */ 2.list类型1234567891011121314151617181920/*列表的所有元素具有相同的类型,元素类型为T的列表类型写成List[T] ,例如:以下四个列表的例子都明确指定了类型:*/val fruit: List[String] = List(&quot;apples&quot;, &quot;oranges&quot;, &quot;pears&quot;)val nums: List[Int] = List(1,2,3,4)val diag3: List[List[Int]] = List( List(1,0,1), List(0,1,0), List(0,0,1) )val empty = List()/*scala里的列表类型是协变的,这意味着对于每一对类型S和T来说,如果S是T的子类型,那么List[S] 是List[T]的子类型,比如说,List[String] 是List[Object]的子类型,这很自然,因为每个字符串列表都同样可以被看做是对象列表注意空列表的类型为List[Nothing] ,Nothing是scala的类层级的底层类型,他是每个scala类型的子类,因为列表时协变的,所以对于任意类型的T的列表List[T]来说,List[Nothing] 都是其子类,因此类型为List[Nothing]的空列表对象,还可以被当做是其他任何形式为List[T] 的列表类型的对象,这也是为何如下的代码是正确的*///List() 同样也是List[String] 的val xs: List[String] = List() 3.构造列表12345678910111213141516/*所有的列表都是由两个基础构造块Nil和:: 构造出来的,Nil代表空列表,中缀操作符:: ,表示列表从前端扩展,也就是说, x::xs 代表了第一个元素为x ,后面跟着xs的列表,因此之前的列表值可以如下的方式定义:*/val fruit = &quot;apple&quot; :: (&quot;orange&quot;::(&quot;pears&quot;::Nil))val nums = 1::(2::(3::(4::Nil)))val diag3 = (1::(0::(1::Nil))):: (0::(1::(0::Nil))):: (0::(0::(1::Nil)))::Nilval empty = Nil//实际上之前的List(...) 形式是对fruits,nums, diag3 和empty的定义只不过是扩展为这些定义的包装,例如:List(1,2,3)创建了列表 1::(2::(3::Nil))/*由于以冒号结尾, :: 操作遵循右结合规则: A::B::C 等同于 A::(B::C) 因此,你可以去掉前面定义里用到的括号,例如:*/val nums = 1::2::3::4::Nil 4.列表的基本操作123456//对列表的所有操作都可以表达Wie以下三种形式head //返回列表的第一个元素tail //返回除第一个之外所有元素组成的列表isEmpty //如果列表为空,则返回真//这些操作都定义了List类的方法,如下表: 操作 行为 empty.isEmpty 返回true fruits.isEmpty 返回false fruits.head 返回”apples” fruits.tail.head 返回”orange” diag3.head 返回List(1,0,0) 1234567891011121314151617181920/*head和tail方法仅能够作用在非空列表上,如果执行在空列表上,会抛出异常:*/scala&gt; Nil.headjava.util.NoSuchElementException: head of empty list/*可以考虑把数值列表以升序的方式排序,较简单的做法是插入排序工作方式如下:排序非空列表x::xs , 可以先排序列表xs,然后把x插入正确的地方,而对空列表的排序结果还是空列表:插入排序算法大致如下:*/def isort(xs: List[Int]): List[Int] = if (xs.isEmpty) Nil else insert(xs.head, isort(xs.tail))def insert(x: Int, xs: List[Int]): List[Int] = if (xs.isEmpty || x &lt;= xs.head) x::xs else xs.head :: insert(x, xs.tail) 5.列表模式1234567891011121314151617/*列表还可以使用模式匹配做拆分,这时列表模式需要逐一匹配要拆分的列表表达式,你既可以用List(...) 形式的模式对列表所有的元素做匹配,也可以用:: 操作符和Nil常量组成的模式逐位拆分列表*/scala&gt; val fruit = &quot;apple&quot; :: (&quot;orange&quot;::(&quot;pears&quot;::Nil))fruit: List[String] = List(apple, orange, pears)scala&gt; val List(a,b,c) = fruita: String = appleb: String = orangec: String = pears//如果起先不知道列表元素的数量,那么最好还是使用:: 做匹配,例如, 模式a::b::rest可以匹配长度至少为2的列表scala&gt; val a::b::rest = fruita: String = appleb: String = orangerest: List[String] = List(pears) List的模式匹配123456789101112131415//使用模式匹配实现插入排序def isort(xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List() case x::xsl =&gt; insert(x, isort(xsl))&#125;def insert(x: Int, xs: List[Int]): List[Int] = xs match &#123; case List() =&gt; List() case y::ys =&gt; if (x&lt;=y) x::xs else y::insert(x,ys)&#125;/*通常,采用模式匹配做拆分会比使用那些基本方法更为清晰,因此你的列表处理工具箱中应该加入模式匹配这样的工具*/ 6.list类的一阶方法1//一阶方法是指不以函数作为传入参数的方法 连接列表12345678910111213141516//连接操作是与 :: 接近的一种操作,写做&quot; ::&quot; 不过不像 ::: 的两个操作都是列表, xs:::ys 的结果依次包含xs和ys所有元素的新列表scala&gt; List(1,2):::List(3,4,5)res20: List[Int] = List(1, 2, 3, 4, 5)scala&gt; List():::List(1,2,3)res21: List[Int] = List(1, 2, 3)scala&gt; List(1,2,3):::List(4)res22: List[Int] = List(1, 2, 3, 4)//与:: 一样,列表的连接操作也是右结合的,如下:xs:::ys:::zs//等价于xs:::(ys:::zs) 分治原则12345678910111213/*连接操作(:::) 被实现为List类的方法,下面是通过使用模式匹配来&quot;手工&quot;实现*/def append[T](xs: List[T], ys: List[T]): List[T]/*列表的许多算法首先使用模式匹配把输入列表拆分为更简单的样本,这是原则里所说的&quot;分&quot;,然后根据每个样本构建结果,如果结果是非空列表,那么一块块部件将通过同样的递归遍历算法构建出来,这就是原则里说的&quot;治&quot;*/def append[T](xs: List[T], ys: List[T]): List[T] = xs match &#123; case List() =&gt; ys case x::xsl =&gt; x::append(xsl,ys)&#125; 计算列表的长度:length方法123456scala&gt; List(1,2,3).lengthres23: Int = 3/*相对于数组来说,列表的length方法是较费时的操作,为了找到尾部,需要遍历整个列表,因此其花费的时间和列表元素数量成正比,这也是在判断列表是否为空时,应当采用xs.isEmpty方法,而不采用xs.length==0的理由,虽然两种测试的结果一致,但是第二种更加的慢,尤其是列表xs较长的时候*/ 访问列表的尾部:init方法和last方法123456789101112131415161718192021222324/*你已经知道了head和tail基本操作,相应可以获得列表的第一个元素及除了第一个元素之外余下的列表,他们都有成对的操作: last返回(非空)列表的最后一个元素,init返回除了最后一个元素之外余下的列表*/scala&gt; val abcde = List(&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;)abcde: List[Char] = List(a, b, c, d, e)scala&gt; abcde.lastres24: Char = escala&gt; abcde.initinit initsscala&gt; abcde.initres25: List[Char] = List(a, b, c, d)//与head和tail一样的是,对空列表调用这些方法的时候,会抛出异常scala&gt; List().initjava.lang.UnsupportedOperationException: empty.init//不一样的是,head和tail运行的时间都是常量,但是init和last需要遍历整个列表以计算结果,因此所耗的时间与列表长度成正比//组织好数据,以便让所有的访问都集中在列表的头部,而不是尾部 反转列表:reverse方法12345678910//如果出于某种原因,某种算法需要频繁的访问列表的尾部,那么可以首先把列表反转过来然后再处理,如下:scala&gt; abcderes29: List[Char] = List(a, b, c, d, e)scala&gt; abcde.reverseres27: List[Char] = List(e, d, c, b, a)//与所有其他列表操作一样,reverse创建了新的列表而不是就地改变被操作列表scala&gt; abcderes28: List[Char] = List(a, b, c, d, e) 前缀与后缀:drop,take,和splitAt123456789101112131415161718/*drop和take操作泛化了tail和init,他们可以返回列表任意长度的前缀或后缀,表达式&quot; xs take n &quot; 返回xs列表的前n个元素,如果n大于xs.length,则返回整个xs,操作 &quot; xs drop n&quot;返回xs列表除了前n个元素之外的所有元素,如果n大于xs.length,则返回空列表splitAt操作在指定位置拆分列表,并返回对偶列表xs splitAt n //等价于 ( xs take n, xs drop n)*/scala&gt; abcderes29: List[Char] = List(a, b, c, d, e)scala&gt; abcde take 2res30: List[Char] = List(a, b)scala&gt; abcde drop 2res31: List[Char] = List(c, d, e)scala&gt; abcde splitAt 2res32: (List[Char], List[Char]) = (List(a, b),List(c, d, e)) 元素选择:apply方法和indices方法1234567891011//apply方法实现了随机元素的元素,不过与数组中的同名方法相比,他使用的并不广泛scala&gt; abcderes29: List[Char] = List(a, b, c, d, e)abcde(2) // c//indices 方法可以返回指定列表的所有有效索引值组成的列表scala&gt; abcde.indicesres35: scala.collection.immutable.Range = Range(0, 1, 2, 3, 4) 齿合列表:zip123456789101112//zip操作可以把两个列表组成一个对偶列表scala&gt; abcde.indices zip abcderes36: scala.collection.immutable.IndexedSeq[(Int, Char)] = Vector((0,a), (1,b), (2,c), (3,d), (4,e))//如果两个列表的长度不一致,那么任何不能匹配的元素将被丢弃scala&gt; val zipped = abcde zip List(1,2,3)zipped: List[(Char, Int)] = List((a,1), (b,2), (c,3))//常用到的情况是把列表元素与索引值啮合在一起,这是使用zipWithIndex方法更为有效scala&gt; abcde.zipWithIndexres37: List[(Char, Int)] = List((a,0), (b,1), (c,2), (d,3), (e,4)) 显示列表:toString方法和mkString方法1234567891011121314151617181920212223242526272829303132//toString操作返回列表的标准字符串表达形式scala&gt; abcde.toStringres39: String = List(a, b, c, d, e)/*xs mkString (pre, sep, post) 操作有四个操作元,带显示的列表xs ,需要显示在所有元素之前的前缀字符串pre,需要显示在每两个元素之间的分隔符字符串sep,以及显示在最后面的后缀字符串post,操作的结果就是字符串*///mkString方法有两个重载的变体以便让你可以忽略部分乃至全部参数,第一个变体仅带有分隔符字符串:xs mkString sep //等价于 xs mkString (&quot;&quot;, sep, &quot;&quot;)//第二个变体让你可以忽略所有的参数:xx.mkString //等价于 xs mkString &quot;&quot;scala&gt; abcde mkString(&quot;++++&quot;, &quot;,&quot;, &quot;%%%%&quot;)res40: String = ++++a,b,c,d,e%%%%scala&gt; abcde mkString &quot;&quot;res42: String = abcdescala&gt; abcde.mkStringres44: String = abcde//mkString方法还有addString的变体,他可以把构建好的字符串添加到StringBuilder对象中,而不是作为结果返回scala&gt; val buf = new StringBuilderbuf: StringBuilder =scala&gt; abcde addString (buf,&quot;(&quot;,&quot;:&quot;,&quot;)&quot;)res45: StringBuilder = (a:b:c:d:e)//mkString和addString方法都继承自List的超特质Iterable ,因此他们可以应用到各种可枚举的集合类上 转换列表:elements,toArray. copyToArray1234567891011121314151617181920212223242526//要想让数据存储格式在连续存放的数组和递归存放的列表之间进行转换,可以使用List类的toArray方法和Array类的toList方法scala&gt; abcderes46: List[Char] = List(a, b, c, d, e)scala&gt; val arr = abcde.toArrayarr: Array[Char] = Array(a, b, c, d, e)scala&gt; arr.toListres47: List[Char] = List(a, b, c, d, e)//另外还有一个方法叫copyToArray ,可以把列表元素复制到目标数组的一段连续空间,操作如下:xs copyToArray (arr, start)/*这将把列表xs的所有元素复制到数组arr中,填入位置开始为start,必须确保目标数组arr有足够的空间可以全部放下列表元素,如下:*/scala&gt; val arr2 = new Array[Int](10)arr2: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)scala&gt; List(1,2,3) copyToArray (arr2,3)scala&gt; arr2res51: Array[Int] = Array(0, 0, 0, 1, 2, 3, 0, 0, 0, 0)//如果你需要使用枚举器访问列表元素,可以使用elements方法 举例:归并排序123456789101112131415161718192021//前面看到的插入排序编写起来较为简洁,但是效率不高,归并排序是更为有效的排序算法/*归并排序的工作原理:首先如果列表长度为零或仅有一个元素,他就已经是排好序的了,因此可以不加改变的返回,长列表可以拆分为两个子列表,每个包含大概一半的原列表元素,每个子列表采用对排序函数的递归调用完成排序,然后再用归并操作把生产的两个排好序的列表合并在一起*/def msort[T](less: (T, T) =&gt; Boolean)(xs: List[T]):List[T] = &#123; def merge(xs:List[T], ys:List[T]): List[T] = (xs,ys) match &#123; case (Nil,_) =&gt; ys case (_,Nil) =&gt; xs case (x::xsl, y::ysl) =&gt; if (less(x,y)) x::merge(xsl,ys) else y::merge(xs,ysl) &#125; val n = xs.length/2 if (n==0) xs else &#123; val (ys,zs) = xs splitAt n merge(msort(less)(ys), msort(less)(zs)) &#125;&#125; 7.list类的高级方法123/*对于列表的许多操作都具有相同的结构,模式重复不断的出现,例如:用某种方式转变列表的所有元素,检查列表的所有元素是否都具有某种特质,取出列表中满足特定条件的元素,或使用某种操作符合并列表的元素,在java中,这样的模式通常需要用for或while循环的固定组合表达,scala中,可以通过使用以List类的方法实现的高阶操作符来更为简洁和直接的表达这些模式*/ 列表见映射:map , flatMap, 和foreach1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950xs map f //操作以类型为List[T] 的列表xs和类型为T=&gt;U的函数f为操作单元,返回把函数f应用在xs的每个列表元素之后由此组成的新列表scala&gt; List(1,2,3) map (_+1)res0: List[Int] = List(2, 3, 4)scala&gt; val words = List(&quot;the&quot;, &quot;quick&quot;, &quot;brown&quot;, &quot;fox&quot;)words: List[String] = List(the, quick, brown, fox)scala&gt; words map (_.length)res1: List[Int] = List(3, 5, 5, 3)scala&gt; words map (_.toList.reverse.mkString)res2: List[String] = List(eht, kciuq, nworb, xof)//flatMap操作符与map类似,不过他的右操作元是能够返回元素列表的函数,他对列表的每个元素调用该方法,然后连接所有方法的结果并返回,map和flatMap的差异如下:scala&gt; words map (_.toList)res3: List[List[Char]] = List(List(t, h, e), List(q, u, i, c, k), List(b, r, o,w, n), List(f, o, x))scala&gt; words flatMap (_.toList)res4: List[Char] = List(t, h, e, q, u, i, c, k, b, r, o, w, n, f, o, x)//可以发现map返回的是包含列表的列表,而flatMap返回的是把所有元素列表连接之后的单个列表scala&gt; List.range(1,5) flatMap ( i=&gt; List.range(1,i) map (j =&gt; (i,j)) )res5: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))//List.range是可以创建某范围内所有整数列表的工具方法scala&gt; List.range(1,1)res6: List[Int] = List()scala&gt; List.range(1,2)res7: List[Int] = List(1)//请注意:使用for表达式也能得到同样的列表scala&gt; for(i&lt;-List.range(1,5);j&lt;-List.range(1,i)) yield (i,j)res8: List[(Int, Int)] = List((2,1), (3,1), (3,2), (4,1), (4,2), (4,3))/*foreach是第三种与映射类似的操作,然而不像map和flatMap,foreach的右操作元是过程(返回类型为Unit的函数),他只是对每个列表元素都调用一遍过程,操作的结果仍然是Unit,不会产生结果列表*/var sum = 0List(1,2,3,4,5) foreach (sum += _) 列表过滤:filter,partition, find, takeWhile, dropWhile和span123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354/*xs filter p //操作把类型为List[T]的列表xs和类型为T=&gt;Boolean的判断函数作为操作元,产生xs中符合p(x) 为true的所有元素x组成的列表*/scala&gt; List(1,2,3,4,5) filter (_ % 2==0)res9: List[Int] = List(2, 4)scala&gt; wordsres10: List[String] = List(the, quick, brown, fox)scala&gt; words filter(_.length == 3)res11: List[String] = List(the, fox)/*partition方法与filter方法类似,不过返回的是列表对,其中一个包含所有论断为真的元素,另外一个包含所有论断为假的元素*/xs partition p //等价于 (xs filter p, xs filter (!p(_))) //相当于分组,返回p成立的组,和p不成立的组scala&gt; List(1,2,3,4,5) partition (_%2==0)res12: (List[Int], List[Int]) = (List(2, 4),List(1, 3, 5))xs find p/*find方法同样与filter类似,不过返回的是第一个满足给定论断的元素,而并非全部, xs find p 操作以列表xs和论断p为操作元,返回可选值,如果xs中存在元素x使得p(x) 为真, Some(x) 将被返回,若p对所有的元素都不成立,None将返回*/scala&gt; List(1,2,3,4,5) find (_ %2 ==0)res14: Option[Int] = Some(2)scala&gt; List(1,2,3,4,5) find (_ &lt;= 0)res15: Option[Int] = None/*takeWhile和dropWhile操作符同样带有论断做右操作元,xs takeWhile p 操作返回列表xs 中最长的更够满足p的前缀,类似的,xs dropWhile p 操作符最长的不满足p的前缀*/scala&gt; List(1,2,3,-4,5) takeWhile (_ &gt; 0)res16: List[Int] = List(1, 2, 3)scala&gt; wordsres17: List[String] = List(the, quick, brown, fox)scala&gt; words dropWhile (_ startsWith &quot;t&quot;)res18: List[String] = List(quick, brown, fox)/*span 方法把takeWhile和dropWhile组合成一个操作,就好像splitAt组合了take和drop一样,他返回一对列表*/xs span p // 等价于 (xs takeWhile p, xs dropWhile p)scala&gt; List(1,2,3,-4,5) span (_ &gt; 0)res19: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5)) 列表的论断:forall和exists12345/*操作xs forall p 以列表xs和论断p为参数,如果列表的所有元素满足p则返回true,与之相对,在xs exists p操作里,xs 中只要有一个值满足论断p就返回true*/def hasZeroRow(m: List[List[Int]]) = m exists(row =&gt; row forall (_ == 0)) 折叠列表: /: 和:\\12345678910111213141516/*其他常用的操作会对列表元素始终执行某种操作*/sum(List(a, b, c)) //等价于 0+a+b+c//下面可以认为是折叠操作的一个具体实例def sum(xs: List[Int]): Int = (0 /: xs)(_ + _)product(List(a, b, c)) //等价于 1*a*b*c//下面是具体实现:def produce(xs: List[Int]): Int = (0 /: xs)(_ * _)/*左折叠(fold left) 操作 (z /: xs)(op) 与三个对象有关,开始值z , 列表xs , 以及二元操作op,折叠的结果是op应用到前缀值z及每个相邻元素上,如下:*/(z /: List(a, b, c))(op) //等价于 下面的图形操作 1234567891011121314//另一个例子: 用空格连接所有字符串列表中的所有单词scala&gt; (&quot;&quot; /: words)(_ + &quot; &quot; + _)res21: String = &quot; the quick brown fox&quot;//结果在最开始的地方多了一个空格,要去掉他,可以稍微改变为:scala&gt; (words.head /: words.tail)(_ +&quot; &quot;+ _)res22: String = the quick brown fox/*与上面类似的操作是: :\\ ,表示如下:*/(List(a, b, c) :\\ z)(op) //等价于 下图 123456789/*看冒号在哪一边,与冒号连接时列表,用于连接的字符在左侧就是左折叠,用于连接的字符在右侧就是右折叠*//*最后,尽管/:和:\\操作符已经从斜杠的方向上描绘了他们的操作树的倾斜方向,并且冒号字符的结合性也保证了开始值在操作树和表达式中同样的位置,但还是有很多人感觉产生与直觉相去甚远,因此如果你喜欢,可以使用名为foldLeft和foldRight的方法,他们同样定义在List类中*///foldLeft的方法签名def foldLeft[B](z: B)(f: (B, A) =&gt; B): B = &#123; 例子:使用折叠操作完成列表反转12345678910111213/*通过折叠实现列表反转*/def reverseLeft[T](xs: List[T]) = (startValue /: xs)(operation)//实现如下:def reverseLeft[T](xs: List[T]) = (List[T]() /: xs)&#123; (ys,y) =&gt; y::ys&#125;//或者:下面使用了圆括号def reverseLeft[T](xs: List[T]) = (List[T]() /: xs)( (ys,y) =&gt; y::ys ) 列表排序: sort123456789/*对列表xs的操作, xs sort before 可以对列表的元素执行排序,其中 &quot;before&quot; 是比较元素的方法,表达式 x beforey在x应按照顺序处于y之前的时候要能够返回true */List(1, -3, 4, 2, 6) sort (_ &lt; _) //List(-3, 1, 2, 4, 6)/*解析: fn(x,y) = _ &lt; _ 是一个函数最后的结果是 x before y 为true ,所以可以看出是升序*/ 8.list对象的方法&emsp;目前为止,上面所看到的所有操作都实现为List类的方法,因此你是在独立的类对象上调用他们,还有些方法是定义在全局可访问对象scala.List上的,他是List类的伴生对象,其中的一些操作是创建列表的工厂方法,另外一些是对某些特定类型列表的操作 通过元素创建列表: List.apply1234/*你已经在很多场合看到过以List(1,2,3) 形式出现的列表字面量,他的语法没有什么特别的,类似于List(1,2,3)这样的字面量只是List对象对元素1,2,3的简单应用,也就说,他等价于List.apply(1,2,3)*/List.apply(1,2,3) 创建数值范围: List.range12345678910111213/*最简单的形式是List.range(from, until) ,可以创建从from开始到until-1 的所有数值的列表,因此尾部值until不再范围之内还有一个版本的range可以带step值作为第三参数,这个操作可以产生from开始的,间隔为step的列表元素,step可以为正,也可以为负*/scala&gt; List.range(1,5)res24: List[Int] = List(1, 2, 3, 4)scala&gt; List.range(1,9,2)res25: List[Int] = List(1, 3, 5, 7)scala&gt; List.range(9,1,-3)res26: List[Int] = List(9, 6, 3) 创建统一的列表: List.make123456789101112/*make方法创建由相同元素的零份或多份拷贝组成的列表,他带两个参数: 待创建列表的长度,需重复的元素*/scala&gt; List.make(5,&apos;a&apos;)warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres28: List[Char] = List(a, a, a, a, a)scala&gt; List.make(3,&quot;hello&quot;)warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres29: List[String] = List(hello, hello, hello) 解除啮合列表:List.unzip12345678/*unzip操作是zip的相反,zip把两个列表组成对偶列表,unzip把对偶列表拆分还原为两个列表,其中一个列表由每对对偶的第一个元素组成,另一个由第二个元素组成*/scala&gt; val zipped = &quot;abcde&quot;.toList zip List(1,2,3)zipped: List[(Char, Int)] = List((a,1), (b,2), (c,3))scala&gt; List.unzip(zipped)res31: (List[Char], List[Int]) = (List(a, b, c),List(1, 2, 3)) 连接列表: List.flatten, List.concat123456789101112131415161718/*flatten方法以列表的列表做参数,并把所有的元素列表连接在一起*/scala&gt; List.flatten(test)warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres32: List[Char] = List(a, b, c, d, e)//concat方法与flatten类似,它能够连接多个元素列表,将多个列表以重复参数的形式直接传递给方法,数量不限scala&gt; List.concat(List(&apos;a&apos;,&apos;b&apos;), List(&apos;c&apos;))res33: List[Char] = List(a, b, c)scala&gt; List.concat(List(),List(&apos;b&apos;),List(&apos;c&apos;))res34: List[Char] = List(b, c)scala&gt; List.concat()res35: List[Nothing] = List() 映射及测试配对列表: List.map2, List.forall2 , List.exists212345678910111213// map2方法与map相似,不过他同时带两个列表及能够把两个元素值映射为结果的函数做参数,函数会应用到两个列表相关的元素上,然后把这些结果值变为列表scala&gt; List.map2(List(10,20),List(7,4,5))(_ * _) //分别取两个list的参数,然后交给 _*_ warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres37: List[Int] = List(70, 80)scala&gt; List.forall2(List(&quot;abc&quot;,&quot;de&quot;),List(3,2))(_.length == _) //分别取两个list的参数,交给 _.length == _warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres38: Boolean = truescala&gt; List.exists2(List(&quot;abc&quot;,&quot;de&quot;),List(3,2))(_.length != _)warning: there were 1 deprecation warning(s); re-run with -deprecation for detalsres40: Boolean = false","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十五章 样本类和模式匹配","date":"2017-04-16T04:47:25.911Z","path":"2017/04/16/scala编程/第十五章 样本类和模式匹配/","text":"1.简单的例子12345678910111213141516171819202122232425abstract class Exprcase class Var(name: String) extends Exprcase class Number(num: Double) extends Exprcase class UnOP(operator: String, arg: Expr) extends Exprcase class BinOp(operator: String, left: Expr, right: Expr) extends Expr/*上述每个class类都有一个case修饰符,带有这种修饰符的类被称为样本类(case class) ,这种修饰符可以让scala的编译器自动为你的类添加一些语句上的便捷设定1.他会添加与类名一致的工厂方法,比如:写成Var(&quot;x&quot;)来构造Var对象以替代稍长一些的new Var(&quot;x&quot;) val v = Var(&quot;x&quot;)2.样本类参数列表中的所有参数隐式获得了val前缀,因此他被当做你字段维护v.nameop.left3.编译器为你的类添加了方法toString,hashCode和equals的自然实现,他们能够打印,哈希和比较由类及其所有参数组成的整棵树,因为scala里的==始终直接转到equals,这也就特别意味着样本类的元素一直是在做结构化的比较val op = BinOp(&quot;+&quot;, Number(1), v)println(op) //BinOp(+, Number(1.0), Var(x))op.right == Var(&quot;x&quot;) //true*//*所有这些转换以极低的代价带来了大量的便利,代价就是必须写case修饰符并且你的类和对象都会变得稍微大一点,变大的原因是因为产生了附加的方法及对于每个构造器参数添加了隐含的字段,不过样本类最大的好处还在于它们能够支持模式匹配*/ 模式匹配&emsp;假如你想简化之前看到的数学表达式,可能有许多的简化规则,以下三条规则只是作为演示:12345678910111213141516171819202122232425Unop(&quot;-&quot;, Unop(&quot;-&quot;, e)) =&gt; e //双重负号BinOp(&quot;+&quot;, e, Number(0)) =&gt; e //加0BinOp(&quot;*&quot; , e, Number(1)) =&gt;e //乘1//在函数方面的应用def simplifyTop(expr: Expr): Expr = expr match &#123; case UnOp(&quot;-&quot;, UnOp(&quot;-&quot;, e)) =&gt; e //双重负号 case BinOp(&quot;+&quot;, e, Number(0)) =&gt; e //加0 case BinOp(&quot;*&quot;, e, Number(1)) =&gt; e //乘1 case _ =&gt; expr&#125;/*simplifyTop右侧的部分组成了match表达式,match对应于Java里的switch,不过他写在选择器表达式之后,也就是说://java选择器 match &#123;备选项&#125;//scalaswitch &#123;选择器&#125; &#123;备选项&#125;*//*一个模式匹配包含了一系列备选项,每个都开始于关键字case,每个备选项都包含了一个模式及一到多个表达式,他们将在模式匹配过程中被计算,箭头符号 =&gt; 隔开了模式和表达式*/ match与switch的比较1234567891011121314/*匹配表达式可以被看做java风格switch的泛化,当每个模式都是常量并且最后一个模式可以是通配(表示为switch的default情况)的时候,java风格的switch可以被自然的表达为match表达式,需要记住三点1.match是scala的表达式,也就是说,他始终以值作为结果2.scala的备选项表达式永远不会&quot;掉到&quot; 下一个case,这一点不同于java的switch3.如果没有模式匹配,MatchError异常会被抛出,所以对模式匹配至少添加一个默认情况*/expr match &#123; case BinOp(op, left, right) =&gt; println(expr + &quot;is a binary operation&quot;) case _ =&gt;&#125;/*上述代码中第二个情况是必须的,否则的话,match表达式将在每个expr参数不是BinOp的时候抛出MatchError,在这个例子里,对于第二种情况没有指定代码,因此如果跑到了这里就什么都不做,每个情况的结果都是unit值 () ,因此这也就是整个match表达式的结果*/ 2.模式的种类 通配模式 1234567891011//通配模式(_)匹配任意对象,他被用作默认的&quot;全匹配&quot;的备选项expr match &#123; case BinOp(op, left, right) =&gt; println(expr + &quot;is a binary operatoin&quot;) case _ =&gt;&#125;//通配模式还可以用来忽略对象中你不关心的部分,如,前一个例子实际上并不关心二元操作符的元素是什么,只是检查是否为二元操作符,因此使用了通配符指代如下:expr match &#123; case BinOp(_, _, _) =&gt; println(expr + &quot;is a binary operatoin&quot;)//并没有使用匹配到的元素,所以使用了通配符去 case _ =&gt;println(&quot;it&apos;s something else &quot;)&#125; 常量模式123456789101112131415161718192021222324252627/*常量模式仅匹配自身,任何字面量都可以用作常量.例如:5, true 还有&quot;hello&quot; 都是常量模式,还有任何val或单例对象也可以被用作常量,例如单例对象Nil是只匹配空列表的模式*/def describe(x: Any) = x match &#123; case 5 =&gt; &quot;five&quot; case true =&gt; &quot;truth&quot; case &quot;hello&quot; =&gt; &quot;hit&quot; case Nil =&gt; &quot;the empty list&quot; case _ =&gt; &quot;something else&quot;&#125;//打印结果scala&gt; describe(5)res0: String = fivescala&gt; describe(true)res1: String = truthscala&gt; describe(&quot;hello&quot;)res2: String = hitscala&gt; describe(Nil)res3: String = the empty listscala&gt; describe(List(2,3))res4: String = something else 变量模式123456789101112131415161718192021222324252627282930313233343536373839404142/*变量模式类似于通配符,可以匹配任意对象,scala把变量绑定在匹配的对象上,因此可以使用这个变量操作对象*/expr match &#123; case 0 =&gt; &quot;zero&quot; case somethingElse =&gt; &quot;not zero&quot; + somethingElse&#125;//常量模式也可以有符号名,我们使用Nil表示空列表,以下是一个与之相关的例子,这里模式匹配采用了常量E(2.71828)和常量Pi(3.1415926)import Math.&#123;E,PI&#125;E match &#123; case PI =&gt; &quot;strange math ? pi = &quot; + PI case _ =&gt; &quot;Ok&quot;&#125;/*scala用小写的字母开始的简单名被当做是模式变量,所有其他的引用被认为是常量*/val pi = Math.PIE match &#123; case pi =&gt; &quot;strange math? Pi=&quot; + pi case _ =&gt; &quot;ok&quot;&#125;&lt;console&gt;:13:error: unreachable code due to variable pattern &apos;pi&apos; on line 12 case _ =&gt; &quot;ok&quot; ^/*在这里编译器甚至都不会让你添加默认情况,因为pi是变量模式,他可以匹配任意输入,因为之后的_ 将永远不能访问到*//*解决的方式有两种:1.如果常量是某个对象的字段,可以在其之上用限定符前缀,例如:pi是变量模式,但是this.pi 或obj.pi虽然都开始于小写字母但都是常量,2.如果这不起作用(比如说,pi是本地变量) 还可以反引号包住变量名,例如: `pi`会再次被解释为常量,而不是变量*/E match &#123; case `pi` =&gt; &quot;strange math ? pi = &quot; + PI case _ =&gt; &quot;Ok&quot;&#125; 构造器模式12345678/*这种模式意味着scala模式支持深度匹配,这种模式不只是检查顶层对象是否一致,还会检查对象的内容是否匹配内层的模式,由于额外的模式自身可以形成构造器模式,因此可以使用它们检查到对象内部的任意深度,例如:如下的代码,检查了对象的顶层是BinOp ,以及他的第三个构造器参数是Number ,以及他的值为数字0,这个模式仅有一行却能够检查三层深度*/expr match &#123; case BinOp(&quot;+&quot;, e, Number(0)) =&gt; println(&quot;a deep match&quot;) case _ =&gt;&#125; 序列模式12345678910111213//你也可以像匹配样本类那样匹配如List, 或 Array这样的序列类型,不过同样的语法现在可以指定模式内任意数量的元素,例如:下面的代码展示了检查开始于零的三元素列表的模式expr match &#123; case List(0, _, _) =&gt; println(&quot;found list&quot;) case _ =&gt;&#125;//如果你想匹配一个不指定长度的序列,可以指定_*作为模式的最后元素,这种古怪的模式能匹配序列中零到任意数量的元素,如下代码,展示了匹配由零开始,不计长度的任意序列的模式expr match &#123; case List(0, _*) =&gt; println(&quot;found list&quot;) case _ =&gt;&#125; 元组模式123456//如下,你可以匹配类似(a,b,c)这样的模式可以匹配任意的3-元组def tupleDemo(expr: Any) = expr match &#123; case (a, b, c) =&gt; println(&quot;matched&quot; + a + b + c) case _ =&gt;&#125; 类型模式123456789101112131415161718192021222324252627//你可以把类型模式当做类型测试和类型转换的简易替代def generalSize(x: Any) = x match &#123; case s: String =&gt; s.length case m:Map[_, _] =&gt; m.size case _ =&gt; 1&#125;//使用scala&gt; generalSize(&quot;abc&quot;)res8: Int = 3scala&gt; generalSize(Map(1-&gt;&apos;a&apos;, 2 -&gt;&apos;b&apos;))res9: Int = 2/*在上面的例子中,尽管x和s是指代了同样的值,不过x是Any而s是String,因此可以在模式对应的备选项表达式中写成s.length但是不能写成x.length,因为Any类型没有length成员*///替代模式匹配的例子,类型转换if (x.isInstanceOf[String])&#123; //isInstanceOf 类型测试 val s = x.asInstanceOf[String] //asInstanceOf 类型转换 s.length&#125;else //.../*上面的代码中,scala里类型测试和转换的代码真的很冗长,所以我们并不鼓励这样去做,使用带有类型模式的模式匹配通常就能够满足你的需求,尤其在需要同时做类型测试和转换的场景时,因为这两种操作都被融入一个模式匹配之中了*/ 类型擦除1234567891011121314151617181920212223242526272829303132//特定元素类型的映射能匹配吗?比如说测试给定值是否是从Int到Int的映射,让我们试一下def isIntToIntMap(x: Any) = x match &#123; case m: Map[Int, Int] =&gt; true case - =&gt; false&#125;//在scala命令行测试结果如下:&lt;console&gt;:9: warning: non-variable type argument Int in type pattern Map[Int,Int] is unchecked since it is eliminated by erasure case m: Map[Int, Int] =&gt; true ^/*scala使用了泛型的擦除模式,就如java那样,也就是说类型参数信息没有保留到运行期,因此运行期没有办法判断给定的Map对象创建时带了两个Int参数,还是带了两个其他类型的参数,系统所能做的只是判断这个值是某种任意类型参数的Map擦除规则的唯一例外就是数组,因为在scala里和java里,他们都被特殊处理了,数组的元素类型与数组值保存在一起,因此他可以做模式匹配*/def isStringArray(x: Any) = x match &#123; case a: Array[String] =&gt; &quot;yes&quot; case _ =&gt; &quot;no&quot;&#125;//测试结果scala&gt; val as = Array(&quot;abc&quot;)as: Array[String] = Array(abc)scala&gt; isStringArray(as)res11: String = yesscala&gt; val al = Array(1,3,5)al: Array[Int] = Array(1, 3, 5)scala&gt; isStringArray(al)res12: String = no 变量绑定12345678910/*除了独立的模式变量模式之外,你还可以对任何其他模式添加变量,只要简单的写上变量名,一个@符号,以及这个模式,这种写法创造了变量绑定模式,这种模式的意义在于他能够像通常的那样做模式匹配,并且如果匹配成功,则把变量设置成匹配的对象,就像使用简单的变量模式那样*/expr match &#123; case UnOp(&quot;abc&quot;, e @ UnOp(&quot;abc&quot;, _)) =&gt; e case _ =&gt;&#125;/*上述代码中使用e作为变量及UnOp(&quot;abc&quot;, _) 作为模式的变量绑定模式,如果整个模式匹配成功,那么符合UNOp(&quot;abc&quot;,_) 的部分就可以使用e指代*/ 3.模式守卫12345678910111213141516171819202122232425262728/*需求:如果你想要简化e+e的操作,写成 e*2 ,表示成Expr树的语言,就是表达式*/BinOp(&quot;+&quot;, Var(&quot;x&quot;), Var(&quot;x&quot;))//简化为BinOp(&quot;*&quot;, Var(&quot;x&quot;), Number(2))//获取你尝试如下的方式定义规则:def simplifyAdd(e: Expr) = e match &#123; case BinOp(&quot;+&quot;, x, x) =&gt; BinOp(&quot;*&quot;, x, Number(2))&#125;//编译器会报如下的错误:&lt;console&gt;: 10: error: x is already defined as value x case BinOp(&quot;+&quot;, x, x) =&gt; BinOp(&quot;*&quot;, x, Number(2)) ^/*模式变量仅允许在模式中出现一次,也就是说 case BinOp(&quot;+&quot;, x, x) 中第二个x是重复出现的,不过你可以使用模式守卫重新定制这个匹配规则*/def simplifyAdd(e: Expr) = e match &#123; case BinOp(&quot;+&quot;, x, y) if x==y =&gt; BinOp(&quot;*&quot;, x, Number(2))&#125;/*模式守卫接在模式之后,开始于if,守卫可以是任意的引用模式中变量的布尔表达式,如果存在模式守卫,那么只有在守卫返回true的时候匹配才成功,*///下面是模式匹配的其他例子case n: Int if 0&lt;n =&gt; .... //仅仅匹配正整数case s: String if s(0) == &apos;a&apos; =&gt; ... //仅仅匹配以字母&apos;a&apos;开始的字符串 4.模式重叠1234567891011121314def simplifyAll(expr: Expr): Expr = expr match &#123; case UnOp(&quot;-&quot;, UnOp(&quot;-&quot;, e)) =&gt; simplifyAll(e) case BinOp(&quot;+&quot;, e, Number(0)) =&gt; simplifyAll(e) case BinOp(&quot;*&quot;, e, Number(1)) =&gt; simplifyAll(e) case UnOp(op,e) =&gt; UnOp(op,simplifyAll(e)) case BinOp(op, left, right) =&gt; BinOp(op, simplifyAll(left), simplifyAll(right)) case _ =&gt; expr&#125;//全匹配的样本要跟在更具体的简化方法之后,例如下面的match表达式不会编译成功,因为一地个样本匹配任何能匹配第二个样本的东西def simplifyBad(expr: Expr): Expr = expr match &#123; case UnOp(op, e) =&gt; UnOp(op,simplifyBad(e)) case UnOp(&quot;-&quot;, UnOp(&quot;-&quot;, e)) =&gt; e&#125; 5.封闭类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*一旦你写好了模式匹配,你就需要确信已经考虑了所有可能的情况,有些时候你可以通过在匹配的最后添加默认处理做到这点,不过这仅仅在的确有一个合理的默认行为的情况下有效,如果没有默认的情况该怎么办?你怎样才能保证包括了所有的情况呢?可选的方案就是让样本类的超类被封闭,封闭类除了类定义所在的文件之外不能再添加任何新的子类,这意味着你仅需要关心你已经知道的子类即可*//*如果你打算做模式匹配的类层级,你应当考虑封闭他们,只要把关键字sealed放在最顶层类的前边即可,如下*/sealed abstract class Exprcase class Var(name: String) extends Exprcase class Number(num: Double) extends Exprcase class UnOp(operator: String, arg: Expr) extends Exprcase class BinOp(operator: String, left: Expr, right: Expr) extends Expr//使用实例def describe(e: Expr): String = e match &#123; case Number(_) =&gt; &quot;is a number&quot; case Var(_) =&gt; &quot;is a variable&quot;&#125;//执行结果warning: match may not be exhaustive.missing combination UnOpmissing combination BinOp/*这样的警告向你表明你的代码会有产生MatchError异常的风险,因为某些可能的模式(UnOp, BinOp)没有被处理,警告指出了潜在的运行期故障的源头,因此他通常能够帮助你正确的编程*/ ^/*然而,有些时候你或许会碰到这样的情况,编译器弹出太过挑剔的警告,例如:或许可以通过上下文知道你只会把上面的describe方法应用在仅仅是Number或Var的表达式上,因此你知道实际上不会产生MatchError,要让这些警告不再发生,你可以为方法添加用作全匹配的第三个样本,如下:*/def describe(e: Expr): String = e match &#123; case Number(_) =&gt; &quot;is a number&quot; case Var(_) =&gt; &quot;is a variable&quot; case _ =&gt; throw new RuntimeException //不会发生&#125;/*上面的做法的确有效,但是不理想,你或许对被迫添加不会被执行(至少你是这么认为的),而只是让编译器闭嘴的代码感到不爽,更轻量级的做法是给匹配的选择器表达式添加 @unchecked 注解,如下*/def describe(e: Expr): String = (e: @unchecked) match &#123; case Number(_) =&gt; &quot;is a number&quot; case Var(_) =&gt; &quot;is a variable&quot;&#125;//如果match的选择器表达式带有这个注解,那么对于随后的模式的穷举性检查将被抑制掉 6.option类型12345678910111213141516171819202122232425262728293031/*scala为可选值定义了一个名为Option的标准类型,这种值可以有两种形式,可以是Some(x) 的形式,其中x是实际值,或者也可以是None对象,代表缺失的值*/scala&gt; val capitals = Map(&quot;France&quot;-&gt;&quot;Pairs&quot;, &quot;Japan&quot;-&gt;&quot;Tokyo&quot;)capitals: scala.collection.immutable.Map[String,String] = Map(France -&gt; Pairs, Japan -&gt; Tokyo)scala&gt; capitals get &quot;France&quot;res13: Option[String] = Some(Pairs)scala&gt; capitals get &quot;North Pole&quot;res14: Option[String] = None//分离可选值,可以通过模式匹配进行def show(x: Option[String]) = x match &#123; case Some(s) =&gt; s case None =&gt; &quot;?&quot;&#125;//在scala命令行的执行结果如下scala&gt; show(capitals get &quot;Japan&quot;)res15: String = Tokyoscala&gt; show(capitals get &quot;North Pole&quot;)res16: String = ?/*java里最常用的是代表没有值的null,例如,java.util.HashMap的get方法要么返回存储在HashMap里的值,要么每找到返回null,这种方式对java起效,不过可能会隐藏错误,因为很难实际记得程序中哪个变量可以允许是null,如果变量允许为null,那么你就必须记住每次使用它的时候检查是否为null,一旦你忘记了检查,就难以避免运行时发生NullPointerException异常,又因为这种异常可能不是经常发生,所以想要通过测试发现故障是非常困难的,对于scala来说,这种方式根本不起作用,因为可以在哈希映射中存储值类型,而null不是值类型的合法元素,举例来说,HashMap[Int,Int] 不能返回null以表明&quot;没有元素&quot;*/ 7.模式无处不在&emsp;在scala中模式可以出现在很多的地方,而不单单在match表达式里 模式在变量定义中123456789101112131415161718192021222324252627/*你在定义val或var的任何时候,都可以使用模式替代简单的标识符,例如,你可以使用模式拆分元组并把其中的每个值分配给变量,如下:*/scala&gt; val myTuple = (123, &quot;abc&quot;)myTuple: (Int, String) = (123,abc)scala&gt; val (number, string) = myTuplenumber: Int = 123string: String = abcscala&gt; numberres17: Int = 123scala&gt; stringres18: String = abc//使用样本类时这种构造非常有用,如果你知道正在用的样本类的精确结构,那就可以使用模式解构他scala&gt; val exp = new BinOp(&quot;*&quot;, Number(5), Number(1))exp: BinOp = BinOp(*,Number(5.0),Number(1.0))scala&gt; val BinOp(op, left, right) = expop: String = *left: Expr = Number(5.0)right: Expr = Number(1.0) 用作偏函数的样本序列12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/*花括号的样本序列(就是备选项) 可以用在能够出现函数字面量的任何地方,实质上,样本序列就是函数字面两,而且只有更普遍,函数字面量只有一个入口点和参数列表,但样本序列可以有多个入口点,每个都有自己的参数列表,每个样本都是函数的一个入口点,参数也被模式化特化,每个入口点的函数体都在样本的右侧*/val withDefault: Option[Int] =&gt; Int = &#123; case Some(x) =&gt; x case None =&gt; 0&#125;//使用withDefault(Some(10))res25: Int = 10withDefault(None)res26: Int = 0val second: List[Int] =&gt; Int = &#123; case x::y::_ =&gt; y&#125;/*如果你编译上述代码,编译器会正确的提示说匹配并不全面,如果你传给一个三元素的列表,他的执行没有问题,但是如果传给他一个空列表就不行了*/second(List(5, 6, 7)) //6second(List()) //MatchError: List() /*你必须首先告诉编译器你知道正在使用的是偏函数,类型: List[Int] =&gt; Int 包含了不管是否为偏函数的,从整数列表到整数的所有函数仅包含从整数列表到整数的偏函数,那么应该写成PartialFunction[List[Int], Int] ,下面还是second函数,这次写成了使用偏函数类型*/val second: PartialFunction[List[Int], Int] = &#123; case x::y::_ =&gt; y&#125;/*偏函数有一个isDefinedAt 方法,可以用来测试是否函数对某个特定值有定义,本例中,函数对任何至少两个元素的列表有定义*/seconde.isDefinedAt(List(1,3,4)) //trueseconde.isDefinedAt(List()) //false/*上面的偏函数会对模式执行两次翻译,其中一次时真实函数的实现,另一次时测试函数是否有定义的实现,例如:上面的函数字面量(case x::y::_ =&gt; x) 会被翻译成下列的偏函数值*/new PartialFunction[List[Int], Int] &#123; def apply(xs: List[Int]) = xs match &#123; case x::y::_ =&gt; y &#125; def isDefinedAt(xs: List[Int]) = xs match &#123; case x::y::_ =&gt; true case _ =&gt; false &#125;&#125;//上面的翻译只有在函数字面量的声明类型为PartialFunction的时候才有效,如果声明的类型只是Function,或者根本不存在,那么函数字面量就会带而转义为完整的函数 for 表达式的模式123456789101112131415161718/*for表达式里也可以使用模式,这个for表达式从capitals映射中获得所有的键/值对,每一对都匹配模式(country,city) 并定义了两个变量country,和city*/for((country, city) &lt;- capitals) println(&quot;The capitals of &quot; + country + &quot; is &quot; + city)//不过模式同样也可能无法匹配产生的值val results = List(Some(&quot;apple&quot;), None, Some(&quot;orange&quot;))for(Some( fruit) &lt;- results ) println(fruit)appleorange/*如你在这个例子中所见,产生出来的不能匹配于模式的值被丢弃,例如:results列表里第二个元素None不匹配模式Some(Fruits) ,因此就没有出现在输出中*/ 8.一个更大的例子123/*需求:写一个计算数学表达式的例子,如&quot; x/x + 1 &quot;这样的除法,应该能够通过把被除数放在除数的顶上这种方式,被垂直打印如下*/ 1//另一个例子是:表达式 ( a/(b*c) + (1/n) ) /3的二维布局: 第一步:集中注意力在水平布局上,结构化的表达式如:1234567891011121314151617BinOp(&quot;+&quot;, BinOp(&quot;*&quot;, BinOp(&quot;+&quot;, Var(&quot;x&quot;), Var(&quot;y&quot;)), Var(&quot;z&quot;) ), Number(1))//(x+y)*z + 1 /*要想知道哪里应该加括号,代码必须知道每个操作符相对的优先级,所以首先处理他是个好主意,你可以用如下形式的映射字面量直接表达相对优先级*/Map( &quot;|&quot; -&gt; 0, &quot;||&quot; -&gt; 0, &quot;&amp;&quot; -&gt; 1, &quot;&amp;&amp;&quot; -&gt;1, ...) 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package org.stairwaybook.exprimport org.stairwaybook.exprsealed abstract class Exprcase class Var(name: String) extends Exprcase class Number(num: Double) extends Exprcase class UnOp(operator: String, arg: Expr) extends Exprcase class BinOp(operator: String, left: Expr, right: Expr) extends Exprclass ExprFormatter &#123; //包含了递增优先级的组中的操作符 private val opGroups = Array( Set(&quot;|&quot;,&quot;||&quot;), Set(&quot;&amp;&quot;, &quot;&amp;&amp;&quot;), Set(&quot;^&quot;), Set(&quot;==&quot;, &quot;!=&quot;), Set(&quot;&lt;&quot;, &quot;&lt;=&quot;, &quot;&gt;&quot;, &quot;&gt;=&quot;), Set(&quot;+&quot;, &quot;-&quot;), Set(&quot;*&quot;, &quot;%&quot;) ) //操作符到优先级的映射 private val precedence = &#123; val assocs = for ( i &lt;- 0 until opGroups.length; op &lt;- opGroups(i) ) yield op -&gt; i &#125; private val unaryPrecedence = opGroups.length private val fractionPrecedence = -1 // 表示 / 的优先级 private def format(e: Expr, enclPred: Int): Element = e match &#123; case Var(name) =&gt; elem(name) case Number(num) =&gt; def stripDot(s: String) = if (s endsWith &quot;.0&quot;) s.substring(0, s.length-2) case UnOp(op, arg) =&gt; elem(op) beside format(arg, unaryPrecedence) case BinOp(&quot;/&quot;, left, right) =&gt; val top = format(left, fractionPrecedence) val bot = format(right, fractionPrecedence) val line = elem(&quot;-&quot;, top.width max bot.wiedht, 1) val frac = top above line above bot if (enclPred != fractionPrecedence) frac else elem(&quot; &quot;) beside frac beside elem(&quot; &quot;) case BinOp(op, left, right) =&gt; val opPrec = precedence(op) val l = format(left, opPrec) val r = format(right, opPrec+1) val oper = 1 beside elem(&quot; &quot;+op+&quot; &quot;) beside r if (enclPred &lt;= opPrec) oper else elem(&quot;(&quot;) beside oper beside elem(&quot;)&quot;) &#125; def format(e: Expr): Element = format(e,0)&#125; 测试12345678910111213141516171819import org.stairwaybook.expr._object Express extends Application &#123; val f = new ExprFormatter val e1 = BinOp(&quot;*&quot;, BinOp(&quot;/&quot;, Number(1), Number(2)), BinOp(&quot;+&quot;, Var(&quot;x&quot;),Number(1)) ) val e2 = BinOp(&quot;+&quot;, BinOp(&quot;/&quot;,Var(&quot;x&quot;),Number(2)), BinOp(&quot;/&quot;, Number(1.5), Var(&quot;x&quot;)) ) val e3 = BinOp(&quot;/&quot;, e1, e2) def show(e: Expr) = println(f.format(e)+&quot;\\n\\n&quot;) for (e &lt;- Array(e1,e2,e3)) show(e)&#125; 输出结果","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十二章 特质","date":"2017-04-16T04:47:25.909Z","path":"2017/04/16/scala编程/第十二章 特质/","text":"1.特质是如何工作的?1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//特质的定义除了使用关键字trait之外,与类的定义无异trait Philosophical &#123; def philosophize() &#123; println(&quot;I consumne memory , therefore i am!&quot;) &#125;&#125;/*这个特质名为Philosophical ,他没有声明超类,因此和类一样,有个默认的超类AnyRef*///一旦特质被定义了,就可以使用extends或with关键字,把它混入类中,是&quot;混入&quot;特质而不是继承他们,因为特质的混入与那些其他语言中的多重继承有重要的差别class Frog extends Philosophical &#123; override def toString = &quot;green&quot;&#125;/*你可以使用extends关键字混入特质:这种情况下隐式地继承了特质的超类,如:Frog类是AnyRef(Philosohpical的超类)的子类并混入了Philosohpical,从特质继承的方法可以像从超类继承的方法那样使用*/val frog = new Frogfrog.philosophize()//特质同样也是类型,以下是把Philosophical用作类型的例子val phil: Philosohpical = frog //变量phil可以被初始化为任何混入了Philosohpical特质的类的对象//如果想要混入多个特质,都加在with子句里就可以了class Animaltrait HasLegsclass Frog extends Animal with Philosohpical with HasLegs &#123; override def toString = &quot;green&quot;&#125;//类Frog重写Philosohpical的philosophize方法,语法与重写超类总定义的方法一样class Animalclass Frog extends Animal with Philosohpical &#123; override def toString = &quot;green&quot; override def philosophize()&#123; println(&quot;I am easy being a man&quot;) &#125;&#125;/*你或许会得出以下的结论:特质就像是带有具体方法的Java接口,不过他其实能做更多的事情,例如:特质可以声明字段和维持状态值,但是特质有两点不同:1.特质不能有任何&quot;类&quot;参数,即传递给类的主构造器的参数class Point(x:Int, y:Int)trait NoPoint(x:Int, y:Int) //不能编译通过2.类和特质的另一个差别在于不论在类的哪个角落,super调用都是静态绑定的,而在特质中,他们是动态绑定的,如果你在类中写下&quot;super.toString&quot; ,你很明确哪个方法实现将被调用,然而如果你在特质中写了同样的东西,在你定义特质的时候super调用的方法实现尚未定义,调用的实现将在每一次特质被混入到具体类的时候才被决定(根据多态的形式决定的),这种处理super的有趣的行为是使得特质能以可堆叠的改变方式工作的关键*/ 2.瘦接口对阵胖接口略 3.样例:长方形对象1234567891011121314151617trait Rectangular &#123; def topLeft: Point def bottomRight: Point def left = topLeft.x def right = bottomRight.x def width = right - left //...&#125;abstract class Component extends Rectangular&#123; //其他方法...&#125;class Rectangle(val topLeft:Point, val bottomRight: Point) extends Rectangular&#123; //其他方法...&#125; 4.ordered特质1234567891011121314151617181920212223242526272829303132333435/*当你比较两个排序对象时,如果一个方法调用就能获知精确的比较结果将非常便利,如果你想要&quot;小于&quot;,你会调用&lt;,如果你想要&quot;小于等于&quot;,你会调用&lt;=,对于瘦接口来说,你或许只有&lt;方法,所以或许什么时候你会不得不写出类似于&quot;(x&lt;y)||(x==y)&quot; 这样的东西*///在第六章里,我们知道了Rational表示的是一个分数(有理数)class Rational(n:Int, d:Int) &#123; //... def &lt; (that: Rational) = this.number * that.denom &gt; that.number * this.denom def &gt; (that:Rational) = that &lt; this def &lt;= (that:Rational) = (this&lt;that)||(this==that) def &gt;= (that:Rational) = (this&gt;that)||(this==that)&#125;/*注意到三个比较操作符都定义在使用第一个的基础上,例如:&gt;被定义为&lt;的反转,&lt;=被定义为句法上的&quot;小于或等于&quot; ,另外,还可以注意到所有这三个方法对于任何可比较的类来说都是一样的,所以讨论&lt;=的时候不会有任何对于分数来说特别的东西,在比较的上下文中,&lt;=永远表示着&quot;小于或等于&quot;,总而言之,这个类的代码中存在着与任何其他实现了比较操作符的类一样的大量的固定格式写法*///由于比较操作时如此的常见,以至于scala专门提供了一个特质解决他,这个特质就是Ordered,要使用它,你首先要用一个compare方法替换所有独立的比较方法(相当于上面的&lt;方法),然后Ordered特质就会利用这个方法为你定义&lt;,&gt;,&lt;=和&gt;= ,Ordered特质让你可以通过仅仅实现了一个方法--compare,使你的类具有了全套的比较方法class Rational(n: Int, d: Int) extends Ordered[Rational] &#123; //... def compare(that: Rational) = //这个compare就是所有比较的基础,就像上面的&lt;方法 (this.number * that.denom) - (that.number * this.denom)&#125;/*这个版本的Rational混入了Ordered特质,不像你之前看到过的特质,Ordered需要你在混入的时候设定类型参数:type parameter,所以实际上混入的是Ordered[C] ,这里的C是你比较的元素的类,在本例中Rational混入了Ordered[Rational]你要做的第二件事就是compare方法来比较两个对象,这个方法应该能比较方法的接受者this和当做方法参数传入的对象,如果对象相同应该返回一个整数零,否则返回正数或者是负数*/val hafl = new Rational(1,2)val third = new Rational(1,3)half &lt; third //falsehalf &gt; third //true/*混入Ordered特质,你可以实现某种排序的类,请当心,Ordered特质并没有为你定义equals方法,因为他无法做到,问题在于要通过使用compare实现equals需要检查传入对象的类型,但是因为类型擦除,Ordered本身无法做这种测试,因此,即使你继承了Ordered,也还是需要自己定义equals*/ 5.特质用来做可堆叠的改变123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/*需求:对一个整数队列堆叠改动,队列有两种操作,put把整数放入队列,get从尾部取出他们,队列是先进先出的假设有一个类实现了这样的队列,你可以定义特质执行如下的改动:Dobling: 把所有放入到队列的数字加倍Incrementing: 把所有放入队列的数字增值&apos;Filtering:从队列中过滤掉负整数以上三种特质代表了改动,因为他们改变了原始队列的行为而非定义了全新的队列类,这三种同样也是可堆叠的,你可以选择三者中的若干,把他们混入类中,并获得你所需改动的新类*///下面是抽象的IntQueue类,使用了ArrayBuffer的实现队列abstract class IntQueue&#123; def get(): Int def put(x:Int)&#125;import scala.collection.mutable.ArrayBufferclass BasicIntQueue extends IntQueue&#123; private val buf = new ArrayBuffer[Int]() def get() = buf.remove(0) def put(x: Int)&#123;buf += x&#125;&#125;//调用val queue = new BasicIntQueuequeue.put(10)queue.put(20)queue.get() //10//下面的方法是使用特质改变行为trait Doubling extends IntQueue&#123; abstract override def put(x: Int)&#123; super.put(2 * x) &#125;&#125;/*上面的特质Doubling在声明为抽象的方法中有一个super调用,这种调用对于普通的类来说是非法的,因为他们在执行时必然失败,然而对于特质来说,这样的调用实际能够成功,因为特质里的super调用是动态绑定的,特质Doubling的super调用将直接被混入另一个特质或类之后例如:使用super.put(2*x)是对超类的调用,所以具体是看超类是怎么样实现的为了告诉编译器你的目的,比必须对这种方法打上abstract override的标志,这种标识符的组合仅在特质成员的定义中被认可,在类中则不行,他意味着特质必须被混入某个具有期待方法的具体定义的类中:因为是重写put方法,所以使用override,因为方法没有实现(因为super.put没有实现,所以说是abstract的)所以定义为abstract的*/class MyQueue extends BasicIntQueue with Doubling//因为BasicIntQueue是MyQueue的超类,所以在特质Doubling中super.put就是调用BasicIntQueue的put方法val queue = new MyQueuequeue.put(10)queue.get() //20/*注意:MyQueue没有定义一行新代码,只是简单的指明了一个类混入了一个特质,这种情况下,你甚至可以直接new 一个 &quot; BasicIntQueue with Doubling &quot;以替代命名类 */val queue = new BasicIntQueue with Doublingqueue.put(10)queue.get() //20//以下是Incrementing和Filtering,这两个特质的实现展示如下:trait Incrementing extends IntQueue&#123; abstract override def put(x: Int) &#123; super.put(x + 1)&#125;&#125;trait Filtering extends IntQueue&#123; abstract override def put(x: Int)&#123; if(x&gt;=0) super.put(x) &#125;&#125;/*有了上面的改动,你现在可以挑选想要的组成特定的队列,比方说,这里有一个队列能够过滤负数有对每个队列的数字增量*/val queue = (new BasicIntQueue with Incrementing with Filtering)queue.put(-1);queue.put(0); queue.put(1)queue.get() //1queue.get() //2/*混入的次序非常重要,越靠近右侧的特质越先其作用,当你调用带混入的类的方法时,最右侧特质的方法首先被调用,如果那个方法调用了super,他调用其左侧特质的方法,以此类推,其中Filtering的super.put调用的是Incrementing的put,Incrementing的super.put调用的是类BasicIntQueue的put*/ 6.为什么不是多重继承123456789101112/*对于多重继承来说,super调用导致的方法调用可以在调用发生的地方明确决定,而对于特质来说,方法调用时由类和被混入到类的特质的线性化所决定的*/class Animaltrait Furry extends Animaltrait HasLegs extends Animaltrait FourLegged extends HasLegsclass Cat extends Animal with Furry with FourLegged/*Cat类的继承层级和线性化次序展示在下图中,其中白色三角箭头表名继承,箭头指向超类型黑底非三角箭头说明线性化次序,牵头指向super调用解决的方法*/ 1//当上述类和特质中的任何一个通过super调用了方法,那么被调用的实现将是他线性化的右侧的第一个实现 7.特质,用还是不用12345/*当你实现了一个可重用的行为集合时,你将必须决定是使用特质还是抽象类,这里没有固定的规律,但是本节包含了一条可供考虑的规则1.如果行为不被重用,那么就把它当做具体类,具体类没有可重用的行为2.如果要在多个不相关的类中重用,就做成特质,只有特质可以混入到不同的类层级中*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十九章 类型参数化","date":"2017-04-16T04:47:25.907Z","path":"2017/04/16/scala编程/第十九章 类型参数化/","text":"1.queue函数式队列123456789//函数式队列是一种具有以下三种操作方式的数据结构head //返回队列的第一个元素tail //返回除第一个元素之外的队列append //返回尾部添加了指定元素的新队列//不像可变队列,函数式队列在添加元素的时候不会改变其内容,而是返回包含了这个元素的新队列scala&gt; import scala.collection.immutable.Queuescala&gt; val q = Queue(1,2,3)scala&gt; val q1 = q append 4 2.信息隐藏 私有构造器及工厂方法12345678910111213141516171819202122232425262728293031323334/*Java中,你可以把构造器声明为私有的使其不可见,scala中,主构造器无需明确定义,不过虽然他的定义隐含于类参数及方法体中,还是可以通过private修饰符添加在类参数列表的前边把主构造器隐藏起来*/class Queue[T] private ( private val leading: List[T], private val trailing: List[T] )//夹在类名与其参数之间的private修饰符表名Queue的构造器是私有的,他只能被类本身及伴生对象访问,类名Queue仍然是公开的,因此你可以继续使用这个类,但不能调用他的构造器scala&gt; new Queue(List(1,2), List(3))&lt;console&gt;:24: error: constructor Queue in class Queue cannot be accessed in object $iw new Queue(List(1,2), List(3)) ^//上面的代码的主构造器不能调用,那么可以使用辅助构造器,如下:def this() = this(Nil, Nil)//上面的辅助构造器可以构建空队列,通过改良,他可以带上初始化队列元素列表:def this(elems: T*) = this(elems.toList, Nil) //T*是重复参数的标注//另一种可能性是添加可以用初始元素序列创建队列的工厂方法,比较简洁的做法是定义与类同名的Queue对象及apply方法object Queue &#123; //用初始化元素&quot;xs&quot;构造队列 def apply[T](xs: T*) = new Queue[T](xs.toList, Nil)&#125;/*再把这个对象放在类Queue的同一个源文件中,你就把他变成了类的伴生对象,在13,4节已经知道,伴生对象与类具有相同的访问权,据此,即使Queue类的构造器是私有的,对象Queue的apply方法也可以创建新的Queue对象 *//*注意:工厂方法名为apply,因此客户可以使用类似于Queue(1,2,3) 这样的表达式创建队列,由于/Queue是对象而不是函数,这个表达式会被扩展为Queue.apply(1,2,3) , 结果,对于客户来说,Queue就好像是全局定义的工厂方法,实际上,scala没有全局可见的方法,每个方法都必须被包含在对象或类中,然而,使用定义在全局对象中的名为apply的方法,你就能够提供这种看上去好像是对全局方法调用的使用模式了*/ 可选方案: 私有类12345678910111213141516171819202122232425262728293031/*私有构造器和私有成员是隐藏类的初始化代码和表达代码的一种方式,另一种更为彻底的方式是直接把类本身隐藏掉,仅提供能够暴露类公共接口的特质*/trait Queue[T] &#123; def head: T def tail: Queue[T] def append(x: T): Queue[T]&#125;object Queue&#123; def apply[T](xs: T*): Queue[T] = new QueueImpl[T](xs.toList, Nil) private class QueueImpl[T](private val leading: List[T], private val trailing: List[T]) extends Queue[T]&#123; def mirror = if (leading.isEmpty) new QueueImpl(trailing.reverse, Nil) else this def head: T = mirror.leading.head def tail: QueueImpl[T] = &#123; val q = mirror new QueueImpl(q.leading.tail, q.trailing) &#125; def append(x: T) = new QueueImpl(leading, x::trailing) &#125;&#125;//这个版本隐藏的是全体实现类 3.变化型注解12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364//在上面的代码中Queue是特质,因为他带类型参数,结果,你将不能创建类型为Queue的变量scala&gt; def doenNotCompile(q: Queue)&#123;&#125;&lt;console&gt;:23: error: trait Queue takes type parameters def doenNotCompile(q: Queue)&#123;&#125; ^//取而代之,特质Queue能够制定参数化的类型,如:Queue[String], Queue[Int], 或Queue[AnyRef], 也就是说,Queue是特质,而Queue[String]是类型,Queue也被称为类型构造器,因为有了他你就能够通过指定参数类型构造新的类型scala&gt; def doenNotCompile(q: Queue[AnyRef])&#123;&#125;/*你也可以认为Queue是泛型的特质,类型参数的组合与子类型化产生了一些有趣的问题,例如:在Queue[T] 产生的类型家族的成员之间是否有任何特定的子类型关系?更具体的说就是:是否把Queue[String] 当做是Queue[AnyRef] 的子类型?或从更广泛的意义上来说,如果S是类型T的子类型,那么是否可以把Queue[S] 当做Queue[T]的子类型?如果是,你可以认为Queue特质是与他的类型参数T保持协变的,由于他只有一个类型参数,你可以简单的说Queue是协变的,协变的Queue将意味着,你可以把,比方说Queue[String] ,传递给之前看到过的值参数类型为Queue[AnyRef] 的doesCompile方法直观上,这些看上去都很合理,因为String队列似乎就是AnyRef的特例,然而在scala中,泛型类型缺省的是非协变的,子类型化,也就是说,根据定义的Queue,不同元素类型的队列之间没有子类型关系,Queue[String] 对象不能被用作Queue[AnyRef], 然而,可以用如下方式改变Queue类定义的第一行,以要求队列协变的子类型化*/trait Queue[+T] &#123;....&#125;/*在正常的类型参数前加上+号标明这个参数的子类型是协变的,这个符号是向scala说明你希望可以把,比方说Queue[String] ,当做Queue[AnyRef] 的子类型*//*除了+号以外,还可以前缀加上-号,这标明是需要逆变的子类型化,如果Queue定义如下:*/trait Queue[-T] &#123; ....&#125;/*那么如果T是类型S的子类型,这将意味着Queue[S] 是Queue[T]的子类型,无论类型参数是协变的,逆变的,还是非协变的,都被称为参数的变化型,可以放在类型参数前的+号和-号被称为变化型注解*/class Cell[T](init: T) &#123; private[this] var current = init def get = current def set(x: T) &#123; current = x&#125;&#125;/*代码中的cell类型被声明为非协变的,为了方便讨论,假设暂时声明为协变的--也就是说,声明为Cell[+T] , 并发给scala编译器,(实际上没有,我们会在之后解释),*/class Cell[+T](init: T) &#123; private[this] var current = init def get = current def set(x: T) &#123; current = x&#125;&#125;//报如下错误:&lt;console&gt;:24: error: covariant type T occurs in contravariant position in type T of value x def set(x: T) &#123; current = x&#125;//下面是测试代码val c1 = new Cell[String](&quot;abc&quot;)val c2 = Cell[Any] = c1c2.set(1)val s:String = c1.get/*上面的四条语句从自身来看,每条都很正常,第一行创建String单元格并保存在c1中,第二行定义c2类型是Cell[Any],并初始化为c1,这没问题,因为前提设定Cell为协变的,第三行把c2的值设为1,这也没问题,因为被赋予的值1是c2的元素类型Any的实例,最后,第四行把c1的元素值赋给字符串,这没什么奇怪的,两边的值都是同样的类型,但把他们放在一起,这四行代码要完成的是把整数1赋给字符串s,这明显是对类型声明的破坏根本的原因是:String类型的Cell并不是Any类型的Cell,因为有些事可以对Any的Cell做,但不能对String的cell做,比如:不能以Int类型的入参调用String的Cell的set方法*/ 变化型和数组1234567891011121314151617181920212223242526272829/*与java数组比较会很有趣,基本上,除了数组可以有超过一个元素之外,他与单元格没什么差别,然而,java中数组被认为是协变的,例如下面是用Java数组模拟了上面的单元格交互操作*///在java里String[] a1 = &#123;&quot;abc&quot;&#125;Object[] a2 = a1a2[0] = new Integer(17) //执行程序会在把Integer对象赋值给a2[0]时引发异常String s = a1[0]//下面是使用scala的方式去翻译上面的代码scala&gt; val a1 = Array(&quot;abc&quot;)a1: Array[String] = Array(abc)scala&gt; val a2: Array[Any] = a1&lt;console&gt;:22: error: type mismatch; found : Array[String] required: Array[Any]Note: String &lt;: Any, but class Array is invariant in type T.You may wish to investigate a wildcard type such as `_ &lt;: Any`. (SLS 3.2.10) val a2: Array[Any] = a1 ^/*这里的情况是scala把数组当做是非协变的,因此Array[String] 对象不能被当做与Array[Any] 一致,然而,有时需要使用对象数组作为模拟泛型数组的手段与java的遗留方法执行交互*/val a2: Array[Object] = a1.asInstanceOf[Array[Object]] 4.检查变化型注解1234567891011121314151617181920212223242526272829303132333435class StrangeIntQueue extends Queue[Int] &#123; override def append(x: Int) = &#123; println(Math.sqrt(x)) super.append(x) &#125;&#125;//测试如下:val x:Queue[Any] = new StrangeIntQueuex.append(&quot;abc&quot;)/*第一行是有效的,因为StrangeIntQueue是Queue[Int]子类,并且假设队列是协变的,即Queue[Int]是Queue[Any]的子类型,第二行是有效的因为你可以对Queue[Any]添加String对象,然而,这两行放在一起的效果就是取字符串的平方根,毫无意义*//*只要泛型的参数类型被当做方法参数的类型,那么包含他的类或特质就有可能不能与这个类型参数一起协变,对于对垒来说,append方法违反了以下情况:*/class Queue[+T] &#123; def append(x: T) = //...&#125;//编译上面的代码将报错/*可重新赋值的字段是&quot; 不允许使用+号注解的类型参数用作方法参数类型&quot; 这条规则的具体例子,如在18.2节提到过可重新赋值的字段, &quot; var x:T&quot;在scala里被看做一种getter方法,&quot; def x:T&quot; 和setter方法,&quot; def x_ = (y:T) , 正如你所见,setter方法带有字段类型为T的参数,因此类型将不是协变的*//*为了核实变化型注解的正确性,scala编译器会把类或特质结构体的所有位置分类为正,负,或中立,所谓的&quot;位置&quot; 是指类(或特质)的结构体内可能会用到类型参数的地方,例如,任何方法的值参数都是这种位置,因为方法值参数具有类型,所以类型参数可以出现在这个位置上,编译器检查的类型参数的每一个用法,注解了+号的类型参数只能被用在正的位置上,而注解了-号的类型参数只能用在负的位置上,没有变化型注解的类型参数可以用于任何位置,因此它是唯一能被用在类结构体的中性位置上的类型参数*/ 5.下界12345678910111213141516171819/*回到Queue类中来,你已经看到了前面演示的Queue[T]定义中不能实现对T的协变,因为T作为参数类型出现在append方法中,而这里是负的位置有一个办法可以打开这个结: 可以通过把append变为多态以使其泛型化,并使用他的类型参数的下界*/class Queue[+T](private val leading:List[T],private val trailing:List[T])&#123; def append[U&gt;:T](x: U) new Queue[U](leading, x::trailing)&#125;/*新的定义指定了append的类型参数U,并通过语法&quot;U&gt;:T&quot;, 定义了T为U的下界,结果U必须是T的超类型,append的参数现在变为类型U而不是类型T,而方法的返回值现在也变为Queue[T] ,取代了Queue[T]假设存在类Fruit及两个子类,Apple和Orange,通过使用Queue类的新定义,现在可以把Orange对象加入到Queue[Apple],而返回的结果为Queue[Fruit]类型append的改进定义是类型正确的,直观的看,如果T比预期的类型更为特化(例如:用Apple替代Fruit),那么append的调用就仍然正确,因为U(Fruit)是T(Apple)的超类型上述情况说明:变化型注解与下界可以相互协作,这是很好的类型驱动设计的例子,由接口的类型引导其细节的设计和实现*/ 6.逆变12345678trait OutputChannel[-T] &#123; def write(x: T)&#125;//同时存在逆变和协变trait Function1(-S, +T)&#123; def apply(x: S): T&#125; 7.对象私有数据(略)12 8.上界1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class Person(val firstName: String, val lastName: String) extends Ordered[Person] &#123; def compare(that: Person) = &#123; val lastNameComparison = lastName.compareToIgnoreCase(that.lastName) if (lastNameComparison != 0) lastNameComparison else firstName.compareToIgnoreCase(that.firstName) &#125; override def toString = firstName + &quot; &quot; + lastName&#125;//测试scala&gt; val robert = new Person(&quot;Robert&quot;, &quot;Jones&quot;)robert: Person = Robert Jonesscala&gt; val sally = new Person(&quot;Sally&quot;, &quot;Smith&quot;)sally: Person = Sally Smithscala&gt; robert &lt; sallyres0: Boolean = truedef orderedMergeSort[T &lt;: Ordered[T]](xs: List[T]):List[T] = &#123; def merge(xs: List[T], ys: List[T]): List[T] = (xs, ys) match &#123; case (Nil, _) =&gt; ys case (_, Nil) =&gt; xs case (x::xs1, y::ys1) =&gt; if (x&lt;y) x::merge(xs1,ys) else y::merge(xs,ys1) &#125; val n = xs.length/2 if (n == 0) xs else&#123; val (ys, zs) = xs splitAt n merge(orderedMergeSort(ys), orderedMergeSort(zs)) &#125;&#125;//测试val people = List( new Person(&quot;Larry&quot;, &quot;Wail&quot;), new Person(&quot;Anders&quot;, &quot;Hejlsberg&quot;), new Person(&quot;Guide&quot;, &quot;van Rossum&quot;), new Person(&quot;Alan&quot;, &quot;Kay&quot;), new Person(&quot;Yukihiro&quot;, &quot;matsumoto&quot;))orderedMergeSort(people)/*尽管上述代码很好的诠释了上界的用法,但他实际上并非scala中设计能够充分利用Ordered特质的排序函数的通用方式,比方说,你不能够用OrderedMergeSort函数对整数列表做排序,因为Int类不是Ordered[Int]的子类型*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十三章 包和引用","date":"2017-04-16T04:47:25.906Z","path":"2017/04/16/scala编程/第十三章 包和引用/","text":"&emsp;做程序的时候,尤其是很大的程序,使耦合最小化是很重要的,低耦合能降低程序一部分的细微改变影响到另一部分的正常执行这样的风险,减少耦合的方式之一是使用模块化风格编写代码,把程序分解成若干比较小的模块,把每块分成内部和外部,在模块的内部(即:模块的实现部分) 工作时,你只需要和同样工作于这个模块的程序员交互,只有当你必须改变模块的外部(即模块的接口)时,才需要和工作于其他模块的开发人员交互 1.包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172/*你可以用两种方式把代码放在命名包中,一种是通过把package子句放在文件顶端的方式把整个文件内容放进包里,如下:*/package bobsrockets.navigationclass Navigator/*由于scala代码是Java生态系统的一部分,在你发布到公开场合的时候,推荐遵循Java的反域名习惯设置scala包名,因此,更好的Navigator包名应该是: com.bobsrockets.navigation ,然而在本章里我们是为了让例子容易理解我们去掉了&quot;com.&quot; *///scala里另一种把代码放进包里的方式更像C#的命名空间,可以在package子句之后把要放到包里的定义用花括号括起来,除此之外,这种语法还能让你把文件的不同部分放在不同的包里,例如:你或许会把类的测试与原始代码一起放在同一个文件,但在不同的包里,如下:package bobsrockets &#123; package navigation &#123; class Navigator //bobsrockets.navigation包中 package tests &#123; class NavigatorSuite //bobsrockets.navigation.tests包中 &#125; &#125;&#125;//实际上如果一个包只是用来嵌入另一个包的话,你可以使用如下的方式:package bobsrockets.navigation &#123; class Navigator //bobsrockets.navigation包中 package tests &#123; class NavigatorSuite //bobsrockets.navigation.tests包中 &#125;&#125;/*正如你看到的一样,scala的包的确是嵌套的,也就是说,包navigation从语义上讲在包bobsrockets的内部,Java包尽管是分级的,却不是嵌套的,在Java里,当你命名一个包的时候,你必须从包层级的根开始*/package bobsrockets &#123; package navigation &#123; class Navigator //bobsrockets.navigation包中 &#125; package launch &#123; class Booster&#123; //不用写bobsrockets.navigation.Navigator val nav = new navigation.Navigator &#125; &#125;&#125;/*上面的方式之所以不用写全路径,是因为Booster类包含在bobsrockets包汇总,而这个包又含有navigationy成员,因此可以直接写navigation,务必使用前缀,就好像类方法里的代码可以直接用类的其他方法而不用前缀*///文件launch.scalapackage launch &#123; class Booster3&#125;//文件bobsrockets.scalapackage bobsrockets &#123; package navigation &#123; package launch &#123; class Booster1 &#125; class MissionControl &#123; val booster1 = new launch.Booster1 val booster2 = new bobsrockets.launch.Booster2 val booster3 = new _root_.launch.Booster3 &#125; &#125; package launch &#123; class Booster2 &#125;&#125;/*为了处理这种情况,scala在所有用户可创建的包之外提供了一个名为_root_的包,换句话说,任何你能写出来的顶层包都被当做是_root_包的成员,例如上述代码中的launch和bobsrockets都是_root_包的成员,因此,_root_.launch让你能访问顶层的launch包,_root_.launch.Booster3指向的就是最外面的booster类 */ 2.引用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889/*在scala里,包和其他成员可以用import子句来引用,之后引用的项目就可以用File这样的简单名访问,否则就要用Java.io.File这样的全称*/package bobsdelightsabstract class Fruit(val name: String, val color: String)object Fruits &#123; object Apple extends Fruit(&quot;apple&quot;, &quot;red&quot;) object Orange extends Fruit(&quot;orange&quot;, &quot;orange&quot;) object Pear extends Fruit(&quot;pear&quot;, &quot;yellowish&quot;) val menu = List(Apple, Orange, Pear)&#125;//易于访问Fruitimport bobsdelights.Fruit//易于访问bobsdelights的所有成员,在Java中使用的是*号,在scala里使用的是_import bobsdelights._//易于访问Fruits的所有成员import bobsdelights.Fruits._/*scala引用可以出现在任何地方,而不是仅仅在编译单元的开始处,同样,他们可以指向任意值,例如,*/def showFruit(fruit: Fruit) &#123; import fruit._ println(name + &quot;s are &quot; + color)&#125;/*方法showFruit引用了他的参数(fruit)的所有成员,之后println语句就可以直接使用name和color了,这两个引用值等价于fruit.name 和fruit.color,当你把对象当做模块使用时这种语法尤其有用*//*scala的import子句比Java的更为灵活,在他们之间存在三点主要差异,在scala中,引用:1.可以出现在任何地方2.可以指的是(单例或正统的)对象及包3.可以重命名或隐藏一些被引用的成员*//*scala的引用还可以重命名或隐藏成员,这可以在被引用成员的对象之后加上括号里的引用选择器子句来做到*///此例只引用了对象Fruits的Apple和Orange成员import Fruits.&#123;Apple, Orange&#125;//此例从对象Fruits引用了Apple和Orange两个成员,不过,Apple对象重命名MacIntosh,重命名子句的格式&quot;&lt;原始名&gt; =&gt; &lt;新名&gt;&quot;import Fruits.&#123;Apple =&gt;McIntosh, Orange&#125;/*此例以SDate的名字引用了sql的日期类,以便同时以Date的名字引用普通的Java日期类 */import java.sql.&#123;Date =&gt; SDate&#125;/*此例以名称S引用了Java.sql包,这样你就可以写成S.Date */import java.&#123;sql =&gt; S&#125;/*此例引用了对象Fruits的所有成员,这与import Fruits._同义 */import Fruits.&#123;_&#125;//此例从Fruits对象引用所有成员,不过重命名Apple为McIntoshimport Fruits.&#123;Apple =&gt; McIntosh, _&#125;/*此例引用了Fruits的所有成员,Pear除外,&quot; &lt;原始名&gt; =&gt; _ &quot; 格式的子句会从被引用的名字中排除&lt;原始名&gt; ,从某种意义上来说,把某样东西重命名为 &quot;_&quot; 就是表示把它隐藏掉,这对避免出现混淆的局面有所帮助,比如说你有两个包,Fruits和Notebooks ,他们都定义了类Apple如果你只是想得到名为Apple的笔记本,而不是水果,你可以使用如下的方式: */import Fruits.&#123;Pear =&gt; _, _&#125;import Notebooks._import Fruits.&#123;Apple =&gt; _, _&#125;/*总结:1.简单名x,把x包含进引用名集2.重命名子句x=&gt;y 让名为x的程艳以名称y出现3.隐藏子句x=&gt;_ 把x排除在引用名集之外4.全包括 &apos;_&quot; 引用除了前面子句提到的之外的全体成员,如果存在全包括,那么必须是引用选择的最后一个*/ 3.隐式引用123456789101112131415/*scala为每个程序隐式的添加了一些引用,就好像每个以&quot;.scala&quot;为扩展名的源文件的顶端都加在了下列的三个引用子句*/import Java.lang._ //java.lang包的所有东西import scala._ //scala包的所有东西import Predef._ //Predef对象的所有东西/*因为Java.lang是隐式引用的,所以说你就可以直接使用Thread而不需要写成java.lang.Thread又如:scala包被隐式引用, 你可以直接写LIst, 而不用写成scala.LIstPredef对象包含了许多scala程序中常用到的类型,方法和隐式的定义,比方说:因为Predef是隐式引用,所以你可以直接写assert而不用写成Predef.assert上面的这三个引用子句与其他的稍有不同,出现在靠后位置的引用将覆盖靠前的引用,例如:StringBuilder类被定义在scala包及包java.lang中,因为scala引用覆盖了java.lang引用,所以StringBuilder简单名被看做scala.StringBuilder,而不是java.lang.StringBuilder*/ 4.访问修饰符&emsp;包/类或对象的成员可以访问修饰符private和protected做标记,这些修饰符把对成员的访问限制在代码确定的区域中,scala大体上遵守java对访问修饰符的对待方式,但也有一些重要的差异 私有成员12345678910111213141516/*私有成员的处理方式与java的相同,标记为private的成员仅在包含了成员定义的类或对象内部可见*/class Outer &#123; class Inner &#123; private def f() &#123; println(&quot;f&quot;)&#125; class InnerMost &#123; f() //Ok &#125; &#125; (new Inner).f() //f不可访问&#125;/*在scala里, (new Inner).f() 访问非法,因if在Inner中被声明为private而访问不再类Inner之内,相反,在类InnerMost里访问f没有问题,因为这个访问包含在Inner类之内,java允许这两种访问,因为他允许外部类访问其内部类的私有成员*/ 保护成员123456789101112131415161718/*scala里,保护成员只在定义了成员的类的子类中可以被访问,而在java中还允许同一个包的其他类中进行这种访问*/package p&#123; class Super&#123; protected def f() &#123; println(&quot;f&quot;)&#125; &#125; class Sub extends Super &#123; f() &#125; class Other &#123; (new Super).f() //error ,f不可访问 &#125;&#125;/*例子中,Sub类对f的访问没有问题,因为f在Super中被声明为protected,而Sub是Super的子类,相反Other对f的访问不被允许,因为Other没有继承自Super,java里,后者同样会被认可,因为Other和Sub在同一个包里*/ 公开成员123/*任何默认标记为private或protected的成员都是公开的,公开成员没有显示的修饰符,这样的成员可以在任何地方被访问*/ 保护的作用域123456789101112131415161718192021222324/*在scala里的访问修饰符可以通过使用限定词强调,格式为private[X] ,或protected[X] 的修饰符表示 &quot;直到&quot; X的私有或保护,这里X指代某个所属的包,类或单例对象在这段代码中,类Navigator被标记为private[bobsrockets] ,就是说这个类对包含在bobsrockets包里的所有的类和对象可见这种技巧在横跨若干包的大型项目中非常有用,他允许你定义一些在你项目的若干子包中可见但对于项目外部的客户却始终不可见的东西*/package bobsrockets &#123; package navigation &#123; private [bobsrockets] class Navigator &#123;//可以在bobsrockets包中访问 protected [navigation] def useStarChar()&#123;&#125;//该方法能被Navigator所有子类及包含在navigation包里的所有代码访问 class LegOfJourney &#123; private[Navigator] val distance = 100 //在类Navigator的任何地方都可见 &#125; private[this] var speed = 200 //仅能在包含了定义的同一个对象中被访问 &#125; &#125; package launch &#123; import navigation._ object Vehicle &#123; private[launch] val guide = new Navigator &#125; &#125;&#125; &emsp;下表罗列了private限定字的效果,每一行说明了一个被限定的私有修饰符及如果这个修饰符被附加在LegOfJourney类里声明的distance变量上意味着什么 没有修饰符 公开访问 private[bobsrockets] 在外部包中访问 private[navigation] 与java的包可见度相同 private[Navigator] 与java的private相同 private[LegOfJourney] 与scala的private相同 private[this] 仅在同一个对象中可以访问 可见性和伴生对象12345678910111213141516171819202122232425/*java里,静态成员和实例成员属于同一个类,因此访问修饰符可以统一的应用在他们之上,你已经知道在scala里没有静态成员,作为替代,可以拥有包含成员的单例的伴生对象*/class Rocket &#123; import Rocket.fuel private def canGoHomeAgain = fuel &gt; 20&#125;object Rocket &#123; private def fuel = 10 def chooseStrategy(rocket: Rocket)&#123; if (rocket.canGoHomeAgain) goHome() else pickAstar() &#125; def goHome()&#123;&#125; def pickAstar()&#123;&#125;&#125;/*对于私有或保护访问来说,scala的访问规则给予了伴生对象和类一些特权,类的所有访问权限都对伴生对象开放,反过来也是如此,具体的说,就是对象可以访问所有他的伴生类的私有成员,就好像类也可以访问伴生对象的所有私有成员一样如上面的例子:Rocket类可以访问fuel方法,而他在Rocket对象中是被声明为private的,类似的,Rocket对象也可以访问Rocket类里面的私有方法canGetHome*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十七章 集合类型","date":"2017-04-16T04:47:25.904Z","path":"2017/04/16/scala编程/第十七章 集合类型/","text":"1.集合库概览 1234567891011121314/*Iterable是主要特质,他同时还是可变和不可变序列(Seq) , 集(Set), 以及映射(Map)的超特质,序列是有序的集合,例如:数组和列表,集可以通过==方法确定对每个对象最多只包含一个,映射则包含了键值映射关系的额集合命名为Iterable是为了说明集合对象可以通过名为elements的方法产生Iterator(枚举器),*/def elements: Iterator[A]/*例子中的A是Iterator的类型参数,他指代集合中包含的元素的类型, elements返回的Iterator被参数化为同样的类型,,例如:Iterable[Int] 的elements方法将创建Iterator[Int]Iterable包含几十个有用的具体方法,所有这些方法都是使用了elements返回的Iterator实现的,而elements是Iterable唯一的抽象方法,Iterable定义的方法中,许多是高阶方法,多数都已经在前面的章节中出现过,其中包含map, flatMap,filter, exists及find Iterator有许多与Iterable相同的方法,包括哪些高阶方法,但他们不属于同一层级,如图*/ 12345678910/*特质Iterator扩展了AnyRef,Iterable与Iterator之间的差异在于特质Iterable指代的是可以被枚举的类型(如集合类型),而特质Iterator是用来执行枚举操作的机制,尽管Iterable可以被枚举若干次,但Iterator仅能使用一次,一旦你使用Iterator枚举遍历了集合对象,你就不能再使用它了,如果你需要再次枚举该集合对象,你需要对他调用elements方法获得新的Iterator*//*Iterator提供的具体方法都使用了next和hasNext抽象方法实现*/def hasNext: Booleandef next: A 2.序列&emsp;序列是继承自特质Seq的类,他可以让你处理一组线性分布的数据,因为元素是有序的,所以你可以请求第一个元素,第二个元素,…第n个元素 列表12345678scala&gt; val colors = List(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)colors: List[String] = List(red, blue, green)scala&gt; colors.headres42: String = redscala&gt; colors.tailres43: List[String] = List(blue, green) 数组123456789101112131415/*数组能够让你保留一组元素序列并可以基于零的索引高效访问(无论是获取还是添加)处于任意位置的元素,下列代码说明了如何创建长度已知但内容未知的数组*/scala&gt; val fiveInts = new Array[Int](5)fiveInts: Array[Int] = Array(0, 0, 0, 0, 0)scala&gt; val fiveToOne = Array(5,4,3,2,1)fiveToOne: Array[Int] = Array(5, 4, 3, 2, 1)//正如之前提到的,scala中数组的访问方式是通过把索引值放在圆括号里,而不是像java里那样放在方括号里,下面的例子和更新了数组元素:scala&gt; fiveInts(0) = fiveToOne(4)scala&gt; fiveIntsres45: Array[Int] = Array(1, 0, 0, 0, 0) 列表缓存1234567891011121314151617181920212223242526272829303132333435/*List类能够提供对列表头部,而非尾部的快速访问,因此,如果需要通过向结尾添加对象的方式建造列表,你应该考虑先以对表头前缀元素的方式反向构造列表,完成之后再调用reverse使得元素反转为你需要的顺序*//*另一种方式是使用ListBuffer, 这可以避免reverse操作,ListBuffer是可变对象(包含在scala.collection.mutable包中),他可以更高效的通过添加元素的方式构建列表,ListBuffer能够支持常量的添加和前缀操作,元素的添加使用+= 操作符,前缀使用+: 操作符,完成之后,可以通过对ListBuffer调用toList方法获得List,举例如下:*/scala&gt; import scala.collection.mutable.ListBufferimport scala.collection.mutable.ListBufferscala&gt; val buf = new ListBuffer[Int]buf: scala.collection.mutable.ListBuffer[Int] = ListBuffer()scala&gt; buf += 1res46: buf.type = ListBuffer(1)scala&gt; buf += 2 //向ListBuffer的后面添加元素res47: buf.type = ListBuffer(1, 2)scala&gt; bufres48: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2)scala&gt; 3 +: buf //在前面添加新的元素,生成新的ListBufferres49: scala.collection.mutable.ListBuffer[Int] = ListBuffer(3, 1, 2)scala&gt; bufres50: scala.collection.mutable.ListBuffer[Int] = ListBuffer(1, 2)scala&gt; buf.toListres51: List[Int] = List(1, 2)/*使用ListBuffer替代List的另一个理由是为了避免栈溢出的风险,即使你能够使用前缀的方式以正确的次序构建列表,但是所需的递归算法不是尾递归,那么你也可以使用for表达式或while循环及ListBuffer做替代*/ 数组缓存1234567891011121314151617181920212223/*ArrayBuffer与数组类似,只是额外还允许你在序列的开始或结束的地方添加和删除元素,所有的Array操作都被保留,只是由于实现中的包装层导致执行的稍微有些慢,*///在使用ArrayBuffer之前,你必须首先从可变集合包中引用它scala&gt; import scala.collection.mutable.ArrayBuffer//创建ArrayBuffer的时候,你必须指定他的类型参数,但可以不用指定长度,ArrayBuffer可以自动调整分配的空间:scala&gt; val buf = new ArrayBuffer[Int]()buf: scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer()//ArrayBuffer还能使用 += 操作添加元素scala&gt; buf += 12 res0: buf.type = ArrayBuffer(12)scala&gt; buf += 15res1: buf.type = ArrayBuffer(12, 15)scala&gt; buf.length //获得数组的长度res2: Int = 2scala&gt; buf(0) //通过索引访问元素res3: Int = 12 队列(Queue)1234567891011121314151617181920212223242526272829303132333435363738394041/*如果你需要先进先出序列,可以使用Queue,scala的集合库提供了可变和不可变的Queue*/import scala.collection.immutable.Queueval empty = new Queue[Int]//你可以使用enqueue为不可变队列添加元素val has1 = empty.enqueue(1)//如果要添加多个元素的话,可以把集合当做enqueue调用的参数val has123 =has1.enqueue(List(2,3))//从队列的头部移除元素,可以使用dequeueval (element, has23) = has123.dequeue //element =1 has23 = Queue(2,3)//对于不可变队列来说,dequeue方法将返回由队列头部元素和移除该元素之后的剩余队列组成的对偶(Tuple2)//可变队列的使用方式与不可变队列一样,只是代之以enqueue方法,你可以使用 += ,及 ++= 操作符添加元素,还有,对于可变队列来说,dequeue方法将只从队列移除元素头并返回scala&gt; import scala.collection.mutable.Queueimport scala.collection.mutable.Queuescala&gt; val queue = new Queue[String]queue: scala.collection.mutable.Queue[String] = Queue()//添加元素scala&gt; queue += &quot;a&quot;res4: queue.type = Queue(a)//添加Listscala&gt; queue ++= List(&quot;b&quot;, &quot;c&quot;)res5: queue.type = Queue(a, b, c)//返回头部scala&gt; queue.dequeueres6: String = ascala&gt; queueres7: scala.collection.mutable.Queue[String] = Queue(b, c) 栈12345678910111213141516171819202122232425262728293031//如果需要的是先进后出的序列,你可以使用Stack,他同样在scala的集合库中也有可变和不可变版本,元素的推入使用push,弹出使用pop,只获取栈顶的元素而不移除可以使用top,下面是使用的可变栈的例子scala&gt; import scala.collection.mutable.Stackimport scala.collection.mutable.Stackscala&gt; val stack = new Stack[Int]stack: scala.collection.mutable.Stack[Int] = Stack()scala&gt; stack.push(1)res8: stack.type = Stack(1)scala&gt; stackres9: scala.collection.mutable.Stack[Int] = Stack(1)scala&gt; stack.push(2)res10: stack.type = Stack(2, 1)scala&gt; stackres11: scala.collection.mutable.Stack[Int] = Stack(2, 1)scala&gt; stack.topres12: Int = 2scala&gt; stackres13: scala.collection.mutable.Stack[Int] = Stack(2, 1)scala&gt; stack.popres14: Int = 2scala&gt; stackres15: scala.collection.mutable.Stack[Int] = Stack(1) 字符串(经RichString隐式转换)123456789101112131415161718/*RichString也是应该知道的序列,他的类型是Seq[Char] ,因为Predef包含了从String到RichString的隐式转换,所以你可以把任何字符串字符当做Seq[Char],举例如下:*/scala&gt; def hasUpperCase(s: String) = s.exists(_.isUpperCase)&lt;console&gt;:12: error: value isUpperCase is not a member of Char def hasUpperCase(s: String) = s.exists(_.isUpperCase) ^scala&gt; def hasUpperCase(s: String) = s.exists(_.isUpperCase) scala&gt; hasUpperCase(&quot;Robert Frost&quot;) // truescala&gt; hasUpperCase(&quot;e e cummings&quot;) // false/*本例中的hasUpperCase方法体中,字符串s调用了exists方法,而String类本身并没有定义名为&quot;exists&quot;的方法,因此scala编译器会把s隐式转换为含有这个方法的RichString类,exists方法把字符串看做Seq[Char] ,并且如果所有的字符都是大写字母则返回值*/ 3.集(set)和映射(map)1234567891011121314151617181920/*默认情况下在你使用&quot;Set&quot; 或&quot; Map&quot; 的时候,获得的都是不可变对象,如果需要的是可变版本,你需要首先写明引用,scala让你更易于使用不可变的版本,期望能够以此方式而并非相对的可变版本,这种访问易于来自Predef对象的支持,他被每个scala源文件隐含引用*/object Predef &#123; type Set[T] = scala.collection.immutable.Set[T] type Map[K,V] = scala.collection.immutable.Map[K, V] type Set = scala.collection.immutable.Set //默认 type Map = scala.collection.immutable.Map //默认///.....&#125;//如果同一个源文件中既要用到可变版本,也要用到不可变版本的集合或映射,方式之一是引用包含了可变版本的包名scala&gt; import scala.collection.mutableimport scala.collection.mutablescala&gt; val mutaSet = mutable.Set(1,2,3)mutaSet: scala.collection.mutable.Set[Int] = Set(1, 2, 3) 使用集1234567891011/*集的关键特性在于他可以使用对象的==操作检查,确保任何时候每个对象只在集中保留最多一个副本,*/scala&gt; val text = &quot;See Spot run, Run, Spot, Run!&quot;scala&gt; val wordsArray = text.split(&quot;[!,. ]+&quot;)wordsArray: Array[String] = Array(See, Spot, run, Run, Spot, Run)scala&gt; for(word &lt;- wordsArray) words += word.toLowerCase 集的常用操作 操作 行为 val nums = Set(1,2,3) 创建不可变集(nums.toString) 返回Set(1,2,3) nums += 5 添加元素(返回Set(1,2,3,5)) nums -= 3 删除元素(返回Set(1,2)) nums ++ List(5,6) 添加多个元素(返回Set(1,2,3,5,6) nums – List(1,2) 删除多个元素(返回Set(3)) nums ** Set(1,3,5,7) 获得交集(返回Set(1,3)) nums.size 返回集中包含的对象数量(返回3) nums.contains(3) 检查是否包含(返回true) import scala.collection.mutable 引用可变集合类型 val words = mutable.Set.empty[String] 创建空可变集(words.toString, 返回Set()) words += “the” 添加元素(words.toString返回Set(the)) words -= “the” 如果存在元素,则删除(words.toString 返回Set()) words ++= List(“do”, “re”, “md”) 添加多个元素(words.toString ,返回Set(do,re,md) words –= List(“do”, “re”) 删除多个元素(words.toString 返回Set(md)) words.clear 删除所有元素(words.toString 返回Set()) 使用映射 1234567891011121314151617scala&gt; val map = scala.collection.mutable.Map.empty[String, Int]map: scala.collection.mutable.Map[String,Int] = Map()/*在创建映射的时候,你必须指定两个类型,第一个类型是用来定义映射的键(key) , 第二个用来定义值(value), 在这个例子中,键是字符串,值是整数*/scala&gt; map(&quot;hello&quot;) = 1scala&gt; map(&quot;there&quot;) = 2scala&gt; mapres5: scala.collection.mutable.Map[String,Int] = Map(hello -&gt; 1, there -&gt; 2)scala&gt; map(&quot;hello&quot;)res6: Int = 1 映射的常用操作 操作 行为 val nums = Map(“i” -&gt; 1, “ii” -&gt; 2) 创建不可变映射 nums + (“vi” -&gt; 6) 添加条目(返回Map(i-&gt;1, II-&gt;2, vi-&gt;6) nums - “ii” 删除条目(返回Map(i-&gt;1)) nums += List(“iii” -&gt; 3, “v”-&gt;5) 添加多个条目 nums – List(“i”, “ii”) 删除多个条目 nums.size 返回映射的条目的数量 nums(“ii”) 获取指定键的关联值(返回2) nums.key 返回键枚举器(返回字符串”i”, 和”ii”的Iterator) nums.keySet 返回键集 nums.values 返回值枚举器(返回整数1,2 的Iterator) nums.isEmpty 指明映射是否为空(返回false) import scala.collection.mutable 引用可变集合类型 val words = mutable.Map.empty[String,Int] 创建空的可变集合 words += (“one”-&gt;1) 添加一条映射 words -= “one” 若存在映射条目,则删除 words ++= List(“one” -&gt;1, “two”-&gt;2, “three”-&gt;3) 添加多个映射条目 words –= List(“one”, “two”) 删除多个对象 默认的(Default)集和映射12345678/*工厂方法提供的实现都使用了快速查找算法,通常都涉及哈希表,因此他们能够快速反应对象是否存在于集合中,如scala.collection.mutable.Set() 工厂方法返回scala.collection.mutable.HashSet,则其在内部使用了哈希表类似的,scala.collection.mutable.Map() 工厂方法返回了scala.collection.mutable.HashMap不可变集和映射的情况更为复杂一些,例如:scala.collection.immutable.Set() 工厂方法返回的类,取决于你传递给他的元素, 具体说明参加下表,对于少于5个元素的集,类型完全取决于他的元素数量,以获得最优的性能,然而一旦你请求的集包含了5个元素以上,工厂方法返回的将是不可变的HashSet*/ 1类似的,scala.collection.immutable.Map()工厂方法返回的类取决于传递进去的键值对数量,参见下表,对于少于5个元素的不可变映射,类型完全取决于其键值对数量,以获得最优的性能,但如果包含了5个或以上的键值对,则使用的是不可变的HashMap 有序的(Sorted) 集和映射123456789101112131415161718192021/*有时,可能你需要集或映射的枚举器能够返回那特定顺序排序的元素,为此,scala的集合库提供了SortedSet和SortedMap特质,这两个特质分别有类TreeSet和TreeMap实现,他们都使用了红黑树有序的保存元素(TreeSet类) 或键(TreeMap)类,具体的顺序取决于Ordered特质,集的元素类型或映射的键类型要么混入,要么能够隐式的转换成Ordered的特质,这些类只有不可变类型的版本*/scala&gt; import scala.collection.immutable.TreeSetimport scala.collection.immutable.TreeSetscala&gt; val ts = TreeSet(1,2,3,8,9)ts: scala.collection.immutable.TreeSet[Int] = TreeSet(1, 2, 3, 8, 9)scala&gt; val cs = TreeSet(&apos;t&apos;,&apos;u&apos;,&apos;n&apos;)cs: scala.collection.immutable.TreeSet[Char] = TreeSet(n, t, u)scala&gt; import scala.collection.immutable.TreeMapimport scala.collection.immutable.TreeMapscala&gt; val tm = TreeMap(3-&gt;&apos;x&apos;, 1-&gt;&apos;x&apos;, 4-&gt;&apos;x&apos;)tm: scala.collection.immutable.TreeMap[Int,Char] = Map(1 -&gt; x, 3 -&gt; x, 4 -&gt; x)scala&gt; tmres12: scala.collection.immutable.TreeMap[Int,Char] = Map(1 -&gt; x, 3 -&gt; x, 4 -&gt; x) 同步的集和映射1234567891011121314151617181920212223242526272829303132333435363738/*我们曾经提到过如果需要线程安全的映射,可以把SynchronizedMap特质混入到你想要的特定类实现中,例如,把SynchronizedMap混入HashMap,*/import scala.collection.mutableimport scala.collection.mutable.&#123;HashMap, Map, SynchronizedMap&#125;object MapMaker &#123; def makMap:Map[String,String] = &#123; new HashMap[String,String] with SynchronizedMap[String,String] &#123; override def default(key: String): String = &quot;why do you want to know?&quot; &#125; &#125;&#125;/*scala编译器将产生混入了SynchronizedMap的HashMap合成子类,并创建他的返回实例,这个合成子类还重载了名为default的方法如果你请求映射返回与特定键关联的值,而该键的映射实际不存在,默认你将得到NoSuchElementException,然而如果你定义了新的映射类并重载了default方法,那么这个新的映射将在查询不存在的键时返回default方法的返回值,这里是返回&quot;why do you want to know?&quot;*//*由于makeMap方法返回的可变映射混入了SynchronizedMap特质,因此可以立即用于多线程环境,每次对映射的访问都被同步操作,下面是单线程访问映射的情况:*/val capital = MapMaker.makeMapcapital ++ List(&quot;us&quot;-&gt;&quot;Washington&quot;, &quot;paris&quot;-&gt;&quot;France&quot;,&quot;Japan&quot;-&gt;&quot;Tokyo&quot;)capital(&quot;Japan&quot;) // Tokyocapital(&quot;New Zealand&quot;) //why do you want to know?/*对于同步的Set,同理可以创建SynchronizedSet特质创建同步的HashSet*/import scala.collection.mutableval synchroSet = new mutable.HashSet[Int] with mutable.SynchronizedSet[Int]/*对于同步,你也可以考虑使用java.util.concurrent的并发集合,又或者,还可以使用非同步的集合及scala的actor*/ 4.可变(mutable)集合vs不可变(immutable)集合1234567891011121314151617181920212223242526272829303132333435/*不可变集合比可变集合更为紧促,节省大量的空间*/scala&gt; val people = Set(&quot;Nancy&quot;, &quot;Jane&quot;)people: scala.collection.immutable.Set[String] = Set(Nancy, Jane)scala&gt; people += &quot;Bob&quot;&lt;console&gt;:11: error: value += is not a member of scala.collection.immutable.Set[String] people += &quot;Bob&quot; //因为是val的,所以不能重新赋值 ^scala&gt; var people = Set(&quot;Nancy&quot;, &quot;Jane&quot;)people: scala.collection.immutable.Set[String] = Set(Nancy, Jane)scala&gt; people += &quot;Bob&quot;scala&gt; peopleres15: scala.collection.immutable.Set[String] = Set(Nancy, Jane, Bob)/*尽管集合是不可变类型的,过程是:首先,创建集合,然后,people将被重新赋值为新集合经过一系列操作之后,people变量现在指向新的不可变集合,其中包含了添加的字符串&quot;Bob&quot;,同样的理念可以应用于以=结尾的方法,而不仅是+=方法,*/people -= &quot;Jane&quot;people ++= List(&quot;Tom&quot;, &quot;Harry&quot;)/*如果你想使用可变集合,仅需要引用可变版本的Map即可,这样就可以重写对不可变Map的默认引用*/import scala.collection.mutable.Map //唯一的改变var capital = Map(&quot;Us&quot;-&gt;&quot;Washington&quot;, &quot;France&quot;-&gt;&quot;Paris&quot;)capital += (&quot;Japan&quot;-&gt;&quot;Tokyo&quot;) 5.初始化集合12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/*最常见的创建和初始化集合的办法是把初始值传递给要用的集合类型的伴生对象的工厂方法,你只需把元素放在伴生对象名后面的括号中,scala编译器就会把它转化为该伴生对象的apply方法调用*/scala&gt; List (1,2,3)res16: List[Int] = List(1, 2, 3)scala&gt; Set(&apos;a&apos;,&apos;b&apos;,&apos;c&apos;)res17: scala.collection.immutable.Set[Char] = Set(a, b, c)scala&gt; import scala.collection.mutableimport scala.collection.mutablescala&gt; mutable.Map(&quot;hi&quot;-&gt;2,&quot;there&quot;-&gt;5)res19: scala.collection.mutable.Map[String,Int] = Map(hi -&gt; 2, there -&gt; 5)scala&gt; Array(1.0, 2.0, 3.0)res20: Array[Double] = Array(1.0, 2.0, 3.0)/*尽管通常都可以让scala编译器从传递给工厂方法的元素推断集合的元素类型,但有些时候或许你会希望指定以不同于编译器所选的类型创建集合,尤其对于可变集合来说更为如此*/scala&gt; import scala.collection.mutableimport scala.collection.mutablescala&gt; val stuff = mutable.Set(42)stuff: scala.collection.mutable.Set[Int] = Set(42)scala&gt; stuff += &quot;abcde&quot;&lt;console&gt;:14: error: type mismatch; found : String(&quot;abcde&quot;) required: Int stuff += &quot;abcde&quot; ^/*上面的问题在于stuff被指定元素类型为Int,如果想要让他的类型为Any,你需要明确的说明,把元素类型放在方括号中*/scala&gt; val stuff = mutable.Set[Any](42)stuff: scala.collection.mutable.Set[Any] = Set(42)/*另一种特殊情况是,你想要把集合初始化为指定类型,例如:设想你要把列表中的元素保存在TreeSet中*/scala&gt; val colors = List(&quot;blue&quot;, &quot;yellow&quot;,&quot;red&quot;)colors: List[String] = List(blue, yellow, red)//你不能把colors列表传递给TreeSet工厂方法scala&gt; import scala.collection.immutable.TreeSetimport scala.collection.immutable.TreeSetscala&gt; val treeSet = TreeSet(colors)&lt;console&gt;:14: error: No implicit Ordering defined for List[String]. val treeSet = TreeSet(colors) ^//需要创建空的TreeSet[String] 对象并使用TreeSet的++ 操作符把列表元素加入其中scala&gt; val treeSet = TreeSet[String]() ++ colorstreeSet: scala.collection.immutable.TreeSet[String] = TreeSet(blue, red, yellow) 数组与列表之间的互转12345678910111213141516171819202122232425262728/*如果你需要用集合初始化列表或数组,使用集合初始化列表,只需对集合调用toList方法*/scala&gt; treeSetres22: scala.collection.immutable.TreeSet[String] = TreeSet(blue, red, yellow)scala&gt; treeSet.toListres23: List[String] = List(blue, red, yellow)//或者你需要的是数组scala&gt; treeSet.toArrayres24: Array[String] = Array(blue, red, yellow)/*对TreeSet调用toList产生的列表元素是按照字母顺序排列的,如下*/.scala&gt; val test = TreeSet(&quot;ff&quot;, &quot;bb&quot;, &quot;ee&quot;, &quot;cc&quot;)test: scala.collection.immutable.TreeSet[String] = TreeSet(bb, cc, ee, ff)scala&gt; test.toListres25: List[String] = List(bb, cc, ee, ff)/*请牢记:转变为列表或数组同样需要复制集合的所有元素,因此对于大型集合来说可能比较慢,所以toList和toArray对于小的Set转成List或者Array还是可以的*/ 集和映射的可变与不可变互转123456789101112131415161718192021222324252627/*另一种偶尔发生的情况是:把可变集或映射转换成不可变类型,或者反向转换,可以先创建空不可变集合,然后把可变集合的元素用++操作符添加进去*/scala&gt; import scala.collection.mutableimport scala.collection.mutablescala&gt; treeSetres26: scala.collection.immutable.TreeSet[String] = TreeSet(blue, red, yellow)scala&gt; val mutaSet = mutable.Set.empty ++ treeSetmutaSet: scala.collection.mutable.Set[String] = Set(red, blue, yellow)scala&gt; val immutaSet = Set.empty ++ mutaSetimmutaSet: scala.collection.immutable.Set[String] = Set(red, blue, yellow)/*使用同样的技巧实现可变映射与不可变映射之间的转换*/scala&gt; val muta = mutable.Map(&apos;i&apos;-&gt;1, &quot;ii&quot;-&gt;2)muta: scala.collection.mutable.Map[Any,Int] = Map(ii -&gt; 2, i -&gt; 1)scala&gt; val immu = Map.empty ++ mutaimmu: scala.collection.immutable.Map[Any,Int] = Map(ii -&gt; 2, i -&gt; 1) 6.元组123456789101112131415161718192021222324252627282930313233343536373839404142/*元组可以把固定数量的条目组合在一起以便于作为整体传送,不像数组或列表,元组可以保存不同类型的对象,下面是可以作为整体保存整数,字符串,和控制台的元组*/(1, &quot;hello&quot;, Console)/*由于元组可以组合不同类型的对象,因此他不能继承自Iterator,如果你发现自己想要的是把&quot;一个&quot;整数和&quot;一个&quot;字符串组合在一起,那么你需要的就是元组,不是List,也不是Array*///元组常用来返回方法的多个值,如:下面的方法找到集合中的最长单词并返回他的索引def longestWord(words: Array[String]) = &#123; var word = words(0) var idx = 0 for (i &lt;- 1 until words.length) if (words(i).length &gt; word.length) word = words(i) idx = 1 (word,idx)&#125;//使用scala&gt; val longest = longestWord(&quot;the quick brown fox&quot; split(&quot; &quot;))longest: (String, Int) = (quick,1)//访问元组的元素scala&gt; longest._1res27: String = quickscala&gt; longest._2res28: Int = 1//而且,你可以把元组的每个元素赋值给他自己的变量(这种模式实际上是模式匹配的特例)scala&gt; val (word, idx) = longestword: String = quickidx: Int = 1scala&gt; val word, idx = longest //相当于为每个变量赋值word: (String, Int) = (quick,1)idx: (String, Int) = (quick,1)//每个变量被初始化为右侧表达式的单次执行结果","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第十一章 scala的层级","date":"2017-04-16T04:47:25.902Z","path":"2017/04/16/scala编程/第十一章 scala的层级/","text":"&emsp;在scala里,每个类都继承自通用的名为Any的超类,因为所有的类都是Any的子类,所以定义在Any中的方法就是”共同”的方法,他们可以被任何对象调用,scala还在层级的底端定义了一些有趣的类,如:Null,和Nothing,来扮演通用的子类,例如:如同Any是所有其他类的超类,Nothing是所有其他类的子类 1.scala的类层级123456789101112131415161718/*层级的顶端是Any类,定义了如下的方法*/final def ==(that: Any): Booleanfinal def !=(that: Any): Booleandef equals(that: Any): Booleandef hashCode: Intdef toString: String/*因为每个类都继承自Any,所以scala程序里的每个对象都能用== , != , 或equals比较,用hashCode来做散列,以及用toString来格式化,Any类里的等号和不等号方法被声明为final,因此他们不能在子类里重写,实际上,==总是和equals相同,!=总是与equals相反因此独立的类可以通过重写equals方法改变==或!=的意义*//*根类Any有两个子类,AnyVal和AnyRef,AnyVal是scala里每个内建值类的父类,有9个这样的值类,:Byte,Short,Char, Int, Long, Float, Double, Boolean, 和Unit, 其中的前8个都对应到Java的基本类型,他们的值在运行时表示成Java的基本类型的值,scala里这些类的实例都写成字面量,如: 42是Int的实例, &apos;x&apos; 是Char的实例, false是Boolean的实例,你不能使用new创造这些类的实例,因为值类被定义成既是抽象的又是final的,因此不能写成 new Int*/ 12345678910111213/*另一个值类,Unit,大约对应于Java的void类型,被作用于不返回任何有趣结果的方法的结果类型,Unit类型只有一个实例值,写成()*//*类Any的另一个子类是类AnyRef,这个是scala里所有引用类的基类,正如前面提到的,在Java平台上AnyRef实际就是类java.lang.Object 的别名,因此Java里写的类和scala里写的类都继承自AnyRef,推荐使用AnyRef*//*scala类与Java类的不同在于他们还继承自一个名为ScalaObject的特别的记号特质,ScalaObject只是包含了一个方法,名为$tag,在内部使用以加速模式匹配*/ 2.原始类型是如何实现的1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//java5的自动装箱//Java代码boolean isEqual(int x , int y)&#123; return x == y&#125;system.out.println(isEqual(444,444))//打印结果true//现在讲int变成java.lang.Integer (或者Object 对象), 以下是Java代码boolean isEqual(Integer x , Integer y)&#123; return x == y&#125;system.out.println(isEqual(444,444))//打印结果false //原因是数字444被装箱了两次,因此参数x和y是两个不同的对象,而==表示的是引用相等,所以结果为false//上述情况说明Java不是纯粹的面向对象语言的一个方面,因为我们能够清楚的观察到基本类型和引用类型之间的差别//scala里的尝试试验def isEqual(x: Int, y:Int) = x == y //使用的是基本类型//调用isEquals(444,444) //truedef isEqual(x: Any, y:Any) = x == y //使用的是引用类型//调用isEquals(444,444) //true/*在scala里为何基本类型和引用类型的结果是一样的?原因是基本类型和引用类型都是继承自Any,所以结果一样*//*实际上在scala里的相等操作==被设计为对类型表达式透明,对值类型来说,就是自然的(数学或布尔)相等,对引用类型,==被视为继承自Object的equals方法的别名,equals就是比较的是内容*//*然而,有些情况你需要使用引用相等代替用户定义的相等,例如:某些时候效率是首要因素,你想要把某些类散列合并然后通过引用相等比较他们的实例*/val x = new String(&quot;abc&quot;)val y = new String(&quot;abc&quot;)x == y //truex eq y //falsex ne y //true/*上述代码中,==比较的是值相等(和equals类似),而eq使用的是引用相等*/ 3.底层类型123456789101112131415161718192021222324252627/*在scala类型层级的底部有两个雷scala.Null 和 scala.Nothing 他们是用统一的方式处理scala面向对象类型系统的某些&quot;边界问题&quot; 的特殊类型Null类是null引用对象的类型,他是每个引用类的子类,Null不兼容值类型,如,你不能把null值赋给整数变量*/val i: Int = null //error/*Nothing 类型在scala的类层级的最底端,他是任何其他类型的子类型,然而,根本没有这个类型的任何值,那么要一个没有值的类型有什么意思呢?Nothing的一个用处是他标明了不正常的终止,例如scala的标准库中的Predef对象有一个error方法,如下定义:*/def error(message: String): Nothing = throw new RuntimeException(message)/*error的返回类型是Nothing,告诉用户方法不是正常返回的,因为Nothing是任何其他类型的子类,所以你可以非常灵活的使用像error这样的方法:*/def divide(x: Int, y: Int): Int = if(y != 0) x/y else error(&quot;can&apos;t divide by zero&quot;)/*如果执行了else,调用了error,类型为Nothing,因为Nothing是任何类型的子类型,也是Int的子类型,所以整个状态语句的类型是Int,正如需要的那样*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第六章 函数式对象","date":"2017-04-16T04:47:25.901Z","path":"2017/04/16/scala编程/第六章 函数式对象/","text":"1.类ration的规格说明书1有理数是一种可以表达式比率 n/d 的数字,这里n和d是数字,其中d不能为零,n被称为分子,d被称为分母 2.创建rational12345678910111213141516class Rational(n:Int, d:Int)/*这行代码里首先应当注意到的是如果类没有主体,就不需要指定一对空的花括号(当然,如果你想指定也是可以的),在类名Rational之后的括号里的n和d,被称为类参数,scala编译器会收集这两个参数并创造出同样的两个参数的主构造器Java类具有可以带参数的构造器,而scala类可以直接带参数,scala的写法更简洁,类参数可以直接在类的主体中使用没有必要定义字段然后写赋值函数把构造器的参数复制到字段里,这里无形省略了很多固定写法,尤其是对小类scala编译器将把类内部的任何既不是字段也不是方法定义的代码编译至主机构造器中,例如:*/class Rational(n:Int, d:Int) &#123; println(&quot;Create &quot;+n+&quot;/&quot;+d) //主构造器中的内容&#125;/*scala编译器把这段代码的println调用放进Rational的主构造器,因此,println调用将在每次创建新的Rational实例时打印这条信息*/ 不可变对象的权衡12345678910/*不可变对象提供了若干可变对象的优点和一个潜在的缺点1.不可变对象常常比可变对象更易理清头绪,因为他们没有回随着时间变化的复杂的状态空间2.其次,你可以很自由的传递不可变对象,但是对于可变对象而言,传递给其他代码之前,需要先建造一个以防万一的副本3.一旦不可变对象完成构造之后,就不会有线程因为并发访问而破坏对象内部状态,因为根本没有线程可以改变不可变对象的状态4.不可变对象让哈希表键值更安全,比方说,如果可变对象在进入HashSet之后被改变,那么你下一次查找这个HashSet时就找不到这个对象了#缺点有时需要复制很大的对象表而可变对象的更新可以在原址发生,有些情况下这会变成难以快速完成而可能产生性能瓶颈*/ 3.重新实现toString方法123class Rational(n:Int, d:Int) &#123; override def toString = n + &quot;/&quot; + d //override修饰符说明这是对原有方法定义的重载&#125; 4.检查先决条件123456789//确保分母不能为零,两种做法://方式一class Rational(n:Int, d:Int) &#123; require(d != 0) //require方法定义在对象Predef中 override def toString = n + &quot;/&quot; + d &#125;/*require方法带有一个布尔型参数,如果传入的值为真,require将正常返回,反之,require将抛出llegalArgumentException阻止对象被构造*/ 5.添加字段123456789101112131415161718192021222324252627282930313233343536373839class Rational(n:Int, d:Int) &#123; require(d != 0) override def toString = n + &quot;/&quot; + d def add(that: Rational): Rational =&#123; new Rational(n*that.d + that.n*d, d*that.d) //n和d是构造器中的n和d,add方法中是拿不到该属性的,但是对象可以拿到 &#125;&#125;/*尽管类参数n和d都在add代码可引用的范围内,但是add方法仅能访问调用对象自身的值,并不能访问that对象的d和n,因为that并不是调用add的Rational对象,所以上述代码回报下面的异常:*/Error:(5, 25) value d is not a member of Rational new Rational(n*that.d + that.n*d, d*that.d)Error:(5, 46) value d is not a member of Rational new Rational(n*that.d + that.n*d, d*that.d)/*如果想要访问that的d和n,需要把他们放在字段中,如下*/class Rational(n:Int, d:Int) &#123; require(d != 0) val number = n val denom = d override def toString = number + &quot;/&quot; + denom def add(that: Rational): Rational =&#123; new Rational(n*that.denom + that.number*d, d*that.denom) &#125;&#125;/*尽管n和d在类范围内有效,但因为他们只是构造器的一部分,所以scala编译器不会为他们自动构造字段,所以我们要手动添加字段(即number和denom)*//*我们之前不能在对象外部直接访问有理数的分子和分母,现在可以了,只要访问公共的number,denom字段即可*/val r = new Rational(1, 2)r.numberr.denom 6.自指向123456789101112/*关键字this指向当前执行方法被调用的对象实例*///下面的方法测试有理数是否小于传入的参数def lessThan(that:Rational) = &#123; this.number * that.denom &lt; that.number * this.denom&#125;//下面的方法返回有理数和参数中的较大者def max(that: Rational) = &#123; if (this.lessThan(that)) that else this //如果返回的是this,表示返回的是当前对象&#125; 7.辅助构造器123456789101112131415161718192021222324252627/*有时候一个类里需要多个构造器,scala里主构造器之外的构造器被称为辅助构造器,比如:分母为1的有理数只写分子的话就更为简洁,写成这样Rational(5), 这就需要给Rational添加只传分子的辅助构造器并预设分母为1,如下:*/class Rational(n:Int, d:Int) &#123; require(d != 0) val number = n val denom = d def this(n:Int) = this(n,1) //辅助构造器 override def toString = number + &quot;/&quot; + denom def add(that: Rational): Rational =&#123; new Rational(n*that.denom + that.number*d, d*that.denom) &#125; def lessThan(that:Rational) = &#123; this.number * that.denom &lt; that.number * this.denom &#125; def max(that: Rational) = &#123; if (this.lessThan(that)) that else this &#125;&#125;/*scala里的每个辅助构造器的第一个动作都是调用同类的别的构造器,换句话说,每个scala类的每个辅助构造器都是以 this(...) 形式开头的,被调用的构造器就可以是主构造器,也可以是源文件中早于调用构造器定义的其他辅助构造器,规则的根本结果就是每个scala的构造器调用最终结束于对主构造器的调用,因此主构造器是类的唯一入口点scala的类里面只有主构造器可以调用超类的构造器*/ 8.私有字段和方法12345678910111213class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125;&#125; 9.定义操作符12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/*有理数(分数)的加法写成如下的形式:x + y而不用写成:x.add(y)*/class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125; def +(that: Rational): Rational = &#123; new Rational( number * that.denom + that.number * denom, denom * that.denom ) &#125; def *(that: Rational): Rational = &#123; new Rational(number*that.number,denom * that.denom) &#125; override def toString = number + &quot;/&quot; + denom&#125;/*测试*/object Rational&#123; def main(args: Array[String]): Unit = &#123; val x = new Rational(3,4) val y = new Rational(1,2) val z = x + y //也可以写成 x.+(y)//不过这样写可读性不佳 //按照scala的优先级规则,Rational的*方法比+方法优先级更高 x + x * y //等同于 x + (x * y) &#125;&#125; 10.scala的标识符和命名规范1234567891011121314151617181920#字母数字标识符:以字母或下划线开始,之后可以跟字母,数字,或下划线/*scala遵循Java的驼峰式标识符习惯,例如:toString和HashSet,,尽管下划线在标识符内是合法的,但是scala程序里并不常用,部分原因是为了保持与Java一致,同样也由于下划线在scala代码里有许多其他非标识符用法,因此,最好避免使用像to_string, _init_ 这样的标识符字段/方法参数/本地变量/还有函数的驼峰式名称,应该以小写字母开始,如:length, flatMap类和特质的驼峰式名称应该以大写字母开始,如:BigInt, List scala与Java的习惯不一致的地方在于常量名,scala里constant这个词并不等同于val,尽管val被初始化之后的确保持不变,但他仍然是变量在Java里,习惯上常量名称全都是大写的,用下划线分割单词,如:MAX_VALUE或PI,在scala里,习惯只是第一个字母必须大写,因此Java风格的常量名在scala里也可以用,但是scala的习惯是常数也用驼峰式风格,如:XOffset操作符标识符由一个或多个操作符字符组成,操作符字符是一些+, :, ?, ~, # 混合标识符:由字母数字组成,后面跟着下划线和一个操作符标识,如: unary_+ 被用作定义一元的&quot;+&quot; 操作符的方法名,或 myvar_= 被用作定义赋值操作符的方法名,字面量标识符,使用反引号 `....` 包括的任意字符串,如:`x` `&lt;clinit&gt;` `yield`在Java的Thread类中访问静态的yield方法是他的典型用例,你不能写Thread.yield() ,因为yield是scala的保留字,然而可以在反引号里引用方法的名称,例如:Thread.`yield`()*/ 11.方法 重载12345678910111213141516171819202122232425262728293031323334353637/*上述 * 的操作符数只能是有理数,所以对于有理数r不能写r * 2 ,只能写成r* new Rational(2) ,这样写很不美观,为了让Rational用起来方便,可以在类上增加能够执行有理数和整数之间的加法和乘法的新方法*/class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125; def +(that: Rational): Rational = &#123; new Rational( number * that.denom + that.number * denom, denom * that.denom ) &#125; def + (i: Int): Rational = &#123; new Rational(number + i*denom, denom) &#125; def *(that: Rational): Rational = &#123; new Rational(number*that.number,denom * that.denom) &#125; def * (i: Int):Rational = &#123; new Rational(number * i, denom) &#125; override def toString = number + &quot;/&quot; + denom&#125; 12.隐式转换12345678910/*上面的做法可以对 r*2 进行计算了,但是 2 * r 还是不能进行,因为2*r等同于2.*(r),因此这是在整数2上的方法调用,但Int类没有带Rational参数的乘法不过scala有另外的方法解决这个问题,可以创建在需要的时候自动把整数转换为有理数的隐式转换,如下:*/implicit def intToTational(x: Int) = new Rational(x)/*上述代码定义了从Int到Rational的转换方法,方法前面的implicit修饰符告诉编译器可以在一些情况下自动调用*/val x = new Rational(3,4)val z = 2 * x","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第八章 函数和闭包","date":"2017-04-16T04:47:25.899Z","path":"2017/04/16/scala编程/第八章 函数和闭包/","text":"1.方法12345678910111213141516/*作为某个对象的成员的函数,被称之为方法(method),*/import scala.io.Sourceobject LongLines&#123; def processFile(fileName: String, width: Int): Unit =&#123; val source = Source.fromFile(fileName) for (line &lt;- source.getLines()) processLine(fileName, width, line) &#125; private def processLine(fileName: String, width: Int, line: String): Unit =&#123; if (line.length&gt;width) println(fileName + &quot;: &quot; + line.trim) &#125;&#125; 2.本地函数12345678910111213141516171819202122232425262728293031//把函数定义在别的函数之内,就好像本地变量那样,这种本地函数仅在包含他的代码块中可见,外部无法访问import scala.io.Sourceobject LongLines&#123; def processFile(fileName: String, width: Int): Unit =&#123; //函数中的函数 def processLine(fileName: String, width: Int, line: String): Unit =&#123; if (line.length&gt;width) println(fileName + &quot;: &quot; + line.trim) &#125; val source = Source.fromFile(fileName) for (line &lt;- source.getLines()) processLine(fileName, width, line) &#125;&#125;/*因为本地函数可以访问包含其函数的参数,你可以直接使用外部processLine 函数的参数 */import scala.io.Sourceobject LongLines&#123; def processFile(fileName: String, width: Int): Unit =&#123; //函数中的函数 def processLine(line: String): Unit =&#123; if (line.length&gt;width) println(fileName + &quot;: &quot; + line.trim) &#125; val source = Source.fromFile(fileName) for (line &lt;- source.getLines()) processLine(line) &#125;&#125; 3.头等函数1234567891011121314151617181920212223242526272829303132333435363738/*scala的函数是头等函数,你不仅 可以定义和调用函数,还可以把他们写成匿名的字面量,并把他们作为值传递*///简单的函数(x: Int) =&gt; x+1 //=&gt;指明这个函数把左边的东西(任何整数x)转变成右边的东西(x+1) ,所以这个函数可以把任意整数x映射为 x+1//函数值是对象,所以如果你愿意,可以将其存入变量,当然也是可以使用括号的写法对其进行调用scala&gt; var increase = (x:Int) =&gt; x+1increase: Int =&gt; Int = &lt;function1&gt;scala&gt; increase(10)res0: Int = 11/*如果你想让函数字面量包含多条语句,可以用花括号包住函数体,一行放一条语句,这样就组成了代码块,与方法一样,当函数值被调用时,所有的语句将被执行,而函数的返回值就是最后一行表达式产生的值*/increase = (x: Int) =&gt;&#123; println(&quot;We&quot;) println(&quot;are&quot;) println(&quot;here&quot;) x + 1&#125;//执行结果increase(10)Wearehererest4: Int = 11//将函数作为字面量传递给函数的参数val someNumbers = List(-11, -10, -5, 0, 5, 10)someNumber.foreach((x: Int) =&gt; println(x) ) 4.函数字面量的短格式12345//去除参数类型someNumber.foreach( (x) =&gt; println(x) ) //因为x的类型在someNumber中已经确定了,所有可以知道x的类型,所以就没有必要指定x的类型//更加简洁的做法someNumber.foreach( x =&gt; println(x) ) 5.占位符语法1234567891011121314151617181920/*可以把下划线当做是一个或多个参数的占位符,这样让函数字面量更简洁,只要每个参数在函数字面量内仅出现一次*/someNumber.filter(x =&gt; x&gt;0)//简写为:someNumber.filter(_ &gt; 0) //即_ 相当于 x=&gt;x//有时你把下划线当做参数的占位符,编译器可能无法推断缺失的参数类型,例如,假设你只是写:scala&gt; val f = _ + _&lt;console&gt;:7: error: missing parameter type for expanded function ((x$1, x$2) =&gt;x$1.$plus(x$2)) val f = _ + _ ^//上述情况下你可以使用冒号指定类型,如下:val f = (_: Int) + (_: Int)f(5,10)/*请注意_+_将扩展成带两个参数的函数字面量,这样也解释了为何仅当每个参数在函数字面量中最多出现一次时,你才能使用这种短格式,多个下划线指代多个参数,而不是单个参数的重复使用,第一个下划线代表第一个参数,第二个下划线代表第二个参数...*/ 6.部分应用的函数1234567891011121314151617181920212223/*尽管前面的例子里下划线替代的只是单个参数,你还可以使用单个下划线替换整个参数列表,例如:写成println(_) ,或者更好的方法你还可以写成println_ */someNumber.foreach(println _)//等价于someNumber.foreach( x =&gt; println(x) )/*在上述例子中,下划线不是单个参数的占位符,他是整个参数列表的占位符,请记住要在函数名和下划线之间留一个空格,因为不这样做编译器会认为你是在说明一个不同的符号(将println_当做一个整体)*//*像上面的方式使用下划线,你就正在写一个部分应用函数,部分应用函数是一种表达式,你不需要提供函数需要的所有参数,代之以仅提供部分,或不提供所需参数,比如要创建调用sum的部分应用表达式,而不提供任何3个所需参数,只要在sum之后放一个下划线即可,然后可以把得到的函数存入变量,如下:*/def sum(a: Int, b:Int, c: Int) = a + b +cval a = sum _ //实例化一个带3个缺失整数参数的函数值,并把这个新的函数值的索引赋给变量a,那么就可以使用a了a(1, 2, 3)/*上述的过程如下:名为a的变量指向一个函数值对象,这个函数值是由scala编译器依照部分应用函数表达式sum _ ,自动产生的类的一个实例,编译器产生的类有一个apply方法带3个参数,之所以带3个参数是因为sum _ 表达式缺少的参数数量为3,scala编译器把表达式a(1,2,3)翻译成对函数值的apply方法的调用,传入3个参数1,2,3,因此a(1,2,3)是下列代码的短格式:a.apply(1,2,3)*/ _形式的偏函数 1234567891011121314151617181920212223242526272829303132333435/*在sum _ 的例子里,他没有应用于任何参数,不过还可以通过提供某些但不是全部需要的参数表达一个偏函数,如下:*/val b = sum(1, _: Int, 3)//调用b(2) //b.apply调用了sum(1,2,3)/*scala的编译器会产生一个新的函数类,其apply方法带有一个参数,在使用一个参数调用的时候,这个产生的函数的apply方法调用sum,传入(1, 参数, 3)*/b(5) //b.apply调用了sum(1, 5, 3)//如果可以省略所有的参数,则可以写成println _ 或则sum _ 这样的形式, 更进一步,如果在代码的那个地方正需要一个函数,你可以去掉下划线从而表达得更简明someNumbers.foreach(println _)//简写someNumbers.foreach(println)/*最后一种格式仅在需要写函数的地方,如例子中的foreach调用,才能使用,编译器知道这种情况下需要一个函数,因为foreach需要一个函数作为参数传入,在不需要函数的情况下,尝试使用这种格式将引发一个编译错误:*/scala&gt; def sum(a:Int, b:Int, c:Int) = a+b+csum: (a: Int, b: Int, c: Int)Intscala&gt; val c = sum //编译错误&lt;console&gt;:8: error: missing arguments for method sum;follow this method with `_&apos; if you want to treat it as a partially applied function val c = sum ^scala&gt; val c = sum _ //正确c: (Int, Int, Int) =&gt; Int = &lt;function3&gt; 7.闭包123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384(x: Int) =&gt; x+more/*自由变量:more,因为函数字面量没有给出其含义绑定变量:x,被定义为函数的唯一参数是Int*//*如果你尝试独立使用这个函数字面量,范围内没有任何more的定义,编译器会报错说:*/scala&gt; (x:Int) =&gt; x+more&lt;console&gt;:8: error: not found: value more (x:Int) =&gt; x+more ^//所以首先要对more进行定义scala&gt; var more = 1more: Int = 1scala&gt; val addMore = (x:Int) =&gt; x+moreaddMore: Int =&gt; Int = &lt;function1&gt;/*不带自由变量的函数字面量,如:(x:Int)=&gt;x+1被称为封闭项同理:(x:Int)=&gt;x+more都是开放项任何以(x:Int)=&gt;x+more为模板在运行期创建的函数值将必须捕获对自由变量more的绑定,因此得到的函数值将包含指向捕获的more变量的索引,又由于函数值是关闭这个开放项(x:Int)=&gt;x+more的行动的最终产物,因此被称之为闭包*/scala&gt; var more = 1more: Int = 1scala&gt; val addMore = (x:Int) =&gt; x+moreaddMore: Int =&gt; Int = &lt;function1&gt;scala&gt; addMore(10)res1: Int = 11scala&gt; more = 999more: Int = 999scala&gt; addMore(10)res2: Int = 1009/*直觉上闭包捕获的是变量本身 ,而不是变量指向的值,所以自由变量的改变,在闭包内可以看到,即:在(x:Int)=&gt;x+more 中是可以看到外面对于more的改变的同理,如果在闭包内部将自由变量more改变,那么在闭包的外部也是可以看到的,如下:*/scala&gt; val someNumbers = List(-11, -5, -10, 0, 10)someNumbers: List[Int] = List(-11, -5, -10, 0, 10) ^scala&gt; var sum = 0sum: Int = 0scala&gt; someNumbers.foreach(sum += _)scala&gt; sumres5: Int = -16/*上述例子中变量sum处于函数字面量sum+=_的外围,函数字面量把数累加到sum上,尽管这是一个在运行期改变sum的闭包,作为结果的累加值,-16,仍然在闭包之外可见*//*闭包使用了某个函数的本地变量*/scala&gt; def makeIncreaser(more:Int) = (x:Int)=&gt;x+moremakeIncreaser: (more: Int)Int =&gt; Intscala&gt; val inc1 = makeIncreaser(1)inc1: Int =&gt; Int = &lt;function1&gt;scala&gt; val inc999 = makeIncreaser(999)inc999: Int =&gt; Int = &lt;function1&gt;scala&gt; inc1(10)res6: Int = 11scala&gt; inc999(10)res7: Int = 1009/*因为闭包是依赖的是函数的本地变量,所以即使对函数本地变量传递不同的值,也是得到不同的闭包函数,而不是改变了原有创建的闭包*/ 8.重复参数12345678910111213141516171819202122232425262728293031323334353637383940/*在scala中你可以指明函数的最后一个参数是重复的,从而允许客户向函数传入可变长度参数列表,想要标注一个重复参数,可以在参数的类型之后放一个星号*/scala&gt; def echo(args:String*) = for(arg&lt;-args) println(arg)echo: (args: String*)Unitscala&gt; echo()scala&gt; echo(&quot;one&quot;)onescala&gt; echo(&quot;one&quot;,&quot;two&quot;,&quot;three&quot;)onetwothree/*函数内部,重复参数的类型是声明参数类型的数组,如:echo函数里被声明为类型&quot;String&quot;的args的类型实际上是Array[String],然而,如果你有一个合适类型的数组,并尝试把它当做重复参数传入,你会得到一个编译器错误:*/scala&gt; val arr = Array(&quot;what&quot;,&quot;is&quot;,&quot;up&quot;)arr: Array[String] = Array(what, is, up)scala&gt; echo(arr)&lt;console&gt;:10: error: type mismatch; found : Array[String] required: String echo(arr) ^//要实现上诉做法,需要在数组参数后添加一个冒号和一个_*符号,如下:scala&gt; echo(arr:_*) //这个标注告诉编译器把arr的每个元素当做参数,而不是当做单一的参数传给echowhatisup 9.尾递归123456789101112131415def approximate(guess: Double): Double =&#123; if (isGoodEnough(guess)) guess else approximate(improve(guess))&#125;def isGoodEnough(guess: Double) =&#123; true //假设实现&#125;def improve(guess: Double)=&#123; 1.0 //假设实现&#125;/*这样的函数,带合适的isGoodEnough和improve的实现,经常在查找问题中,在上面approximate的例子中,scala编译器可以应用一个重要的优化,注意递归调用时approximate函数体执行的最后一件事,像approximate这样,在他们最后一个动作调用自己的函数,被称为尾递归(tail recursive)*/ 尾递归函数的追踪1234567891011121314151617181920212223242526272829303132333435/*尾递归函数将不会为每个调用制造新的堆栈结构,所有的调用将在一个结构内执行,*/scala&gt; def boom(x:Int):Int= if(x==0) throw new Exception(&quot;boom Exception&quot;) else boom(x-1)+1/*上述函数不是尾递归,因为在递归调用之后执行了递增操作,如果执行他,会得到如下的结果:*/scala&gt; boom(3)java.lang.Exception: boom Exception at .boom(&lt;console&gt;:7) at .boom(&lt;console&gt;:7) at .boom(&lt;console&gt;:7) at .boom(&lt;console&gt;:7)....//现在修改boom从而让他变成尾递归scala&gt; def bang(x:Int):Int= if(x==0) throw new Exception(&quot;bang Exception&quot;) else bang(x-1)//执行结果如下:scala&gt; bang(5)java.lang.Exception: bang Exception at .bang(&lt;console&gt;:9) at .&lt;init&gt;(&lt;console&gt;:9)....../*上述打印结果中只是看到了bang的一个堆栈结构,*/ 尾递归的局限1234567891011121314151617/*在scala里尾递归的使用局限很大,因为JVM指令集使实现更加先进的尾递归形式变得很困难*///如果递归是间接的,就像在下面的例子里两个相互递归的函数,就没有优化的可能性了def isEven(x: Int):Boolean= if (x==0) true else isOdd(x-1)def isOdd(x: Int): Boolean= if (x==0) false else isEven(x-1)//如果最后一个调用是一个函数值,你也不能获得优化val funValue = nestedFun _def nestedFun(x: Int): Unit =&#123; if (x!=0)&#123; println(x) funValue(x-1) &#125;&#125;","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第五章 基本类型和操作","date":"2017-04-16T04:47:25.897Z","path":"2017/04/16/scala编程/第五章 基本类型和操作/","text":"1.基本类型12345/*在scala中有如下的基本类型:Byte,Short,Int,Long,Char,String, Float, Double, Boolean,除了String归于java.lang包之外,其余所有的基本类型都是包scala的成员,如,Int的全名是scala.Int,然而,由于包scala和Java.lang的所有成员都被每个scala源文件自动引用,因此可以在任何地方只用简化名(也就是说,直接写成Boolean,Char或String)目前实际上scala值类型可以使用与Java的原始类型一致的小写化名称,比如,scala程序里可以用int替代Int,但请记住他们都是一回事,scala.Int, scala的社区实践提出的推荐风格是一直使用大写形式,这也是本书推荐的,将来scala的版本可能不再支持乃至移除小写化名称,因此跟随社区的趋势,在scala代码中使用Int而非int才是明智之举*/ 2.字面量123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/*字面量就是直接下载代码里的常量*//*整数字面量类型Int,Long, Short和Byte的整数字面量有三种格式,十进制,十六进制和八进制,整数字面量的开头方式说明了数值的进制,如果数开始于0x或0X,那他是十六进制,并且可能包含从0到9,及大写或小写的从A到F的数字,如下:*/val hex = 0x5val hex2 = 0x00FFval magic = 0xcafebabe/*八进制如果数开始于0,就是八进制的,并且只可以包含数字0到7*/val oct = 035val no = 0777val dec = 0321/*十进制如果数字开始于非零数字,并且没有被修饰过,就是十进制的,例如:*/val dec1 = 11val dec2 = 255/*长整形:Long如果整数字面量结束于L或者l*/val prog = 0xcafebaelval tower = 35Lval of = 31l //小写l/*Short, Byte如果Int类型的字面量被赋值给Short或Byte类型的变量,字面量就会被当做是赋值的类型,以便让字面量值处于有效范围内,如:*/val little: Short = 367val little: Byte = 38/*浮点数字面量浮点数字面量是由十进制数字,可选的小数点,可选的E或e及指数部分组成的,举例如下:*/val big = 1.2345val bigger = 1.2343Lval biggerStill = 123E45 //123E45就是123乘以10的45次幂/*如果浮点数字面量以F或f结束,就是Float类型的,否则就是Double类型的,可选的,Double浮点数字面量也可以是D或者d结尾的,举例如下:*/val little = 1.234F //1.234val littleBigger = 3e5f //300000.0val yetAnother = 3e5D/*字符字面量,可以是在单引号之间的任何Unicode字符,如:*/val a = &apos;A&apos;val c = &apos;\\101&apos; //单引号之间除了可以摆放字符之外,还可以提供一个前缀反斜杠的八进制或十六进制的表示字符编码号的数字,八进制数必须在&apos;\\0&apos; 和&apos;\\377&apos;之间val d = &apos;\\u0041&apos; //\\u表示十六进制 特殊的转义字符12345678910111213/*\\n 换行\\b 回退\\t 制表符\\f 换页\\r 回车\\&quot; 双引号\\&apos; 单引号\\\\ 反斜杠*/val backslash = &apos;\\\\&apos; 123456789101112131415161718192021222324252627282930313233/*字符串字面量:字符串字面量是由双引号包括的字符组成*/val escapes = &quot;\\\\\\&quot;\\&apos;&quot; //字符串: \\&quot;&apos;/*三引号:以一行里的三个引号作为开始和结束,内部的原始字符串可以包含无论何种任意字符,包括新行,引号,和特殊字符,举例如下:*/println(&quot;&quot;&quot;Welcome to Ultamix 3000.Type &quot;Help&quot; for help.&quot;&quot;&quot;)/*#输出如下:Welcome to Ultamix 3000. Type &quot;Help&quot; for help. 第二行的前导的空格被包含在了字符串里,为了解决这个常见情况,字符串类引入了stripMargin方法,使用的方式是,把管道符号(|)放在每行前面,然后对整个字符串调用stripMargin,如下:*/println( &quot;&quot;&quot; |Welcome to Ultamix 3000. |Type &quot;help&quot; for help &quot;&quot;&quot;.stripMargin)/*输出如下:Welcome to Ultamix 3000.Type &quot;help&quot; for help*/ 3.操作符和方法123456789101112131415161718192021222324252627282930313233343536373839val sum = 1+2 //scala调用了(1).+(2)/*实际上+包含了各种类型的重载方法,像+这样的操作符叫做中缀操作符,这样的操作符还有indexOf*/val s = &quot;hello, world!&quot;s indexOf &apos;0&apos; //调用s.indexOf(&apos;o&apos;)//另外String还提供了重载的indexOf方法,带两个参数,分别是要搜索的字符和从哪个索引开始搜索s indexOf (&apos;0&apos;, 5) //调用了s.indexOf(&apos;0&apos;, 5)/*任何方法都可以是操作符scala里的操作符不是特殊的语法,任何方法都可以是操作符到底是方法还是操作符取决于你如何使用它,如果写成s.indexOf(&apos;o&apos;) , indexOf是方法,但是如果写成s indexOf &apos;o&apos; ,那么indexOf就是操作符,因为你以操作符标注方式使用它*//*前缀标注和后缀标注方法名放在调用的对象之前,如, -7里的&quot;-&quot; 后缀标注中,方法放在对象之后,如 7 toLong 里的&quot;toLong&quot;*//*方法名在操作符上前缀&quot;unary_&quot;,例如scala表达式-2.0 转换成方法调用 (2.0).unary_- 操作符中能作为前缀操作符用的有值+.-,!,和~ 因此,如果对类型定义了名为unary_! 的方法,就可以对值或变量用!p这样的前缀操作符方式调用方法,但是即使定义了名为unary_*的方法,也没有办法将其使用成操作符了,因为*不是四种可以当做前缀操作符用的标识符之一,你可以向平时那样调用它,如:p.unary_* ,但是如果尝试像*p这么调用,scala就会把它理解为*.p ,者或许就不是你所期望的了*//*后缀操作符是不用点或括号调用的不带任何参数的方法,在scala里,方法调用的空括号可以省略,惯例是如果方法带有副作用就加上括号,如println(), 如果没有副作用就去掉括号,如String 的toLowerCase*/val s = &quot;hello, world&quot;s.toLowerCase //方法里没带参数,因此还可以去掉点,采用后缀操作符标注方式s toLowerCase //toLowerCase被当做操作数s的后缀操作符 4.对象相等性123456789101112131415161718/*如果要比较一下两个对象是否相等,可以使用==,或者他的反义 != */1 == 2 //false1 != 2 //true2 == 2 //true//以上这些操作对所有对象都起作用,而不仅仅是基本类型,List(1, 2, 3) == List(1, 2, 3) //trueList(1, 2, 3) == List(4,5,6) //false1 == 1.0 //trueList(1, 2, 3) == &quot;hello&quot; //falseList(1, 2, 3) == null //falsenull == List(1, 2, 3) //false/*== 已经被仔细加工过,因此多数情况下都可以实现合适的相等性比较,这种比较遵循一种非常简单的规则,首先检查左侧是否为null,如果不是调用左操作数的equals方法,而精确的比较取决于左操作数的equals方法定义,由于有了自动的null检查,因此不需要手动再检查一次了这种比较即使发生在不同的对象之间也会产生true,只要比较的两者内容相同并且equals方法是基于内容编写的,例如,以下是恰好都有五个同样字母的两个字符串的比较:(&quot;he&quot; + &quot;llo&quot;) == &quot;hello&quot; //true*/ scala的==与Java的有何区别123/*Java里==既可以比较原始类型也可以比较引用类型,对于原始类型,Java的==比较值的相等性,与scala一致,而对于引用类型,Java的==比较了引用相等性,也就是说比较的是这两个变量是否都指向JVM堆里的同一个对象,scala也提供了这种机制,scala也提供了这种机制,名字为eq, 不过eq和他的反义词ne,仅仅应用于可以直接映射到Java的对象*/ 5.操作符的优先级和相关性12345/*操作符的优先级决定了表达式的哪个部分先于其他部分被评估,举例来说,表达式2+2 * 7 计算得16,而不是28,因为*操作符比+操作符有更高的优先级,由于scala没有操作符,实际上,操作符只是方法的一种表达方式, 对于操作符形式使用的方法,scala根据操作符的第一个字符判断方法的优先级(这个有个例外) 比方说,如果方法名开始于* ,那么就比开始于+的方法有更高的优先级,因此2+2*7将被评估为2+(2*7), 而a+++b***c 将被看做是a+++(b***c) ,因为***方法比+++方法有更高的优先级*/ 操作符优先级123456789101112131415161718192021/*(所有其他的特殊字符)* / %+:= !&lt;&gt;&amp;^|(所有字母)(所有赋值操作符)以上以降序方式列举了以方法第一个字符判断的优先级,同一行的字符具有相同的优先级*/2 &lt;&lt; 2+2 //+比&lt;&lt;的优先级高,表达式也要先调用了+方法之后在调用&lt;&lt;方法,如: 2 &lt;&lt; (2+2)//特例: *=的操作符的优先级与赋值符号(=)相同,也就是说,他比任何其他操作符的优先级都低,类似的操作符还有 += -= /= ,虽然*比+的优先级高,但是我们这里将 *= 当做是一个整体x *= y + 1 123456789101112131415/*当同样优先级的多个操作符并列出现在表达式里时,操作符的关联性决定了操作符分组的方式,scala里操作符的关联性取决于他的最后一个字符,例如:任何以&quot;:&quot; 字符结尾的方法由他的右操作数调用,并传入左操作数,以其他字符结尾的方法与之相反,他们都是被左操作数调用,并传入右操作数的,因此a*b 变成 a.*(b) 但是a:::b 变成b.:::(a)*//*然而不管操作符具有什么样的关联性,它的操作数总是从左到右评估的,如:a:::b将会被当做是:&#123;val x = a; b.:::(x) &#125;在这个代码块中,a仍然在b之前被评估,然后评估结果被当做操作数传给b的:::方法a:::b:::c // a:::(b:::c)a*b*c //(a*b)*c*/ 6.富包装类&emsp;下面方法的使用要通过隐式转换,现在需要知道的是本章介绍过的每个基本类型,都对应着一个”富包装器”提供的许多额外的方法 代码 结果 基本类型 富包装 0 max 5 5 Byte scala.runtime.RichByte 0 min 5 0 Short scala.runtime.RichShort -2.7 abs 2.7 Int scala.runtime.RichInt -2.7 round -3L Long scala.runtime.RichLong 1.5 isInfinity false Char scala.runtime.RichChar (1.0/0) isInfinity true String scala.runtime.RichString 4 to 6 Range(4,5,6) Float scala.runtime.RichFloat “bob” capitalize “Bob” Double scala.runtime.RichDouble “robet” drop 2 “bert’ Boolean scala.runtime.RichBoolean","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第二十章 抽象成员","date":"2017-04-16T04:47:25.895Z","path":"2017/04/16/scala编程/第二十章 抽象成员/","text":"1.抽象成员的快速浏览123456789101112131415161718192021222324/*我们称不完全定义的类或特质的成员为抽象成员,抽象成员将被声明类的子类实现*//*下面的特质对每种抽象成员各声明了一个例子,他们分别是:类型(T), 方法(transform), val(initial) , 以及var(current):*/trait Abstrant &#123; type T def transform(x: T): T val initial: T val current: T&#125;//Abstract的具体实现需要对每种抽象成员填入定义,下面的例子是提供这些定义的实现class Concrete extends Abstrant&#123; type T = String override def transform(x: String) = x + x val initial= &quot;hi&quot; override var current = initial&#125;/*这个实现为类型T提供了具体的含义,他被定义为类型String的别名,transform被定义为参数字符串与其自身连接的操作,而initial和current值都被设置为&quot;hi&quot;*/ 2.类型成员123456789/*抽象类型这个术语在scala中是指不带具体定义的,由&quot;type&quot;关键字声明为类或特质的成员的类型,类本身可以是抽象的,而特质本来就是抽象的,但不论哪种都不是scala中所指的抽象类型,scala的抽象乐行永远都是某个类或者特质的成员,就好像特质Abstract里的类型T那样你可以把非抽象的类型成员,如Concrete类里的类型T,想象成是类型定义新的名称,或别名的方式,例如Concrete类中,类型String被指定了别名T,因此,任何出现在Concrete定义中的T指的都是String,这也包含了transform的参数和结果类型initial,以及current,这些在Abstract超特质中声明的时候提到的T的成员,因此当Concrete类实现这些抽象成员的时候,所有的T都被解释为String使用类型成员的理由之一是为类型定义短小的,具有说明性的别名,因为类型的实际名称可能比别名更冗长,或语义不清,这种类型成员有助于净化类或特质的代码,类型成员的另一种主要用途是声明必须被定义为子类的抽象类型*/ 3.抽象val12345678910111213141516171819202122232425262728//抽象val以如下形式定义val initial: String //他指明了val的名称和类型,但不指定值,该值必须由子类的具体val定义提供,例如:Concrete类以如下方式实现了val:val initial = &quot;hi&quot;//抽象的val声明类似于抽象的无参数方法声明,如:def initial: String/*如果initial是抽象val,那么客户就获得了保证,每次引用都将得到同样的值,如果initial是抽象方法,就不会获得这样的保证,因为在这种情况下initial可以实现为每次调用时都返回不同值的具体方法*/abstract class Fruit&#123; val v: String //&apos;v&apos;代表value(值) def m: String //&apos;m&apos;代表method方法&#125;abstract class Apple extends Fruit&#123; val v: String val m: String //可以用&quot;val&quot; 重写def&#125;abstract class BadApple extends Fruit&#123; def v: String //error , 不能用&quot;def&quot;重写val def m: String&#125;/*换句话说,抽象的val限制了合法的实现方式,任何实现都必须是val类型的定义,不可以是var或def,另一方面,抽象方法声明可以被实现为具体的方法定义或具体的val定义,*/ 4.抽象var123456789101112131415//与抽象val类似,抽象var只声明名称和类型,没有初始值,如下:trait AbstractTime&#123; var hour: Int var minute: Int&#125;/*类似于hour和minute这样的抽象var表达的是什么意思呢?在18.2节看到声明为类成员的var实际配备了getter和setter方法,对于抽象var来说也是如此,比如如果你声明了名为hour的抽象var,实际上是隐式声明了抽象getter方法,hour及抽象setter方法(hour_=) */trait AbstractTime&#123; def hour: Int //&apos;hour&apos; 的getter方法 def hour_=(x:Int) //&quot;hour&quot; 的setter方法 def minute: Int //&quot;minute&quot;的getter方法 def minute_=(x:Int) //&apos;minute&apos;的setter方法&#125; 5.初始化抽象val1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/*抽象val有时会扮演类似于超类的参数这样的角色:他们能够让你在子类中提供超类缺少的细节信息,这对于特质来说尤其重要,因为特质缺少能够用来传递参数的构造器,因此通常参数化特质的方式就是通过需要在子类中实现的抽闲val完成*/trait RationalTrait &#123; val numerArg: Int val denomArg: Int&#125;//实现new RationalTrait &#123; override val denomArg: Int = 1 override val numerArg: Int = 1&#125;/*这里关键字new出现在特质名称(RationalTrait)之前,然后是花括号包围的类结构体,这个表达式可以产生混入了特质并被结构体定义的匿名类实例,这种特使的匿名类实例化结果与使用new Rational(1,2)的方式创建的实例具有类似的效果,不过这种类比并非完美,两者在表达式的初始化顺序方面存在着微妙的差别*/ //当你写下:new Rational(expr1, expr2)//两个表达式,expr1和expr2会在类Rational初始化之前计算,因此在执行类的初始化操作是expr1和expr2的值已经可用,然而对于特质来说,情况正好相反,当你写下:new RationalTrait&#123; val numerArg = expr1 val denomArg = expr2&#125;//表达式expr1和expr2被作为匿名初始化的一部分计算,但匿名类的初始化在RationalTrait之后,因此numerArg和denomArg的值在RationalTrait初始化期间还没有主备好(更为精确的说,选用任何值都将得到Int类型的默认值,0),对于之前的RationalTrait定义来说,这不是问题,因为特质的初始化没有用到numerArg和denomArg的值,但是对于下面的代码来说就成为一个问题,因为其中定义了经过约分的分子和分母trait RationalTrait&#123; val numerArg: Int val denomArg: Int require(denomArg != 0) private val g = gcd(numerArg, denomArg) val number = numerArg / g val denom = denomArg / g private def gcd(a:Int, b:Int): Int = if (b == 0) a else gcd(b, a%b) override def toString = number + &quot;/&quot; + denom&#125;//测试scala&gt; new RationalTrait&#123; | val numerArg = 1*x | val denomArg = 2*x | &#125;java.lang.IllegalArgumentException: requirement failed at scala.Predef$.require(Predef.scala:221) at RationalTrait$class.$init$(&lt;console&gt;:10) at $anon$1.&lt;init&gt;(&lt;console&gt;:10) at .&lt;init&gt;(&lt;console&gt;:10) at .&lt;clinit&gt;(&lt;console&gt;) at .&lt;init&gt;(&lt;console&gt;:7)/*出错的原因是:当类RationalTrait初始化的时候,denomArg仍然为他的默认值0,使得require调用失败上述例子演示了类参数和抽象字段的初始化顺序并不一致,类参数在被传递给构造器之前计算(除非参数是传名的),相反子类对于val定义的实现,是在超类完成了初始化之后执行的*/ fields预初始化字段1234567891011121314151617181920212223242526272829303132333435363738394041/*第一种解决方案,预初始化字段,可以让你在调用超类之前初始化子类的字段,操作的方式是把字段加上花括号,放在超类构造器调用之前,*/new &#123; val numerArg = 1*x val denomArg = 2*x&#125; with RationalTrait//预初始化字段不仅限于匿名类,他们还可以被用于对象或有名称的子类//预初始化段落在每个例子中都被定义的对象或类的extends关键字之后object twoThirds extends &#123; val numerArg = 2 val denomArg = 3&#125; with RationalTrait/*由于预初始化的字段在超类构造器调用之前被初始化,因此他们的初始化器不能引用正被构造的对象,相应的结果是,如果有引用this的这种初始化器,那么实际指向的是包含了正被构造的类或对象的对象,而不是被构造对象本身*/scala&gt; new &#123; | val numerArg = 1 | val denomArg = this.numerArg*2 | &#125; with RationalTrait&lt;console&gt;:11: error: value numerArg is not a member of object $iw val denomArg = this.numerArg*2 ^/*上述例子编译通过的原因在于this.numerArg引用是在包含new的对象中numerArg字段(这个例子中是指名为$iw的合成对象,解释器会把用户输出的语句放在这个对象中)*///示例了如何在超特质的初始化过程中使用类参数的通用模式class RationalClass(n: Int, d:Int) extends &#123; val numerArg = n val denomArg = d&#125;with RationalTrait&#123; def + (that: RationalClass) = new RationalClass( number * that.denom + that.number*denom, denom * that.denom )&#125; 懒加载val123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/*如果你把lazy修饰符前缀val定义上,那么右侧的初始化表达式直到val第一次被使用的时候才计算*/scala&gt; object Demo&#123; | val x = &#123;println(&quot;initializing x&quot;); &quot;done&quot;&#125; | &#125;defined module Demoscala&gt; Demoinitializing xres6: Demo.type = Demo$@3d0035d2scala&gt; Demo.xres7: String = done/*发现,用到Demo的时候,他的x字段就完成了初始化,x的初始化成为了Demo初始化的一部分*///将x字段定义为lazyscala&gt; object Demo&#123; | lazy val x = &#123;println(&quot;initialing x&quot;); &quot;done&quot;&#125; | &#125;defined module Demoscala&gt; Demores8: Demo.type = Demo$@3c78e551scala&gt; Demo.xinitialing xres9: String = donescala&gt; Demo.xres10: String = done/*初始化Demo不会执行初始化x的调用,x的初始化将延迟到第一次使用x的时候第一次计算懒加载val的时候结果就被保存了下来,以备同样的val后续使用*/trait RationalTrait&#123; val numerArg: Int val denomArg: Int private lazy val g = &#123; require(denomArg != 0) gcd(numerArg, denomArg) &#125; lazy val numer = numerArg / g lazy val denom = denomArg / g override def toString = numer + &quot;/&quot; + denom private def gcd(a:Int, b:Int): Int = if (b == 0) a else gcd(b, a%b)&#125;//测试scala&gt; val x = 2x: Int = 2scala&gt; new RationalTrait&#123; | val numerArg = 1*x | val denomArg = 2*x | &#125;res13: RationalTrait = 1/2//执行过程/*1.首先,RationalTrait的新实例被创建出来,特质的初始化代码被运行,该初始化代码为空,没有任何字段被初始化2.之后,有new表达式定义的匿名子类的主构造器被执行,他把numerArg初始化为2,把denomArg初始化为43.之后,解释器调用了构造器对象的toString方法,结果值被打印出来4.之后,numer字段被特质RationalTrait的toString方法首次访问,因此它的初始化器执行计算5.numer的初始化器访问了私有字段g,因此g接下来被初始化计算,这次计算访问了numerArg和denomArg,他们定义在第二步6.之后,toString方法访问了denom值,引发denom的计算,这次访问计算了denomArg和g的值,g字段的初始化器不再重新计算,因为他已经在第五步执行过7.最终,结果字符串&quot;1/2&quot;被构造出来并被打印*//*请注意,在RationalTrait类中,g的定义在代码文本中处于numer和denom定义之后,尽管如此,因为所有的三个值都是懒加载的,所以g将在numer和denom完成初始化之前被初始化,这说明了懒加载val的一个很重要的属性,定义的文本顺序不用多加考虑,因为初始化是按需的,从而,懒加载val可以免去你作为程序员不得不认真考虑的问题,及如何安排val定义顺序,以确保所有东西在需要的时候已经完成定义*/ 6.抽象类型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/*与所有其他抽象声明一样,抽象类型声明也是将在子类中具体定义的事务的占位符,这里,他是将在之后的类层次中定义的类型,因此上文的T是对在声明点尚不可知的类型的引用,不同的子类可以提供不同的T实现*///假设给了你一个为动物饮食习惯建模的任务,你或许会以Food类和带有eat方法的Animal类开始工作class Foodabstract class Animal &#123; def eat(food: Food)&#125;//然后你或许尝试把这两个类特化为Cow类吃Grass类(牛吃草)class Grass extends Foodclass Cow extends Animal&#123; override def eat(food: Grass) = &#123;&#125;//不能编译&#125;/*这里的情况是Cow类的eat方法不能重写Animal类的eat方法,因为参数类型不同---Cow类里是Grass, 而Animal类里是Food*///为什么要做这样的限制?class Foodabstract class Animal &#123; def eat(food: Food)&#125;class Grass extends Foodclass Cow extends Animal&#123; override def eat(food: Grass) = &#123;&#125;//不能编译,不过如果能够编译通过的话,...&#125;class Fish extends Foodval bessy: Animal = new Cowbessy eat (new Fish) //.....你将能用鱼喂牛/*你应该做的是采用更为精确的建模方式,Animal的确吃Food,但Animal具体吃什么类型的Food取决于Animal,这可以使用抽象类型干净的表示出来*/class Foodabstract class Animal&#123; type SuitableFood &lt;: Food def eat(food: SuitableFood)&#125;/*有了新的定义,Animal就可以只吃适合的食物了,不过到底什么食物合适,这并不在Animal类的层面决定,这也就是SuitableFood被建模为抽象类型的原因,具体具有上界约束:Food,表达为&quot; &lt;:Food &quot; 子句,说明任何(Animal子类中的)SuitableFood的具体实例化结果都必须是Food的子类*/class Grass extends Foodclass Cow extends Animal&#123; type SuitableFood = Grass def eat(food: Grass)&#123;&#125;&#125; 7.路径依赖类型12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455/*通常情况下,不同的路径将产生不同的依赖*/class DogFood extends Foodclass Dog extends Animal&#123; override type SuitableFood = DogFood override def eat(food: DogFood) = &#123;&#125;&#125;//如果你尝试把牛的饲料用来喂狗,你的代码将无法通过编译scala&gt; val bessy = new Cowbessy: Cow = Cow@59edb4f5scala&gt; val lassie = new Doglassie: Dog = Dog@7ea2412cscala&gt; lassie eat (new bessy.SuitableFood)&lt;console&gt;:16: error: type mismatch; found : Grass required: DogFood lassie eat (new bessy.SuitableFood) ^/*问题在于传递给eat方法的SuitableFood对象的类型(bessy.SuitableFood), 不能匹配eat的参数类型,lassie.SuitableFood,然而如果同样是Dog的话,情况会不一样,因为Dog的SuitableFood类型被定义为DogFood类的别名,所以对于两条Dog来说,他们的SuitableFood类型实际上是一样的,*/scala&gt; val bootsie = new Dogbootsie: Dog = Dog@11381415scala&gt; lassie eat (new bootsie.SuitableFood)scala&gt;/*路径依赖类型会让我们想起java中的内部类语法,但两者有决定性的差别:路径依赖类型表达了外在的对象,而内部类表达了外在的类*/class Outer&#123; class Inner&#125;/*scala中,内部类的表达形式为Outer#Inner, 而不是java的Outer.Inner ,&quot;.&quot; 语法保留给对象使用,例如,假设你实例化了类型Outer的两个对象*/val o1 = new Outerval o2 = new Outer/*这里o1.Inner和o2.Inner是两个路径依赖类型,o1.Inner类型是指特定(o1引用的)外部对象的Inner类*///实例化内部类new o1.Inner //因为o1.Inner是属于o1对象的内部类,所以new o1.Inner是new出来的对象//返回的内部对象将包含其外部对象的引用,即o1的对象引用,相反Outer#Inner没有指明任何特定Outer实例,因此你不能创建他的实例new Outer#Inner //error 8.枚举12345678910111213141516171819202122232425262728293031323334353637383940414243/*scala中如果想要创建新的枚举,只需要定义扩展scala.&apos;Enumeration这个类的对象即可*/object Color extends Enumeration&#123; val Red = Value val Green = Value val Blue = Value&#125;//等价于object Color extends Enumeration&#123; val Red, Green, Blue = Value&#125;/*这个对象定义提供了三个值:Color.Red, Color.Green, Color.Blue,你可以引用Color的全部内容:*/import Color._//然后简单写成Red, Green, 和Blue ,但这些值的类型是什么?Enumeration定义了内部类,名为Value,以及同名的无参方法Value返回该类的新对象,也就是说诸如Color.Red类的值类型是Color.Value,而Color.Value也正是定义在对象Color中的所有枚举值的类型,他是路径依赖类型,其中Color是路径,Value是依赖类型,这里很重要的一点是他是全新的类型,与其他所有的类型都不一样object Direction extends Enumeration&#123; val North, East, South, West = Value&#125;//Direction.Value与Color.Value不同,因为两种类型的路径部分不同//scala的Enumeration类还提供了其他语言的枚举设计中所拥有的许多其他特质,你可以通过使用Value方法不同的重载变体把名称与枚举值联系起来object Direction extends Enumeration&#123; val North = Value(&quot;North&quot;) val East = Value(&quot;East&quot;) val South = Value(&quot;South&quot;) val West = Value(&quot;West&quot;)&#125;//遍历枚举的所有值for(d &lt;-Direction) print(d+&quot; &quot;) //North East South West//枚举值从0开始计数,你可以用枚举值的id方法获得他的计数值scala&gt; Direction.East.idres20: Int = 1//也可以反过来,通过非零的整数获得id为该数的枚举值scala&gt; Direction(1)res19: Direction.Value = East 9.案例研究:货币123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151//本章的剩余部分提供了一个解释scala中如何使用抽象类型的案例研究,任务是设计Currency类,典型的Currency实例可以代表美元,欧元,日元,或其他货币种类的一笔金额,而且还有可能需要一些关于货币方面的计算,比方说,相同货币的两笔金额应该能够相加,或金额应该能够乘上代表利率的因子abstract class Currency&#123; val amount: Long //金额大小 def designation: String //标识货币的字符串 override def toString: String = amount + &quot; &quot; + designation def +(that: Currency): Currency = ... def *(x:Double): Currency = ...&#125;//产生的结果如下:79 USD11000 Yen99 Euro//抽象的实现new Currency &#123; override val amount: Long = 79L override def designation = &quot;USD&quot;&#125;//如果我们仅仅是对单一货币建模,那么这种设计不存在问题,可是一旦我们需要处理若干货币种类,这样做就不行了,假设你对美元和欧元建模为货币类的两个子类abstract class Dollar extends Currency&#123; override def designation = &quot;USD&quot;&#125;abstract class Euro extends Currency&#123; override def designation = &quot;Euro&quot;&#125;/*上面的做法看上去很有道理,但是在执行加法的时候将两种货币放在一起感觉古怪,你要的应该是+方法更具体化的版本,实现在Dollar类中的时候,他应该带Dollar参数并产生Dollar结果,实现在Euro类中的时候,应该带Euro参数并产生Euro结果,因此加法的类型应该依赖于所在类而改变,尽管如此,你还是希望方法只写一次即可,而不是每次定义新的货币都要重写*///第二版abstract class AbstractCurrency&#123; type Currncy &lt;:AbstractCurrency val amount: Long def designation: String override def toString: String = amount + &quot; &quot; + designation def +(that: Currency): Currency = ... def *(x: Double): Currency = ...&#125;//第二版与前面的一版的却别在于:类现在成为AbstractCurrency,并且包含了抽象类型Currency,代表未知的真实货币种类,每种AbstractCurrency的具体子类将需要把Currency类型修改为这个类本身,从而能够把两者结合在一起//使用abstract class Dollar extends AbstractCurrency&#123; override type Currncy = Dollar override def designation = &quot;USD&quot;&#125;/*这个设计仍不完美,问题之一是隐藏在AbstractCurrency类省略号中的方法定义+和* 如何具体化? 像下面这样吗?*/def +(that: Currency): Currency = new Currency &#123; val amount = this.amount + that.amount&#125;//编译不通过,因为scala对待抽象类型的一种限制是你既不能创建抽象类型的实例,也不能把抽象类型当做其他类的超类型,因此编译器将拒绝上面例子的代码实例化Currency的尝试//解决的方法是通过工厂方法abstract class CurrencyZone&#123; type Currency &lt;: AbstractCurrency def make(x: Long): Currency abstract class AbstractCurrency&#123; val amount: Long def designation: String override def toString: String = amount + &quot; &quot; + designation def +(that: Currency): Currency = new Currency &#123; make((this.amount + that.amount)) &#125; def *(x: Double): Currency = make((this.amount*x).toLong) &#125;&#125;//实现object US extends Currency&#123; abstract class Dollar extends AbstractCurrency&#123; def designation = &quot;USD&quot; &#125; type Currency = Dollar def make(x:Long) = new Dollar &#123;val amount = x&#125;&#125;/*上面的情况是:每种货币都仅用一个测量单位:美元,欧元,或日元,然而大多数货币都有子单位,例如:在美国有美元和美分,下面将引入CurrencyUnit字段,以包含货币一个标准单位的金额*///实现2object US extends Currency&#123; abstract class Dollar extends AbstractCurrency&#123; def designation = &quot;USD&quot; &#125; type Currency = Dollar def make(cents:Long) = new Dollar &#123;val amount = cents&#125; val Cent = make(1) //美分 val Dollar = make(100) //美元 val CurrencyUnit = Dollar&#125;//改进toString方法,例如:10美元与23美分的总和应该打印成小数:10.23USDoverride def toString: String = ((amount.toDouble/CurrencyUnit.amount.toDouble) formatted ( &quot;%.&quot;+decimals(CurrencyUnit.amount) + &quot;f&quot;))private def decimals(n: Long): Int = if (n == 1) 0 else 1+decimals(n/10)//改进:添加货币特征转换,首先,你可以编写Converter对象,以包含适用的货币汇率object Converter&#123; var exchangeRate = Map&#123; &quot;USD&quot; -&gt; Map(&quot;USD&quot;-&gt;1.0, &quot;EUR&quot;-&gt;0.7596, &quot;JPY&quot;-&gt;1.211, &quot;CHF&quot;-&gt;1.223) &quot;EUR&quot;-&gt; Map(&quot;USD&quot;-&gt;1.316, &quot;EUR&quot;-&gt;1.0, &quot;JPY&quot;-&gt;1.594, &quot;CHF&quot;-&gt;1.623) &quot;JPY&quot;-&gt; Map(&quot;USD&quot;-&gt;0.8257, &quot;EUR&quot;-&gt;0.6272, &quot;JPY&quot;-&gt;1.0, &quot;CHF&quot;-&gt;1.018) &quot;CHF&quot;-&gt; Map(&quot;USD&quot;-&gt;0.8108, &quot;EUR&quot;-&gt;0.6160, &quot;JPY&quot;-&gt;0.982, &quot;CHF&quot;-&gt;1.0) &#125;&#125;def from(other: CurrencyZone#AbstractCurrency): Currency = make(Math.round( other.amount.toDouble * Converter.exchangeRate(other.designation)(this.designation) ))//总代码abstract class CurrencyZone&#123; type Currency &lt;: AbstractCurrency def make(x: Long): Currency abstract class AbstractCurrency&#123; val amount: Long def designation: String override def toString: String = amount + &quot; &quot; + designation def +(that: Currency): Currency = new Currency &#123; make((this.amount + that.amount)) &#125; def *(x: Double): Currency = make((this.amount*x).toLong) &#125; def from(other: CurrencyZone#AbstractCurrency): Currency = make(Math.round( other.amount.toDouble * Converter.exchangeRate(other.designation)(this.designation) )) private def decimals(n: Long): Int = if (n == 1) 0 else 1+decimals(n/10) override def toString: String = ((amount.toDouble/CurrencyUnit.amount.toDouble) formatted ( &quot;%.&quot;+decimals(CurrencyUnit.amount) + &quot;f&quot; )) val CurrencyUnit: Currency&#125;","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第九章 控制抽象","date":"2017-04-16T04:47:25.893Z","path":"2017/04/16/scala编程/第九章 控制抽象/","text":"1.减少代码重复1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//需求:查询当前路径下的所有的文件,找到指定字符结束的文件或目录object FileMatcher&#123; private def fileHere = (new java.io.File(&quot;.&quot;)).listFiles def filesEnding(query: String) = for (file &lt;-fileHere if file.getName.endsWith(query)) yield file&#125;/*由于fileHere是私有的,filesEnding方法是定义在提供用户的API,FileMatcher中唯一可以访问的方法*///需求:查询当前路径下的所有的文件,找到指定字符的文件或目录(并不一定是文件结尾)def filesContaining(query: String) = for (file &lt;-fileHere if file.getName.contains(query)) yield file//后来客户又有了一个需求:他们要求基本正则表达式匹配文件,为了支持他们,于是你,写了下面的代码:def filesRegex(query: String) = for (file &lt;-fileHere if file.getName.matches(query)) yield file/*综合上面的三个需求,他们之间唯一的不同就是最后匹配文件的方法不同,第一个是endsWith,第二个是contains,第三个是matches,所以你或许希望有这样的代码:*/def filesMatching(query: String, method) = for (file &lt;-fileHere if file.getName.method(query)) yield file//具体实现如下:def filesMatching(query: String, matcher: (String, String)=&gt;Boolean) = for (file &lt;-fileHere if matcher(file.getName, query)) yield file//有了新的filesMatching帮助方法,上面的三个搜索方法可以简化成如下:def filesEnding(query: String) = filesMatching(query,_.endsWith(_))def filesContaining(query: String) = filesMatching(query, _.contains(_))def filesRegex(query: String) = filesMatching(query, _.matches(_))/*其实像_.endsWith(_)这样的函数是使用了占位符语法,原函数可以写成如下的形式:*/(fileName: String, query: String) =&gt; fileName.endsWith(query)//filesMatching函数需要一个参数,这个参数是函数,类型为(String, String)=&gt;Boolean,所以我们在传参的时候可以不用指定参数类型,因此写成下面的样子:(fileName, query) =&gt; fileName.endsWith(query)//在传参的过程中,第一个参数fileName在方法体重被第一个使用,第二个参数query被第二个使用,因此你可以使用占位符语法:_.endsWith(_)//更加简化的形式object FileMatcher&#123; private def fileHere = (new java.io.File(&quot;.&quot;)).listFiles def filesMatching( matcher: (String)=&gt;Boolean) = for (file &lt;-fileHere if matcher(file.getName)) yield file def filesEnding(query: String) = filesMatching(_.endsWith(query)) def filesContaining(query: String) = filesMatching(_.contains(query)) def filesRegex(query: String) = filesMatching(_.matches(query))&#125;/*以上代码就使用了闭包的特性,其中的query就是一个自由变量在Java中的做法就是将公共的部分抽取出来形成接口,然后对接口进行实现*/ 2.简化客户代码1234567891011121314151617181920//一个判断传入的值是否包含在集合中的方法:#指令式编程的做法def containsNeg(nums: List[Int]): Boolean=&#123; var exists = false for (num &lt;- nums)&#123; if (num&lt;0) exists = true &#125; exists&#125;//调用containsNeg(List(1, 2, 3, 4))//函数式编程def containsNeg2(nums: List[Int]) = nums.exists(_ &lt; 0)containsNeg2(List(1, 2, 3, 4))/*传过去的是一个函数: _&lt;0, 该函数值需要一个参数,*/ 3.柯里化1234567891011121314151617181920212223242526272829/*柯里化的函数被应用于多个参数列表,而不仅仅一个*///未被柯里化的函数def plainOldSum(x: Int, y:Int) = x + y//调用plainOldSum(1, 2)//被柯里化的函数,把这个函数应用于连个列表的各一个参数def curriedSum(x: Int)(y: Int) = x + y//调用curriedSum(1)(2)/*这里发生的事情是当你调用curriedSum时,实际上连接调用了两个传统函数,第一个函数调用带单个的名为x的Int参数,并返回第二个函数的函数值,第二个函数带Int参数y,下面的名为first的函数实质上执行了curriedSum的第一个传统函数调用会做的事情:*/def first(x: Int) = (y: Int)=&gt;x+y//在第一个函数上应用1,会产生第二个函数val second = first(1)//执行第二个函数second(2)/*first和second函数只是柯里化过程的一个演示,他们并不直接连接在curriedSum函数上,可以使用下面的函数来获取第二个参数的参考*/val onePlus = curriedSum(1) _//调用onePlus(2) //curriedSum(1)_里的下划线是第二个参数列表的占位符,结果及时指向一个函数的参考 4.编写新的控制结构1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/*在拥有头等函数的语言中,即使语言的语法是固定的,你也可以有效的制作新的控制结构,所有你需要做的就是创建带函数做参数的方法*///下面是&quot;双倍&quot;控制结构,能够重复一个操作两次并返回结果def twice(op: Double=&gt;Double, x:Double) = op(op(x))twice(_ + 1, 5) //7//在这个例子里op的类型是Double=&gt;Double,就是说他是带一个Double做参数并返回另一个Double的函数,而 _+1就是这个函数的实现,而下划线就是参数(用占位符表示)//需求:打开一个资源,对他进行操作,然后关闭资源,如下的代码:def withPrintWriter(file: java.io.File, op: PrintWriter =&gt;Unit): Unit =&#123; val writer = new PrintWriter(file) try &#123; op(writer) &#125; finally &#123; writer.close() &#125;&#125;//在客户端调用方法withPrintWriter( new java.io.File(&quot;data.txt&quot;), writer =&gt; writer.println(new java.util.Date) //客户端只需要去提供方法,并不需要去关心文件流的关闭与否)/*这个方法的好处就是:由withPrintWriter而并非客户端代码,去确认文件在结尾被关闭,因此忘记关闭文件是不可能的,这个技巧被称为借贷模式,因为控制抽象函数,如:withPrintWriter,打开了资源并&quot;借贷&quot;出函数,例如,前面例子里的withPrintWriter把PrintWriter借给函数op,当函数完成的时候,他发送信号说明他不在需要&quot;借&quot;的资源,于是资源被关闭在finally块中,以确认其确实被关闭,而忽略函数是正常结束还是抛出了异常*//*让客户端看上去更像内建控制结构的另一种方式是:使用花括号代替小括号包围参数列表,scala的任何方法调用,如果你确实之传入一个参数,就能可选的使用花括号替代小括号包围参数*/println(&quot;hello, world!&quot;)//替换为println&#123;&quot;hello, world!&quot;&#125; //仅在一个参数的时候有效//在多个参数时,可以使用柯里化的方式来使用花括号构建控制抽象val file = new File(&quot;data.txt&quot;)withPrintWriter(file)&#123; writer =&gt; writer.println(new java.util.Date)&#125;/*在上述代码中,第一个参数列表包含了一个File参数,被写成包围在小括号中,第二个参数列表包含了一个函数列表,被包围在花括号中*/ 5.传名参数(by-name parameter)123456789101112131415161718192021222324252627282930313233343536373839404142/*上面的描述中可以自花括号中使用参数,但是如果传入的函数的没有参数的情况下,该如何呢?*/var assertionsEnabled = truedef myAssert(predicate: ()=&gt;Boolean) = if (assertionsEnabled &amp;&amp; !predicate()) throw new AssertionError//使用myAssert(()=&gt; 5&gt;3) //看上去有点难看,或许你想写成下面的样子myAssert(5&gt;3) //不会有效,因为缺少()=&gt;/*传名函数恰好就是为了实现上述愿望而出现的,要实现一个传名函数,要定义参数的类型开始于 =&gt;,而不是()=&gt; ,例如:上述代码&quot; ()=&gt;Boolean &quot; 变为 &quot;=&gt;Boolean&quot; */var assertionsEnabled = truedef myAssert(predicate: =&gt;Boolean) = //因为没有参数,所以就不写() if (assertionsEnabled &amp;&amp; !predicate) throw new AssertionError//使用myAssert(5&gt;3) //或许你想对上述的函数还进一步的简化,如下:def boolAssert(predicate: Boolean) = if (assertionsEnabled &amp;&amp; !predicate) throw new AssertionError//调用boolAssert(5&gt;3) /*虽然可以使用上述的方式,但是: &quot; predicate:=&gt;Boolean &quot; 和 &quot; predicate:Boolean &quot;是两种不同的方式因为boolAssert的参数类型是Boolean,在boolAssert(5&gt;3)里括号中的表达式先于boolAssert的调用被评估,表达式5&gt;3产生true,被传给boolAssert而在myAssert的predicate参数的类型是 =&gt;Boolean, myAssert(5&gt;3) 里括号中的表达式不是先于myAssert的调用被评估的,而是代之以先创建一个函数值,其apply方法将被评估 5&gt;3 ,而这个函数值将被传递给myAssert*///如果断言被禁用var assertionsEnabled = falsemyAssert(x/0 == 0)boolAssert(x/0 == 0)//抛出异常(被0除),因为其中的表达式是先于函数被评估的,所以会先抛出异常","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第七章 内建控制结构","date":"2017-04-16T04:47:25.891Z","path":"2017/04/16/scala编程/第七章 内建控制结构/","text":"&emsp;scala的几乎所有的控制结构都会产生某个值,这是函数式语言所采用的方式scala的if可以向Java的三元操作符一样产生值,同样for, try, match也产生值, 程序员能够用结果值来简化代码,就如同用函数的返回值那样,如果没有这种机制,程序员就必须创建零时变量来保存控制结构中的计算结果 1.if表达式123456789101112var filename = &quot;default.txt&quot;if(!args.isEmpty) filename = args(0)//改进代码val filename = if (!args.isEmpty) args(0) else &quot;default.txt&quot;/*使用val是函数式的风格,并且具有与Java的final变量类型类似的效果*/ 2.while循环1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950def gcdLoop(x: Long, y: Long): Long = &#123; var a = x var b = y while (a!=0)&#123; val tmp = a a = b % a b = tmp &#125; b //返回b&#125;//或者do ..whilevar line = &quot;&quot; do&#123; line = readLine() println(&quot;Head: &quot; + line)&#125;while(line != &quot;&quot;)/*while 和do-while结构之所以被称为&quot;循环&quot;,而不是表达式,是因为他们不能产生有意义的结果,结果的类型是Unit,是表名存在并且唯一存在类型为Unit的值,称为unit value, 写成() */scala&gt; def greet()&#123;println(&quot;hi&quot;)&#125;greet: ()Unitscala&gt; greet() == () ^hires0: Boolean = true/*对var再赋值等式本身也是unit值,这是另一种与此类似的架构,比如:*/var line = &quot;&quot;while ((line = readLine()) != &quot;&quot;) println(&quot;read: &quot; + line)/*虽然在Java里,赋值语句可以返回被赋予的那个值,本例中是标准输入读取的文本行,但同样的情况下scala的赋值语句只能得到unit的值(),因此赋值语句&quot; line = readLine() &quot; 的值将永远返回 () 而不是&quot;&quot; ,结果是这个循环的状态将永远不会是假,循环永远无法结束*//*由于while循环不产生值,因此他经常被纯函数式语言所舍弃,这种语言只有表达式,没有循环,但是while的结构对指令式的程序员来说更容易读懂而对于函数式风格的话,只能使用递归实现,或许对某些代码的读者来说这就不是那么显而易见的了*/def gcd(x: Long, y: Long): Long = if (y == 0) x else gcd()/*上述代码是函数式风格,采用了递归写法,相对于指令式风格,函数式不需要var,因为while的指令式编程,所以建议在代码中更为审慎的使用while循环*/ 3.for表达式&emsp;for表达式可以让你用不同的方式把若干简单的成分组合起来以表达各种各样的枚举,例如:枚举整数序列,枚举不同类型的多个集合,使用任意条件过滤元素,制造新的集合 枚举集合类1234567891011121314151617/*for能做的最简单的事情就是把集合中所有元素都枚举一遍*/object Rational&#123; def main(args: Array[String]): Unit = &#123; val filesHere = (new File(&quot;.&quot;)).listFiles for (file &lt;- filesHere) println(file) &#125;&#125;/*new File(&quot;.&quot;) 创建指向当前目录的对象listFiles 返回File对象数组,每个元素都代表目录或文件file &lt;- filesHere 这样的语法被称之为发生器,我们遍历了filesHere的元素每一次遍历,名为file的新的val就被元素值初始化,编译器能够推断出file的类型是File,因为filesHere是Array[File]的,所以能够推断出file的类型是File*/ for中添加过滤条件1234567891011121314151617/*有时你并不想要枚举集合的全部元素,而只想过滤出某个子集,这就可以通过for表达式的括号中添加过滤器(filter),即 if字句来实现*/val filesHere = (new File(&quot;.&quot;)).listFilesfor (file &lt;- filesHere if file.getName.endsWith(&quot;.scala&quot;)) println(file)/*也可以有多个if条件过滤*/val filesHere = (new File(&quot;.&quot;)).listFilesfor ( file &lt;- filesHere if file.isFile; if file.getName.endsWith(&quot;.scala&quot;)) println(file)/*如果发生器中加入超过一个过滤器,if字句必须用分号,如上代码所示*/ 嵌套枚举123456789101112131415161718192021/*如果加入多个&lt;-字句,你就得到了嵌套的&quot;循环&quot;,如下,外层的循环是循环枚举filesHere ,内层的遍历的是所有以.scala结尾的文件*/def fileLines(file: java.io.File) = scala.io.Source.fromFile(file).getLines().toListdef grep(pattern: String) = &#123; val filesHere = (new File(&quot;.&quot;)).listFiles for &#123; file &lt;- filesHere if file.getName.endsWith(&quot;.scala&quot;) line &lt;- fileLines(file) if line.trim.matches(pattern) //找到匹配的行 &#125; println(file + &quot;:&quot; + line.trim)//使用grep(&quot;.*gcd.*&quot;)/*你可以使用花括号代替小括号包裹发生器和过滤器,使用花括号的好处是可以省略使用小括号时必须加的分号*/ 流间(mid-stream)变量绑定 123456789101112131415161718/*前面的代码中重复出现的表达式line.trim 或许你希望只计算一遍,用等号(=)把结果绑定到新变量实现,绑定的变量被当做val引入和使用,不过不带关键字val*/def grep(pattern: String) = &#123; val filesHere = (new File(&quot;.&quot;)).listFiles for &#123; file &lt;- filesHere if file.getName.endsWith(&quot;.scala&quot;) line &lt;- fileLines(file) trimmed = line.trim if trimmed.matches(pattern) &#125; println(file + &quot;:&quot; + trimmed)&#125;/*代码中,名为trimmed的变量被从半路引入for表达式,并被初始化为line.trim的结果值,于是之后的for表达式在两个地方使用了新的变量,一次在if中,另一次在println中*/ 制造新集合1234567891011121314151617181920/*上述的例子都只是对枚举值进行操作然后就释放,你还可以创建一个值去记住每一次的迭代,只要在for表达式之前加上关键字yield,比如:下面的函数鉴别出.scala文件并保存在数组里*/def scalaFiles(pattern: String) = &#123; val filesHere = (new File(&quot;.&quot;)).listFiles for ( file &lt;- filesHere if file.getName.endsWith(&quot;.scala&quot;); ) yield file&#125;/*for表达式在每次执行的时候都会产生一个新值,本例中是file,当for表达式完成的时候,结果将是包含了所有值的集合对象,本例中结果为Array[File]*///for-yield表达式的语法是这样的for &#123;子句&#125; yield &#123;循环体&#125;//以下写法是错误的for (file &lt;- fileHere if file.getName.endsWith(&quot;.scala&quot;))&#123; yield file //语法错误&#125; 4.使用try表达式处理异常&emsp;scala的异常和许多其他的语言一样,方法除了能以通常的方式返回值以外,还可以通过抛出异常中止执行,方法的调用者要么可捕获并处理这个异常,或者也可以只是简单的中止掉,并把异常上升到调用者处,异常以这种方式上升,逐层释放调用堆栈,直到某个方法接手处理或不再剩下其他的方法 抛出异常123456789//首先创建一个异常对象,然后用throw关键字抛出throw new IllegalArgumentException//下面代码的意思是:如果n是偶数,half将被初始化为n的一半,如果n不是偶数,那么异常将在half被初始化为任何值之前被抛出val half = if (n%2 == 0) n / 2 else throw new RuntimeException(&quot;n must be event!&quot;) 捕获异常123456789101112131415import java.io.FileReaderimport java.io.FileNotFoundExceptionimport java.io.IOExceptiontry &#123; val f = new FileReader(&quot;input.txt&quot;) //使用了,但是并未关闭文件&#125; catch &#123; case ex: FileNotFoundException =&gt; //处理丢失的文件 case ex: IOException =&gt; //处理其他IO错误&#125;/*这个try-catch表达式的处理方式与其他语言中的异常处理一致,首先执行程序体,如果抛出异常,则依次尝试每个catch子句,本例中,如果异常是FileNotFoundException,那么第一个字句将被执行,如果是IOException类型,第二个字句将被执行,如果都不是,那么try-catch将终结并把异常上升出去注意:scala与Java的区别在于scala里不需要捕获异常,或者把他们声明在throws子句中*/ finally子句12345678910111213141516//某些代码无论方法如何终止都要执行的话,那么可以将表达式放在finally子句里import java.io.FileReaderval f = new FileReader(&quot;input.txt&quot;)try &#123; //使用了,但是并未关闭文件&#125;finally &#123; f.close() //确保关闭文件&#125;/*上面的代码演示了确保非内存资源(如:文件,套接字,或者数据库链接)被关闭的惯例方式:首先,占有资源然后,开始try代码块使用资源最后,在finally代码块中关闭资源上述方式和在Java中的一样,但是在scala中还有一种被称之为出借模式的技巧将更加的简洁来到达同样的目的,出借模式在9.4节描述*/ 生成值123456789101112/*和其他大多数scala控制结构一样,try-catch-finally也产生值,如果没有抛出异常,返回的结果是try子句中的new URL(path),如果抛出异常并被捕获,则对应于catch中的子句*/import java.net.URLimport java.net.MalformedURLExceptiondef urlFor(path: String) = try &#123; new URL(path) &#125; catch &#123; case e: MalformedURLException =&gt; new URL(&quot;http://www.scala-lang.org&quot;) &#125; 5.匹配(match)表达式1234567891011121314151617181920212223242526272829303132/*类似于其他语言中的switch语句,他可以提供给你在多个备选项中做选择*/object ScalaDemo&#123; def main(args: Array[String]): Unit = &#123; val firstArg = if (args.length&gt;0) args(0) else &quot;&quot; firstArg match &#123; case &quot;salt&quot; =&gt; println(&quot;pepper&quot;) case &quot;chips&quot; =&gt; println(&quot;salsa&quot;) case &quot;eggs&quot; =&gt; println(&quot;bacon&quot;) case _ =&gt; println(&quot;huh?&quot;) //默认情况用下划线说明,这是常用在scala里作为占位符来表达未知值的通配符 &#125; &#125;&#125;/*和Java不同的是scala里的每个备选项的最后并没有break,但是在scala里并不会从上一个备选项落入到下一个备选项里面去的情况发生,*//*match表达式的每个备选项不但可以通过打印输出值,还可以只生成返回值而不打印*/object ScalaDemo&#123; def main(args: Array[String]): Unit = &#123; val firstArg = if (args.length&gt;0) args(0) else &quot;&quot; val friend = firstArg match &#123; case &quot;salt&quot; =&gt; &quot;pepper&quot; case &quot;chips&quot; =&gt; &quot;salsa&quot; case &quot;eggs&quot; =&gt; &quot;bacon&quot; case _ =&gt; &quot;huh?&quot; &#125; println(friend) &#125;&#125; 6.不再使用break和continue12345678910111213141516171819202122232425262728//Java写法int i = 0;boolean foundIt = false;while (i&lt;args.length)&#123; if (args[i].startsWith(&quot;-&quot;))&#123; i=i+1; continue; &#125; if (args[i].endsWith(&quot;.scala&quot;))&#123; foundIt = true; break; &#125; i = i+1&#125;//scala写法(使用递归去循环,去掉var)def searchFrom(i: Int): Int = if (i&gt;=args.length) -1 else if (args(i).startsWith(&quot;-&quot;)) searchFrom(i+1) else if (args(i).endsWith(&quot;.scala&quot;)) i else searchFrom(i+1)val i = searchFrom(0) //输入整数值做输入,从该值向前搜索,并返回想要的参数的索引/*用递归替换了循环,每个continue都被带有i+1做参数的递归调用替换掉了,并快速跳转处理下一个整数*/ 7.变量范围123456789101112131415161718/*scala的变量的作用域和Java的基本相同*///一旦变量被定义了,在同一个范围内就无法定义同样的名字了val a = 1val a = 1//编译不过//然而,你可以在内部范围内定义与外部范围里名称相同的变量val a = 1;&#123; val a = 2 //编译通过,仅在花括号内有效 println(a)&#125;println(a)/*在内部变量被认为是遮蔽了同名的外部变量,因为在内部范围内中外部变量变得不可见,虽然可以在内部定义和外部同名的变量,但是不建议这样做,因为这样做对读者来说会很混乱,通常,选择新的,有意义的变量名比不时遮蔽外部变量的做法更为妥当*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第一章 scala简介","date":"2017-04-16T04:47:25.889Z","path":"2017/04/16/scala编程/Scala简介/","text":"本书的所有例子能在scala 版本的2.7.2下面编译通过 1.函数式编程&emsp;Eric Raymond 把大教堂和集市作为软件开发的两个隐喻,大教堂是几乎近于完美的建筑物,要花很长的时间建设,而一旦建成了就长时间保持不变,相对来说,集市则天天在被工作在其中的人调整和扩展Raymond 的文章中,集市是对于开源软件开发的隐喻, Scala更像是一个集市而不是大教堂,因为他被设计为可以让使用它的人扩展和修改,Scala并没有提供那种”完美齐全”语言中可能需要的所有的东西,而是把制作这些东西的工具放在了你的手中 &emsp;Scala是纯粹的面向对象语言,每个值都是对象,每个操作都是方法,例如:如果使用Scala描述:1+2 ,实际上等于条用定义在Int类里的+方法,方法名可以用像操作符一样的名字定义,以便于API的使用者像使用操作符那样使用方法 &emsp;Scala还是一种成熟的函数式语言,函数式编程有两种指导理念,第一种理念是函数是头等值,在函数式语言中,函数也是值,与整数或字符串处于同一个地位,函数可以被当做参数传递给其他函数,可以当做结果从函数中返回或保存在变量里,可以在函数里定义其他函数,就好像在函数里定义整数一样,还可以定义匿名函数,并随意的插入到代码的任何地方,就好像使用43这样的整数字面量一样函数式编程的第二种理念是程序的操作应该把输入值映射为输出值而不是就地修改数据,在Java和scala里,字符串是一种数学意义上的额字符序列,使用s.replace(‘:’, ‘.’) 这样的表达式替换字符串里的字符会产生一个新的,不同于原字符串s的对象,换句话说就是Java里字符串是不可变的而在一些其他的语言中是可变的,因此单就字符串来说,Java是函数式的语言,不可变数据结构是函数式语言的一块基石,Scala库在Java API之上定义了更多的不可变数据类型,例如:Scala有不可变的列表 , 元组, 映射表, 和集 &emsp;函数式编程第二种理念的另一种解释是,方法不应该有任何的副作用,方法与其所在的环境交流的唯一方式应该是获得参数和返回结果,比如Java里String类的replace方法就是符合这种说法,他的参数包含一个字符串和两个字符串,返回的是一个新的字符串,其中的指定字符串都替换成了另一个,调用replace不会有其他的结果,类似于replace这样的方法被称为 指称透明 , 就是说对于任何输入来讲,都可以用方法的结果替代对他的调用,而不影响程序的语义 2.Why Scala? Scala是兼容的&emsp;scala不需要你脱离Java平台重新学习开发,他可以让你保全现存的代码并添加新东西因为他被设计成可以与Java实施无缝的互操作,Scala程序会被编译成JVM的字节码,其运行时的性能通常与Java程序不分上下,scala代码可以调用Java方法,访问Java字段,继承Java类和实现Java接口,这些都不需要特别的语法,外部接口描述或胶水代码,实际上,程序员们根本都没有意识到,几乎所有Scala代码都大量使用了Java库代码与Java的全交互操作性的另一个方面是Scala大量重用了Java类型,Scala的Int类型代表了Java的原始整数类型Int,Float代表了float等等,Scala不仅重用了Java的类型,而且还把他们打扮得更好看,如Scala的字符串类似于toInt 和toFloat的方法,可以把字符串转换成整数或浮点数,因此可以用str.toInt 代替 Integer.parseInt(str) 方法,不过他在不打破互操作性的基础上是怎么做到这点的呢?Java的String类可没有toInt方法,实际上,Scala有一个通用方案可以解决这种高级库设计和互操作性相对立的问题,Scala允许定义类型失配或者选用不存在的方法时使用隐式转换,在上面的例子里,当在字符串中寻找toInt方法时,Scala编译器会发现String类里没有这种方法,但他会发现Java的String对象转换为Scala的RichString 类实例的隐式转换,而RichString类中定义了这么个方法,于是在执行toInt操作之前,转换被隐式应用了 Scala是简洁的有一种极端的例子是,Scala的代码可以减少到Java的1/10,较为保守的估计大概标准的Scala程序代码行在Java里写的同样程序的一般左右,更少的行数不仅意味着打更少的字,同样意味着更少的对程序的阅读和理解,以及更少出错的机会,如:Java和scala的构造函数的对比就可以看出来了 scala是高级的 12345678910111213#java 写法boolean nameHasUpperCase = false;for (int i=0; i&lt;name.length(); i++)&#123; if(Character.isUpperCase(name.charAt(i)))&#123; nameHasUpperCase = true; break; &#125;&#125;#scala写法val nameHasUpperCase = name.exists(_.isUpperCase) scala是静态类型的&emsp;scala是以Java的内嵌类型系统为基础,允许使用泛型参数化类型,用交集组合类型及抽象类型影藏类型细节,这都为自建类型打下了坚实的基础,从而能够设计出既安全又灵活使用的接口,静态类型检查可以提供一些基础的编码上的检查,从而避免在程序运行时出错而重新排错的可能","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"第二章 scala入门初探","date":"2017-04-16T04:47:25.888Z","path":"2017/04/16/scala编程/Scala入门初探/","text":"1.变量定义先写变量后写类型12345678scala&gt; val msg: java.lang.String = &quot;hello world!&quot;msg: String = hello world!//或者scala&gt; val msg: String = &quot;hello world!&quot; #先写变量名称,再写变量类型msg: String = hello world! #定义了一个名称为msg的变量,类型为String, 值为 &quot;hello world!&quot;scala&gt; 自动类型推断12345#也可以省略类型的定义,scala会自动推断出变量的类型scala&gt; val msg = &quot;hello world!&quot; #因为值的类型是String,所以可以推断出msg的类型是String,所以在定义变量的时候可以省略对msg类型的推断msg: String = hello world! scala&gt; val类型变量不能修改,var可以123456789101112131415161718#val类型的变量不能再次对其进行赋值操作scala&gt; msg = &quot;goodby cruel world!&quot;&lt;console&gt;:8: error: reassignment to val msg = &quot;goodby cruel world!&quot; ^#var可以重复赋值scala&gt; var msg_var: String = &quot;hello world!&quot;msg_var: String = hello world!#对变量重新赋值scala&gt; msg_var = &quot;hello 2222 world!&quot;msg_var: String = hello 2222 world!scala&gt; 2.函数定义函数的基本结构1234567def max(x: Int, y:Int): Int = &#123; if(x &gt; y) x else y&#125;#scala的条件表达式可以像Java的三元操作符那样生成结果值if(x &gt; y) x else y #scala中的if/else不仅控制语句的执行流程,同时有返回值,所以他不等同于Java中的if/else#等同于Java中的:(x &gt; y)? x: y 函数的返回值类型123#函数的返回值类型可以不用写,因为可以使用函数体推断出来,但是如果函数是递归的,那么必须明确的说明返回值的类型#尽管如此,显示的说明函数结果类型也经常是一个好主意,这种类型标注可以使代码便于阅读,因为读者不用研究函数体之后再去猜测结果类型 函数体12如果函数仅包含一个语句,那么连花括号都可以选择不写,这样max函数就可以写成:def max2(x: Int, y:Int): Int = if(x &gt; y) x else y 没有参数和返回值的函数123456 scala&gt; def max2(x: Int, y:Int): Int = if(x &gt; y) x else ymax2: (x: Int, y: Int)Int scala&gt; def greet() = println(&quot;Hello world!&quot;)greet: ()Unit #greet是函数名, ()说明函数不带参数, Unit是greet的结果类型,指的是函数没有有效的返回值 3.编写scala脚本1234567891011121314151617#在hello.scala文件中,有如下的代码:println(&quot;hello world , from a script! &quot;)#执行上述脚本文件$ scala hello.scala#系统输出hello world , from a script! #传参,scala脚本的命令行参数保存在名为args的scala数组中println(&quot;hello world , from &quot; + args(0) + &quot; a script! &quot;) //注意scala中的数组是以()去取元素的,而不是像Java中是以[]去取元素#再次执行脚本$ scala hello.scala xxxxhello world , from xxxx a script! 4.用while循环,用if做判断123456789#在printargs.scala文件里,输入以下代码测试whilevar i = 0while (i &lt; args.length) &#123; println(args(i)) i += 1&#125;#上述代码并不是scala推荐的代码风格,在这里只是有助于解释while循环#在scala中并没有++/--,必须写成+= / -=这样的操作 5.用foreach和for做枚举foreach 1234567891011121314151617181920#上面所写的while循环的编码风格被称之为指令式编程(即:逐条执行指令,并经常改变不同函数之间的共享状态,在Java/C++/C这些语言中常见),在scala中更偏向的是函数式编程,如下:#在pa.scala文件中,如下代码:args.foreach(arg=&gt;println(arg))#执行$ scala pa.scala xx1 xx2 xx3#打印xx1xx2xx3#在上述例子中,scala解释器可以推断arg的类型为String,因为String是调用foreach的那个数组的元素类型,当然也可以更明确的给args加上类型名:args.foreach( (arg: String) =&gt;p rintln(arg))#更加简洁的写法args.foreach(p rintln(arg)) for123456#scala也是提供了像指令式的forfor (arg &lt;- args)&#123; println(arg)&#125;#&lt;- 右侧是已经在前面见过的args数组,&lt;-左侧的arg是val的名称(不是var,这里一定是val),尽管arg可能感觉像var,因为每次枚举都会得到新的值,但这的确是val,因为他不能在for表达式的函数体中被重新赋值,所以,对于args数组的每个元素,枚举的时候都会创建并初始化新的arg值,然后调用执行for的函数体 6.数组Array12345678910111213141516171819val greetStrings = new Array[String](3)greetStrings(0) = &quot;hello&quot; //访问数组元素是使用圆括号,而不是像Java一样使用的是[]greetStrings(1) = &quot;,&quot;greetStrings(2) = &quot;world!&quot;for (i &lt;- 0 to 2) &#123; println(greetStrings(i))&#125;#当同时使用类型和值参数化实例的时候,应该首先写方括号和类型参数,然后再写圆括号和值参数#以上代码并不是scala推荐的创建和初始化数组的推荐方式#因为scala的类型推断,所以有如下的代码val greetStrings: Array[String] = new Array[String](3)// 完整定义的形式能够更有效的说明类型参数(方括号包含的类型名)是实例类型的组成部分,而值参数(圆括号包含的值)不是,即:greetString的类型是Array[String] ,不是Array[String](3)#变量val用val定义的变量不能被重新赋值,但变量指向的对象内部却可以改变,所在在本例中,greetString对象不能被重新赋值成别的数组,他将永远指向初始化时指定的那个Array[String]实例,但是Array[String]的内部元素始终能被修改,因此数组本省是可变的 scala中没有运算符重载的问题12scala没有操作符重载的问题,因为它根本没有传统意义上的操作符,取而代之的是,诸如:+/-/*等遮掩的字符,可以用来做方法名因此在scala中输入 1+2的时候,实际上是在Int对象1上调用名称为+的方法,并把2当做参数传给他,如下图: 使用圆括号访问数组元素的原因12345678910111213141516171819202122232425val greetStrings = new Array[String](3)greetStrings(0) = &quot;hello&quot; //访问数组元素是使用圆括号,而不是像Java一样使用的是[]greetStrings(1) = &quot;,&quot;greetStrings(2) = &quot;world!&quot;/*数组只是类的实例(将greetStrings看做是对象),用括号传递给变量一个或多个值参数时,scala会把它转换成对apply方法的调用,于是greetString(i)转换成greetString.apply(i),所以scala里访问数组的元素也只不过是跟其他方法一样的调用,这个原则不是只对数组: 任何对于对象的值参数应用将都被转换为对apply方法的调用,当然前提是这个类型实际定义过apply方法,所以这不是特例,而是通用法则与之相对应,当对带有括号并包含一到若干参数的变量赋值时,编译器将使用对象的update方法对括号里的参数(索引值)和等号右边的对象执行调用*/greetStrings(0) = &quot;hello&quot;#将被转化为:greetStrings.update(0, &quot;hello&quot;) #也是讲greetStrings看做是对象,update只是调用对象的方法#综上代码可以写成:val greetStrings = new Array[String](3)greetStrings.update(0,&quot;hello&quot;)greetStrings.update(1,&quot;,&quot;)greetStrings.update(2,&quot;world!&quot;)for(i &lt;- 0.to(2))&#123; println(greetStrings.apply(i))&#125; 简介的构建数组的方式1234val numNames = Array(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;) //编译器根据传递的值类型(字符串)推断数组的类型是Array[String]#实际上是调用val numNames2 = Array.apply(&quot;zero&quot;, &quot;one&quot;, &quot;two&quot;) //在Array的伴生对象中有apply方法 7.列表list123456789/*scala数组是可变的同类对象序列,例如:Array[String]的所有对象都是String,而且尽管数组在实例化之后长度固定,但是他的元素值却是可变的,所以说数组是可变的list是不可变的同类对象序列,scala的scala.List不同于Java的java.util.List,一旦创建了就不可改变,实际上scala的列表是为了实现函数式风格的编程而设计的,*/val oneTwoThree = List(1, 2, 3)val oneTwo = List(1, 2)val threeFour = List(3,4)val oneTwoThreeFour = oneTwo ::: threeFour //用新的值重建了列表然后返回 向列表添加单个元素123456val twoThree = List(1, 2)val oneTwoThree = 1 :: twoThree/*表达式&quot;1 :: twoThree&quot;中, ::是右操作数twoThree的方法,即:方法的调用者是twoThree,1是方法的传入参数,因此可以写成:twoThree.::(1)* */ Nil形成新的列表12//因为Nil是空列表的简写,所以可以使用::操作符把所有元素都串联起来,并以Nil作结尾来定义新列表,例如可以用以下方法产生与上文同样的输出:&quot;List(1,2,3)&quot;val oneTwoThree = 1::2::3::Nil 列表不支持append操作的解决方法123List类没有提供append操作,因为随着列表变长,append的耗时将呈现线性增长,而使用::做前缀则仅仅耗时固定的时间,如果你想通过添加元素来构造列表,你的选择可以如下:1.先使用前缀(::)去构建一个list,在调用reverse2.使用ListBuffer,一种提供append操作的可变列表,完成之后用toList List 的一些方法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061//空ListList() //或者 Nil//创建带有三个值的新的List[String]List(&quot;cool&quot;, &quot;tools&quot;, &quot;rule&quot;)//创建带有三个值的新的List[String]val thrill = &quot;Will&quot;::&quot;fil&quot;::&quot;until&quot;::Nil//叠加两个列表(返回带&quot;a&quot;,&quot;b&quot;, &quot;c&quot;, &quot;d&quot;的新List[String] )List(&quot;a&quot;, &quot;b&quot;) ::: List(&quot;c&quot;, &quot;d&quot;)//获取list元素(返回thrill列表上索引为2的元素,从list是0开始)thrill(2) //&quot;until&quot;//count函数,计算满足条件的list的元素的个数thrill.count(s=&gt;s.length == 4) //计算长度为4的String元素的个数//删除元素并返回新列表thrill.drop(2) //返回去掉前两个元素的thrill列表(List(&quot;until&quot;) )thrill.dropRight(2) //返回去掉后两个元素的thrill列表( List(&quot;Will&quot;) )//exists判断是否存在某个值thrill.exists(s=&gt;s==&quot;until&quot; ) //判断是否有值为&quot;until&quot;的字符串元素在thrill里//过滤掉指定条件的元素,返回新的listthrill.filter(s=&gt;s.length==4) //新的列表: List(&quot;Will&quot;, &quot;fill&quot;)//thrill.forall(s =&gt; s.endsWith(&quot;1&quot;))//判断是否thrill列表里的所有元素都以&quot;1&quot;结尾//遍历每一个list元素thrill.foreach(s =&gt; print(s))thrill.foreach(print) //上面形式的简写//取第一个元素thrill.head //返回thrill列表的第一个元素( &quot;Will&quot; )//返回列表的最后一个元素thrill.last //&quot;until&quot;thrill.init //返回thrill列表除最后一个以外其他元素组成的列表( List(&quot;Will&quot;, &quot;fill&quot;) )//返回除第一个元素之外依次组成的新列表thrill.tail // List(&quot;fill&quot;, &quot;until&quot;)//判断thrill列表是否为空thrill.isEmpty // false//返回列表元素的数量thrill.length //3//遍历每一个元素,形成新的列表thrill.map(s =&gt; s+&quot;y&quot; ) //返回由thrill列表里每一个String元素都加了&quot;y&quot;构成的列表 (LIst(&quot;Willy&quot;,&quot;filly&quot;, &quot;untily&quot;) )//返回由list元素组成的字符串thrill.mkString(&quot;, &quot;) //&quot;Will, fill, until&quot;//翻转list,形成新列表thrill.reverse //返回翻转之后的新列表: List(&quot;until&quot;,&quot;fill&quot;,&quot;Will&quot;) 8.元组Tuple&emsp;元组也是很有用的容器对象,与列表一样,元组也是不可变的,但与列表不同元组可以返回不同类型的元素,例如列表只能写成LIst[Int] 或者 LIst[String] 但元组可以同时拥有Int和String, &emsp;元组的应用场景:如:方法里返回多个对象1234567val pair = (99, &quot;Luftballons&quot;)//元组的访问println(pair._1) //99//元组的实际类型取决于他含有的元素数量和这些元素的类型(99.&quot;ddd&quot;) 的类型是Tuple2[Int, String](&apos;u&apos;,&apos;r&apos;,&quot;ddd&quot;,1)的类型是Tuple4[Char,Char,String,Int] 访问元组的元素123456你或许想知道为什么不能用列表的方法来访问元组,如pair(0) ,那是因为列表的apply方法始终返回同样的元素,但元组的类型不尽相同, _1的结果类型可能与_2的不一致,诸如此类,因此两个的访问方法也不一样,此外,这些 _N 的索引是基于1的,而不是基于0的,这是因为对于拥有静态类型元组的其他语言,如Hashkell和ML,从1开始是传统的设定val pair = (99, &quot;Luftballons&quot;)//元组的访问println(pair._1) //99 9.使用集(set) 和映射(map)12345/*在scala中的set和map来说,同样有可变和不可变,不过并非提供两种类型,而是通过继承的差别把可变差异蕴含其中如下图HashSet类,各有一个扩展了可变的和另一个扩展不可变的Set特质,(Java里面称为&quot;实现&quot;接口,而在scala中称为&quot;扩展&quot; 或者&quot;混入&quot; 了特质)*/ 简单的实例代码123456789101112131415var jetSet = Set(&quot;Booing&quot;, &quot;Airbus&quot;)jetSet += &quot;Lear&quot;println(jetSet.contains(&quot;Cessna&quot;))/*第一行: 定义了名为jetSet的新变量,并初始化为包含两个字符串&quot;Booing&quot; 和&quot;Airbus&quot;的不可变交集scala中创建set的方法与创建list和array的类似,通过调用set伴生对象的apply工厂方法,在上面的例子中,对scala.collection.immutable.Set的伴生对象调用了apply方法,返回了默认的不可变Set的实例,scala编译器推断其类型为不可变Set[String]第二行: 加入新变量,可以对jetset调用,并传入新元素,可变的和不可变的集都提供了+方法,但结果不同,可变集把元素加入到自身,而不可变集则创建并返回包含了添加元素的新集,上述程序中使用的是不可变集,因此+调用将产生一个全新的集,所以只有可变集提供的才是正真的+=方法,不可变集不是:jetSet += &quot;Lear&quot; 实际上是下面方法的简写形式jetSet = jetSet + &quot;Lear&quot;因此这里实际是用包含了 &quot;Booing&quot; , &quot;Airbus&quot; , &quot;Lear&quot; 的新集重新赋值给了jetSet变量(因为jetSet变量是可变的,所以可以重新赋值) */ 定义可变Set,需要加入引用import1234import scala.collection.mutable.Setval movieSet = Set(&quot;hitch&quot;, &quot;Poltergeist&quot;)movieSet += &quot;Shrok&quot;println(movieSet) map&emsp; map和set一样,也是采用了类继承机制,提供了可变和不可变的两种版本的额Map,如下图: 12345678910111213import scala.collection.mutable.Mapval treasureMap = Map[Int,String]()treasureMap += &#123;1 -&gt; &quot;Go to island.&quot;&#125; //1 -&gt; &quot;Go to island.&quot; 转换为(1).-&gt;(&quot;Go to island.&quot;) ,scala的任何对象都能调用-&gt;方法,并返回包含键值对的二元组treasureMap += &#123;2 -&gt; &quot;Find big X on ground&quot;&#125;treasureMap += &#123;1 -&gt; &quot;Dig.&quot;&#125;println(treasureMap(2))//如果没有import,那么默认使用的Map是不可变的val romanNumberal = Map(1-&gt;&quot;1&quot;, 2-&gt;&quot;II&quot;, 3-&gt;&quot;III&quot;,4-&gt;&quot;IV&quot;, 5-&gt;&quot;V&quot;)println(romanNumberal(4)) //打印 &quot;IV&quot; 10.认识函数式风格123456789101112131415161718192021222324252627282930313233343536373839404142434445/*首先要理解指令式编程和函数式编程在代码风格上的差异,大致上说,如果代码包含了var变量,那么他可能就是指令式的风格,如果代码根本没有var ----就是说仅仅包含val ---- 那他或许是函数式的风格,因此向函数式风格转变的方式之一,就是尝试不用任何var编程*/#指令式风格def printArgs(args: Array[String]):Unit = &#123; var i = 0 while(i&lt;args.length)&#123; println(args(i)) i += 1 &#125;&#125;#函数式风格//通过去掉var的办法把代码变得更函数式风格def printArgs2(args: Array[String]):Unit = &#123; for(arg &lt;- args)&#123; println(arg) &#125;&#125;//或者像这样def printArgs3(args: Array[String]):Unit = &#123; args.foreach(println)&#125;/*这个例子说明了减少使用var的一个好处,重构后的代码比原来(指令式)的代码更加的简洁,明白,也更少有机会犯错,上述的代码有修改的余地,重构之后的printArgs方法并不是纯函数式的,因为他有副作用----本例中的副作用就是打印到标准输出流,识别函数是否有副作用的地方就在于其结果类型是否为Unit,如果某个函数不返回任何有用的值,也就是说如果返回类型为Unit,那么这个函数唯一能产生的作用就只能是通过某种副作用,而函数风格的方式应该是定义对打印的arg进行格式化的方法,不过仅返回格式化之后的字符串,如下:*/def formArgs(args: Array[String]) = args.mkString(&quot;\\n&quot;)/*以上才是正真的函数式编程的风格,完全没有副作用或var的mkString方法,能在任何可枚举的集合类型上调用没有副作用的好处:举例来说:要测试前面给出的任何一个有副作用的printArgs方法,你将需要重新定义println,捕获传递给他的输出,在检查结果,相反,formatArgs来说你可以直接检查他的返回结果,如下:*/def formArgs(args: Array[String]) = args.mkString(&quot;\\n&quot;)val res = formArgs(Array(&quot;one&quot;, &quot;two&quot;, &quot;zero&quot;))assert(res == &quot;one\\ntwo\\nzero&quot;) //assert方法检查传入的Boolean表达式,如果结果为假,抛出AssertionError,否则assert就什么也不做,安静的返回 scala程序员的平衡感12崇尚val, 不可变对象和没有副作用的方法首先想到他们,只有在特定需要和并加以权衡之后才选择var, 可变对象和有副作用的方法 11.从文件里读取文本行12345678910111213import scala.io.Sourceif(args.length&gt;0)&#123; //Source.fromFile(args(0))尝试打开指定的文件并返回Source对象,之后对他调用, getLines函数,返回Iterator[String] ,枚举每次提供一行文本,包括行结束符 for(line &lt;- Source.fromFile(args(0)).getLines())&#123; println(line.length + &quot;&quot; +line) &#125;&#125;else&#123; Console.err.println(&quot;Please enter filename&quot;)&#125;/*toList 是必须加的,因为getLines方法返回的是枚举器,一旦完成遍历,枚举器就失效了,而通过调用toList 把它转换为LIst,我们把文件中的所有行全部存储在内存中,因此可以随时使用,lines变量因此就指向着包含了指定文件的文本字符串列表*/","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala编程","slug":"scala编程","permalink":"http://yoursite.com/tags/scala编程/"}]},{"title":"面向对象编程-属性和方法","date":"2017-04-16T04:47:25.885Z","path":"2017/04/16/python/面向对象编程-属性和方法/","text":"实例化步骤 调用__new__()方法创建实例 __new__()方法自动从object继承 调用init()方法对其初始化 __init__()方法在类中定义 添加类说明 紧跟类名之后,以三引号包围的字符串 查看类说明 类名.__doc__ help(类名) 新式类与经典类 Python 3.x版本统一为新式类,不用继承object 区别 经典类继承为深度优先 新式类继承为广度优先 实例属性 类被实例化以后才会具有的属性 一般在__init__方法中创建并初始化 直接使用即定义:self.&lt;属性名&gt; 引用方法:self.&lt;属性名&gt; self用来代表类的实例的 类外用实例名.属性名方式定义和引用 相同类的不同实例其实例属性是不相关的 一般不建议在__init__()方法之外创建和初始化实例属性 一般不推荐类外定义和修改,修改可以单独定义方法 123456789101112131415class TestCss: def __init__(self): self.a = 0 self.b = 10 def info(self): print(&quot;a:&quot;, self.a, &quot; b:&quot;,self.b)if ___name__ == &apos;___name__&apos;: tc = TestCss() tc.info() tc.color = &quot;red&quot; //实例属性 print(tc.color)# 可以在类外动态的添加实例属性，如上面的color就是一个动态添加的实例属性 类属性 类定义后就存在,且不需要实例化 类属性使得相同类的不同实例共同持有相同变量1234567891011121314151617class TestCss: cssa = &quot;class-attribute&quot; def __init__(self): self.a = 0 self.b = 10 def info(self): print(&quot;a:&quot;, self.a, &quot; b:&quot;,self.b, &quot;cssa:&quot;, TestCss.cssa) def define_a(self): self.c = 19if __name__ == &apos;___name__&apos;: tc = TestCss() tc.info() tc.color = &quot;red&quot; print(tc.color) 私有属性 不提供限制属性访问的关键字(无法限制类的各种属性在类外直接访问) 使用__开头的变量名加以标志,只有对象自己能访问 使用_开头的变量名加以标志,应该只有类对象及其子类能访问(非强制性)123456789101112131415161718192021222324252627282930313233343536373839404142434445class A: def __init__(self): self.__ab = 0 def info(self): print(self.__ab)if __name__ == &apos;__main__&apos;: a = A() a.info() a.__ab = 3 a.info() print(a.__ab)#执行结果003[Finished in 0.2s]class A: def __init__(self): self._ab = 0 def info(self): print(self._ab)if __name__ == &apos;__main__&apos;: a = A() a.info() a._ab = 3 a.info() print(a._ab)#执行结果:033[Finished in 0.2s]#单下划线在外部仍然是可以访问的，所以上面说是非强制性的 特殊属性 __doc__ __name__ __dict__ __module__ __base__ 可以使用dir(Student)看student类有哪些特殊属性，注意这些特殊属性是使用两个下划线开头和结束的 同名的类属性与实例属性 以实例名.属性名引用时,优先引用实例属性 以类名.属性名引用时,只能引用类属性123456789101112131415161718&gt;&gt;&gt; class A:... a=0... def __init__(self):... self.a=10... self.b=100...&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.a10&gt;&gt;&gt; A.a0&gt;&gt;&gt; A.bTraceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AttributeError: type object &apos;A&apos; has no attribute &apos;b&apos;&gt;&gt;&gt; 属性访问的特殊方法(反射) 提供用字符串来操作类的属性/方法的方式 主要工具函数 hasattr(obj_name, ‘属性名’) setattr(obj_name, “属性名”, 值) getattr(obj_name, “属性名”) 1234567891011121314&gt;&gt;&gt; class A:... a=0... def __init__(self):... self.a=10... self.b=100...&gt;&gt;&gt; getattr(a,&quot;a&quot;)10&gt;&gt;&gt; setattr(a,&quot;a&quot;,30)&gt;&gt;&gt; hasattr(a, &quot;b&quot;)True&gt;&gt;&gt; hasattr(a, &quot;cc&quot;)False 属性包装 将方法包装成属性,以隐藏相关实例 控制属性的类型或范围 虚拟属性(由其他属性处理后得来) 三种属性操作 可读:@property 可写:@.setter 可删: @.deleter 12345678910111213141516171819class Watcher: def __init__(self, water=10,scour=2): self._water = water self.scour = scour @property def water(self): return self._water def set_water(self, water): self.water = water @water.setter def water(self, water): if 0&lt;water&lt;=500: self._water = water else: print(&quot;set failure&quot;)#将方法装饰成为属性，在外部看来是访问属性，其实就是访问类的中的方法 描述符 将实现特殊协议方法的类作为另一个类的类属性 用来拦截和控制属性访问并可以重复使用 协议方法 __get__() __set__() __delete__() 分类 数据描述符(实现全部协议方法) 非数据描述符(实现部分协议方法) 说明:所有类成员函数都是非数据描述符 同名的实例属性和非数据描述符(以方法为例)访问优先级 注意:只能在新式类中使用1234567891011121314151617181920212223242526272829303132333435class NonNeg: def __init__(self, default=0): self.default = default def __get__(self, instance, owner): return self.default def __set__(self, instance, val): if val &gt; 0: self.default = val else: print(&quot;The value must be NonNegative&quot;)class Movie: rating = NonNeg() score = NonNeg()if __name__ == &quot;__main__&quot;: m = Movie() print(&quot;rating: &quot;, m.rating) print(&quot;score: &quot;, m.score) m.rating = 80 //在set的时候会将self作为隐藏的参数,传给方法 print(&quot;rating:&quot;, m.rating) m.score = -3 print(&quot;score:&quot;, m.score)#打印结果:rating: 0score: 0rating: 80The value must be NonNegativescore: 0[Finished in 0.3s] __call__()让类的实例如函数一样可调用 12345678&gt;&gt;&gt; class Test:... def __call__(self):... print(&quot;call.....&quot;)...&gt;&gt;&gt; t = Test() //相当于将__call__返回返回了&gt;&gt;&gt; t() //调用方法call.....&gt;&gt;&gt; 类方法,静态方法 静态方法 定义方法 @staticmethod装饰,参数不用self 访问特性 不能引用或访问实例属性,可以通过了类,类变量访问属性 调用方法 可以用类,类实例调用 本质 在类中的一个普通函数而已,使面向对象程序中函数归属于类,易于代码管理 用法 与类相关,但不依赖或改变类与实例 创建不同的实例 把类相关工具方法放入类中 类方法 定义方法 @classmethod,必须提供参数cls*访问特性 不能引用或访问实例属性 调用方法 可以用类,类实例调用 继承特性 继承时,传入的类变量cls是子类,而非父类 用途 与类相关,但不依赖或改变类的实例 工厂方法,创建类实例,完成有关预处理 在类内调用静态方法时不能硬编码类名 12345678910111213141516171819202122232425262728class Watcher(object): &quot;&quot;&quot;docstring for Watcher&quot;&quot;&quot; def __init__(self,water=10, scour=2): super(Watcher, self).__init__() self._water = water self.scour = scour self.year = 2010 @staticmethod def spins_ml(spins): print(&quot;company: &quot;, Watcher.company) print(&quot;year: &quot;, self.year) return spins * 0.4 @classmethod def get_water(cls, watter, scour)://cls 是指代的实例对象 return cls(water,cls.spins_ml(scour)) @property def water(): return self._waterif __name__ == &quot;__main__&quot;: print(Watcher.spins_ml(8)) w = Watcher() print(w.spins_ml(8))","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"用图片解释：进程（process）和线程（thread)","date":"2017-04-16T04:47:25.884Z","path":"2017/04/16/python/用图片解释：进程（process）和线程（thread）/","text":"转自:用图片解释：进程（process）和线程（thread） 进程（process）和线程（thread）是操作系统的基本概念，但是它们比较抽象，不容易掌握。最近，我读到一篇材料，发现有一个很好的类比，可以把它们解释地清晰易懂。1. 计算机的核心是CPU，它承担了所有的计算任务。它就像一座工厂，时刻在运行。2. 假定工厂的电力有限，一次只能供给一个车间使用。也就是说，一个车间开工的时候，其他车间都必须停工。背后的含义就是，单个CPU一次只能运行一个任务。3. 进程就好比工厂的车间，它代表CPU所能处理的单个任务。任一时刻，CPU总是运行一个进程，其他进程处于非运行状态。4. 一个车间里，可以有很多工人。他们协同完成一个任务。5. 线程就好比车间里的工人。一个进程可以包括多个线程。6. 车间的空间是工人们共享的，比如许多房间是每个工人都可以进出的。这象征一个进程的内存空间是共享的，每个线程都可以使用这些共享内存。7. 可是，每间房间的大小不同，有些房间最多只能容纳一个人，比如厕所。里面有人的时候，其他人就不能进去了。这代表一个线程使用某些共享内存时，其他线程必须等它结束，才能使用这一块内存。8. 一个防止他人进入的简单方法，就是门口加一把锁。先到的人锁上门，后到的人看到上锁，就在门口排队，等锁打开再进去。这就叫”互斥锁”（Mutual exclusion，缩写 Mutex），防止多个线程同时读写某一块内存区域。9. 还有些房间，可以同时容纳n个人，比如厨房。也就是说，如果人数大于n，多出来的人只能在外面等着。这好比某些内存区域，只能供给固定数目的线程使用。10. 这时的解决方法，就是在门口挂n把钥匙。进去的人就取一把钥匙，出来时再把钥匙挂回原处。后到的人发现钥匙架空了，就知道必须在门口排队等着了。这种做法叫做”信号量”（Semaphore），用来保证多个线程不会互相冲突。不难看出，mutex是semaphore的一种特殊情况（n=1时）。也就是说，完全可以用后者替代前者。但是，因为mutex较为简单，且效率高，所以在必须保证资源独占的情况下，还是采用这种设计。11. 操作系统的设计，因此可以归结为三点：（1）以多进程形式，允许多个任务同时运行；（2）以多线程形式，允许单个任务分成不同的部分运行；（3）提供协调机制，一方面防止进程之间和线程之间产生冲突，另一方面允许进程之间和线程之间共享资源。","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"Python面向对象编程指南(转)","date":"2017-04-16T04:47:25.883Z","path":"2017/04/16/python/Python面向对象编程指南(转)/","text":"转自:Python面向对象编程指南 抽象是隐藏多余细节的艺术。在面向对象的概念中，抽象的直接表现形式通常为类。虽然Python是解释性语言，但是它是面向对象的，从设计之初就已经是一门面向对象的语言。Python基本上提供了面向对象编程语言的所有元素，如果你已经至少掌握了一门面向对象语言，那么利用Python进行面向对象程序设计将会相当容易。下面就来了解一下如何在Python中进行对象编程。 1.如何定义一个类在进行python面向对象编程之前，先来了解几个术语：类，类对象，实例对象，属性，函数和方法。类是对现实世界中一些事物的封装，定义一个类可以采用下面的方式来定义：12class className: block 注意类名后面有个冒号，在block块里面就可以定义属性和方法了。当一个类定义完之后，就产生了一个类对象。类对象支持两种操作：引用和实例化。引用操作是通过类对象去调用类中的属性或者方法，而实例化是产生出一个类对象的实例，称作实例对象。比如定义了一个people类： 12345class people: name = &apos;jack&apos; #定义了一个属性 #定义了一个方法 def printName(self): print self.name people类定义完成之后就产生了一个全局的类对象，可以通过类对象来访问类中的属性和方法了。当通过people.name（至于为什么可以直接这样访问属性后面再解释，这里只要理解类对象这个概念就行了）来访问时，people.name中的people称为类对象，这点和C++中的有所不同。当然还可以进行实例化操作，p=people( )，这样就产生了一个people的实例对象，此时也可以通过实例对象p来访问属性或者方法了(p.name). 理解了类、类对象和实例对象的区别之后，我们来了解一下Python中属性、方法和函数的区别。 在上面代码中注释的很清楚了，name是一个属性，printName( )是一个方法，与某个对象进行绑定的函数称作为方法。一般在类里面定义的函数与类对象或者实例对象绑定了，所以称作为方法；而在类外定义的函数一般没有同对象进行绑定，就称为函数。 2.属性在类中我们可以定义一些属性，比如：123456class people: name = &apos;jack&apos; age = 12 p = people() print p.name,p.age 定义了一个people类，里面定义了name和age属性，默认值分别为’jack’和12。在定义了类之后，就可以用来产生实例化对象了，这句p = people( )实例化了一个对象p，然后就可以通过p来读取属性了。这里的name和age都是公有的，可以直接在类外通过对象名访问，如果想定义成私有的，则需在前面加2个下划线 ‘ __’。123456class people: __name = &apos;jack&apos; __age = 12 p = people() print p.__name,p.__age 这段程序运行会报错：1234Traceback (most recent call last): File &quot;C:/PycharmProjects/FirstProject/oop.py&quot;, line 6, in &lt;module&gt; print p.__name,p.__age AttributeError: people instance has no attribute &apos;__name 提示找不到该属性，因为私有属性是不能够在类外通过对象名来进行访问的。在Python中没有像C++中public和private这些关键字来区别公有属性和私有属性，它是以属性命名方式来区分，如果在属性名前面加了2个下划线’__’，则表明该属性是私有属性，否则为公有属性（方法也是一样，方法名前面加了2个下划线的话表示该方法是私有的，否则为公有的）。 3.方法 在类中可以根据需要定义一些方法，定义方法采用def关键字，在类中定义的方法至少会有一个参数，，一般以名为’self’的变量作为该参数（用其他名称也可以），而且需要作为第一个参数。下面看个例子：1234567891011class people: __name = &apos;jack&apos; __age = 12 def getName(self): return self.__name def getAge(self): return self.__age p = people() print p.getName(),p.getAge() 如果对self不好理解的话，可以把它当做C++中类里面的this指针一样理解，就是对象自身的意思，在用某个对象调用该方法时，就将该对象作为第一个参数传递给self。 4.类中内置的方法 在Python中有一些内置的方法，这些方法命名都有比较特殊的地方（其方法名以2个下划线开始然后以2个下划线结束）。类中最常用的就是构造方法和析构方法。 构造方法init(self,….)：在生成对象时调用，可以用来进行一些初始化操作，不需要显示去调用，系统会默认去执行。构造方法支持重载，如果用户自己没有重新定义构造方法，系统就自动执行默认的构造方法。 析构方法del(self)：在释放对象时调用，支持重载，可以在里面进行一些释放资源的操作，不需要显示调用。 还有其他的一些内置方法，比如 cmp( ), len( )等。下面是常用的内置方法： 内置方法 说明 init(self,…) 初始化对象，在创建新对象时调用 del(self) 释放对象，在对象被删除之前调用 new(cls,args,*kwd) 实例的生成操作 str(self) 在使用print语句时被调用 getitem(self,key) 获取序列的索引key对应的值，等价于seq[key] len(self) 在调用内联函数len()时被调用 cmp(stc,dst) 比较两个对象src和dst getattr(s,name) 获取属性的值 setattr(s,name,value) 设置属性的值 delattr(s,name) 删除name属性 getattribute() getattribute()功能与getattr()类似 gt(self,other) 判断self对象是否大于other对象 lt(slef,other) 判断self对象是否小于other对象 ge(slef,other) 判断self对象是否大于或者等于other对象 le(slef,other) 判断self对象是否小于或者等于other对象 eq(slef,other) 判断self对象是否等于other对象 call(self,*args) 把实例对象作为函数调用 init():init方法在类的一个对象被建立时，马上运行。这个方法可以用来对你的对象做一些你希望的初始化。注意，这个名称的开始和结尾都是双下划线。代码例子: 123456789101112# Filename: class_init.py class Person: def __init__(self, name): self.name = name def sayHi(self): print &apos;Hello, my name is&apos;, self.name p = Person(&apos;Swaroop&apos;) p.sayHi() 输出： Hello, my name is Swaroop new():new()在init()之前被调用，用于生成实例对象。利用这个方法和类属性的特性可以实现设计模式中的单例模式。单例模式是指创建唯一对象吗，单例模式设计的类只能实例化一个对象。12345678910class Singleton(object): __instance = None # 定义实例 def __init__(self): pass def __new__(cls, *args, **kwd): # 在__init__之前调用 if Singleton.__instance is None: # 生成唯一实例 Singleton.__instance = object.__new__(cls, *args, **kwd) return Singleton.__instance getattr()、setattr()和getattribute():当读取对象的某个属性时，python会自动调用getattr()方法。例如，fruit.color将转换为fruit.getattr(color)。当使用赋值语句对属性进行设置时，python会自动调用setattr()方法。getattribute()的功能与getattr()类似，用于获取属性的值。但是getattribute()能提供更好的控制，代码更健壮。注意，python中并不存在setattribute()方法。代码例子：123456789101112131415161718# -*- coding: UTF-8 -*- class Fruit(object): def __init__(self, color=&quot;red&quot;, price=0): self.__color = color self.__price = price def __getattribute__(self, item): # &lt;span style=&quot;font-family:宋体;font-size:12px;&quot;&gt;获取属性的方法&lt;/span&gt; return object.__getattribute__(self, item) def __setattr__(self, key, value): self.__dict__[key] = value if __name__ == &quot;__main__&quot;: fruit = Fruit(&quot;blue&quot;, 10) print fruit.__dict__.get(&quot;_Fruit__color&quot;) # &lt;span style=&quot;font-family:宋体;font-size:12px;&quot;&gt;获取color属性&lt;/span&gt; fruit.__dict__[&quot;_Fruit__price&quot;] = 5 print fruit.__dict__.get(&quot;_Fruit__price&quot;) # &lt;span style=&quot;font-family:宋体;font-size:12px;&quot;&gt;获取price属性&lt;/span&gt; Python不允许实例化的类访问私有数据，但你可以使用object._classNameattrName访问这些私有属性。 getitem():如果类把某个属性定义为序列，可以使用getitem()输出序列属性中的某个元素.假设水果店中销售多钟水果，可以通过getitem__()方法获取水果店中的没种水果。代码例子：123456789101112# -*- coding: UTF-8 -*- class FruitShop: def __getitem__(self, i): # 获取水果店的水果 return self.fruits[i] if __name__ == &quot;__main__&quot;: shop = FruitShop() shop.fruits = [&quot;apple&quot;, &quot;banana&quot;] print shop[1] for item in shop: # 输出水果店的水果 print item, 输出12banana apple banana str():str()用于表示对象代表的含义，返回一个字符串.实现了str()方法后，可以直接使用print语句输出对象，也可以通过函数str()触发str()的执行。这样就把对象和字符串关联起来，便于某些程序的实现，可以用这个字符串来表示某个类。代码例子：1234567891011# -*- coding: UTF-8 -*- class Fruit: &apos;&apos;&apos;&apos;&apos;Fruit类&apos;&apos;&apos; #为Fruit类定义了文档字符串 def __str__(self): # 定义对象的字符串表示 return self.__doc__ if __name__ == &quot;__main__&quot;: fruit = Fruit() print str(fruit) # 调用内置函数str()触发__str__()方法，输出结果为:Fruit类 print fruit #直接输出对象fruit,返回__str__()方法的值，输出结果为:Fruit类 call():在类中实现call()方法，可以在对象创建时直接返回call()的内容。使用该方法可以模拟静态方法。代码例子:123456789101112# -*- coding: UTF-8 -*- class Fruit: class Growth: # 内部类 def __call__(self): print &quot;grow ...&quot; grow = Growth() # 调用Growth()，此时将类Growth作为函数返回,即为外部类Fruit定义方法grow(),grow()将执行__call__()内的代码 if __name__ == &apos;__main__&apos;: fruit = Fruit() fruit.grow() # 输出结果：grow ... Fruit.grow() # 输出结果：grow ... 5.类属性、实例属性、类方法、实例方法以及静态方法 在了解了类基本的东西之后，下面看一下python中这几个概念的区别。 先来谈一下类属性和实例属性 在前面的例子中我们接触到的就是类属性，顾名思义，类属性就是类对象所拥有的属性，它被所有类对象的实例对象所共有，在内存中只存在一个副本，这个和C++中类的静态成员变量有点类似。对于公有的类属性，在类外可以通过类对象和实例对象访问。12345678910class people: name = &apos;jack&apos; #公有的类属性 __age = 12 #私有的类属性 p = people() print p.name #正确 print people.name #正确 print p.__age #错误，不能在类外通过实例对象访问私有的类属性 print people.__age #错误，不能在类外通过类对象访问私有的类属性 实例属性是不需要在类中显示定义的，比如：12345678910class people: name = &apos;jack&apos; p = people() p.age =12 print p.name #正确 print p.age #正确 print people.name #正确 print people.age #错误 在类外对类对象people进行实例化之后，产生了一个实例对象p，然后p.age = 12这句给p添加了一个实例属性age，赋值为12。这个实例属性是实例对象p所特有的，注意，类对象people并不拥有它（所以不能通过类对象来访问这个age属性）。当然还可以在实例化对象的时候给age赋值。12345678910111213class people: name = &apos;jack&apos; #__init__()是内置的构造方法，在实例化对象时自动调用 def __init__(self,age): self.age = age p = people(12) print p.name #正确 print p.age #正确 print people.name #正确 print people.age #错误 如果需要在类外修改类属性，必须通过类对象去引用然后进行修改。如果通过实例对象去引用，会产生一个同名的实例属性，这种方式修改的是实例属性，不会影响到类属性，并且之后如果通过实例对象去引用该名称的属性，实例属性会强制屏蔽掉类属性，即引用的是实例属性，除非删除了该实例属性。 123456789101112class people: country = &apos;china&apos; print people.country p = people() print p.country p.country = &apos;japan&apos; print p.country #实例属性会屏蔽掉同名的类属性 print people.country del p.country #删除实例属性 print p.country 下面来看一下类方法、实例方法和静态方法的区别。 类方法：是类对象所拥有的方法，需要用修饰器”@classmethod”来标识其为类方法，对于类方法，第一个参数必须是类对象，一般以”cls”作为第一个参数（当然可以用其他名称的变量作为其第一个参数，但是大部分人都习惯以’cls’作为第一个参数的名字，就最好用’cls’了），能够通过实例对象和类对象去访问。1234567891011class people: country = &apos;china&apos; #类方法，用classmethod来进行修饰 @classmethod def getCountry(cls): return cls.country p = people() print p.getCountry() #可以用过实例对象引用 print people.getCountry() #可以通过类对象引用 类方法还有一个用途就是可以对类属性进行修改：123456789101112131415161718192021222324252627class people: country = &apos;china&apos; #类方法，用classmethod来进行修饰 @classmethod def getCountry(cls): return cls.country @classmethod def setCountry(cls,country): cls.country = country p = people() print p.getCountry() #可以用过实例对象引用 print people.getCountry() #可以通过类对象引用 p.setCountry(&apos;japan&apos;) print p.getCountry() print people.getCountry() #运行结果：china china japan japan 结果显示在用类方法对类属性修改之后，通过类对象和实例对象访问都发生了改变。 实例方法：在类中最常定义的成员方法，它至少有一个参数并且必须以实例对象作为其第一个参数，一般以名为’self’的变量作为第一个参数（当然可以以其他名称的变量作为第一个参数）。在类外实例方法只能通过实例对象去调用，不能通过其他方式去调用。1234567891011class people: country = &apos;china&apos; #实例方法 def getCountry(self): return self.country p = people() print p.getCountry() #正确，可以用过实例对象引用 print people.getCountry() #错误，不能通过类对象引用实例方法 静态方法：需要通过修饰器”@staticmethod”来进行修饰，静态方法不需要多定义参数。12345678910class people: country = &apos;china&apos; @staticmethod #静态方法 def getCountry(): return people.country print people.getCountry() 对于类属性和实例属性，如果在类方法中引用某个属性，该属性必定是类属性，而如果在实例方法中引用某个属性（不作更改），并且存在同名的类属性，此时若实例对象有该名称的实例属性，则实例属性会屏蔽类属性，即引用的是实例属性，若实例对象没有该名称的实例属性，则引用的是类属性；如果在实例方法更改某个属性，并且存在同名的类属性，此时若实例对象有该名称的实例属性，则修改的是实例属性，若实例对象没有该名称的实例属性，则会创建一个同名称的实例属性。想要修改类属性，如果在类外，可以通过类对象修改，如果在类里面，只有在类方法中进行修改。 6.继承和多重继承 上面谈到了类的基本定义和使用方法，这只体现了面向对象编程的三大特点之一：封装。下面就来了解一下另外两大特征：继承和多态。 在Python中，如果需要的话，可以让一个类去继承一个类，被继承的类称为父类或者超类、也可以称作基类，继承的类称为子类。并且Python支持多继承，能够让一个子类有多个父类。 Python中类的继承定义基本形式如下：1234567#父类 class superClassName: block #子类 class subClassName(superClassName): block 在定义一个类的时候，可以在类名后面紧跟一对括号，在括号中指定所继承的父类，如果有多个父类，多个父类名之间用逗号隔开。以大学里的学生和老师举例，可以定义一个父类UniversityMember，然后类Student和类Teacher分别继承类UniversityMember： 1234567891011121314151617181920212223242526272829303132333435363738394041# -*- coding: UTF-8 -*- class UniversityMember: def __init__(self,name,age): self.name = name self.age = age def getName(self): return self.name def getAge(self): return self.age class Student(UniversityMember): def __init__(self,name,age,sno,mark): UniversityMember.__init__(self,name,age) #注意要显示调用父类构造方法，并传递参数self self.sno = sno self.mark = mark def getSno(self): return self.sno def getMark(self): return self.mark class Teacher(UniversityMember): def __init__(self,name,age,tno,salary): UniversityMember.__init__(self,name,age) self.tno = tno self.salary = salary def getTno(self): return self.tno def getSalary(self): return self.salary 在大学中的每个成员都有姓名和年龄，而学生有学号和分数这2个属性，老师有教工号和工资这2个属性，从上面的代码中可以看到： 1）在Python中，如果父类和子类都重新定义了构造方法init( )，在进行子类实例化的时候，子类的构造方法不会自动调用父类的构造方法，必须在子类中显示调用。 2）如果需要在子类中调用父类的方法，需要以”父类名.方法“这种方式调用，以这种方式调用的时候，注意要传递self参数过去。 对于继承关系，子类继承了父类所有的公有属性和方法，可以在子类中通过父类名来调用，而对于私有的属性和方法，子类是不进行继承的，因此在子类中是无法通过父类名来访问的。 Python支持多重继承。对于多重继承，比如 class SubClass(SuperClass1,SuperClass2) 此时有一个问题就是如果SubClass没有重新定义构造方法，它会自动调用哪个父类的构造方法？这里记住一点：以第一个父类为中心。如果SubClass重新定义了构造方法，需要显示去调用父类的构造方法，此时调用哪个父类的构造方法由你自己决定；若SubClass没有重新定义构造方法，则只会执行第一个父类的构造方法。并且若SuperClass1和SuperClass2中有同名的方法，通过子类的实例化对象去调用该方法时调用的是第一个父类中的方法。 7.多态 多态即多种形态，在运行时确定其状态，在编译阶段无法确定其类型，这就是多态。Python中的多态和Java以及C++中的多态有点不同，Python中的变量是弱类型的，在定义时不用指明其类型，它会根据需要在运行时确定变量的类型（个人觉得这也是多态的一种体现），并且Python本身是一种解释性语言，不进行预编译，因此它就只在运行时确定其状态，故也有人说Python是一种多态语言。在Python中很多地方都可以体现多态的特性，比如内置函数len(object)，len函数不仅可以计算字符串的长度，还可以计算列表、元组等对象中的数据个数，这里在运行时通过参数类型确定其具体的计算过程，正是多态的一种体现。这有点类似于函数重载（一个编译单元中有多个同名函数，但参数不同），相当于为每种类型都定义了一个len函数。这是典型的多态表现。有些朋友提出Python不支持多态，我是完全不赞同的。 本质上，多态意味着可以对不同的对象使用同样的操作，但它们可能会以多种形态呈现出结果。len(object)函数就体现了这一点。在C++、Java、C#这种编译型语言中，由于有编译过程，因此就鲜明地分成了运行时多态和编译时多态。运行时多态是指允许父类指针或名称来引用子类对象，或对象方法，而实际调用的方法为对象的类类型方法，这就是所谓的动态绑定。编译时多态有模板或范型、方法重载（overload）、方法重写（override）等。而Python是动态语言，动态地确定类型信息恰恰体现了多态的特征。在Python中，任何不知道对象到底是什么类型，但又需要对象做点什么的时候，都会用到多态。 能够直接说明多态的两段示例代码如下： 多态方法1234567891011121314# -*- coding: UTF-8 -*- _metaclass_=type # 确定使用新式类 class calculator: def count(self,args): return 1 calc=calculator() #自定义类型 from random import choice obj=choice([&apos;hello,world&apos;,[1,2,3],calc]) #obj是随机返回的 类型不确定 print type(obj) print obj.count(&apos;a&apos;) #方法多态 对于一个临时对象obj，它通过Python的随机函数取出来，不知道具体类型（是字符串、元组还是自定义类型），都可以调用count方法进行计算，至于count由谁（哪种类型）去做怎么去实现我们并不关心。 有一种称为”鸭子类型（duck typing）“的东西，讲的也是多态：123456789101112131415161718192021222324_metaclass_=type # 确定使用新式类 class Duck: def quack(self): print &quot;Quaaaaaack!&quot; def feathers(self): print &quot;The duck has white and gray feathers.&quot; class Person: def quack(self): print &quot;The person imitates a duck.&quot; def feathers(self): print &quot;The person takes a feather from the ground and shows it.&quot; def in_the_forest(duck): duck.quack() duck.feathers() def game(): donald = Duck() john = Person() in_the_forest(donald) in_the_forest(john) game() 就in_the_forest函数而言，参数对象是一个鸭子类型，它实现了方法多态。但是实际上我们知道，从严格的抽象来讲，Person类型和Duck完全风马牛不相及。 多态运算符12345678def add(x,y): return x+y print add(1,2) #输出3 print add(&quot;hello,&quot;,&quot;world&quot;) #输出hello,world print add(1,&quot;abc&quot;) #抛出异常 TypeError: unsupported operand type(s) for +: &apos;int&apos; and &apos;str&apos; 上例中，显而易见，Python的加法运算符是”多态“的，理论上，我们实现的add方法支持任意支持加法的对象，但是我们不用关心两个参数x和y具体是什么类型。 Python同样支持运算符重载，实例如下：1234567891011121314class Vector: def __init__(self, a, b): self.a = a self.b = b def __str__(self): return &apos;Vector (%d, %d)&apos; % (self.a, self.b) def __add__(self,other): return Vector(self.a + other.a, self.b + other.b) v1 = Vector(2,10) v2 = Vector(5,-2) print v1 + v2 一两个示例代码当然不能从根本上说明多态。普遍认为面向对象最有价值最被低估的特征其实是多态。我们所理解的多态的实现和子类的虚函数地址绑定有关系，多态的效果其实和函数地址运行时动态绑定有关。在C++, Java, C#中实现多态的方式通常有重写和重载两种，从上面两段代码，我们其实可以分析得出Python中实现多态也可以变相理解为重写和重载。在Python中很多内置函数和运算符都是多态的。 参考文献：http://www.cnblogs.com/dolphin0520/archive/2013/03/29/2986924.htmlhttp://www.cnblogs.com/jeffwongishandsome/archive/2012/10/06/2713258.html","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之进程间通讯——消息队列Queue","date":"2017-04-16T04:47:25.881Z","path":"2017/04/16/python/python进程之进程间通讯——消息队列Queue/","text":"1. 简介 Queue：是一个消息队列，队列的长度可为无限或者有限 用于父子进程通讯，两个没有关系的进程不能使用Queue通信 使用实例： from mulitprocessing import Queue 进程之间同步：lock.acquire 当多个进程同时对队列写的时候需要进行同步，保证一个时刻只有一个进程对队列进行写操作 2.查看Queue帮助123456In [36]: from multiprocessing import QueueIn [37]: help(Queue)Help on function Queue in module multiprocessing: Queue(maxsize=0) Returns a queue object 3.Queue常用方法12345678910111213In [38]: q = Queue() In [39]: q.q.cancel_join_thread q.empty q.get #取出队列中的消息 q.join_thread q.put_nowaitq.close q.full q.get_nowait q.put #向队列中放入消息 q.qsize 4.Queue举例123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/python import osfrom multiprocessing import Queue Qmsg = Queue() #创建一个队列pid = os.fork() #启动子进程 if pid==0: #说明是子进程 msg = Qmsg.get() #从队列中获取数据 print(msg)else: #父进程 Qmsg.put(&quot;msg 1&quot;) #向队列中放入数据 os.wait() #等待子进程退出-----------------------------------------------------------------------------------------------------------------[root@backup python]# vim queue2.py import time#!/usr/bin/python import osfrom multiprocessing import Queue,Processimport time Qmsg = Queue() #创建队列 def child_func(name): print(&quot;child pid=&#123;0&#125;&quot;.format(os.getpid())) #打印子进程的pid msg = Qmsg.get() #获取队列数据 print(&quot;name=&#123;0&#125;,msg=&#123;1&#125;&quot;.format(name,msg)) print(&quot;Main pid = &#123;0&#125;&quot;.format(os.getpid())) #打印父进程的pid p = Process(target=child_func,args=(&quot;chilid_1&quot;,)) #创建一个子进程，并传递参数Qmsg.put(&quot;msg_1&quot;) #向队列中放入数据p.start() #开启进程p.join() #等待子进程结束 #执行 [root@backup python]# python queue2.pyMain pid = 3742child pid=3744name=chilid_1,msg=msg_1 将start方法换成run方法，那么child_func相当于在主进程中执行，此时并没有开一个子进程 5.Lock锁的使用1234567891011In [39]: from multiprocessing import Lock #导入模块In [40]: help(Lock)Help on function Lock in module multiprocessing: Lock() Returns a non-recursive lock object In [41]: l = Lock()In [43]: l. #其中的方法l.acquire l.release #获取锁、释放锁 6.解决多进程之间的互斥问题加上1s的延时，则当前进程会被挂起，去执行其他的进程，所以打印的进程消息将不会有序 ，挂起之后，子进程得到锁的机会是随机的，但是在一个子进程中，因为有锁的存在，所以的能够保证一个进程中的所有的内容会一起执行，以下是解决方案：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162[root@backup python]# cat queue3.py #!/usr/bin/python import os #导入相应的模块from multiprocessing import Queue,Process,Lockimport time Qmsg = Queue()lock = Lock() def child_func(name): lock.acquire() #加锁 Qmsg.put(&quot;child_&quot;+str(name)+&quot;_msg_1:&quot;+&quot;Pid=&quot;+str(os.getpid)) time.sleep(1) Qmsg.put(&quot;child_&quot;+str(name)+&quot;_msg_2:&quot;+&quot;Pid=&quot;+str(os.getpid)) lock.release() #释放锁，这样就能保证每一个子进程中的两次put是相邻放入到队列中的 listp = []for i in range(10): p = Process(target=child_func, args=(i,)) #创建子进程 p.start() #开启子进程 listp.append(p) while True: msg = Qmsg.get() #获取队列消息 print(msg) #打印 for i in range(10): listp[i].join() #等待子进程结束-------------------------------------------------------------------------------------------------#执行结果[root@backup python]# python queue3.pychild_0_msg_1:Pid=&lt;built-in function getpid&gt; #child_i_msg 总是在一起执行child_0_msg_2:Pid=&lt;built-in function getpid&gt;child_3_msg_1:Pid=&lt;built-in function getpid&gt;child_3_msg_2:Pid=&lt;built-in function getpid&gt;child_4_msg_1:Pid=&lt;built-in function getpid&gt;child_4_msg_2:Pid=&lt;built-in function getpid&gt;child_5_msg_1:Pid=&lt;built-in function getpid&gt;child_5_msg_2:Pid=&lt;built-in function getpid&gt;child_2_msg_1:Pid=&lt;built-in function getpid&gt;child_2_msg_2:Pid=&lt;built-in function getpid&gt;child_1_msg_1:Pid=&lt;built-in function getpid&gt;child_1_msg_2:Pid=&lt;built-in function getpid&gt;child_6_msg_1:Pid=&lt;built-in function getpid&gt;child_6_msg_2:Pid=&lt;built-in function getpid&gt;child_7_msg_1:Pid=&lt;built-in function getpid&gt;child_7_msg_2:Pid=&lt;built-in function getpid&gt;child_9_msg_1:Pid=&lt;built-in function getpid&gt;child_9_msg_2:Pid=&lt;built-in function getpid&gt;child_8_msg_1:Pid=&lt;built-in function getpid&gt;child_8_msg_2:Pid=&lt;built-in function getpid&gt;^CTraceback (most recent call last): File &quot;queue3.py&quot;, line 28, in &lt;module&gt; msg = Qmsg.get() File &quot;/usr/lib/python2.6/multiprocessing/queues.py&quot;, line 91, in get res = self._recv()KeyboardInterrupt 7.使用Queue实现最简单的本地CS架构读取客户端输入，并将数据保存到文件中，并在Server端回显 基本思路： 创建小心队列 创建子进程 子进程读取输入并放到消息队列中 父进程读取并处理消息","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之进程间通讯——无名管道","date":"2017-04-16T04:47:25.880Z","path":"2017/04/16/python/python进程之进程间通讯——无名管道/","text":"管道：是一种半双工的通信机制，他一端用来读，另一端用来写，管道只能用来在具有公共祖先的两个进程之间通信，管道通信消息先进先出的原理，并且数据只能被读取一次，当此段数据被读取后就会被清空，管道的实质是内存的一页（page） 相关函数：os.pipe()：返回读写通道文件描述符组成的元组（read_end，write_end 读端和写端) 管道示意图 管道通讯示意图： 公共祖先：父进程和子进程，具有相同的父进程之间 无名管道实例： 使用上面的方式，我们可以处理一堆父子进程，如果我们这里有N个子进程，而且全双工通讯，那么我们如何处理呢？ 如果要全双工通信（读写同时进行），那么要创建两条管道，因为如果父子进程之间只有一个管道的话，父进程写了之后要读，那么，就会有父进程读取到的是自己写的内容，而一旦内容被读取之后，管道中的内容就没有了，子进程就不会读取到，所以就会出现错乱的情况，解决的办法就是创建两条管道，父进程在一条管道中读，在另一条管道中写，同时子进程在一条管道中写，在另一条管道中读。但是这样的解决方式也是有弊端的，就是每增加一个子进程，就会有2条管道增加，而管道的数量是有上限的。 全双工：在父进程和每一个子进程之间创建两个管道，一条管道用于写，一条管道用于读 弊端：当两个没有关系的进程不能实现通讯，如下： 1234567891011121314151617181920212223242526#!/usr/bin/python import osimport time p = os.pipe() #start a pipepid = os.fork() #开始一个子进程 if pid == 0:#only read ,so close write end os.close(p[1]) while True: msg = os.read(p[0],1024) print(msg) if msg == &apos;q&apos;: os.close(p[0]) breakelse: os.close(p[0]) #only write ,so close read end while True: str1 = raw_input(&quot;&gt;&quot;) os.write(p[1],str1) #write &apos;str1&apos; to p[1] if str1 == &apos;q&apos;: os.close(p[1]) os.wait() #wait subprocess exit break time.sleep(1) #sleep 1s","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之进程间通讯——命名管道","date":"2017-04-16T04:47:25.879Z","path":"2017/04/16/python/python进程之进程间通讯——命名管道/","text":"1.简介 命名管道：无名管道的一个扩展，无名管道是程序运行时存在，命名管道是持久的，一旦创建，所有有权限进程都可以访问 命名管道是单向管道，只能以只读或只写方式打开，如果要实现双向通信，必须打开两个管道 命名管道创建：os.mkfifo(path) ，读写和操作文件一样 命名管道通信模型： 代码示例 1234567891011121314151617181920212223242526272829#process1import osp_name = &quot;pipe1&quot;os.mkfifo(p_name) try: fp=open(p_name,&apos;w&apos;) #写方式打开管道except IOError: print &quot;open %s Error&quot; % p_nameelse: msg = os.write(fp, &quot;hello python&quot;) #发送消息finally: if (fp): fp.close()#process2import osp_name = &quot;pipe1&quot;os.mkfifo(p_name) #创建管道,管道相当于文件描述符,所以可以像文件一样可以用open进行读写try: fp=open(p_name,&apos;r&apos;) #读方式打开管道except IOError: print &quot;open %s Error&quot; % p_nameelse: msg = os.read(fp, 1024) print msgfinally: if (fp): fp.close() 阻塞：自行设备操作时，如果不能获得资源就会挂起进程，知道获取资源后在进行操作，被挂起的进程进入休眠状态 非阻塞：执行设备操作时，如果不能获取资源直接返回，可以使用轮询的方式进行设备操作 一个管道可以多个进程打开，是否安全 2.os.access测试路径的访问权1234567891011121314151617os.access(path, mode) 使用实际的uid和gid去测试路径的访问权。实际的uid和gid指的是用户登录到系统使用的uid和当前用户所在的gid，这和有效用户id和有效组id是有区别的，有效用户id和有效组id是对应于进程的。 mode参数指定测试路径的方式： os.F_OK - 测试路径是否存在 os.R_OK - 测试文件是否可读 os.W_OK - 测试文件是否可写 os.X_OK - 测试文件是否可执行 其中的R_OK，W_OK，X_OK是可以使用OR操作合起来进行一起测试的。 #函数返回True如果测试成功，否则返回False。在系统的C API中可以使用access系统调用。 3.创建pipe，读写举例（以系统的open） 如果进程以只读方式打开管道，那么会阻塞，直到有进程以只写方式打开管道为止 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#写[root@backup python]# cat pipe_w.py#!/usr/bin/python import os p_name = &quot;./pipe&quot; #管道名称if os.access(p_name,os.F_OK)==False: #判断路径是否存在 os.mkfifo(p_name) #创建有名管道 print(&quot;before open&quot;) fp_w = open(p_name,&apos;w&apos;) #以只写的方式打开一个管道文件，返回管道对象 print(&quot;end open&quot;) msg = &quot;&quot; while True: msg1 = raw_input(&quot;&gt;&quot;) fp_w.write(msg1) #向管道中写入数据 fp_w.flush() #刷新缓存数据到管道中，因为使用的是系统的open函数打开的管道，所以相当于是C的方法，而C中是有缓存存在的 if msg1==&apos;q&apos;: break fp_w.close()---------------------------------------------#读[root@backup python]# cat pipe_r.py#!/usr/bin/python import os p_name = &quot;./pipe&quot;if os.access(p_name,os.F_OK)==False: os.mkfifo(p_name) print(&quot;before open&quot;) fp_r = open(p_name,&apos;r&apos;) # 其实open打开的虽然是管道，但是他相当于一个文件一样，用ls可以在本地看到 print(&quot;end open&quot;) while True: msg = fp_r.read(1) #每次读取一个字节 print(msg) if msg == &apos;q&apos;: breakfp_r.close() 123[root@backup python]# ll#os.mkfifo(p_name) ，会生成一个pipe的文件prw-r--r-- 1 root root 0 10月 5 16:30 pipe 4.os.open/read/write简绍12345678910111213141516171819202122232425262728293031323334353637383940414243In [1]: import os In [2]: help(os.open)Help on built-in function open in module posix: open(...) open(filename, flag [, mode=0777]) -&gt; fd #返回fd，即文件描述符 Open a file (for low level IO). #flag 有以下方式：如 os.O_WRONLY(读写) os.O_RDONLY（只读） os.O_WRONLY （只写）In [3]: os.O_os.O_APPEND os.O_DIRECT os.O_EXCL os.O_NOATIME os.O_NONBLOCK os.O_RSYNC os.O_WRONLY os.O_ASYNC os.O_DIRECTORY os.O_LARGEFILE os.O_NOCTTY os.O_RDONLY os.O_SYNC os.O_CREAT os.O_DSYNC os.O_NDELAY os.O_NOFOLLOW os.O_RDWR os.O_TRUNC #readIn [3]: help(os.read)Help on built-in function read in module posix: read(...) read(fd, buffersize) -&gt; string Read a file descriptor. #读文件描述符 #write In [4]: help(os.write)Help on built-in function write in module posix: write(...) write(fd, string) -&gt; byteswritten Write a string to a file descriptor. #写string 到文件描述符 #closeIn [5]: help(os.close)Help on built-in function close in module posix: close(...) close(fd) Close a file descriptor (for low level IO). #关闭一个文件描述符 5.创建pipe，读写举例（以os模块的open）使用os.open的方式打开，返回的是一个文件描述符123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#写#!/usr/bin/python import os p_name = &quot;./pipe&quot;if os.access(p_name,os.F_OK)==False: os.mkfifo(p_name) #没有就创建管道 print(&quot;before open&quot;) fp_w = os.open(p_name, os.O_WRONLY) #os.open返回一个文件描述符 print(&quot;end open&quot;) msg = &quot;&quot; while True: msg1 = raw_input(&quot;&gt;&quot;) &apos;&apos;&apos; #fp_w.write(msg1) #如果使用的是os.open就不用刷新，os中并没有缓冲的存在，而是直接读取的。所以是写端写多少，在读端读多少 #fp_w.flush() &apos;&apos;&apos; os.write(fp_name,msg1) #向文件描述符中写 if msg1==&apos;q&apos;: break os.close(fp_w) #关闭文件描述符---------------------------------------------------------------------#读[root@backup python]# cat pipe_os_r.py #!/usr/bin/python import os p_name = &quot;./pipe&quot;if os.access(p_name,os.F_OK)==False: os.mkfifo(p_name) print(&quot;before open&quot;) fp_r = os.open(p_name,os.O_RDONLY) #打开一个文件描述符 print(&quot;end open&quot;) while True: msg = os.read(fp_r, 1024) #读入文件描述符中的内容 if msg == &apos;&apos;: #如果杀掉写端的进程，那么在读端会继续读，所以我们对读取的内容进行判断，如果为空字符串，就退出 break print(msg) if msg == &apos;q&apos;: break os.close(fp_r) #关闭文件描述符","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之进程间通讯——共享内存","date":"2017-04-16T04:47:25.878Z","path":"2017/04/16/python/python进程之进程间通讯——共享内存/","text":"1. 共享内存简介 共享内存：进程间的一种通讯方式，他允许多个进程访问相同的内存，一个进程改变其中的数据后，其他的进程都可以看到数据的变化 Linux的内存模型： 每个进程的虚拟内存被分为页（page） 每个进程维护自己的内存地址到虚拟内存页之间的映射 实际的数据存在于进程的内存地址上 每个进程都有自己的地址空间，多个进程的映射还是可以指定相同的页 数据可以使用Value或Array类型存储在共享内存映射中，需要导入：from multiprocessing import Process, Value, Array Manger()返回的管理者，支持类型包括：list 、dict、Namespance、Lock、RLock、Semaphore、BoundedSemaphore、Condition、Event、Queue、Avalue and Array 2.Value and Array 帮助1234567891011121314In [5]: from multiprocessing import Process,Value,Array #导入模块 In [6]: help(Value) #ValueHelp on function Value in module multiprocessing: Value(typecode_or_type, *args, **kwds) #typecode_or_type是指定数据类型，见下文 Returns a synchronized shared object In [7]: help(Array) #ArrayHelp on function Array in module multiprocessing: Array(typecode_or_type, size_or_initializer, **kwds) Returns a synchronized shared array 3.typecode_or_type数据类型 Type code C Type Python Type Minimum size in bytes Notes ‘b’ signed char int 1 ‘B’ unsigned char int 1 ‘u’ Py_UNICODE Unicode character 2 (1) ‘h’ signed short int 2 ‘H’ unsigned short int 2 ‘i’ signed int int 2 ‘I’ unsigned int int 2 ‘l’ signed long int 4 ‘L’ unsigned long int 4 ‘q’ signed long long int 8 (2) ‘Q’ unsigned long long int 8 (2) ‘f’ float float 4 ‘d’ double float 8 1234array(&apos;l&apos;)array(&apos;u&apos;, &apos;hello \\u2641&apos;)array(&apos;l&apos;, [1, 2, 3, 4, 5])array(&apos;d&apos;, [1.0, 2.0, 3.14]) 4.Array、Value常用方法1234567891011121314151617181920212223242526272829#ArrayIn [8]: ay = Array(&apos;i&apos;, range(10)) #指定数据类型，创建ArrayIn [9]: ayOut[9]: &lt;SynchronizedArray wrapper for &lt;multiprocessing.sharedctypes.c_long_Array_10 object at 0x96f48e4&gt;&gt;In [10]: ay. #常用的方法ay.acquire #加锁ay.get_lock #获取锁ay.get_obj #获取对象ay.release #释放锁 In [10]: c = ay.get_obj() #获取对象 In [11]: cOut[11]: &lt;multiprocessing.sharedctypes.c_long_Array_10 at 0x96f48e4&gt;In [12]: c[:] #打印对象分片Out[12]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -------------------------------------------------------------------------------------#ValueIn [13]: value = Value(&apos;i&apos;,11)In [14]: value.value.acquire #加锁 value.get_lock #获取锁value.get_obj #获取对象value.release #释放锁value.value #获取其中的值 In [14]: value.value #打印值 Out[14]: 11 5.Value、Array举例12345678910111213141516171819202122232425262728293031323334353637383940414243444546[root@backup python]# cat gongxiang.py#!/usr/bin/python from multiprocessing import Process,Value,Array #导入相应的模块import timeimport os def child_func(g_value, g_array, ar): g_value.value = ar g_array[ar] = ar*ar print(&quot;g_value.value=&quot;,g_value.value) listp = [] g_value = Value(&apos;i&apos;,0) #初始化共享类型g_array = Array(&apos;i&apos;,range(10)) print(&quot;init g_value=&#123;0&#125;,g_array=&#123;1&#125;&quot;.format(g_value.value,g_array[:])) for i in range(10): p = Process(target=child_func, args=(g_value,g_array,i)) #启动子进程，子进程会调用对应的函数，所有的子进程会共享g_value,g_array，因为他们会继承父进程的内存 p.start() listp.append(p) for i in range(10): listp[i].join() #等待子进程关闭 print(&quot;end vlaue=&quot;,g_value.value)print(&quot;end array=&quot;,g_array[:])----------------------------------------------------------------------------#打印结果[root@backup python]# python gongxiang.pyinit g_value=0,g_array=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9](&apos;g_value.value=&apos;, 1)(&apos;g_value.value=&apos;, 0)(&apos;g_value.value=&apos;, 2)(&apos;g_value.value=&apos;, 3)(&apos;g_value.value=&apos;, 4)(&apos;g_value.value=&apos;, 5)(&apos;g_value.value=&apos;, 6)(&apos;g_value.value=&apos;, 7)(&apos;g_value.value=&apos;, 9)(&apos;g_value.value=&apos;, 8)(&apos;end vlaue=&apos;, 8)(&apos;end array=&apos;, [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]) 6.Manger查看帮助及常用方法因为Value和Array中的数据类型是固定的，所以我们引入了Manger的方式来共享内存12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152In [20]: from multiprocessing import Manager#帮助In [22]: help(Manager)Help on function Manager in module multiprocessing: Manager() Returns a manager associated with a running server process The managers methods such as `Lock()`, `Condition()` and `Queue()` can be used to create shared objects.#常用方法In [23]: ma = Manager() #可以获取下面的数据类型In [25]: ma.ma.Array #数组 ma.JoinableQueue ma.Queue #队列ma.address ma.join ma.startma.BoundedSemaphore ma.Lock ma.RLock ma.connect ma.list #列表ma.Condition ma.Namespace ma.Semaphore ma.dict #字典ma.register ma.Event ma.Pool ma.Value ma.get_server ma.shutdown In [26]: dict = ma.dict() #返回一个字典类型 In [27]: dictOut[27]: &lt;DictProxy object, typeid &apos;dict&apos; at 0x9795aac&gt; In [28]: dict[&apos;name&apos;]=&apos;zhangsan&apos; #为字典赋值In [32]: dict.values()Out[32]: [&apos;zhangsan&apos;] In [34]: dict[&apos;name&apos;] #取出其中的值Out[34]: &apos;zhangsan&apos; 7.Manger举例12345678910111213141516171819202122232425262728293031323334353637[root@backup python]# cat manager.py#!/usr/bin/python from multiprocessing import Process,Value,Array,Managerimport timeimport os def child_func(g_value,g_array,g_dict,ar): g_value.value = ar g_array[ar] = ar*ar g_dict[ar] = ar+ar listp = []g_value = Value(&apos;i&apos;,0)g_array = Array(&apos;i&apos;,range(10))manager = Manager()g_dict = manager.dict() #返回一个dict类型的字典 print(&quot;init g_value=&#123;0&#125;,g_array=&#123;1&#125;,g_dict=&#123;2&#125;&quot;.format(g_value.value,g_array[:],g_dict)) for i in range(10): p = Process(target=child_func, args=(g_value,g_array,g_dict,i)) p.start() listp.append(p) for i in range(10): listp[i].join() print(&quot;init g_value=&#123;0&#125;,g_array=&#123;1&#125;,g_dict=&#123;2&#125;&quot;.format(g_value.value,g_array[:],g_dict))-------------------------------------------------------------#执行结果[root@backup python]# python manager.pyinit g_value=0,g_array=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],g_dict=&#123;&#125;init g_value=8,g_array=[0, 1, 4, 9, 16, 25, 36, 49, 64, 81],g_dict=&#123;0: 0, 1: 2, 2: 4, 3: 6, 4: 8, 5: 10, 6: 12, 7: 14, 8: 16, 9: 18&#125;","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之进程基本概念","date":"2017-04-16T04:47:25.876Z","path":"2017/04/16/python/python进程之进程基本概念/","text":"进程：存储在磁盘上的程序被读取到内存的一次执行 进程是一个实体，每个进程都会有自己的地址空间，内存、数据栈 进程之间的数据不能共享，通过进程间通讯进行交互 python进程机制基于系统机制实现 Linux进程调度： 通过合理的调度，最大限度的利用处理器时间和系统资源，如果进程数大于处理器个数，某一时刻调度不到的进程会等待运行，调度程序会在这些等待运行的进程中选择一个合适的来执行。 python进程： 基于系统进程实现，启动一个进程相当于启动一个python虚拟机 每创建一个进程，就相当于复制一个python虚拟机 有两种方式来实现并发性，一种方式是让每个“任务”或“进程”在单独的内在空间中工作，每个都有自已的工作内存区域。不过，虽然进程可在单独的内存空间中执行，但除非这些进程在单独的处理器上执行，否则，实际并不是“同时”运行的。是由操作系统把处理器的时间片分配给一个进程，用完时间片后就需退出处理器等待另一个时间片的到来。另一种方式是在在程序中指定多个“执行线程”，让它们在相同的内存空间中工作。这称为“多线程处理” 子进程是父进程的副本，它将获得父进程数据空间、堆、栈等资源的副本。注意，子进程持有的是上述存储空间的“副本”，这意味着父子进程间不共享这些存储空间，它们之间共享的存储空间只有代码段。 python程序运行的状态： 运行 休眠 等待 僵尸进程","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之创建子进程及进程池","date":"2017-04-16T04:47:25.874Z","path":"2017/04/16/python/python进程之创建子进程及进程池/","text":"1.python创建进程12345678910import osret = os.fork() #创建进程,返回两个值,(如果是子进程,那么返回0,如果是父进程,那么返回子进程的pid)if(ret == 0): print &quot;child process&quot; print os.getpid() #子进程的pidelse: print &quot;Parent process ret=%d&quot; % ret print os.getpid()print os.getpid() #父进程和子进程都执行的代码 父进程创建子进程后，子进程会继承父进程的代码段，数据空间、堆和栈 通过fork()函数的返回值区分父进程和子进程 如果父进程没有回收子进程，那么当父进程退出后，由系统回收子进程资源 2.子进程回收函数：os.wait()1234567891011import osret = os.fork() #创建进程,返回两个值,(如果是子进程,那么返回0,如果是父进程,那么返回子进程的pid)if(ret == 0): print &quot;child process&quot; print os.getpid() #子进程的pidelse: print &quot;Parent process ret=%d&quot; % ret print os.getpid() os.wait() #父进程等待子进程退出print os.getpid() #父进程和子进程都执行的代码 父进程不显示调用os.wait()，当父进程退出，系统的第一个进程会把子进程回收 父进程显示的调用os.wait()，子进程退出后，就会被父进程回收 父进程没有显示调用os.wait()，而且父进程是一个守护或者死循环（如果父进程退出，那么子进程会由系统进程回收，但是此时父进程没有退出，所以系统就不会管理子进程），那么他创建的子进程退出后，就会成为僵尸进程，当父进程不断的创建进程的时候，就会出现错误 如果父进程先于子进程退出，那么守护进程init会将子进程回收，但是如果父进程在子进程的后面退出，并且此时在父进程中没有os.wait()，那么就会使子进程成为僵尸进程，即子进程的内存将不会被回收。os.wait函数用于等待子进程结束(只适用于UNIX兼容系统)。该函数返回包含两个元素的元组，包括已完成的子进程号pid，以及子进程的退出状态，返回状态为0，表明子进程成功完成。返回状态为正整数表明子进程终止时出错。如没有子进程，会引发OSError错误。os.wait要求父进程等待它的任何一个子进程结束执行，然后唤醒父进程。 要指示父进程等候一个指定的子进程终止，可在父进程中使用os.waitpid函数(只适用于unix兼容系统)。它可等候一个指定进程结束，然后返回一个双元素元组，其中包括子进程的pid和子进程的退出状态。函数调用将pid作为第一个参数传递，并将一个选项作为第二个选项，如果第一个参数大于 0，则waitpid会等待该pid结束，如果第一个参数是-1，则会等候所有子进程，也就和os.wait一样 3.图形展示父进程和子进程的执行过程 4.multiprocessing 方式创建子进程fork 方式是仅在linux 下才有的接口， 在windows下并没有， 那么在windows下如何实现多进程呢， 这就用到了multiprocessingmultiprocessing 模块的Process 对象表示的是一个进程对象， 可以创建子进程并执行制定的函数 12345678910111213141516from multiprocessing import Processimport os def pro_do(name, func): print &quot;This is child process %d from parent process %d, and name is %s which is used for %s&quot; %(os.getpid(), os.getppid(), name, func) if __name__ == &quot;__main__&quot;: print &quot;Parent process id %d&quot; %(os.getpid()) #process 对象指定子进程将要执行的操作方法(pro_do), 以及该函数的对象列表args(必须是tuple格式， 且元素与pro_do的参数一一对应) pro = Process(target=pro_do, args=(&quot;test&quot;, &quot;dev&quot;)) print &quot;start child process&quot; #启动子进程 pro.start() #是否阻塞方式执行， 如果有， 则阻塞方式， 否则非阻塞 pro.join() #if has this, it&apos;s synchronous operation or asynchronous operation print &quot;Process end&quot; 执行结果1234Parent process id 4878start child processThis is child process 4879 from parent process 4878, and name is test which is used for devProcess end 如果没有pro.join()， 则表示非阻塞方式运行， 那么最终的Process end的输出位置就有可能出现在pro_do 方法执行之前了1234Parent process id 4903start child processProcess endThis is child process 4904 from parent process 4903, and name is test which is used for dev 通过multiprocessing 的process对象创建多进程， 还可以从主进程中向子进程传递参数， 例如上面例子中的pro_do的参数 5.Pool 进程池12345678910111213141516from multiprocessing import Poolimport os, time def pro_do(process_num): print &quot;child process id is %d&quot; %(os.getpid()) time.sleep(6 - process_num) print &quot;this is process %d&quot; %(process_num) if __name__ == &quot;__main__&quot;: print &quot;Current process is %d&quot; %(os.getpid()) p = Pool() for i in range(5): p.apply_async(pro_do, (i,)) #增加新的进程 p.close() # 禁止在增加新的进程 p.join() print &quot;pool process done&quot; 输出:123456789101112Current process is 19138child process id is 19139child process id is 19140this is process 1child process id is 19140this is process 0child process id is 19139this is process 2child process id is 19140this is process 3this is process 4pool process done 其中12child process id is 19139child process id is 19140 是立即输出的， 后面的依次在等待了sleep的时间后输出 ， 之所以立即输出了上面两个是因为诶Pool 进程池默认是按照cpu的数量开启子进程的， 我是在虚拟机中运行， 只分配了两核， 所以先立即启动两个子进程， 剩下的进程要等到前面的进程执行完成后才能启动。 不过也可以在p=Poo() 中使用Pool(5)来指定启动的子进程数量， 这样输出就是下面的了： 123456789101112Current process is 19184child process id is 19185child process id is 19186child process id is 19188child process id is 19189child process id is 19187this is process 4this is process 3this is process 2this is process 1this is process 0pool process done 且123456Current process is 19184child process id is 19185child process id is 19186child process id is 19188child process id is 19189child process id is 19187 都是立即输出的","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python进程之IPC-管道(pipe)","date":"2017-04-16T04:47:25.873Z","path":"2017/04/16/python/python进程之IPC-管道(pipe)/","text":"Unix中的进程间通信方式之一是通过管道实现的，管道分为有名管道和无名管道，对于有名管道FIFO，可以实现没有亲缘关系的进程间通信，而对于无名管道，可以实现父子进程间的通信 管道这种IPC存在的意义是为了实现进程间消息的传递。无名管道是Unix最初的IPC形式，但是由于无名管道的局限性，后来出现了有名管道FIFO，这种管道由于可以在文件系统中创建一个名字，所以可以被没有亲缘关系的进程访问 管道打开后的标识是以文件描述符的形式提供的，可以使用Unix系统中的read和write系统调用访问 管道的实现形式有多种，在一些系统中，管道被实现为全双工的，在管道的一端既可以读也可以写，但是Posix.1和Unix 98只要求半双工管道，在Linux系统中，管道是半双工的。 Unix中的无名管道是通过 pipe 函数创建的，该函数创建了一个半双工的管道。12345#include &lt;unistd.h&gt;int pipe(int fd[2]);返回值：成功返回0，出错返回-1 函数通过参数fd[2]返回两个描述符，fd[0]表示管道的读端，fd[1]表示管道的写端 管道一般是由一个父进程创建，然后被用来在父子进程间进行通信： 在父子进程通过管道进行通信的程序中，一般在父进程中先创建一个管道，然后 fork 出一个子进程，然后在两个进程中关闭不写和不读的两端。 由于Unix中的管道默认实现是单向的，为了实现双向的，可以用两个单向的管道模拟","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之迭代器和解析","date":"2017-04-16T04:47:25.871Z","path":"2017/04/16/python/python语法之迭代器和解析/","text":"1.迭代器(Iterator)和可迭代对象(Iterable)1.1判断对象是否可迭代可以使用isinstance()判断一个对象是否是Iterator对象123456789&gt;&gt;&gt; from collections import Iterator&gt;&gt;&gt; isinstance((x for x in range(10)), Iterator)True&gt;&gt;&gt; isinstance([], Iterator)False&gt;&gt;&gt; isinstance(&#123;&#125;, Iterator)False&gt;&gt;&gt; isinstance(&apos;abc&apos;, Iterator)False 1.2.Iterator 和Iterable区别 凡是可作用于for循环的对象都是Iterable类型； 凡是可作用于next()函数的对象都是Iterator类型，它们表示一个惰性计算的序列； 集合数据类型如list、dict、str等是Iterable但不是Iterator，不过可以通过iter()函数获得一个Iterator对象。 Python的for循环本质上就是通过不断调用next()函数实现的 2.迭代器原理1234567891011&gt;&gt;L = [1,2,3]&gt;&gt;I = iter(L) #第一步：使用iter()函数将可迭代的对象转化成为一个迭代器&gt;&gt;I.next()1&gt;&gt;I.next() #第二部：使用迭代器的next()方法( 或者_next_()方法 )进行迭代2&gt;&gt;I.next()3&gt;&gt;I.next()....more ommittedStopIteration #第三部：在迭代过程中遇到StopItreation 异常就结束迭代 3.迭代器原理举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#Example1（列表迭代的过程）&gt;&gt;L = [1,2,3]&gt;&gt;for x in L: print(x, end=&apos; &apos;).......1 2 3#等价于&gt;&gt;L = [1,2,3]&gt;&gt;I = iter(L) #获取迭代器while True: try: x = next(I) #进行迭代 except StopIteration: #迭代结束 break print(x , end=&apos; &apos;).......1 2 3#Example2（字典迭代）&gt;&gt;D = &#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;&gt;&gt;for key in D: print(key, D[key])#等价于&gt;&gt;D = &#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;&gt;&gt;I = iter(D)&gt;&gt;next(I) #对字典的迭代取出的是字典中的key&apos;a&apos;&gt;&gt;next(I)&apos;b&apos;&gt;&gt;next(I)Traceback ......more omitted...StopIteration #迭代结束#Example3（enumerate迭代）&gt;&gt;E = enumerate(&apos;spam&apos;)&gt;&gt;I = iter(E) #得到迭代器&gt;&gt;next(I)(0,&apos;s&apos;)&gt;&gt;next(I)(1,&apos;p&apos;)......&gt;&gt;list( enumerate(&apos;spam&apos;))[(0,&apos;s&apos;) ,(1,&apos;p&apos;) ,(2,&apos;a&apos;) ,(3,&apos;m&apos;)] 4.列表解析4.1基本格式 4.2举例1234567891011121314151617181920212223#Example1L = [x + 1 for x in L]#Example2lines = [line.rstrip() for line in lines] #去掉每一行后面的\\n#Example3[line.upper() for line in opne(&apos;data.txt&apos;)] #将每一行转成大写[&apos;AAA&apos;, &apos;BBB&apos;, &apos;CCC&apos;]#Example4（if条件）lines = [line.rstrip() for line in open(&apos;data.txt&apos;) if line[0] == &apos;p&apos; ] #在列表中只留下以‘p&apos;开头的行#Example5（多个for）[x +y for x in &apos;abc&apos; for y in &apos;123&apos;][&apos;a1&apos;, &apos;b2&apos;, &apos;c3&apos;]#Example6(生成字典)&#123;ix:line for ix,line in enumerate(open(&apos;data.txt&apos;))&#125;&#123;0:&apos;AAA&apos;, 1:&apos;BBB&apos;, 2:&apos;CCC&apos;&#125; 5.支持可迭代协议的函数这样的函数会在内部调用itre（可迭代对象）进行循环12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#listlist(open(&apos;data.txt&apos;)) #循环可迭代对象的每一行[&apos;AAA&apos;, &apos;BBB&apos;, &apos;CCC&apos;]#sorted #返回的是listsorted(open(&apos;data.txt&apos;))[&apos;AAA&apos;, &apos;BBB&apos;, &apos;CCC&apos;]#sumsum( [1,3,4,18] ) #循环可迭代对象list，求sum26#max/min #循环可迭代对象list，找出maxmax( [1,3,4,18] ) 18#any #如果一个迭代对象中的任何或所有项都为真，返回True ?any([2,3,4])True#tupletuple(open(&apos;data.txt&apos;)) #tuple循环可迭代对象(&apos;AAA&apos;, &apos;BBB&apos;, &apos;CCC&apos;)#join&quot;&amp;&amp;&quot;.join(open(&apos;data.txt&apos;)) #join循环可迭代对象,然后在其中加入“&amp;&amp;”’AAA&amp;&amp;BBB&amp;&amp;CCC‘#set#zipzip([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], [1,2,3]) #返回的是可迭代对象list( zip([&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], [1,2,3]) ) #在list中进行循环迭代，生成list列表[(&apos;a&apos;, 1) ,(&apos;b&apos;, 2), (&apos;c&apos;, 3)]#enumerateenumerate(open(&apos;data.txt&apos;)) #返回的是可迭代对象list( enumerate(open(&apos;data.txt&apos;)) ) #在list中进行循环迭代，生成list列表#range 6.多个迭代器VS单个迭代器1234567891011121314151617181920212223242526272829#range 产生的多个迭代器&gt;&gt;R = range(3)&gt;&gt;next(R)TypeError: range object is not an iterator&gt;&gt;I1 = iter(R)&gt;&gt;I2 = iter(R) #可以产生多个互不影响的迭代器&gt;&gt;next(I1) #遍历迭代器10&gt;&gt;next(I1)1&gt;&gt;next(I2) #迭代器20&gt;&gt;next(I2)1#zip 、map、filter 产生的单迭代器&gt;&gt;Z = zip([1,2,3], [10,11,12])&gt;&gt;I1 = iter(Z)&gt;&gt;I2 = iter(Z) &gt;&gt;next(I1)(1,10)&gt;&gt;next(I1)(2,11)&gt;&gt;next(I2) #他会接着上一个迭代器循环(3,12) 7.字典视图迭代器字典有针对key的迭代器，keys、values、items返回都是可迭代的视图对象（可迭代对象）12345678910111213141516171819202122232425&gt;&gt;D = &#123;&apos;a&apos;:1, &apos;b&apos;:2 , &apos;c&apos;:3&#125;&gt;&gt;K = D.keys()&gt;&gt;next(K)TypeError:dict_key object is not an iterator.....&gt;&gt;I = iter(K)&gt;&gt;next(I)&apos;a&apos;&gt;&gt;next(I)&apos;b&apos;for key in D.keys(): print(key,D[key])#因为字典也是可迭代对象&gt;&gt;D = &#123;&apos;a&apos;:1, &apos;b&apos;:2 , &apos;c&apos;:3&#125;&gt;&gt;I = iter(D) #返回对key的迭代器&gt;&gt;next(I)&apos;a&apos;for key in D: #迭代字典的key print(key,D[key])","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之赋值语句","date":"2017-04-16T04:47:25.869Z","path":"2017/04/16/python/python语法之赋值语句/","text":"1.赋值语句的一些简单特性 赋值语句建立对象引用值赋值语句总是建立对象的引用值，而不是复制对象，因此python的变量更像是指针，而不是数据存储区域 变量名在首次赋值时会被创建 变量名在引用前必须先赋值 2.赋值语句的形式 表达式 含义 spam = ‘spam’ 基本形式 spam, ham = ‘yum’, ‘YUM’ 元组赋值运算（位置性） [spam, ham] = [‘yum’, ‘YUM’] 列表赋值运算（位置性） a, b, c, d = ‘spam’ 序列赋值匀速（通用性） a, *b = ‘spam’ 扩展的序列解包 spam = ham = ‘lunch’ 多目标赋值运算 sapm += 43 增强型赋值运算（相当于：spam = spam + 43) 3.序列赋值123456789101112131415161718192021222324252627282930#Example1&gt;&gt;nudge = 1&gt;&gt;wink = 2&gt;&gt;A, B = nudge, wink #like A = nudge , B = wink&gt;&gt;A, B(1, 2)&gt;&gt;[c, d] = [nudge, wink]&gt;&gt;c, d(1,2)#Example2&gt;&gt;str = &apos;SPAM&apos;&gt;&gt;a, b, c, d = str&gt;&gt;a, b(&apos;S&apos;, &apos;P&apos;)&gt;&gt;a, b, c = str #变量和值的个数不匹配ValueError : too many values to unpack#Example3&gt;&gt;((a, b), c ) = (&apos;sp&apos;, &apos;am&apos;) #嵌套赋值&gt;&gt;a, b ,c &gt;&gt;(&apos;s&apos;, &apos;p&apos;, &apos;am&apos;)#Example4 &gt;&gt;for (a, b, c) in [(1 ,2 ,3), (4, 5, 6)] #for循环中的赋值&gt;&gt;for((a, b) ,c ) in [((1,2), 3), ((4, 5), 6)] 4.匹配赋值 *a一个带有星号的名称可以在赋值目标中使用，以指定对于序列的一个更为通用的匹配——一个列表赋值给了带有星号的变量名123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#Example1&gt;&gt;seq = [1,2,3,4]&gt;&gt;a, *b = seq&gt;&gt;a1&gt;&gt;b [2,3,4]#Example2&gt;&gt;seq = [1,2,3,4]&gt;&gt;*b, a = seq #会贪婪匹配&gt;&gt;a 4&gt;&gt;b[1,2,3]#Example3&gt;&gt;seq = [1,2,3,4]&gt;&gt;a, *b ,c = seq #处于中间的贪婪匹配&gt;&gt;a1&gt;&gt;b[2,3]&gt;&gt;c4#Example4&gt;&gt;seq = [1,2,3,4]&gt;&gt;a, b, c, d, *e = seq &gt;&gt;print(a,b,c,d,e) 1 2 3 4 [] #没有匹配到所以只能是一个空的list#Example5&gt;&gt;seq = [1,2,3,4]&gt;&gt;a, b, *c, d, e = seq #同上&gt;&gt;print(a,b,c,d,e) 1 2 [] 3 4#Example6&gt;&gt;seq = [1,2,3,4]&gt;&gt;a , *b , c , *d = seq #对于 b d 不知道如何匹配，所以会报错SyntaxError:two starred expressions in assignment#Example7&gt;&gt;seq = [1,2,3,4]&gt;&gt;*a = seqSyntaxError:starred assignment target must be in a list or tuple#Example8&gt;&gt;seq = [1,2,3,4]&gt;&gt;*a , = seq #左侧必须是一个元组或者是list&gt;&gt;a [1,2,3,4] 5.增强赋值语句12345678910111213141516171819202122232425262728&gt;&gt;L = [1,2]&gt;&gt;L = L + [3,4]&gt;&gt;L[1,2,3,4]&gt;&gt;L.extend([7,8])&gt;&gt;L[1,2,3,4,7,8]&gt;&gt;L += [9,10] #python会将+= 转成extend方法，而不是转成 L = L + [9, 10] &gt;&gt;L[1,2,3,4,7,8,9,10]#增强型赋值被修改&gt;&gt;L = [1,2]&gt;&gt;M = L&gt;&gt;L = L + [3,4]&gt;&gt;M,L([1,2], [1,2,3,4]) #可以看到L 改变了，但是M没有变化&gt;&gt;L = [1,2]&gt;&gt;M = L&gt;&gt;L += [3,4] &gt;&gt;M,L([1,2,3,4], [1,2,3,4]) #因为+=使用的是extend进行的，所以修改是原处修改，所以M 、L 都发生了变化","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之格式简介","date":"2017-04-16T04:47:25.867Z","path":"2017/04/16/python/python语法之格式简介/","text":"1.python中增加了什么？python中新的语法成分是冒号（：），所有python的复合语句都有相同的一般形式，也就是首行以冒号结尾，首行下一行嵌套的代码往往按缩进的格式书写，如下所示：12Header line: #首行以冒号结尾 Nested statement block #缩进的格式书写 2.python删除了什么？2.1括号是可选的123if(x&lt;y)if x&lt;y #推荐使用 2.2.终止行就是终止语句（不需要分号）python中需要要像C语言那样用分号终止一行1x = 1; 字python中一行的结束会自动终止在该行的语句，即，可以省略分号1x = 1 #推荐不使用分号 2.3.缩进的结束就是代码块的结束python不在乎怎么缩进，缩进多少，语法规则只不过是给定一个单独的嵌套块中所有的语句都必须缩颈相同的距离12345678910#C语言写法if (x&gt;y)&#123; x = 1; y = 2;&#125;#python写法if x&gt;y: x = 1 y =2","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之打印语句","date":"2017-04-16T04:47:25.865Z","path":"2017/04/16/python/python语法之打印语句/","text":"1.python2.x 和python3.x的不同 在python2.x中，打印是语句，拥有自己特定的语法 在python3.x中，打印是一个内置的函数，用关键字参数来表示特定的模式 2.python3.x中的print函数2.1调用格式print ([object,….] [,sep=’ ‘] [,end = ‘\\n’] [, file = sys.stdout]) sep 、end和file如果给出的话，必须使用一种特殊的“name = value”的语法来根据名称而不是位置来传递参数 sep 在每一个对象之间插入的字符串，没有指定，默认就是空格（如果传递的是空字符串，则所有的对象文本之间将连接在一起） end 添加在打印文件末尾的一个字符串，如果没有传递的话， 默认是\\n换行符 file 指定了文本将要发送到的文件、标准流、或者其他类似文件的对象，默认是sys.stdout 2.2.Example123456789101112131415161718192021222324252627#Example1&gt;&gt;print() #打印的是空行#Example2&gt;&gt;print(x, y, z, sep=&apos;,&apos;)spam, 99, [&apos;eggs&apos;] #使用逗号去分割文本&gt;&gt;print(x, y, z, sep=&apos;&apos;) #所有的对象文本将连接一起spam99[&apos;eggs&apos;]#Example3&gt;&gt;print(x, y, z, end=&apos;&apos;); print(x, y, z) #两次打印之间没有换行spam 99 [&apos;eggs&apos;]spam 99 [&apos;eggs&apos;] &gt;&gt;print(x, y, z, end=&apos;###\\n&apos;) #指定特殊的结束符spam 99 [&apos;eggs&apos;]####Example4&gt;&gt;print(x, y, z, sep=&apos;...&apos;, file=open(&apos;data.txt&apos;,&apos;w&apos;)) #指定输出到一个打开的文件中去&gt;&gt;print(x, y, z)spam 99 [&apos;eggs&apos;]&gt;&gt;print(open(&apos;data.txt&apos;).read()) #打印文件中的内容spam...99...[&apos;eggs&apos;] 3.打印流重定向3.1.系统的打印方法123&gt;&gt;import sys&gt;&gt;sys.stdout.write(&apos;hello world&apos;)hello world 3.2.sys.stdout和print比较1234print(x, y)#等价于importsys.stdout.write(str(x) + &apos; &apos; + str(y) + &apos;\\n&apos;) 3.3.改变print的重定向流123456import syssys.stdout = open(&apos;log.txt&apos;, &apos;a&apos;)...print(x, y ,z ) #这样print将内容打印到了log.txt文件中#这样改变的一个弊病，就是每次print的时候都是打印内容到log.txt中 3.4.自动化重定向流1234567891011121314151617&gt;&gt;import sys&gt;&gt;temp = sys.stdout&gt;&gt;sys.stdout = open(&apos;log.txt&apos;, &apos;a&apos;)&gt;&gt;print(&apos;spam&apos;)&gt;&gt;print(1, 2, 3)&gt;&gt;sys.stdout.close() &gt;&gt;sys.stdout = temp #重新定向输出流&gt;&gt;print(&quot;back here&quot;)back here&gt;&gt;print(open(&apos;log.txt&apos;).read())spam1 2 3#有了上面的试验，我们可以知道，print的好处，我们只是使用了print的中的file可以临时的改变输出流的指向，当打印完毕之后，输出流又重新回到原来的默认的流","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之变量命名规则","date":"2017-04-16T04:47:25.864Z","path":"2017/04/16/python/python语法之变量命名规则/","text":"1.命名规则 语法：以数字、字母、下划线组成，其中以字母、下划线开头 区分大小写 禁止使用保留字 2.命名惯例 以单一下划线开头的变量名（_X）不会被from module import * 语句导入 前后有下划线的变量名（_X_）是系统定义的变量名，对解释器有特殊意义 以两个下划线开头，但是没有两个下划线结尾的变量名（__X）是类的本地变量 通过交互模式运行时，只有单个下划线的变量名（_）会保存最后表达式的结果 类变量名通常以大写字母开头 模块变量名通常以小写字母开头 变量名self表示类本身","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之while和for循环和range和zip","date":"2017-04-16T04:47:25.863Z","path":"2017/04/16/python/python语法之while和for循环和range和zip/","text":"1.while循环1.1一般格式首行以测试表达式、有一列或者多列的缩进语句的主体，一个可选的else部分（控制权离开循环而又没有碰到break语句时会执行）1234while &lt;test&gt;: &lt;statements1&gt;else: &lt;statements2&gt; 1.2.Example12345678910111213141516171819202122#Example1while True: print(&apos;ha ha&apos;) #一直循环打印#Example2x = &apos;spam&apos;while x: print(x, end=&apos; &apos;) x = x[1:] #重新指向自己的切片。。。spam pam am m #Example3a = 0, b = 6while a&lt;b: print(a, end=&apos; &apos;) a +=1。。。0 1 2 3 4 5 1.3.break、continue、pass break 跳出最近所在的循环（跳出整个循环语句） continue 跳到最近循环所在的开头处（来到循环的首行） pass 什么事也不做，只是空占位语句 循环else块只有当循环正常离开时，才会执行（也就是没有碰到break语句） 2.for循环for循环语句可以用于字符串、列表、元组、其他内置可迭代对象以及之后我们能够通过类所创建的新对象 2.1.一般格式12345678910111213for &lt;target&gt; in &lt;object&gt; &lt;statements1&gt;else: &lt;statements2&gt;#for 配合break、continue使用的例子for &lt;target&gt; in &lt;object&gt; &lt;statements1&gt; if &lt;test&gt; : break if &lt;test&gt; :continueelse: &lt;statements2&gt; 2.2.Example1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#Example1for x in [&apos;spam&apos;, &apos;eggs&apos;, &apos;ham&apos;]: #循环遍历list print(x , end=&apos; &apos;)...spam eggs ham#Example2S = &apos;spam&apos;for char in S: #循环遍历str print(char, end=&apos; &apos;) ..s p a m #Example3T = (&apos;and&apos;, &apos;or&apos; , &apos;okay&apos;)for x in T: #循环遍历tuple print(x , end=&apos; &apos;)...and or okay#Example4T = [(1,2), (3,4), (5,6)]for (a,b) in T: #遍历嵌套元组 print(a,b)...1 23 45 6#Example5D = &#123;&apos;a&apos;:1, &apos;b&apos;:2, &apos;c&apos;:3&#125;for key in D: #遍历dict的key print(key,&apos;=&gt;&apos;,D[key])....a=&gt;1b=&gt;2c=&gt;3for (key,value) in D.items(): print(key,&apos;=&gt;&apos;,value)....a=&gt;1b=&gt;2c=&gt;3#Example6for ((a, b), c) in [((1,2), 3), ((4,5), 6)]: print(a,b,c)1 2 34 5 6for((a, b), c) in [((1, 2), 3), (&apos;xy&apos;, 6)]: print(a, b, c)....1 2 3x y 6#Example7for (a, *b, c) in [(1,2,3,4), (5, 6, 7, 8)]: print(a,b,c)......1 [2,3] 45 [6,7] 8#Example8 for x in L1: #嵌套for循环 for y in L2: print(xxxx)#Example9for line in open(&apos;test.txt&apos;): #循环遍历文件，文件迭代器会自动在每次循环迭代的时候读入一行 print(line) 3.rangerange函数返回一系列连续增加的整数，可用作for的索引 3.1.语法1234list(range(&lt;font color=red&gt;x&lt;/font&gt;, &lt;font color=green&gt;y&lt;/font&gt; , &lt;font color=blue&gt;step&lt;/font&gt;)list(range(0,7,2)[0, 2, 4, 6] x 表示整数序列的起始值（默认0开始） y 表示整数序列的结束值（不包含y本身） step 步进值（相邻元素之间的差值，默认是1） 3.2.例子12345678910111213141516171819list(range(-4,4))[-4,-3,-2,-1,0,1,2,3] #可以是负数序列list(range(4,-4,-1))[4,3,2,1,0,-1,-2,-3] #可以是非递增的x = &apos;spam&apos;for i in range(len(x)): #和for循环配合使用 print(x[i], end=&apos; &apos;)...s p a mL = [1,2,3][x+1 for x in L] #生成一个新的list，其中的每一个元素都加1 4.zipzip会取得一个或多个序列为参数，然后返回一个元组的列表，将这些序列中的并排的元素配成对。如下：12345678910111213141516171819202122232425262728293031323334353637L1 = [1,2,3]L2 = [a,b,c]list(zip(L1, L2))[(1,a), (2,b), (3,c)]T1 = (1,2,3)T2 = (4,5,6)T3 = (7,8,9)list(zip(T1, T2, T3))[(1,4,7), (2,5,8), (3,6,9)]S1 = &apos;abc&apos;S2 = &apos;xyz123&apos;list(zip(S1, S2)) #当长度不同时，zip会以最短序列的长度为准来截取所得到的元组[(&apos;a&apos;,&apos;x&apos;), (&apos;b&apos;,&apos;y&apos;), (&apos;c&apos;, &apos;z&apos;)]#使用zip构造字典keys = [&apos;eggs&apos;, &apos;toast&apos;, &apos;ham&apos;]values = [1,3,5]D2 = &#123;&#125;for (k,v) in list(zip(keys,values)): D2[k] = vD2&#123;&apos;eggs&apos;:1, &apos;toast&apos;:3, &apos;ham&apos;:5&#125;#另一种方式构造字典D3 = dict(list(zip(keys,values))) 5.偏移和元素：enumerate1234567891011121314151617181920212223#手动获取偏移&gt;&gt;s = &apos;spam&apos;&gt;&gt;offset = 0&gt;&gt;for item in s: print(item, &apos;appers at offset&apos;, offset) offset +=1...s appers at offset 0p appers at offset 1a appers at offset 2m appers at offset 3#通过enumerate获取偏移&gt;&gt;s = &apos;spam&apos;&gt;&gt;for (offset, item) in enumerate(s): #offset记录的就是偏移量 print(item, &apos;appers at offset&apos;, offset)...s appers at offset 0p appers at offset 1a appers at offset 2m appers at offset 3","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python语法之if","date":"2017-04-16T04:47:25.861Z","path":"2017/04/16/python/python语法之if/","text":"1.语法格式123456if &lt;test1&gt;: &lt;statement1&gt;elif &lt;test2&gt;: &lt;statement2&gt;else: &lt;statement3&gt; 2.基本例子12345678910111213141516171819202122232425262728#Example1if 1: #只有if print(&apos;true&apos;)#Example2if not 1: #只有if /else print(&apos;true&apos;)else: print(&apos;false&apos;)#Example3if x == &apos;a&apos;: #多路分支 print(&apos;a&apos;)elif x==&apos;b&apos;: print(&apos;b&apos;)else: print(&apos;c&apos;)#Example4branch = &#123;&apos;spam&apos;:1.23 , &apos;ham&apos;:2,33, &apos;eggs&apos;:4.33&#125;choice = &apos;ham&apos;if choice in branch: #判断dict中有没有对应的键匹配 print(branch[choice])else: print(&apos;Bad choice&apos;) 3.语法规则 语句是逐个执行的，除非你不这样编写 块和语句的边界会自动检测 复合语句=首行+“：”+缩进语句 空白行、空格、以及注释通常都会忽略 文档字符串会被忽略，但会保存并由工具显示 4.语句分割符 如果使用语法有括号对,那么语句就可横跨数行，如：封闭的（）、{}、[] 如果语句以反斜线结尾，就可横跨多行（不推荐使用） 字符串常量有特殊规则，如：三引号 其它使用分号终止语句，那么可以把一个以上的简单语句挤进单个行中注释和空白行可以出现在文件的额任意之处 举例1234567891011121314151617181920#Example1L = [&apos;Good&apos;, &apos;Bad&apos;, &apos;Ugly&apos;]#Example2if (a ==b and c ==d and d ==e and e ==f )#Example3x =1; y = 3; print(x)#Example4S = &apos;&apos;&apos; #也可以使用三个双引号chenyansong is a good boybut is a good ...&apos;&apos;&apos; 5.真值测试5.1总结 任何非零数字和非空对象都为空 数字零以及空对象以及特殊对象None都被认为是假 比较和相等测试会递归地引用到数据结构中 比较和相等测试会返回True 或False 布尔and 和or会返回真或假的操作对象X and Y 如果X和Y都为真，就返回第一个对象XX or Y 如果X或Y，有一个为真，就返回第一个为真的对象not X 如果为假，那么就返回真（表达式返回的是True或False） 5.2.Example123456789101112131415161718192021&gt;&gt;2&lt;3, 2&gt;3 #比较返回True 或False(True, False)&gt;&gt;2 or 3 , 3 or 2 #布尔返回的是对象(2, 3)&gt;&gt;[] or 33&gt;&gt;[] or &#123;&#125;&#123;&#125;&gt;&gt;2 and 3, 3 and 2(3, 2)&gt;&gt;[] and &#123;&#125; #第一个对象为空，就不会进行后面的运算，直接返回了[]&gt;&gt;3 and [] #第一个对象不为空，所以返回的是第二个对象[] 6.if/else三元表达式A = Y if X else Z 只有当X为真的时候，python才会执行表达式Y,而只有当X为假的时候，才会执行表达式Z 1234567&gt;&gt;A = &apos;t&apos; if &apos;spam&apos; else &apos;f&apos;&gt;&gt;A&apos;t&apos;&gt;&gt;A = &apos;t&apos; if &apos;&apos; else &apos;f&apos;&gt;&gt;A&apos;f&apos;","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python网络编程之简介","date":"2017-04-16T04:47:25.860Z","path":"2017/04/16/python/python网络编程之简介/","text":"网络客户端和服务端架构服务器：软件和硬件，主要提供自己服务，比如搜索，购物、视频、教育等客户端：用户通过浏览器或者软件访问服务器 网络客户端和服务端模型： 客户端使用浏览器，浏览器基于http协议和域名对服务器进行访问 当我们访问服务器的时候，需要DNS解析域名，然后通过IP通过网络对服务器进行连接 服务器接收http请求内容，返回相应的数据 浏览器根据返回数据，显示相应的图文和视频信息 服务端和客户端编程： 服务器进行配置，建立一个监听端口，监听有没有请求 客户端使用浏览器对域名进行访问，并得到相应的数据 网络套接字（socket）：用于描述IP地址和端口，是一个通信链的句柄，可以用来实现不同虚拟机或不同计算机之间的通信，是支持TCP/IP的网络通信的基本操作单元，可以看做是不同主机之间的进程进行双向通信的端点 网络套接字地址：IP地址+套接字类型+端口号IP地址：本机或者服务端IP端口号：有效范围0-65535，其中0-1024是系统保留端口套接字类型： 流套接字（SOCK_STREAM）：在服务器和客户端通讯之前建立连接的一种特性，在开始通讯前必须先进行一次呼叫和应答，面向连接的通讯是顺序的、可靠的 数据包套接字（SOCK_DGRAM）：服务器和客户端不用建立连接就可以通讯，但是通讯数据到达顺序、可靠性无法保证 原始套接字（SOCK_RAW）：原始套接字与标准套接字（标准套接字指的是前面介绍的流套接字和数据报套接字）的区别在于：原始套接字可以读写内核没有处理的IP数据包 三次握手、四次断开 TCP应用领域：Telnet、FTP、SMTP等 UDP支持的应用层协议主要有：NFS（网络文件系统）、SNMP（简单网络管理协议）、DNS（主域名称系统）、TFTP（通用文件传输协议）等","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python网络编程之多路复用服务器模型","date":"2017-04-16T04:47:25.858Z","path":"2017/04/16/python/python网络编程之多路复用服务器模型/","text":"1.select 模块的作用&emsp;select系统调用用来检测多个文件描述符状态变化，程序会一直在select中等待直到超时或者被监视文件描述符中的一个或者多个状态发生变化 2.select函数12345select(rlist, wlist, xlist[,timeout]) #返回值：（rlist, wlist, xlist）#rlist：读取socket列表，判断是否有可以读的socket#wlist：写入socket列表，判断是否有可以写的socket#xlist：异常socket列表，判断是否有异常的socket#如果socket可以读，可以写，或者异常，select返回相应的socket列表 3.select工作模型 4.select如何判断可读、可写、异常 select如何判断可读 将监测的socket加入到rlist中，然后调用select等待数据 如果有客户端连接或者对方数据，那么select就会立刻返回 如果是新的连接调用accept接受新的socket，并将该socket计入到rlist或者wlist中 如果有数据，那么就接受数据 select如何判断可以写 将检测到的socket加入到wlist，调用select等待 如果socket可以写，返回可以写的socket列表 调用send方法，发送数据 select如何判断异常 将监测到的socket加入到xlist，调用select等待 如果socket有异常，返回异常的xlist列表 处理异常的socket 5.select编程模型 read操作：需要将可读的socket全部读出 write操作：将可以写的socket根据自己需要发送相关消息和数据 socket一般都是可写的，所以要根据自己需求决定是否每次写完之后是否将wlist中的socket移除 因为网络问题，socket并不是一致处于可写状态，如果使用UDP，不去判断socket是否可写，可能会写失败 6.程序实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#服务端[root@backup python]# cat select.py#!/usr/bin/python from socket import *import timeimport osimport sysimport threadingimport selectimport Queue if __name__ == &apos;__main__&apos;: tcp_server = socket(AF_INET, SOCK_STREAM) tcp_server.bind((&apos;localhost&apos;,5002)) tcp_server.listen(5) msg_list = &#123;&#125; #client queue in_list = [] #socket readline out_list = [] #socket writelist in_list.append(tcp_server) while True: ilist,olist,elist = select.select(in_list,out_list,in_list,1) if not (ilist or olist or elist): print(&quot;Wait Timeout : in_list = &quot;+len(in_list)) continue for c in ilist: #读处理 if c == tcp_server: client,client_info = c.accept() in_list.append(client) msg_list[client]=Queue.Queue() else: recv_data = c.recv(1024) if recv_data: print(&quot;recv= &quot;+recv_data) msg_list[c].put(recv_data) #将数据放入消息队列中 if c not in out_list: out_list.append(c) else: #如果接受的数据为空的话（可能client异常了），那么从ilist、outlist中删除 if c in out_list: out_list.remove(c) in_list.remove(c) c.close() del msg_list[c] print(&quot;a client exit&quot;) for c in elist: #异常处理 if c in in_list: in_list.remove(c) if c in out_list: out_list.remove(c) c.close() del msg_list[c] for c in olist: #写处理 if c in msg_list: try: msg = msg_list.get_nowait() #从队列中取出数据 except Queue.Empty: out_list.remove(c) else: print(&quot;send msg= &quot;+msg) c.send(msg) #发送数据 tcp_server.close() 7.select多路服务器优点和缺点 优点 不需要频繁的创建和销毁线程和进程，节省了系统的开销和负担 select采用轮询方式处理收发数据，处理效率高于多进程和多线程模型 缺点 单个进程监控的最多文件描述符是有限的（系统默认1024个） 需要维护一个文件描述符列表 对于文件描述符扫描是线性的，当每次对这个结构进行扫描时时间会增加 内核把文件描述符消息通知给用户空间，空间需要拷贝 8.select和多线程模型 9.epoll模型 是Linux下多路复用IO接口select、poll的增强版本 它所支持的文件描述符的上限是最大可以打开文件的数目 epoll只会对“活跃”的socket进行操作，不会因为文件描述符的增加导致效率线性的下降 epoll是通过内核于用户空间mmap同一块内存实现，使用mmap加速内核于用户空间的消息传递","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python网络编程之多线程多进程服务端模型","date":"2017-04-16T04:47:25.856Z","path":"2017/04/16/python/python网络编程之多线程多进程服务端模型/","text":"1.基本服务器模型 只能同时支持一路 因为创建的socket文件描述符是阻塞的，所以如果该socket一直没有消息，那么我们程序一直处于等待中 2.多进程服务器模型 多进程模型对系统的开销比较大 该模型需要注意： 子进程的回收 子进程的创建数量 3.多进程服务器模型程序实现123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/usr/bin/python import timeimport osimport sysfrom socket import * def client_deal(client,client_info): # while True: msg = client.recv(1024) if msg == &apos;&apos;: client.close() break print(&quot;recv=&quot;+msg+&quot; from :&quot;,client_info) if msg == &apos;q&apos; or msg == &apos;Q&apos;: client.close() break def client_process(client,client_info): pid = os.fork() if pid == 0: #子进程 ppid = os.fork() #子进程中又开启了一个子进程 if ppid == 0: client_deal(client,client_info) #用该进程去处理对应的客户端请求 else: print(&quot;child exit&quot;) sys.exit() else: client.close() os.wait() print(&quot;wait child exit&quot;) if __name__ == &apos;__main__&apos;: tcp_server = socket(AF_INET, SOCK_STREAM) tcp_server.bind((&apos;localhost&apos;,5001)) tcp_server.listen(5) while True: print(&quot;wait client......&quot;) client,client_info = tcp_server.accept() print(client_info) client_process(client,client_info) tcp_server.close() 4.多线程服务器模型 接收到一个请求，创建一个相应的线程 如果该请求结束，关闭该请求，退出执行线程 5.多线程服务器模型程序实现1234567891011121314151617181920212223242526272829303132333435363738394041424344#!/usr/bin/python from socket import *import timeimport osimport sysimport threading def client_deal(client,client_info): while True: msg = client.recv(1024) if msg == &apos;&apos;: client.close() break print(&quot;recv=&quot;+msg+&quot; from :&quot;,client_info) if msg == &apos;q&apos; or msg == &apos;Q&apos;: client.close() break def thread_process(client,client_info): p_thread = threading.Thread(target=client_deal,args=(client,client_info)) p_thread.setDaemon(True) p_thread.start() print(&quot;start on thread&quot;) if __name__ == &quot;__main__&quot;: tcp_server = socket(AF_INET,SOCK_STREAM) tcp_server.bind((&quot;localhost&quot;,5001)) tcp_server.listen(5) while True: print(&quot;pid=&quot;+str(os.getpid())+&quot; wait client&quot;) client,client_info = tcp_server.accept() #每接受到一个客户端的时候，就去开启一个线程 print(client_info) thread_process(client,client_info) #开启一个线程 tcp_server.close()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python网络编程之UDP客户端和服务端实现","date":"2017-04-16T04:47:25.855Z","path":"2017/04/16/python/python网络编程之UDP客户端和服务端实现/","text":"1.UDP服务端和客户端模型： 2.UDP服务器1234567from socket import *udp_server = socket(AF_INET, SOCK_DGRAW) #创建udp socket udp_server.bind((&apos;localhost&apos;, 5002)) #绑定本地IP与端口 recv_data, addr = udp_server.recvfrom(1024) #接收数据和client地址信息udp_server.close() 3.UDP客户端123456from socket import *udp_client = socket(AF_INET, SOCK_DGRAW) #创建udp socket udp_client.sendto(&quot;hello python&quot;, (&apos;localhost&apos;, 5002)) #向指定的地址发送数据 udp_client.close() #关闭连接 4.实例123456789101112131415161718192021222324252627282930313233343536373839#服务端#!/usr/bin/python from socket import *import time udp_server = socket(AF_INET, SOCK_DGRAM)udp_server.bind((&quot;localhost&quot;,5002)) while True: recv_data,addr = udp_server.recvfrom(1024) print(recv_data,addr) if recv_data == &apos;q&apos;: break udp_server.sendto(&quot;server send data:&quot;+recv_data, addr) udp_server.close()-----------------------------------------------------------------#客户端#!/usr/bin/python from socket import *import time udp_client = socket(AF_INET, SOCK_DGRAM) while True: msg = raw_input(&quot;&gt;&quot;) udp_client.sendto(msg, (&apos;localhost&apos;,5002)) if msg == &apos;q&apos;: break server_data,addr = udp_client.recvfrom(1024) print(server_data,addr) udp_client.close() 5.UDP传输的问题 丢包问题 包到达顺序的问题 不能知道对方是否关闭的问题 6.udp客户端中的connect()如果在执行的过程中服务端关闭，然后客户端还在发送数据，但是客户端并不知道服务端关闭了，他还是会不停的发送数据，只不过此时的发送的数据将全部丢包而已。通过connect知道数据是否发送成功12345678910111213141516171819202122232425#!/usr/bin/python from socket import *import time udp_client = socket(AF_INET, SOCK_DGRAM)udp_client.connect((&apos;localhost&apos;,5002)) #在客户端中使用connect while True: msg = raw_input(&quot;&gt;&quot;) udp_client.sendto(msg, (&apos;localhost&apos;,5002)) if msg == &apos;q&apos;: break server_data,addr = udp_client.recvfrom(1024) print(server_data,addr) udp_client.close()#如果服务端关闭，客户端会抛出异常Traceback (most recent call last): File &quot;udp_client.py&quot;, line 14, in &lt;module&gt; server_data,addr = udp_client.recvfrom(1024)socket.error: [Errno 111] Connection refused 7.通过tcpdump查看发送记录","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python网络编程之TCP客户端和服务端实现","date":"2017-04-16T04:47:25.854Z","path":"2017/04/16/python/python网络编程之TCP客户端和服务端实现/","text":"1.socket模块 创建socket： int socket(int domain, int type int protocol) domain：协议域，决定了socket的地址类型，在通信中必须采用对应的地址AF_INET:ipv4协议，ipv4地址与端口号组合(AF:address family)AF_LOCAL：使用一个绝对路径名作为地址（进程间通信）AF_PACKET：来处理以太网包，他能修改以太网包头 type：socket类型：SOCK_STREAM：TCP套接字SOCK_DGRAM：UDP套接字SOCK_RAW：原始套接字 protocol：指定协议，常用协议有IPPROTO_TCP/IPPROTO_UDP、分别对应tcp传输协议、udp传输协议 type和protocol不能随意组合，第三个参数不设置的时候，默认为何第二个参数对应的协议一致 2.socket中常用方法 命令 说明 ss = socket(family,type[,proto]) 创建一个套接字，返回一个socket对象 ss.binds((ip, port)) 绑定ip地址和端口 ss.listen(n) 最大接受连接数 ss.accept() 接受TCP客户端连接，返回一个连接和客户单信息的元组 ss.connect((ip,port)) 主动初始化TCP连接服务器 ss.recv(buffers[,flags]) 返回接受数据（tcp） ss.send(data[,flags]) 发送数据（tcp） recvfrom(buffersize[,flags]) 返回接受数据和发送端的地址信息（udp) sendto(data[,flags],address) 向指定的ip和端口发送数据（udp） ss.close() 关闭套接字 3.Tcp客户端和服务器端模型 4.TCP客户端和服务端实现 tcp服务端编写：监听一个端口，建立请求后接受数据并回复bye,然后关闭连接，继续等待其他客户端的连接 最简单的服务端程序： 1234567891011from socket import * tcp_server = socket(AF_INET, SOCK_STREAM) #创建sockettcp_server.bind((&quot;&quot;, 5001)) #绑定本地IP与端口tcp_server.listen(5) #设置监听的最大连接数client, client_info = tcp_server.accept() #等待连接recv_data = client.recv(1024) #接收数据client.close()tcp_server.close() #关闭连接 最简单的客户端程序：123456from socket import *tcp_client = socket(AF_INET, SOCK_STREAM) #创建socket tcp_client.connect((&apos;localhost&apos;,5001)) #连接服务器tcp_client.send(&quot;hello python&quot;) #发送数据tcp_client.close() #关闭连接 4.1.最简单的实现123456789101112131415161718192021222324252627282930313233#客户端代码#!/usr/bin/python from socket import *import time tcp_server = socket(AF_INET, SOCK_STREAM) #建立一个tcp连接tcp_server.bind((&quot;localhost&quot;,5001)) #绑定一个端口，注意是元组tcp_server.listen(3) print(&quot;wait client connect.........&quot;)client,client_info = tcp_server.accept() #服务端等待接受（阻塞再次）recv_data = client.recv(1024) print(recv_data) client.close()tcp_server.close()-----------------------------------------------------------#服务端代码#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) #指定连接的地址和端口 tcp_client.send(&quot;hello chenyansong&quot;) #发送数据 tcp_client.close() 4.2.客户端输入、服务端连续读取1234567891011121314151617181920212223242526272829303132333435363738394041424344#服务端#!/usr/bin/python from socket import *import time tcp_server = socket(AF_INET, SOCK_STREAM)tcp_server.bind((&quot;localhost&quot;,5001))tcp_server.listen(1) while True: print(&quot;wait client connect.........&quot;) client,client_info = tcp_server.accept() #读取客户端的一个连接 print(client_info) #打印客户端连接信息的元组：(&apos;127.0.0.1&apos;, 35890) while True: recv_data = client.recv(1024) #获取一个连接数据 print(&quot;recv=&quot; + recv_data) if recv_data == &apos;q&apos;: client.close() #关闭一个连接 break tcp_server.close()---------------------------------------------------------#客户端#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) while True: msg = raw_input(&quot;&gt;&quot;) #输入内容 tcp_client.send(msg) if msg == &apos;q&apos;: #退出 break tcp_client.close() 4.3.客户端抛出异常，服务端的处理方式如果客户端程序异常终止，然后就会想服务器端发送“”（空），所以在服务器端要进行判断123456789101112131415161718192021222324252627282930313233#服务端#!/usr/bin/python from socket import *import time tcp_server = socket(AF_INET, SOCK_STREAM)tcp_server.bind((&quot;localhost&quot;,5001))tcp_server.listen(3) while True: print(&quot;wait client connect.........&quot;) client,client_info = tcp_server.accept() print(client_info) while True: recv_data = client.recv(1024) if recv_data == &apos;&apos;: #添加判断，服务端接受的数据，是否是空，如果是，则关闭连接的客户端 client.close() break print(&quot;recv=&quot; + recv_data) if recv_data == &apos;q&apos;: client.close() break tcp_server.close()---------------------------------------------------------------- #客户端，同上 4.4.客户端会发送空数据如果发送的数据是空，那么客户端将不会将数据发送，因为在客户端抛出异常的时候，会请求断开链接，那么此时客户端会发送空数据（此时会发送），所以为了防止误判，客户端正常的发送空数据，将不会发送的1234567891011121314151617181920#客户端#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) while True: msg = raw_input(&quot;&gt;&quot;) if msg == &apos;o&apos;: tcp_client.send(&apos;&apos;) #将不会发送 else: tcp_client.send(msg) if msg == &apos;q&apos;: break tcp_client.close() 4.5.服务端发送数据到客户端123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#服务端#!/usr/bin/python from socket import *import time tcp_server = socket(AF_INET, SOCK_STREAM)tcp_server.bind((&quot;localhost&quot;,5001))tcp_server.listen(3) while True: print(&quot;wait client connect.........&quot;) client,client_info = tcp_server.accept() print(client_info) while True: recv_data = client.recv(1024) if recv_data == &apos;&apos;: client.close() break print(&quot;recv=&quot; + recv_data) client.send(&quot;server send :&quot;+recv_data) #向客户端发送数据 if recv_data == &apos;q&apos;: client.close() break tcp_server.close()-----------------------------------------------------------------------#客户端#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) while True: msg = raw_input(&quot;&gt;&quot;) if msg == &apos;o&apos;: tcp_client.send(&apos;&apos;) else: tcp_client.send(msg) if msg == &apos;q&apos;: break server_data = tcp_client.recv(1024) #接收来自服务端的数据 print(server_data) tcp_client.close() 4.6.服务端断开，客户端的处理情况当服务端断开与客户端的连接的时候（客户端输入Q），服务端会发送一个空数据，然后客户端检测，断开链接12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#服务端#!/usr/bin/python from socket import *import time tcp_server = socket(AF_INET, SOCK_STREAM)tcp_server.bind((&quot;localhost&quot;,5001))tcp_server.listen(3) while True: print(&quot;wait client connect.........&quot;) client,client_info = tcp_server.accept() print(client_info) while True: recv_data = client.recv(1024) if recv_data == &apos;&apos;: client.close() break print(&quot;recv=&quot; + recv_data) client.send(&quot;server send :&quot;+recv_data) if recv_data == &apos;q&apos; or recv_data ==&apos;Q&apos;: #当接收到客户端的数据为“Q&quot;的时候，服务端断开与客户端的连接 client.close() break tcp_server.close()---------------------------------------------------------------------------#客户端1#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) while True: msg = raw_input(&quot;&gt;&quot;) if msg == &apos;o&apos;: tcp_client.send(&apos;&apos;) else: tcp_client.send(msg) if msg == &apos;q&apos;: break server_data = tcp_client.recv(1024) print(server_data) if server_data == &apos;&apos;: #当接收到服务端的空数据的时候，就说明服务端断开了连接，所以此时break print(&quot;socket close.....&quot;) break tcp_client.close()---------------------------------------------------------------------------- #客户端2： 当客户端没有接受数据时，使用异常判断来解决#!/usr/bin/python from socket import *import time tcp_client = socket(AF_INET, SOCK_STREAM)tcp_client.connect((&quot;localhost&quot;,5001)) while True: msg = raw_input(&quot;&gt;&quot;) try: if msg == &apos;o&apos;: tcp_client.send(&apos;&apos;) else: tcp_client.send(msg) if msg == &apos;q&apos;: break except error, emsg: #使用异常检测的方式去处理服务端断开的连接 print(emsg) break tcp_client.close()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之线程池","date":"2017-04-16T04:47:25.853Z","path":"2017/04/16/python/python线程之线程池/","text":"1. 简介 线程池：用来解决线程生命周期开销问题和资源不足问题，通过对多个任务重用线程，线程创建的开销就被分摊到了多个任务上了，线程池中的所有线程主动从工作队列中寻找需要执行的工作。 如何实现线程池 2.原理图 在线程池中创建了5个线程 创建了N个任务列表 让线程池中的每个线程循环去取任务列表中的任务 3. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384[root@backup python]# cat pool.py#!/usr/bin/python import sys,Queue,threading import time #创建具体的线程class _Thread(threading.Thread): def __init__(self,workQueue,resultQueue,timeout=1,**kwargs): threading.Thread.__init__(self,kwargs=kwargs) self.timeout = timeout self.setDaemon(True) self.workQueue = workQueue self.resultQueue = resultQueue def run(self): while True: try: callable,args,kwargs = self.workQueue.get(timeout=self.timeout) #print(self.workQueue.get(timeout=self.timeout)) res = callable(args,kwargs) print(res+&quot; | &quot;+self.getName()) self.resultQueue.put(res+&quot; | &quot;+self.getName()) except Queue.Empty: #任务队列中的值为空，则结束循环 break except: print(sys.exc_info()) raise #线程池class ThreadPool: def __init__(self,num_of_threads=2): self.workQueue = Queue.Queue() #工作任务队列 self.resultQueue = Queue.Queue() #结果队列 self.threads = [] #线程池 self.__createThreadPool(num_of_threads) def __createThreadPool(self,num_of_threads): for i in range(num_of_threads): thread = _Thread(self.workQueue,self.resultQueue) self.threads.append(thread) #创建的线程加入线程池 def wait_for_complete(self): while len(self.threads): thread = self.threads.pop() if thread.isAlive(): thread.join() def start(self): for th in self.threads: th.start() def add_job(self,callable,*args,**kwargs): #向任务队列中添加任务 self.workQueue.put((callable,args,kwargs)) #任务由函数来描述，args,kwargs是向函数传递的参数 #具体的任务def test_job(id,sleep=0.001): time.sleep(0.1) return str(id) #测试函数def test(): print(&quot;start testing.........&quot;) tp = ThreadPool(5) #在线程池中创建了5个线程 for i in range(50): tp.add_job(test_job,i,i) tp.start() tp.wait_for_complete() while tp.resultQueue.qsize(): print(tp.resultQueue.get()) print(&quot;end testing.........&quot;) if __name__ == &quot;__main__&quot;: test()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之线程同步之互斥锁","date":"2017-04-16T04:47:25.851Z","path":"2017/04/16/python/python线程之线程同步之互斥锁/","text":"1.概念介绍 线程互斥：是指某一个资源同时只允许一个访问者对其进行修改，具有唯一性和排他性 线程死锁：死锁就是系统进入到一种阻塞的，无法移动的状态 死锁出现的情况：一个线程占有锁之后又去占有锁 占有锁后，忘记释放 占有锁，然后调用其他函数又去占有锁 线程死锁避免：代码书写小心，保证占有和释放成对出现 线程饿死：一个线程长时间的得不到需要的资源而不能执行的现象 线程饿死避免：线程队列，或者结合其他同步方式 2.互斥锁程序实现123456789101112131415161718192021222324252627282930313233#!/usr/bin/pythonimport threadingimport timetmp = 0g_lock = threading.Lock()def func(): global tmp global g_lock for i in range(20,25): g_lock.acquire() #加锁 tmp -= 1 print(threading.currentThread().getName()+&quot;:tmp=&#123;0&#125;&quot;.format(str(tmp))) g_lock.release() #释放锁 time.sleep(1)if __name__==&quot;__main__&quot;: p = threading.Thread(target=func,args=()) p.setDaemon(True) p.start() for i in range(0,5): g_lock.acquire() #加锁 tmp += 1 print(threading.currentThread().getName()+&quot;:tmp=&#123;0&#125;&quot;.format(str(tmp))) g_lock.release() #释放锁 time.sleep(1) p.join() 打印结果1234567891011Thread-1:tmp=-1MainThread:tmp=0MainThread:tmp=1Thread-1:tmp=0MainThread:tmp=1Thread-1:tmp=0MainThread:tmp=1Thread-1:tmp=0MainThread:tmp=1Thread-1:tmp=0[Finished in 5.3s]","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之多线程练习","date":"2017-04-16T04:47:25.850Z","path":"2017/04/16/python/python线程之多线程练习/","text":"练习 使用多线程统计每门课程的平均分 前提条件：学生成绩保存在TXT文件中：chinese.txt maths.txt 思路： 将文件中你的每门课程的成绩解析出来 统计每门课程分数总和及学生数量，并计算平均值 应用知识点： 文件读写 正则表达式得到学生成绩 将统计过程抽象成类 实现多线程统计以下是多线程和单线程统计过程的区别： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#创建分数文件#!/usr/bin/python import sysimport random &apos;&apos;&apos;#文件模板open file40 databasename_1:90name_2:92.....&apos;&apos;&apos;def construct_data(filename): f = open(filename,&apos;w&apos;) for i in range(40): data = &apos;name_&apos;+str(i)+&apos;:&apos;+str(random.randint(50,100))+&apos;\\n&apos; #格式为：name_i:99 f.write(data) f.close() def create_file(filenames): for filen in filenames: #循环创建多个数据文件 construct_data(filen) if __name__==&quot;__main__&quot;: print(sys.argv[1:]) create_file(sys.argv[1:]) #指定要创建哪些文件-------------------------------------------------------------#测试[root@backup python]# python create_data.py chinese.txt math.txt[&apos;chinese.txt&apos;, &apos;math.txt&apos;][root@backup python]# cat math.txtname_0:65name_1:70name_2:95name_3:54name_4:72name_5:67name_6:74name_7:93name_8:98。。。。。。。。。。。。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#下面的代码是统计每一个文件中的分数平均值[root@backup python]# cat thread_count.py#!/usr/bin/python import threadingimport reimport sys class my_thread(threading.Thread): #继承Thread线程 def __init__(self,filepath): threading.Thread.__init__(self) self.filepath = filepath self.result = 0 self.sumscore = 0 def run(self): f = open(self.filepath) iter_f = iter(f) num = 0 for line in iter_f: score = int(re.split(&quot;:&quot;,line)[1]) #用正则取出分数列 self.sumscore += score num += 1 f.close() self.result = self.sumscore / num #求平均值 print(&quot;file=&#123;0&#125;,sum=&#123;1&#125;,num=&#123;2&#125;,avg=&#123;3&#125;&quot;.format(self.filepath,self.sumscore,num,self.result)) #打印统计结果 def count_score(file_list): thread_list = [] for filel in file_list: #对每个文件都创建一个线程，在线程中对该文件中的分数做统计 p = my_thread(filel) p.start() thread_list.append(p) for p in thread_list: p.join() #等待所有的线程结束 if __name__ == &quot;__main__&quot;: print(sys.argv[1:]) count_score(sys.argv[1:]) #调用统计分数的函数，传过去的是参数list-----------------------------------------------------#测试[root@backup python]# python thread_count.py chinese.txt math.txt[&apos;chinese.txt&apos;, &apos;math.txt&apos;]file=chinese.txt,sum=2892,num=40,avg=72file=math.txt,sum=2996,num=40,avg=74","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之同步之条件变量","date":"2017-04-16T04:47:25.849Z","path":"2017/04/16/python/python线程之同步之条件变量/","text":"1.生产和消费的平衡 线程1负责产生数据，线程2负责分发数据，线程1和线程2同时需要对公共数据进行管理，理想的状态：1产生数据后，2能分发，1和2合理调度，保证公共数据不为0，同时也不会堆积过多 如果使用lock机制，因为调度的关系，造成下面两种情况： 线程1可能一直被调度，导致线程2无法调度，这样造成数据堆积 线程2可能一直被调度，导致线程1无法调度，这样造成数据不足如何来解决？ 2.条件变量允许线程阻塞等待另一个线程发送信号唤醒，条件变量被用来阻塞一个线程，当条件不满足时，线程解开相应的互斥锁并等待条件发生变化，如果其他线程改变了条件变量，并且使用条件变量唤醒一个或多个正被此条件变量阻塞的线程，这些线程将重新锁定互斥锁并重新测试条件是否满足，条件变量被用来进行线程间的同步 1234567891011121314#thread1con = threading.Condition()while true: do_something con.acquire() #获取锁 con.notify() #唤醒等待线程 con.release() #释放锁#thread2con.acquire():while true: con.wait() #等待唤醒,释放锁 do_somethingcon.release() #释放锁 条件变量实质：某一时刻只有一个线程访问公共资源，其他线程做其他任务或者休眠 使用条件变量实现生产者和消费者 3.程序实现1234567891011121314151617181920212223242526272829303132333435363738394041#!/usr/bin/python import threadingimport time tmp = 0 g_cond = threading.Condition() def thread_func(): global g_cond global tmp while True: g_cond.acquire() if tmp &gt;= 10: #当tmp&gt;=10的时候，去消费 tmp -= 1 print(&quot;sub tmp=&#123;0&#125;&quot;.format(str(tmp))) else: g_cond.wait() print(&quot;wake up by another thread&quot;) g_cond.release() if __name__ == &quot;__main__&quot;: p = threading.Thread(target=thread_func,args=()) p.setDaemon(True) p.start() while True: g_cond.acquire() if tmp &gt;= 15: #当tmp&gt;=15的时候，停止生产，同时通知sub thread 去消费 print(&quot;notify sub thread.....&quot;) g_cond.notify() #唤醒wait的线程 else: tmp += 1 print(&quot;main tmp=&#123;0&#125;&quot;.format(str(tmp))) g_cond.release() time.sleep(1) p.join()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之同步之信号量","date":"2017-04-16T04:47:25.848Z","path":"2017/04/16/python/python线程之同步之信号量/","text":"1.简介&emsp;信号量semaphore：管理一个内置的计数器，每当调用acquire()时-1, 调用release()时+1，计数器不能小于0；当计数器为0时，acquire()将阻塞线程至锁定状态，知道其他线程调用release()使计数器大于0，信号量同步机制适用于访问像服务器这样的有限资源（即只能够让有限个(N)资源访问） 创建信号量：sem = threading.Semaphore(n) ;同时允许N个线程对公共资源进行访问 2.举例1(只让一个资源访问)12345678910111213141516171819202122232425262728293031323334353637383940414243#!/usr/bin/python import threadingimport time tmp = 0 g_sem = threading.Semaphore(1) def func(): global tmp global g_sem print(&quot;&#123;0&#125; before get sem &quot;.format(threading.currentThread().getName())) g_sem.acquire() print(&quot;&#123;0&#125; geted sem &quot;.format(threading.currentThread().getName())) time.sleep(3) print(&quot;&#123;0&#125; release sem &quot;.format(threading.currentThread().getName())) g_sem.release() if __name__ == &quot;__main__&quot;: p = threading.Thread(target=func,args=()) p.setDaemon(True) p.start() print(&quot;&#123;0&#125; before get sem &quot;.format(threading.currentThread().getName())) g_sem.acquire() print(&quot;&#123;0&#125; geted sem &quot;.format(threading.currentThread().getName())) time.sleep(5) print(&quot;&#123;0&#125; release sem &quot;.format(threading.currentThread().getName())) g_sem.release() #主线程释放资源 p.join()-----------------------------------------------------------------------#结果[root@backup python]# python se.py MainThread before get semMainThread geted semThread-1 before get sem #主线程拿到了资源，然后子线程会等待主线程释放资源（即同一个时刻只有一个线程访问资源）MainThread release semThread-1 geted semThread-1 release sem[root@backup python]# 3.举例2(只让两个资源访问)123456789101112131415161718192021222324252627282930313233343536373839404142#!/usr/bin/python import threadingimport time tmp = 0 g_sem = threading.Semaphore(2) #初始化了两个信号量 def func(): global tmp global g_sem print(&quot;&#123;0&#125; before get sem &quot;.format(threading.currentThread().getName())) g_sem.acquire() print(&quot;&#123;0&#125; geted sem &quot;.format(threading.currentThread().getName())) time.sleep(3) print(&quot;&#123;0&#125; release sem &quot;.format(threading.currentThread().getName())) g_sem.release() if __name__ == &quot;__main__&quot;: p = threading.Thread(target=func,args=()) p.setDaemon(True) p.start() print(&quot;&#123;0&#125; before get sem &quot;.format(threading.currentThread().getName())) g_sem.acquire() print(&quot;&#123;0&#125; geted sem &quot;.format(threading.currentThread().getName())) time.sleep(5) print(&quot;&#123;0&#125; release sem &quot;.format(threading.currentThread().getName())) g_sem.release() p.join()-----------------------------------------------------#结果[root@backup python]# python se2.pyMainThread before get semMainThread geted sem #主线程可以获取资源Thread-1 before get semThread-1 geted sem #子线程也是可以获取资源的Thread-1 release semMainThread release sem[root@backup python]# 4.release()使信号量加1当调用release函数的时候，信号量就会加1，那么此时就会有N+1个线程可以去访问资源，即：1234threading.Semaphore(2) #等价于：g_sem = threading.Semaphore(1)g_sem.release()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之同步之Queue队列","date":"2017-04-16T04:47:25.846Z","path":"2017/04/16/python/python线程之同步之Queue队列/","text":"1. 简介&emsp;Queue模块中提供了同步的、线程安全的队列类，包括FIFO（先进先出）队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列PriorityQueue，这些队列都实现了原子操作，能够在多线程中直接使用，可以使用队列来实现线程间的同步，内部实现使用了mutex和condition123456#创建队列： number_queue = Queue.Queue()#访问： number_queue.put(N) n = number_queue.get() 2.帮助文档123456789101112In [8]: import Queue #导入模块 In [9]: qu = Queue. #模块下的所有的函数（各种队列类型）Queue.Empty Queue.LifoQueue Queue.Queue Queue.heapq Queue.Full Queue.PriorityQueue Queue.deque In [9]: qu = Queue.Queue() #返回一个先进先出队列 In [10]: qu. #先进先出队列的方法qu.all_tasks_done qu.get qu.maxsize qu.not_full qu.qsize qu.unfinished_tasksqu.empty qu.get_nowait qu.mutex qu.put qu.queue qu.full qu.join qu.not_empty qu.put_nowait qu.task_done 3.举例123456789101112131415161718192021222324252627#!/usr/bin/python import threading, Queueimport time g_que = Queue.Queue() #创建先进先出队列 def thread_func(): global g_que #声明使用全局变量 while True: msg = g_que.get() #子线程取出数据，然后打印 print(msg) if __name__ == &quot;__main__&quot;: p = threading.Thread(target=thread_func,args=()) p.setDaemon(True) p.start() tmp = 0 while True: tmp+=1 g_que.put(tmp) #主线程向队列中放入数据 time.sleep(1) p.join()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之同步之Event事件","date":"2017-04-16T04:47:25.845Z","path":"2017/04/16/python/python线程之同步之Event事件/","text":"1.简介&emsp;一个线程通知事件，其他线程等待事件12345678#创建Event:thread_event = threading.Event()#等待：thread_event.wait()#唤醒：thread_event.set() 2.举例12345678910111213141516171819202122232425262728#!/usr/bin/python import threadingimport time g_event = threading.Event() def thread_func(): global g_event while True: if g_event.isSet(): g_event.clear() print(&quot;Thread_1 start Wait&quot;) g_event.wait() #等待主线程中的set()，将其唤醒 print(&quot;Thread_1 end Wait&quot;) if __name__ == &quot;__main__&quot;: p = threading.Thread(target=thread_func,args=()) p.setDaemon(True) p.start() while True: time.sleep(1) g_event.set() p.join()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之创建线程","date":"2017-04-16T04:47:25.844Z","path":"2017/04/16/python/python线程之创建线程/","text":"1.线程模型1234567891011import threadingdef func(): #线程函数 print &quot;call thread_func&quot;def test(): t1 = threading.Thread(target=func, args=()) #创建线程 t1.start() #启动线程 return t1if __name__==&quot;__main__&quot;: p = test() p.join() #等待线程退出 target 是线程要执行的函数 args是函数的参数（注意是一个元组） 2.threading.Thread帮助文档123456789101112131415161718192021222324252627282930313233In [6]: import threadingIn [10]: help(threading.Thread)Help on class Thread in module threading: class Thread(_Verbose) | Method resolution order: | Thread | _Verbose | __builtin__.object | | Methods defined here: | | __init__(self, group=None, target=None, name=None, args=(), kwargs=None, verbose=None) #初始化构造函数 | | __repr__(self) | | getName(self) | | isAlive(self) | | isDaemon(self) | | is_alive = isAlive(self) | | join(self, timeout=None) | | run(self) | | setDaemon(self, daemonic) #设置为守护线程 | | setName(self, name) | | start(self) #启动线程 3.使用target传参的方式创建线程1234567891011121314151617181920212223242526#!/usr/bin/python import threadingimport time def thread_func(): for i in range(10): print(&quot;call thread_func&quot;) time.sleep(1) def thread_run(): t1 = threading.Thread(target=thread_func,args=()) #使用target的方式来指定线程执行的函数 t1.start() return t1 if __name__ == &quot;__main__&quot;: p1 = thread_run() for i in range(10): print(&quot;in main_func i=&#123;0&#125;&quot;.format(str(i))) time.sleep(1) p1.join()------------------------------------------------将start方法换成run，run方法将不是多线程了，那么会顺序执行 将 t1.start() 换成：t1.run() 4.主线程和子线程共享资源tmp在主线程中创建一个tmp，看在子线程中是否共享tmp123456789101112131415161718192021222324#!/usr/bin/python import threadingimport time tmp = 0 #子线程和主线程共享的资源tmp def thread_func(): for i in range(10): print(&quot;call thread_func,tmp=&#123;0&#125;&quot;.format(tmp)) #使用共享资源 time.sleep(1) def thread_run(): t1 = threading.Thread(target=thread_func,args=()) t1.start() return t1 if __name__ == &quot;__main__&quot;: p1 = thread_run() for i in range(10): tmp = i #改变共享资源 print(&quot;in main_func i=&#123;0&#125;&quot;.format(str(i))) time.sleep(1) p1.join() 执行结果：12345678910111213141516call thread_func,tmp=0in main_func i=0in main_func i=1call thread_func,tmp=1in main_func i=2call thread_func,tmp=2call thread_func,tmp=2in main_func i=3call thread_func,tmp=3in main_func i=4call thread_func,tmp=4call thread_func,tmp=4call thread_func,tmp=4call thread_func,tmp=4call thread_func,tmp=4[Finished in 10.2s] 5.守护线程的作用：setDaemon(True)设置守护线程的作用：当主线程退出的时候，会将子线程也结束，不管此时子线程是否执行完成，这就是守护线程的作用 6.join() 将join注释掉之后，主线程将不会等待子线程结束之后，再结束，而是直接退出，那么在主线程退出之后，子线程由于守护线程的存在也会退出 如果没有守护线程，那么子线程将不会退出（在主线程退出时） 7.通过Thread类的方式创建线程7.1.threading模块的方法 方法 说明 .activeCount() 返回当前活动线程个数 .currentThread() 返回当前线程对象 .enumerate() 返回当前活动线程对象组成的列表 .Thread() 返回线程对象 .Timer(n,func) 返回一个N秒后自动执行的线程函数 .settrace(p_callback) start()前调用该回调函数 .setprofil(func) start()后调用该回调函数 .setDaemon() 设置守护进程 .join() 等待线程结束 7.2.编程实现123456789101112131415161718192021#!/usr/bin/python import threadingimport time class my_thread(threading.Thread): #使用继承线程类Thread方式 def __init__(self,thread_name): threading.Thread.__init__(self,name=thread_name) def run(self): for i in range(10): time.sleep(1) print(i) if __name__==&quot;__main__&quot;: p1 = my_thread(&quot;thread1&quot;) p1.setDaemon(True) p1.start() time.sleep(3) print(&quot;main thread.........end......&quot;) p1.join()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python线程之共享资源","date":"2017-04-16T04:47:25.843Z","path":"2017/04/16/python/python线程之共享资源/","text":"1.程序实现12345678910111213141516171819202122232425#!/usr/bin/python import threadingimport time tmp = 0 def func(): global tmp #声明使用全局变量 for i in range(10,15): tmp = i #改变共享的资源 print(&quot;before &#123;0&#125;, tmp=&#123;1&#125;&quot;.format(threading.currentThread().getName(),str(tmp))) time.sleep(1) print(&quot;after &#123;0&#125;, tmp=&#123;1&#125;&quot;.format(threading.currentThread().getName(),str(tmp))) if __name__ == &quot;__main__&quot;: p = threading.Thread(target=func,args=()) p.start() for i in range(0,3): tmp = i print(&quot;before &#123;0&#125;, tmp=&#123;1&#125;&quot;.format(threading.currentThread().getName(),str(tmp))) time.sleep(1) print(&quot;after &#123;0&#125;, tmp=&#123;1&#125;&quot;.format(threading.currentThread().getName(),str(tmp))) p.join() 打印结果1234567891011121314151617before Thread-1, tmp=10before MainThread, tmp=0after Thread-1, tmp=0before Thread-1, tmp=11after MainThread, tmp=11before MainThread, tmp=1after Thread-1, tmp=1before Thread-1, tmp=12after MainThread, tmp=12before MainThread, tmp=2after Thread-1, tmp=2after MainThread, tmp=2before Thread-1, tmp=13after Thread-1, tmp=13before Thread-1, tmp=14after Thread-1, tmp=14[Finished in 5.3s]","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python的作用域","date":"2017-04-16T04:47:25.841Z","path":"2017/04/16/python/python的作用域/","text":"1.简介python创建、改变、查找变量名都是在所谓的命名空间中进行的，在代码中变量名被赋值的位置决定了这个变量名能被访问的位置。可以在三个地方定义变量 在def函数内定义 在嵌套的def中赋值，对于嵌套的函数来说，他是非本地变量 如果在def之外赋值，他就是整个文件全局的 2.作用域法则 内嵌的模块是全局作用域 每一个模块都是一个全局的作用域，对于外部的全局变量就是一个模块对象的属性，但是在一个模块中能够像简单的变量一样使用 全局作用域的作用范围仅限于单个文件（即模块），在python中听到“全局”，你就应该想到“模块” 每次对函数的调用都创建了一个新的本地作用域 赋值的变量名除非声明为全局变量或非本地变量，否则均为本地变量 如果需要给一个在函数内部，却位于模块文件顶层的变量名赋值，需要在函数内部通过global语句声明，如果需要给一个嵌套在def中的名称赋值，可以通过一条nonlocal语句声明来做到 其他所有的变量名都可以归纳为本地、全局或者内置的 在def内部为本地变量 在一个模块的命名空间内部的顶层为全局变量 有python的预定义的buildin模块提供额为内置变量 3.变量解析：LEGB原则3.1.三条简单的原则： 变量名引用分为三个作用域进行查找：首先是在本地、之后是函数内（如果有的话）、之后是全局、最后是内置 在默认情况下，变量名赋值会创建或改变本地变量 全局申明和非本地声明将赋值的变量名映射到模块文件内部的作用域3.2.LEGB原则 当在函数中使用未认证的变量名时，python搜索4个作用域：本地作用域【L】，之后是上一层结构中def或lambda的本地作用域【E】，之后是全局作用域【G】，最后是内置作用域【B】，如果还是没有找到就会报错，因为变量在使用之前必须先要赋值的 当在函数中给一个变量名赋值的时候，python总是创建或者使用本地作用域的变量名，除非该变量名已经在函数中声明为全局变量 3.3.LEGB四个作用域的关系 3.4.作用域实例1234567891011121314151617181920#Global scope #全局变量名：X 、func (def 语句在这个模块的顶层将一个函数对象赋值给了变量名func)X = 99def func(Y): #local scope #本地变量名：Y、Z Z = X + Y return Zfunc(1) #100#def作用域def f1(): x = 88 #def变量名：x 、f2 def f2(x=x): print(x) #本地变量名：x f2()..f1() #print 88 4.global语句4.1.全局变量名总结 全局变量名时位于模块文件内部的顶层的变量名 全局变量名如果要在函数内部被赋值的话，必须经过global的声明 全局变量名在函数的内部不经过声明是可以被引用的（但是不会对变量做原处修改） 4.2.对全局变量在本地域中做原处修改123456789101112131415161718#1x = 99def func(): x = 100 print(x)func() #100print(x) #99#2x = 99def func(): global x #通过global语句使自己明确地映射到了模块的作用域，如果没有使用global语句的话，x将会由于赋值而被认为是本地变量 x = 100 print(x)func() #100print(x) #100 5.导入模块对象123456789#first.pyX = 99#second.pyimport first.pyprint(first.X)first.X = 88#一个模块文件的全局变量一个被导入就成为了这个模块对象的一个属性 6.工厂函数产生函数的工厂1234567891011121314def maker(N): def action(X): return X ** N return action #将里面的函数返回了#调用上面的函数f = maker(2)f(3) #返回9f(4) #返回16g = maker(3)g(3) #返回27 内嵌的函数记住了整数N ,这里是3f(4) #返回16 内嵌的函数记住了整数N ,这里是2 7.nonlocal语句nonlocal应用于一个嵌套的函数的作用域中的一个名称，而不是所有def之外的全局模块作用域，而且，在声明nonlocal名称的时候，他必须已经存在于嵌套函数的作用域中——他们可能只存在于一个嵌套函数中，并且不能由一个嵌套的def中的第一次赋值创建 7.1.语法格式12def func(): nonlocal name1, name2,... 当一个函数def嵌套在另一个函数中，嵌套的函数引用嵌套的def的作用域中的赋值所定义的任何名称，但是不能修改他们，此时nonlocal就派上用场了 nonlocal使得对该语句中列出的名称的查找从嵌套的def的作用域开始，而不是从本地作用域开始，也就是说：nonlocal也意味着“完全略过我的本地作用域”，所以在一个嵌套的def中必须提前定义过nonlocal的变量名 nonlocal的作用域查找只限定在嵌套的def中，作用域查找不会到全局作用域或者是内置作用域 7.2.应用举例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#被nonlocal使用的变量只能且必须出现在def嵌套中spam = 99def tester(): def nested(): nonlocal spam print(&apos;Current=&apos;, spam) spam + = 1 return nested...SyntaxError: no binding for nonlocal &apos;spam&apos; found #nonlocal限制作用域查找仅为嵌套的def，nonlocal不会在嵌套的模块的全局作用域或者是所有def之外的内置作用域中查找#默认情况下，不允许修改def作用域中的名称def tester(str): state = str def nested(lable, state): print(label, state) state += 1 #因为将state当做是函数nested的本地变量,state在使用之前没有赋值,所以此处是不能使用的 return nested..UnboundLocalError:local variable &apos;state&apos; referenced before assignment#nonlocal语句允许在内存中保持可变状态的多个副本(使用nonlocal进行修改)def tester(str): state = str #赋值操作 def nested(lable, state): nonlocal state #声明为nolocal,那么就去嵌套的def中找 print(label, state) state += 1 return nested...F = tester(0) #F函数记住了内嵌函数中的state，初始化状态为0F(&apos;spam&apos;)&gt;&gt;spam 0F(&apos;ham&apos;)&gt;&gt;ham 1G = tester(42) #G函数记住了内嵌函数中的state，初始化状态为42G(&apos;spam&apos;)&gt;&gt;spam 42G(&apos;ham&apos;)&gt;&gt;ham 43F(&apos;bacon&apos;)&gt;&gt;bacon 3#因为F函数和G函数分别记住了内嵌函数的state的不同的初始化值，所以存在多个副本的情况","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之字符串","date":"2017-04-16T04:47:25.840Z","path":"2017/04/16/python/python数据类型字符串/","text":"&emsp;python和c语言不一样，没有单个字符的数据类型，取而代之的是一个字符的字符串，python的字符串被划分为不可变序列，意味着这些字符串所包含的字符串存在从左到右的位置顺序，并且他们不可以在原处修改。 下表是常见的字符串和表达式 表达式 含义 s = ‘’ 空字符串 s = “sam” 或 s = ‘sam’ 单引号和双引号相同 s = ‘’’ abcde ‘’’ 三引号字符串（单引号和双引号都可） s = “\\temp \\n dfsdfsf” 其中包含转义字符 s1 + s2 合并 s * 3 重复 s[i] 索引 s[i:j] 分片 len(s) 取长度 “my name is {0} ,age is {1}”.format(name,age) 字符串格式化表达式 s.find(“aaaa”) 搜素 s.rstrip() 移除空格 s.split(‘,’) 加入分隔符 s.isdigit() 内容测试（是否是digit） s.lower() 大小写转换 s.endwith(“spam”) 结束测试 ”spam”.join(strlist) 插入分隔符 1.字符串常量 单引号： ‘spa”m’ 双引号：“spa’m” 三引号：’’’ …….spam…’’’ / “””……..spam…….””” 单引号和双引号是可以互换的，可以在一个单引号字符串中嵌入一个双引号，反之，亦然。1&gt;&gt;&apos;knsdfsfsi&quot;ssss&apos; , &quot;dfsfldsfsldf&apos;sdfsdf&quot; Python会自动的合并相邻的字符串常量，尽管可以简单的在他们之间增加+操作符来说明这一合并操作123title = &quot;meaning&quot; &apos;of&apos; &quot;life&quot;&gt;&gt;titlemeaning of life 2.转义字符&emsp;转义序列让我们可以在字符串中嵌入不容易通过键盘输入的字节，字符串常量中字符“\\”，以及在它后面的一个或者多个字符，在最终字符串对象中会被一个单个字符所替代，这个字符通过转义序列定义了一个二进制值例如：有一个五个字符的字符串，其中嵌入了一个换行符和一个制表符12345678&gt;&gt;s = &apos;a\\nb\\tc&apos;&gt;&gt;print(s)ab c&gt;&gt;len(s)5 注意原始的反斜杠字符并不真正的和字符串一起存在在内存中 下面是常见的反斜杠字符表 表达式 含义 \\newline 忽视连续 \\ 反斜杠 \\’ 单引号 \\” 双引号 \\a 响铃 \\n 换行 \\t 水平制表符 \\v 垂直制表符 \\xhh 十六进制值 \\ooo 八进制值 \\0 Null（不是字符串结尾） 123456&gt;&gt;s = &apos;a\\ob\\oc&apos;&gt;&gt;s&apos;a\\x00b\\x00c&apos;&gt;&gt;len(s)5 如果python没有作为一个合法的转义编码识别出在“\\”后的字符，他就直接在最终的字符串中保留反斜杠123456&gt;&gt;x = &quot;C:\\py\\code&quot;&gt;&gt;x&apos;C:\\\\py\\\\code&apos;&gt;&gt;len(x)10 3.raw 转义抑制&emsp;有时我们会像这样打开一个文件1myfile = open(&apos;C:\\new\\test.data&apos;,&apos;w&apos;) #因为有转义的存在，所以最终显示的是： C:(换行）ew(制表符）est.data 的文件 &emsp;&emsp;为了解决上面的问题，使用字母r（大写或小写）放在字符串的第一个引号的前面，这样会自动的关闭转义机制1myfile = open(r&apos;C:\\new\\test.data&apos;,&apos;w&apos;) # r会抑制转义 另外一种方法是，加入反斜杠的转义序列，如下：1myfile = open(&apos;C:\\\\new\\\\test.data&apos;,&apos;w&apos;) 4.三引号作用： 编写多行字符串 12&gt;&gt; a = &apos;&apos;&apos; my name is chenyansong , age is 24 &apos;&apos;&apos; 作为多行注释 12&quot;&quot;&quot; this is print result&quot;&quot;&quot; #表示注释print(s) 5.字符串的基本操作5.1.+12&gt;&gt;&apos;abc&apos; + &apos;def&apos; #字符串合并，两个字符串对象相加创建了一个新的字符串对象&apos;abcdef&apos; 5.2.*12&gt;&gt;print(&apos;------------ ...more....-----------&apos;) #90个横线&gt;&gt;print( &apos;-&apos; * 90) #90个横线，这就是重复的作用，可以避免我们冗余的操作 注意：在应用于数字时，执行加法和乘法的相同的操作符+和* ，python不允许你在+表达式中混合数字和字符串：‘aaa’+9会跑出一个异常 5.3.in&emsp;&emsp;in表达式操作符用于对字符和子字符串进行成员关系的测试12345678910&gt;&gt;myjob = &apos;hacker&apos;&gt;&gt; &apos;k&apos; in myjob True&gt;&gt;&apos;z&apos; in myjobFalse&gt;&gt;&quot;spam&quot; in &quot;abcdAspam&quot; #用来测试子串在父串中是否存在True 6.索引和分片6.1.索引 第一个元素的偏移为0 负偏移索引意味着从最后或右边反向进行计数 s[0] 获取了第一个元素 s[-2] 获取了倒数第二个元素，（就像s[len(s)-2]一样） 123&gt;&gt;s = &apos;spam&apos;&gt;&gt;s[0], s[-2](&apos;s&apos;, &apos;a&apos;) 6.2.分片 上边界并不包含在内 分片的默认边界为0和序列的长度，如果没有给出的话 s[1:3] 获取了从偏移为1的元素，知道但不包含偏移为3的元素 s[1:] 获取了从偏移为1直到结尾（偏移为序列长度）之间的元素 s[:3] 获取了偏移为0直到但不包含偏移为3之间的元素 s[:-1] 获取了从偏移为0直到但是不包含最后一个元素之间的元素 s[:] 获取了从偏移0到末尾之间的元素，这有效的实现了顶层s拷贝 123&gt;&gt;s = &apos;spam&apos;&gt;&gt;s[1:3],s[1:],s[:-1](&apos;pa&apos;, &apos;pam&apos;, &apos;spa&apos;) 6.3.分片扩展：第三个限制值&emsp;在python2.3中，分片表达式增加了一个可选的第三个索引，用作步进，如下x[I:J:K] 表示：索引x对象中的元素，从偏移为I直到偏移为J-1，每隔K元素索引一次，默认K=1 1234567&gt;&gt;s = &apos;abcdefg&apos;&gt;&gt;s[1:5:2]&apos;bdf&apos;&gt;&gt;s = &apos;hello&apos;&gt;&gt;s[::-1]&apos;olleh&apos; #步进-1表示分片将会从右至左进行而不是通常的从左至右，实际的意义就是将取得的字符串反转 7.字符串转换工具1234567891011121314151617181920&gt;&gt;int(&apos;42&apos;),str(42)(42,&apos;42&apos;)&gt;&gt;s = &apos;42&apos;&gt;&gt;I = 1&gt;&gt;s +ITypeError:cannot concatenate &apos;str&apos; and &apos;int&apos; object&gt;&gt;int(s) +I #转换成int43&gt;&gt;s + str(I) #转换成str&apos;431&apos;&gt;&gt;ord(&apos;s&apos;) #ord函数将单个字符转换成ASCII码115&gt;&gt;chr(115) #chr将ASCII码转成对应的字符&apos;s&apos; 8.字符串方法8.1.修改字符串12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#方法1&gt;&gt; s = &apos;spamy&apos;&gt;&gt; s = s[:3] + &apos;xx&apos; + s[5:] #切片+拼接&gt;&gt;s&apos;spaxxy&apos;#方法2&gt;&gt; s = &apos;spammymmy&apos;&gt;&gt; s = s.replace(&apos;mm&apos;,&apos;xx&apos;) #replace直接替换，会替换匹配到的所有&gt;&gt; s&apos;spaxxyxxy&apos;&gt;&gt; s = &apos;spammymmy&apos;&gt;&gt; s = s.replace(&apos;mm&apos;,&apos;xx&apos;,1) #replace直接替换，只是替换第一个&gt;&gt; s&apos;spaxxymmy&apos;#方法3&gt;&gt; s = &apos;xxxxSPAMxxxxSPAMxxxx&apos;&gt;&gt; where = s.find(&apos;SPAM&apos;)&gt;&gt;where5&gt;&gt;s = s[:where] + &apos;EGGS&apos; + s[(where + 4):]&gt;&gt;s&apos;xxxxEGGSxxxxSPAMxxxx&apos;#方法4&gt;&gt; s = &apos;spammy&apos;&gt;&gt; L =list(s)&gt;&gt; L[&apos;s&apos;, &apos;p&apos;, &apos;a&apos;, &apos;m&apos;, &apos;m&apos;, &apos;y&apos;]&gt;L[3] = &apos;x&apos;&gt;L[4] = &apos;x&apos;&gt;&gt;L[&apos;s&apos;, &apos;p&apos;, &apos;a&apos;, &apos;x&apos;, &apos;x&apos;, &apos;y&apos;]&gt;&gt;s = &apos;&apos;.join(L) #使用join函数去拼接&gt;&gt;s&apos;spaxxy&apos;#join的用法&gt;&gt; &apos;SPAM&apos;.join([&apos;eggs&apos;, &apos;sausage&apos;, &apos;ham&apos;, &apos;toast&apos;]) #使用SPAM去连接，组成新的字符串 &apos;eggsSPAMsausageSPAMhamSPAMtoast&apos; 8.2.split&emsp;将一个字符串分割为一个子字符串的列表，以分隔符字符串为标准，默认的分隔符为空格12345678910&gt;&gt; line = &apos;aaa bbb ccc&apos;&gt;&gt; cols = line.split()&gt;&gt; cols[&apos;aaa&apos;, &apos;bbb&apos;, &apos;ccc&apos;]&gt;&gt;line = &quot;i&apos;mSPAMaSPAMboy&quot; &gt;&gt; line.split(SPAM) #使用“SPAM”去分割[&quot;i&apos;m&quot;, &apos;a&apos;, &apos;boy&apos;] 8.3.rstrip 清除每行末尾空白123&gt;&gt;line = &quot;the knigts who say Ni!\\n&quot;&gt;&gt;line.rstrip()&quot;the knights who say Ni!&quot; #去掉每行末尾的换行符 8.4.大小写转换123&gt;&gt;s = &apos;my name is cys&apos;&gt;&gt;s.upper()&apos;MY NAME IS CYS&apos; 8.5.内容检测123&gt;&gt;s = &apos;my name is cys&apos;&gt;&gt;&quot;cys&quot; in sTrue 8.6.检测末尾或起始字符串123&gt;&gt;s = &apos;my name is cys&apos;&gt;&gt;s.endswith(cys)True 9.字符串格式化表达式%12&gt;&gt;&apos;This is %d %s bird&apos; % (1,&apos;dead&apos;)This is 1 dead bird 格式化字符串： 在%操作符的左侧放置一个需要进行格式化的字符串，这个字符串带有一个或多个嵌入的转换目标，都以%开头（例如：%d) 在%操作符右侧放置一个（或多个，嵌入到元组中）对象，这些对象将会插入到左侧想让python进行格式化字符串的一个或多个转换目标的位置上去 注意：当不止一个值待插入的时候，应该在右侧用括号把它们括起来，即将他们放入到元组中去，%格式化表达式操作符在其右侧期待一个或多个项（此时就是元组） 字符串格式化代码 字符 含义 s 字符串（或任何对象） r s,但使用repr ,而不是str c 字符 d 十进制 i 整数 u 无符号整数 o 八进制整数 X 打印大写 e 浮点指数 f 浮点十进制 9.1.基于字典的格式化123456789101112131415&gt;&gt;&quot;%(n)d %(x)s&quot; % &#123;&quot;n&quot;:1, &quot;x&quot;:&quot;spam&quot;&#125; #格式化字符串里（n)和（x）引用了右边字典中的键，并提取他们相应的值&apos;1 spam&apos;&gt;&gt;reply = &apos;&apos;&apos;my name is %(name)smy birthday is %(birthday)smy work is %(work)s&apos;&apos;&apos;&gt;&gt;values = &#123;&quot;name&quot;:&quot;cys&quot;,&quot;birthday&quot;:&quot;1992.12.21&quot;,&quot;work&quot;:&quot;IT worker&quot;&#125;&gt;&gt;print(reply % values)my name is cysmy birthday is 1992.12.21my work is IT worker 10.字符串格式化调用方法：format在主体字符串中，花括号通过位置（例如：{1}）或关键字（例如：{food}）指出替换目标即将要插入的参数。123456789101112131415161718192021222324&gt;&gt;&quot;&#123;0&#125;, &#123;1&#125;, and &#123;2&#125;&quot;.format(&apos;spam&apos;, &apos;ham&apos;, &apos;eggs&apos;) #通过位置&apos;spam , ham and eggs&apos;&gt;&gt;&quot;my name is &#123;name&#125;, old is &#123;age&#125;&quot;.format( name=&quot;cys&quot;, age=&quot;24&quot; ) #通过关键字&apos;my name is cys, old is 24&apos;&gt;&gt;&quot;my name is &#123;name&#125;, old is &#123;0&#125;, and my favious is &#123;food&#125;&quot;.format( 24, name=&quot;cys&quot;, food=[1,3] ) #位置和关键字混合&apos;my name is cys, old is 24, and my favious is [1,3]&apos;&gt;&gt;import sys&gt;&gt;&quot;My &#123;1[spam]&#125; runs &#123;0.platform&#125;&quot;.format(sys, &#123;&apos;spam&quot;:&quot;laptop&quot;&#125;) #1表示第一个参数，而1[spam]表示取第一个参数的属性&apos;My laptop runs win32&apos;&gt;&gt;&quot;My &#123;config[spam]&#125; runs &#123;sys.platform&#125;&quot;.format(sys = sys , config = &#123;&quot;spam&quot;:&quot;laptop&quot;&#125;) &apos;My laptop runs win32&apos;&gt;&gt;someList = list(&apos;SPAM&apos;)&gt;&gt;parts = someList[0],someList[-1], someList[1:3]&gt;&gt;&quot;first=&#123;0&#125;,last=&#123;1&#125;, middle=&#123;2&#125;&quot;.format(*parts)&quot;first=S, last=M, middle=[&apos;P&apos;,&apos;A&apos;]&quot; 具体格式化{fieldname!conversionflag:formatspec} fieldname是指参数的一个数字或关键字，后面跟着可选的“.name” 或者”[index]“成分引用 conversionflag 可以是r、s、或者是a分别表示该值上对repr,str、或ascii内置函数的一次调用 formatspec 指定了如何表示该值，包括字段宽度、对齐方式、不零、小数点精度等细节冒号后的formatspec 组成形式[ [fill]align] [sign] [#] [0] [widht] [.precision] [typecode]align可能是&lt; &gt; = ^ 表示左对齐、右对齐、一个标记字符后的补充或居中对齐 1234567891011121314&gt;&gt; &apos;&#123;0:&lt;10&#125; = &#123;1:&gt;10&#125;&apos;.format(&apos;spam&apos;, 123,4567)&apos;spam = 123.4567&apos;&gt;&gt;&apos;&#123;0:f&#125;, &#123;1:.2f&#125;, &#123;2:06.2f&#125;&apos;.format(3.14159, 3.14159, 3.14159)&apos;3.141590, 3.14, 003.14&apos;&gt;&gt;&apos;&#123;0:.&#123;1&#125;f&#125;&apos;.format(1/3.0, 4)&apos;0.3333&apos;&gt;&gt;format(1.2345, &apos;.2f&apos;)&apos;1.23&apos;","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之文件","date":"2017-04-16T04:47:25.839Z","path":"2017/04/16/python/python数据类型之文件/","text":"1.常见的文件运算 表达式 含义 output = open(r’C:\\spam’, ‘w’) 创建输出文件（w表示写入） input = open(‘data’, ‘r’) 创建输入文件（r表示读） input = open(‘data’) 同上（默认是r) aString = input.read() 把整个文件读进单一字符串 aString = input.read(N) 读取之后的N个字节到一个字符串 aString = input.readline() 读取下一行（包含文末标识符）到一个字符串 aList = input.readlines() 读取整个文件到字符串列表 output.write(aString) 写入字节字符串到文件 output.writelines(aList) 将列表中所有字符串写入到文件 output.close() 手动关闭 output.flush() 将输出缓冲刷到磁盘，但是不关闭 anyFile.seek(N) 修改文件位置到偏移量N处以便进行下一个操作 for line in open(‘data’): use line 文件迭代器一行一行的读取 open(‘f.txt’, encoding=’latin-1’) open(‘f.bin’, ‘rb’) 2.打开文件的模式open(file_name, 处理模式） r 表示输入打开文件（默认） w 表示输出打开文件 a 表示在文件尾部追加内容而打开文件 ｂ表示进行二进制数据处理 3.文件写入123456&gt;&gt; myfile = open(&apos;myfile.txt&apos;, &apos;w&apos;)&gt;&gt; myfile.write(&apos;hello text fle\\n&apos;) #写入方法不会为我们添加终止符，我们必须手动添加行终止符（否则，下次写入时会简单地延长文件的当前行）16 #表示写入了16个字符&gt;&gt; myfile.write(&apos;goodbye text fle\\n&apos;)18&gt;&gt; myfile.close() 4.文件读取123456789101112&gt;&gt; myfile = open(&apos;myfile.txt&apos;) #默认的模式r，读取&gt;&gt;myfile.readline()&apos;hello text fle\\n&apos;&gt;&gt;myfile.readline()&apos;goodbye text fle\\n&apos;&gt;&gt;myfile.readline()&apos;&apos; #已经到了文件的末尾#另一种读取方式&gt;&gt;for line in open(myfile): #open临时创建的文件对象将自动在每次循环迭代的时候读入并返回一行..... print(line,end=&apos;&apos;) 5.在文件中存储并解析python对象123456789101112131415161718192021222324252627282930313233343536373839#步骤1、创建对象&gt;&gt;x, y, z = 43, 44, 45&gt;&gt; s =&apos;spam&apos;&gt;&gt;D = &#123;&apos;a&apos;:1,&apos;b&apos;:2&#125;&gt;&gt;L = [1,2,3]#步骤2：将创建的对象写入文件&gt;&gt;F = open(&apos;datafile.txt&apos;, &apos;w&apos;)&gt;&gt;F.write(s+ &apos;\\n&apos;)&gt;&gt;F.write(&apos;%s,%s,%s\\n&apos; % (x,y,z))&gt;&gt;F.write(str(L) + &apos;$&apos; + str(D) + &apos;\\n&apos;)&gt;&gt;F.close()#步骤3：提取文件字符，并转成对象（还原）&gt;&gt;F = open(&apos;datafile.txt&apos;)&gt;&gt;line = F.readline()&gt;&gt;line&apos;spam\\n&apos;&gt;&gt;line.rstrip()&apos;spam&apos; #还原字符串&gt;&gt;line = F.readline()&gt;&gt;line&apos;43,44,45\\n&apos;&gt;&gt;parts = line.split(&apos;,&apos;)&gt;&gt;parts[&apos;43&apos;, &apos;44&apos;, &apos;45\\n&apos;]&gt;&gt;numbers = [int(p) for p in parts][43, 44, 45] #还原int列表&gt;&gt;line = F.readline()&gt;&gt;line“[1,2,3]$&#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;\\n”&gt;&gt;parts = line.split(&apos;$&apos;)&gt;&gt;parts[&apos;[1,2,3]&apos; , &quot;&#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;&quot;]&gt;&gt;objects = [eval(p) for p in parts] #eval能够将字符串当做可执行程序代码[[1,2,3], &#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;] 6.使用pickle存取python的原生对象1234567891011&gt;&gt;D = &#123;&apos;a&apos;:1, &apos;b&apos;:2&#125;&gt;&gt;F = open(&apos;datafile.pk1&apos;, &apos;wb&apos;)&gt;&gt;import pickle&gt;&gt;pickle.dump(D,F) #将D写入到文件对象F&gt;&gt;F.close()&gt;&gt;F = open(&apos;datafile.pk1&apos;, &apos;rb&apos;)&gt;&gt;E = pickle.load(F)&gt;&gt;E&#123;&apos;a&apos;:1, &apos;b&apos;:2&#125; 7.打包二进制数据的存储与解析struct模块能够构造并解析打包的二进制数据，从某种意义上来说，他是另一个数据转换工具，他能够把文件中的字符串解读为二进制数1234567891011121314151617&gt;&gt;F = open(&apos;data.bin&apos;, &apos;wb&apos;)&gt;&gt;import struct&gt;&gt;data = struct.pack(&apos;&gt;i4sh&apos;, 7, &apos;spam&apos;, 8) #格式化的字符串是：一个4字节的整数，一个包含4个字符的字符串，一个2为整数&gt;&gt;datab&apos;\\xoo\\xoo\\xoo\\xo7spam\\xoo\\xo8&apos;&gt;&gt;F.write(data)&gt;&gt;F.close()#读取&gt;&gt;F = open(&apos;data.bin&apos;, &apos;rb&apos;)&gt;&gt;data = F.read()&gt;&gt;datab&apos;\\xoo\\xoo\\xoo\\xo7spam\\xoo\\xo8&apos;&gt;&gt;values = struct.uppack(&apos;&gt;i4sh&apos;, data) #还原数据&gt;&gt;values(7,&apos;spam&apos;, 8 ) 8.其他文件工具 标准流在sys模块中预先打开的文件对象，例如：sys.stdout os 模块中的描述文件 socket 、pipes 和FIFO文件 通过键来存取的文件 shell命令流","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之布尔型","date":"2017-04-16T04:47:25.838Z","path":"2017/04/16/python/python数据类型之布尔型/","text":"在python中将整数0代表真，整数1代表假，除此之外，python将任意的空数据结构视为假，将任何的非空数据结构视为真 数字如果是非零，则为真 其他对象如果是非空，则为真 对象真假的例子 表达式 含义 “spam” True “” False [] False {} False 1 true 0.0 False None False None对象一般起到一个空的占位符的作用，与C语言中的NULL指针类似","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之字典","date":"2017-04-16T04:47:25.837Z","path":"2017/04/16/python/python数据类型之字典/","text":"字典当成是无序的集合，是通过键来存取的，而不是通过偏移存取，python字典的主要属性如下： 通过键而不是偏移量来读取 任意对象的无序集合 可变长、异构、任意嵌套 常见的字典常量和操作 表达式 含义 D = {} 空字典 D = {“name”:”zhangsan”, “age”:33} D = { “name”:”zhangsan”, “food”:{“ham”:1,”egg”:3}} 嵌套 D = dict(name=”Bob”, age=43) D[“eggs”] 通过键索引值 D[“food”][“egg”] ‘egg’ in D 判断键是否存在 D.keys() 获取所有的键视图 D.values() 获取所有的值视图 D.items() 获取所有的键+值视图 D.copy() D.get(key, default) 根据键获取值，没有返回默认 D.update(D2) 合并 D.pop(key) 删除等 len(D) 长度 D[key] = 33 赋值 del D[key] 根据键删除条目 list(D.keys()) 字典视图转成列表 D = {x : x*2 for x in range(10)} 初始化字典 1.赋值1234&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt; D[&apos;name&apos;] = [&quot;firstName&quot;,&quot;lastName&quot;]&gt;&gt;D&#123;&quot;name“:[&quot;firstName&quot;,&quot;lastName&quot;], &quot;age&quot;:22&#125; 2.取值(根据键，get， for)1234567891011121314151617181920&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt;D[&quot;name&quot;]&apos;zhangsan&apos;&gt;&gt;D.get(&quot;name&quot;) #如果不存在返回None&apos;zhangsan&apos;&gt;&gt;D.get(&quot;birthday&quot;, &quot;no have name&quot;) #如果不存在就返回字符串”no have name&quot;”no have name&quot;&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt; for key in D #直接遍历的是字典的key...... print(key, &apos;\\t&apos;, D[key])&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt; for key in D.keys() #先获取key的集合，然后遍历集合...... print(key, &apos;\\t&apos;, D[key]) 3.获取不存在的值（避免missing-key错误）12345678910111213141516#方式1if key in D : print (D[key])else: print（0）#方式2try: print (D[key])except KeyError: print（0）#方式3D.get(key,defalut) #不存在就给一个默认的值 4.len 长度123&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt;len(D)2 5.删除pop / del1234567891011121314&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;del D[&quot;name&quot;]&gt;&gt;D&#123;&quot;age&quot;:22&#125;&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt;D.pop(&quot;name&quot;) #从字典中删除一个键并返回他的值&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt;D.pop() #删除并返回最后一个的值&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt;D.pop(1) #删除指定的一个，并返回值 6.判断键存在in123&gt;&gt; D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;&gt;&gt; &quot;name&quot; in D #检查某个键是否存在字典中True 7.创建（初始化）字典的方法1234567891011121314151617181920212223242526272829#方式1D = &#123;&quot;name“:&quot;zhangsan&quot;, &quot;age&quot;:22&#125;#方式2D = &#123;&#125;D[&apos;name&apos;] = &quot;zhangsan&quot;D[&apos;age&apos;] = 33#方式3D = dict(name=&quot;zhangsan&quot;, age=33)#方式4D = dict([(&apos;name&apos;,&apos;zhangsan&apos;), (&apos;age&apos;,3)])#方式5D = dict.fromkeys([&apos;a&apos;,&apos;b&apos;],0) #传入一个键列表，以及所有键的初始值（默认值为空）&#123;&apos;a&apos;:0, &apos;b&apos;:0&#125; D = &#123;k:0 for k in [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&#125;&gt;&gt;D&#123;&apos;a&apos;:0, &apos;b&apos;:0, &apos;c&apos;:0&#125;D = &#123;k:None for k in [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&#125;&gt;&gt;D&#123;&apos;a&apos;:None, &apos;b&apos;:None, &apos;c&apos;:None&#125;D =dict.fromkeys(&apos;abc&apos;)&gt;&gt;D&#123;&apos;a&apos;:None, &apos;b&apos;:None, &apos;c&apos;:None&#125; 8.视图（keys / vlaues / items）123456789101112131415161718192021222324252627282930313233&gt;&gt; D = dict(a=1, b=2, c=3)&gt;&gt; D&#123;&apos;a&apos;:1, &apos;b&apos;:2, &apos;c&apos;:3&#125;&gt;&gt;K = D.keys()&gt;&gt;K&gt;&gt;list(K)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&gt;&gt;list(D.values())[1,2,3]&gt;&gt;list(D.items())[(&apos;a&apos;,1), (&apos;b&apos;,2), (&apos;c&apos;, 3)]#python3中字典视图并非创建后不能改变——他们可以动态的反应在视图创建之后对字典做出的修改&gt;&gt; D = dict(a=1, b=2, c=3)&gt;&gt; K = D.keys()&gt;&gt;V = D.values()&gt;&gt;list(K)[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;]&gt;&gt;list(V)[1,2,3]&gt;&gt;del D[&apos;a&apos;]&gt;&gt;list(K)[ &apos;b&apos;, &apos;c&apos;]&gt;&gt;list(V)[2,3] 9.排序字典键因为keys不会返回一个list，所以我们要排序，必须要通过手动转换为一个列表的方式，123456789101112&gt;&gt; D = dict(a=1, b=2, c=3)&gt;&gt;ks = list(D.keys())ks.sort() #列表排序for k in ks: print(k,D[k])&gt;&gt; D = dict(a=1, b=2, c=3)&gt;&gt;ks = D.keys()for k in sorted(ks) #调用排序函数 print(k,D[k])","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之列表","date":"2017-04-16T04:47:25.836Z","path":"2017/04/16/python/python数据类型之列表/","text":"&emsp;列表可以包含任何种类的对象：数字、字符串、其他列表，列表时可变对象，支持在原地修改，常用列表常量和操作 表达式 含义 L = [] 一个空的列表 L = [0, 1, 2, 3, 4] L = [1,3,[4,5]] 列表中嵌套列表 L = list(‘spam’) L[i] 索引某一个元素 L[i][j] 索引的索引 L[i:j] 分片 len(L) 求长度 L1 + L2 合并 L * 3 重复 for x in L : print(x) 迭代 3 in L 检测 L.append(4) 增长 L.extend([4,5,7]) L.insert(1) 插入 L.index(“zhansan”) 求索引 L.sort() 排序 L.reverse() 反转 del L[k] 删除 del L[i:j] 删除一片 L.pop() 移除最后一个 L.remove(2) 移除指定的一个 L[i:j] = [] 清空指定的分片 L[i:j] = [4,5,6] 分片赋值 1.列表的CRUD和其他的操作1.1.求列表长度12&gt;&gt;len([1,3,5])3 1.2.合并列表123456789&gt;&gt;[1,2,3] + [4,5,7][1,2,3,4,5,7]#注意+两边必须是同种类型的序列，否则江湖报错&gt;&gt; str([1,3,4]) + &quot;33&quot;&apos;1,3,4]33&apos;&gt;&gt;[1,2] + list(&quot;33&quot;)[1,2,3,3] 1.3.重复123456789101112&gt;&gt;[&apos;Ni&apos;] * 4[&apos;Ni&apos;, &apos;Ni&apos;, &apos;Ni&apos;, &apos;Ni&apos;]&gt;&gt;res = [c * 4 for c in &apos;spam&apos;]&gt;&gt;res [&apos;ssss&apos;, &apos;pppp&apos;, &apos;aaaa&apos;, &apos;mmmm&apos;]#等价于&gt;&gt;res = []&gt;&gt;for c in &apos;spam&apos;:... res.append(c * 4)&gt;&gt;res[&apos;ssss&apos;, &apos;pppp&apos;, &apos;aaaa&apos;, &apos;mmmm&apos;] 1.4.判断in12345&gt;&gt; 3 in [1,2,3]True&gt;&gt;for x in [1,2,3]:print(x) 1.5.索引、分片、矩阵1234567891011121314&gt;&gt; L = [&apos;spam&apos;, &apos;Spam&apos;, &apos;SPAM&apos;] &gt;&gt; L[2] #索引&apos;SPAM&apos;&gt;&gt;L[1:] #分片[ &apos;Spam&apos;, &apos;SPAM&apos;]&gt;&gt;matrix = [[1,2,3], [4,5,6], [7,8,9]]&gt;&gt;matrix[1] #矩阵[4,5,.6]&gt;&gt;matrix[1][1]5 1.6.赋值当使用列表的时候，可以将它赋值给一个特定项（偏移）或整个片段（分片）来改变他的内容，索引和分片的赋值都是原地修改，他们对列表进行直接修改，而不是·生成一个新的里诶包作为结果123456789&gt;&gt; L = [&apos;spam&apos;, &apos;Spam&apos;, &apos;SPAM&apos;]&gt;&gt; L[1] = &apos;eggs&apos; #特定项&gt;&gt; L[&apos;spam&apos;, &apos;eggs&apos;, &apos;SPAM&apos;]&gt;&gt;L[0:2] = [&apos;eat&apos;, &apos;more&apos;] #整个片段&gt;&gt;L[&apos;eat&apos;, &apos;more&apos;, &apos;SPAM&apos;] 分片赋值最好分成两步来理解 删除，删除等号左边指定的分片 插入，将包含在等号右边对象中的片段插入旧分片被删除的位置实际情况并非如此，但是这有助于我们理解为什么插入元素的数目不需要与删除的数目相匹配，例如：已知一个列表L的值为【1,3,4】，赋值操作L[1:2] = [4,5] 会把L修改成列表【1,4，5,4】,python会先删除3（单项分片），然后在删除3的位置插入4,5，这也解释了为什么L[1:2] = [] 实际上是删除操作——python删除分片，之后什么也不插入 1.7.添加append / extend / insert1234567891011121314&gt;&gt;L.append(&apos;please&apos;) #在末尾添加&gt;&gt;L[&apos;a&apos;,&apos;b&apos;,&apos;please&apos;] &gt;&gt; L = [1,3]&gt;&gt; L.extend([4,5,6]) #在末端插入多个元素&gt;&gt; L[1,3,4,5,6]&gt;&gt;L.insert(1,&quot;toast&quot;) #在指定位置插入元素&gt;&gt; L[1,&apos;toast&apos;,3,4,5,6] 1.8.排序sort12345678910111213141516171819&gt;&gt;L = [&quot;aA&quot;, &quot;dC&quot;,&quot;ab&quot;]&gt;&gt;L.sort() #按照字符的ASCII码进行排序&gt;&gt;L[&quot;aA&quot;,&quot;ab&quot;, &quot;dC&quot;] &gt;&gt;L.sort(key=str.lower) #转化成为小写之后进行排序&gt;&gt;L[“aA&quot;, &quot;ab&quot;, &quot;dC&quot;]&gt;&gt;L.sort(key=str.lower，reverse = True) #转化成为小写之后，降序排列&gt;&gt;L[&quot;dc&quot;, &quot;ab&quot;, &quot;aA&quot;]#通过sorted 内置函数可以实现&gt;&gt; L = [&apos;abc&apos;, &apos;ABD&apos;,&apos;bBe&apos;]&gt;&gt; sorted(L, key=str.lower, reverse = True)[&apos;aBe,&apos;ABD, &apos;abc&apos;] 要当心append和sort原处修改相关的列表对象，而结果并没有返回列表（从技术上来讲，两者返回的是None），如果编辑类似的L=L.append(x）的语句，将不会得到L修改后的值（实际上，会失去整个列表的引用） 1.9.删除pop / remove / del1234567891011121314151617181920212223242526272829303132&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt; L.pop() #删除最后一个元素&gt;&gt; L[&quot;zhangsan&quot;, &quot;lisi&quot;]&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt;L.pop(1) #删除指定的元素&gt;&gt;L [&quot;zhangsan&quot;, &quot;wangwu&quot;]&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt;L.remove(&quot;zhangsna&quot;) #移除 指定值 的元素&gt;&gt;L[&quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt; del L[0] #删除指定的元素&gt;&gt; L[&quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt;del L[1:] #删除指定的分片&gt;&gt;L [&quot;zhangsan&quot;]&gt;&gt; L = [&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;]&gt;&gt; L[1:] = [] #清空指定的分片&gt;&gt;L [&quot;zhangsan&quot;] 1.10.反转1234&gt;&gt; L = [1,2,3]&gt;&gt;L.reverse()&gt;&gt;L[3,2.1]","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据类型之元组","date":"2017-04-16T04:47:25.834Z","path":"2017/04/16/python/python数据类型之元组/","text":"元组与列表类似，只不过元组不能在原处修改，并且写成圆括号中的一系列项，其具有如下属性： 任意对象的有序集合元组是一个位置有序的对象的集合，可以嵌入到任何类别 的对象中 通过偏移存取因为他们支持偏移的操作，所以有：索引和分片 属于不可变序列类型不支持在原处进行修改 固定长度、异构、任意嵌套因为元组的长度是不可变的，在不生成一个拷贝的情况下不能增长或缩短，另一方面，元组可以包含其他的复合对象（如：列表、字典、其他元组等），因为支持嵌套 对象引用的数组 常见的元组常量和运算 表达式 含义 （） 空元组 T = (0,) 单个元素的元组 T = (1,’ni’,3,4) 四个元素的元组 T = 0,’ni’,3,4 另一个四个元素的元组 T = (‘abc’, (‘def’, ‘ghi’)) 嵌套元组 T = tuple(‘spam’) T[i] 索引 T[i][j] 索引的索引 T[i:j] 分片 len(T) 求长度 T1 + T2 合并 T * 3 重复 for x in T : print(x) 遍历 ’spam’ in T 检查 [x ** 2 for x in T] T.index(‘ni’) 索引下标 T.count(‘ni’) 统计计数 1.合并12&gt;&gt; (1,3) + (2,4)(1,3,2,4) 2.重复*12&gt;&gt;(1,3)*4 =========&gt; (1,3)+(1,3)+(1,3)+(1,3)(1,3,1,3,1,3,1,3) 3.切片123&gt;&gt;T = (1,2,3,4)&gt;&gt;T[0] ,T[1:3](1,(2,3)) 4.逗号和圆括号1234567&gt;&gt;x = (40) #（）表示四则运算的基本元素，所以此时只是为整数40加上了括号&gt;&gt;x40&gt;&gt;y = (40,) #表示元组，之所以加上逗号，就是为了区分整数40的原因&gt;&gt;y (40,) 5.排序1234567891011121314&gt;&gt;T = (&apos;cc&apos;,&apos;aa&apos;,&apos;dd&apos;,&apos;bb&apos;)&gt;&gt;tmp = list(T)&gt;&gt;tmp.sort()&gt;&gt;tmp[&apos;aa&apos;,&apos;bb&apos;,&apos;cc&apos;,&apos;dd&apos;]&gt;&gt;T = tuple(tmp)&gt;&gt;T(&apos;aa&apos;,&apos;bb&apos;,&apos;cc&apos;,&apos;dd&apos;)#另外一种方式&gt;&gt;sorted(T)[&apos;aa&apos;,&apos;bb&apos;,&apos;cc&apos;,&apos;dd&apos;] 6.索引下标12345&gt;&gt;T = (1,2,3,2,4,2)&gt;&gt; T.index(2) #第一次出现“2”时的偏移1&gt;&gt;T.index(2,2)3 7.统计计数123&gt;&gt;T = (1,2,3,2,4,2)&gt;&gt;T.count(2) #统计出现的次数3 8. “改变”元组元组的不可变形只适用于元组本身顶层而并非其内容，例如：1234567&gt;&gt; T = (1,[2,3],4)&gt;&gt; T[1] = &apos;spam&apos;TypeError:ojbect doesn&apos;t support item assignment&gt;&gt;T[1][0] = &apos;spam&apos;&gt;&gt;T(1,[&apos;spam&apos;,3], 4)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之sqlite数据库简介","date":"2017-04-16T04:47:25.833Z","path":"2017/04/16/python/python数据库之sqlite数据库简介/","text":"1.简介SQLite是一种嵌入式数据库，它的数据库就是一个文件。由于SQLite本身是C写的，而且体积很小，所以，经常被集成到各种应用程序中，甚至在iOS和Android的App中都可以集成。 2.特点 不需要一个单独的服务器进程或操作的系统（无服务器的）。 SQLite 不需要配置，这意味着不需要安装或管理。 一个完整的 SQLite 数据库是存储在一个单一的跨平台的磁盘文件。 SQLite 是非常小的，是轻量级的，完全配置时小于 400KiB，省略可选功能配置时小于250KiB。 SQLite 是自给自足的，这意味着不需要任何外部的依赖。 SQLite 事务是完全兼容 ACID 的，允许从多个进程或线程安全访问。 SQLite 支持 SQL92（SQL2）标准的大多数查询语言的功能。 SQLite 使用 ANSI-C 编写的，并提供了简单和易于使用的 API。 SQLite 可在 UNIX（Linux, Mac OS-X, Android, iOS）和 Windows（Win32, WinCE, WinRT）中运行。 3.参考教程http://www.runoob.com/sqlite/sqlite-tutorial.html","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之sqlite3模块","date":"2017-04-16T04:47:25.831Z","path":"2017/04/16/python/python数据库之sqlite3模块/","text":"1.使用步骤Python就内置了SQLite3，所以，在Python中使用SQLite，不需要安装任何东西，直接使用。 导入sqlite3模块 连接数据库 sqlite3.connect(“数据库文件路径及其文件”) sqlite3.connect(“:memory:”) —— 内存数据库 以上将返回数据库连接对象 用游标对象方法操作数据库123456cursor.execute()cursor.executemany()cursor.executescript()cursor.fetchone()cursor.fetchall()cursor.fetchmany() 提交事务 12con.commit()con.rollback() 关闭连接 1con.close() 2.实现简单的CRUD12345create table_name(col_name_type,......) #创建表insert into table_name(cola,colb...) vlaues(...) #插入数据select * from table_name #查询数据update table set ..... where ........ #更新数据delete from table_name where ....... #删除数据 12345678910111213141516171819202122232425262728from sqlite3 import connect db_name = &apos;test.db&apos; con = connect(db_name)cur = con.cursor() # cur.execute(&apos;create table star(id integer,name text,age integer,address text)&apos;) #创建表 # rows = [(1,&quot;王俊凯&quot;,16,&quot;重庆&quot;),(2,&quot;王源&quot;,15,&quot;重庆&quot;),(3,&quot;易烊千玺&quot;,15,&quot;怀化&quot;)]# for item in rows:# cur.execute(&quot;insert into star (id,name,age,address) values (?,?,?,?)&quot;,item) #插入数据 # cur.execute(&apos;select * from star&apos;) #查询数据# for row in cur:# print(row) # cur.execute(&apos;update star set age=? where id=?&apos;,(16,3)) #更新数据# cur.execute(&apos;select * from star&apos;)# for row in cur:# print(row)cur.execute(&apos;delete from star where id=?&apos;,(3,)) #删除数据cur.execute(&apos;select * from star&apos;)for row in cur: print(row) con.commit()con.close() 3.行对象（Row）简介3.1.支持的操作 以列名访问 以索引号访问 迭代访问 len()操作 3.2.游标建立前conn.row_factory = sqlite3.Row 3.3.程序实现123456789101112131415161718192021222324 from sqlite3 import connect,Row db_name = &apos;test.db&apos; con = connect(db_name)con.row_factory = Row #设置cur = con.cursor() cur.execute(&apos;select * from star&apos;)row = cur.fetchone() print(type(row)) print(&apos;以列名访问：&apos;,row[&apos;name&apos;]) #以列名访问 print(&apos;以索引号访问：&apos;,row[1]) #以索引号访问 print(&apos;以迭代的访问：&apos;)for item in row: print(item) print(&quot;len():&quot;,len(row)) con.close() 4.批量数据库操作 cur.executemany(sql_string,seq) 1234567891011121314151617181920from sqlite3 import connect,Row db_name = &apos;test.db&apos; con = connect(db_name)con.row_factory = Rowcur = con.cursor() rows = [(14,&apos;Lily&apos;,12,&apos;BeiJing&apos;),(6,&apos;John&apos;,13,&quot;ChongQing&quot;)]cur.executemany(&apos;insert into star (id,name,age,address) values (?,?,?,?)&apos;,rows) #rows 一个列表的参数cur.execute(&apos;select * from star&apos;) for row in cur: for r in row: print(r) con.commit() con.close() 5.批量执行脚本 cur.executescript(sql_string) 1234567891011121314151617181920from sqlite3 import connectdb_name = &apos;testb.db&apos; con = connect(db_name)cur = con.cursor() sql_str = &quot;&quot;&quot;create table test(id integer,name text);insert into test (id,name) values (1,&apos;Lily&apos;);insert into test (id,name) values (2,&apos;Green&apos;);&quot;&quot;&quot;cur.executescript(sql_str) #批量执行 cur.execute(&apos;select * from test&apos;)for item in cur: print(item) con.commit() con.close() 6.自定义函数——创建基本函数 con.create_function(name,params_num,func_name) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from sqlite3 import connect,Rowimport binascii db_name = &apos;test.db&apos; def encrypt(mydata): crc = str(binascii.crc32(mydata.encode())) while len(crc) &lt; 10: crc = &apos;0&apos;+ crc return mydata + crc def check(mydata): if len(mydata) &lt; 11: return None crc_res = str(binascii.crc32(mydata[:-10].encode())) while len(crc_res) &lt; 10: crc_res = &apos;0&apos;+ crc_res if crc_res == mydata[-10:]: return mydata[:-10] con = connect(db_name)con.create_function(&apos;checkk&apos;,1,check) #checkk 是注册函数的名字，注册了之后可以在sql语句中使用的，check是对应的执行的函数（实际就是执行的这个函数） cur = con.cursor() sql_scrpit = &quot;&quot;&quot;drop table if exists testa;create table if not exists testa(id integer,name text);insert into testa (id,name) values (3,&quot;%s&quot;);insert into testa (id,name) values (4,&quot;%s&quot;);&quot;&quot;&quot;names = [&apos;Lily&apos;,&apos;Green&apos;]names = tuple(encrypt(i) for i in names)sql_scrpit = sql_scrpit % namesprint(sql_scrpit)cur.executescript(sql_scrpit) cur.execute(&apos;select id,checkk(name) from testa&apos;) #调用注册的函数for item in cur: print(item) cur.execute(&apos;update testa set name=? where id=?&apos;,(&apos;dfddkkjd1122334455&apos;,4))cur.execute(&apos;select id,checkk(name) from testa&apos;)for item in cur: print(item) con.commit()con.close() 7.自定义函数——创建聚合函数 实现一些方法的一个自定义类 con.create_aggregate(name,params_num,class_name) 协议方法 step() finalize() 123456789101112131415161718192021222324252627282930313233#求绝对值和from sqlite3 import connect,Rowimport binascii db_name = &apos;test.db&apos; class AbsSum: def __init__(self): self.s = 0 def step(self,v): self.s += abs(v) def finalize(self): return self.s con = connect(db_name)con.create_aggregate(&apos;abssum&apos;,1,AbsSum) cur = con.cursor() sql_scrpit = &quot;&quot;&quot;drop table if exists testa;create table if not exists testa(id integer,name text,score integer);insert into testa (id,name,score) values (3,&quot;Lily&quot;,8);insert into testa (id,name,score) values (4,&quot;Jhon&quot;,-7);&quot;&quot;&quot;cur.executescript(sql_scrpit) cur.execute(&apos;select abssum(score) from testa&apos;)for item in cur: print(item) con.commit()con.close() 8.数据类型转换python sqliteNone NULLstr textint integerbytes blob 9.保存文件至数据库 execute(“insert into t values(?) “, (f.read())) 1234567891011121314151617181920212223242526from sqlite3 import connect,Row,Binaryimport binascii db_name = &apos;test.db&apos; con = connect(db_name)cur = con.cursor() sql_scrpit = &quot;&quot;&quot;drop table if exists testa;create table if not exists testa(id integer,data blob);&quot;&quot;&quot;cur.executescript(sql_scrpit) f = open(&apos;test.jpg&apos;,&apos;rb&apos;) cur.execute(&apos;insert into testa (id,data) values (3,?)&apos;,(f.read(),)) cur.execute(&apos;select * from testa where id=3&apos;)record = cur.fetchone()f = open(&apos;tt.jpg&apos;,&quot;wb+&quot;)f.write(record[1])f.close()con.commit()con.close() 练习关于：联系人和分组的增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384import osimport sqlite3from sqlite3 import connect class MySqliteDb(object): &quot;&quot;&quot;Sqlite3 Db Class&quot;&quot;&quot; def __init__(self, dbname=&quot;address.db&quot;): self.dbname = dbname self.con = None self.curs = None def getCursor(self): self.con = sqlite3.connect(self.dbname) if self.con: self.curs = self.con.cursor() def closeDb(self): if self.curs: self.curs.close() if self.con: self.con.commit() self.con.close() def __enter__(self): self.getCursor() return self.curs def __exit__(self, exc_type, exc_val, exc_tb): if exc_val: print(&quot;Exception has generate: &quot;,exc_val) print(&quot;Sqlite3 execute error!&quot;) self.closeDb() def initDb(): sql_script = &apos;&apos;&apos; create table contact (id integer primary key autoincrement not null, name varchar(20) not null, home_tel varchar(20), office_tel varchar(20), mobile_phone varchar(20), memo text); create table mygroup( id integer primary key autoincrement not null, name varchar(20) not null, memo text); create table con_gro( cid integer not null, gid integer not null, FOREIGN KEY(cid) REFERENCES contact(id), FOREIGN KEY(gid) REFERENCES mygroup(id) ); &apos;&apos;&apos; if not os.path.exists(&apos;address.db&apos;): with MySqliteDb() as db: db.executescript(sql_script) class Contact: def __init__(self,name=&apos;&apos;,home_tel=&apos;&apos;,office_tel=&apos;&apos;,mobile_phone=&apos;&apos;,memo=&apos;&apos;): self.id = -1 self.name = name self.home_tel = home_tel self.office_tel = office_tel self.mobile_phone = mobile_phone self.memo = memo self.gid = None self.keys = (&apos;home_tel&apos;,&apos;office_tel&apos;,&apos;mobile_phone&apos;,&apos;memo&apos;) self.__change() def save(self): if self.id == -1: param_dicts = &#123;k:getattr(self,k) for k in self.keys if getattr(self,k)&#125; keys = tuple(k for k in self.keys if k in param_dicts) quotes = &apos;,&apos;.join((&apos;?&apos; for i in range(len(param_dicts)))) param_tuple = [self.name,] for key in keys: param_tuple.append(param_dicts[key]) param_tuple = tuple(param_tuple) sql = &quot;insert into contact (name,%s) values (?,%s)&quot; % (&apos;,&apos;.join(keys),quotes) if self.name: try: with MySqliteDb() as db: db.execute(sql,param_tuple) res = db.execute(&apos;select id from contact where name=?&apos;,(self.name,)) res = res.fetchone() self.id = res[0] return True except: print(&quot;保存失败，请检查服务器！&quot;) else: print(&quot;姓名不能为空！&quot;) return False else: return self.update() def update(self): sql = &quot;update contact set %s=? where id=?&quot; chgs = &#123;k:getattr(self,k) for k in self.keys \\ if getattr(self,k) and getattr(self,k) != self.vals.get(k)&#125; if not chgs: return try: with MySqliteDb() as db: for k,v in chgs.items(): db.execute(sql % k,(v,self.id)) self.__change() return True except: print(&quot;更新失败！&quot;) return False def load(self,id): try: with MySqliteDb() as db: res = db.execute(&apos;select * from contact where id=?&apos;,(id,)) res = res.fetchone() self.name = res[1] self.id = id for i,v in enumerate(res[2:]): setattr(self,self.keys[i-2],v) self.__change() return True except: print(&apos;数据载入失败！&apos;) def load_from_name(self): if self.name: try: with MySqliteDb() as db: res = db.execute(&apos;select * from contact where name=?&apos;,(self.name,)) res = res.fetchone() self.id = res[0] for i,v in enumerate(res[2:]): setattr(self,self.keys[i-2],v) self.__change() return True except: print(&apos;数据载入失败！&apos;) def get_by_name(self,name): try: with MySqliteDb() as db: res = db.execute(&apos;select * from contact where name=?&apos;,(name,)) res = res.fetchall() return res except: print(&apos;数据查询失败！&apos;) def delete(self): try: with MySqliteDb() as db: db.execute(&apos;delete from contact where id=?&apos;,(self.id,)) db.execute(&apos;delete from con_gro where cid=?&apos;,(self.id,)) return True except: print(&apos;数据查询失败！&apos;) def all(self): try: with MySqliteDb() as db: res = db.execute(&apos;select * from contact&apos;) res = res.fetchall() return res except: print(&apos;数据查询失败！&apos;) def add_to_group(self,gid): if gid and self.id != -1: self.gid = gid try: with MySqliteDb() as db: db.execute(&apos;insert into con_gro (cid,gid) values (?,?)&apos;,(self.id,gid)) return True except: print(&apos;数据查询失败！&apos;) def __change(self): self.vals = &#123;k:getattr(self,k) for k in self.keys&#125; class Group: def __init__(self,name=&apos;&apos;,memo=&apos;&apos;): self.id = -1 self.name = name self.memo =memo self.contacts = [] def save(self): if self.name and self.id == -1: try: with MySqliteDb() as db: db.execute(&apos;insert into mygroup (name,memo) values (?,?)&apos;,(self.name,self.memo)) res = db.execute(&apos;select id from mygroup where name=?&apos;,(self.name,)) self.id = res.fetchone()[0] return True except: print(&quot;数据查询失败！&quot;) if self.name and self.id != -1: return self.update() def update(self): try: with MySqliteDb() as db: db.execute(&apos;update mygroup set name=?,memo=? where id=?&apos;,(self.name,self.memo,self.id)) return True except: print(&quot;数据更新失败！&quot;) def load(self,id): try: with MySqliteDb() as db: res = db.execute(&apos;select * from mygroup where id=?&apos;,(id,)) res = res.fetchone() self.id = id self.name = res[1] self.memo = res[2] except: pass def delete(self): if self.id != -1: try: with MySqliteDb() as db: db.execute(&apos;delete from mygroup where id=?&apos;,(self.id,)) db.execute(&apos;delete from con_gro where gid=?&apos;,(self.id,)) return True except: print(&quot;数据查询失败！&quot;) def load_from_name(self): if self.name: try: with MySqliteDb() as db: res = db.execute(&apos;select * from mygroup where name=?&apos;,(self.name,)) res = res.fetchone() self.id = res[0] self.memo = res[2] except: pass def all(self): try: with MySqliteDb() as db: res = db.execute(&apos;select * from mygroup&apos;) return res.fetchall() except: print(&quot;数据查询失败！&quot;) def add_contact(self,cid): if cid and self.id != -1: try: with MySqliteDb() as db: db.execute(&apos;insert into con_gro (cid,gid) values (?,?)&apos;,(cid,self.id)) return True except: print(&apos;数据查询失败！&apos;) def del_contact(self,cid): try: with MySqliteDb() as db: db.execute(&apos;delete from con_gro where cid=? and gid=?&apos;,(cid,self.id)) return True except: pass def all_contacts(self): # return objects of members if self.id != -1: try: with MySqliteDb() as db: cts = [] cids = db.execute(&apos;select cid from con_gro where gid=?&apos;,(self.id,)) cids = cids.fetchall() for cid in cids: c = Contact() c.load(cid[0]) cts.append(c) return cts except: print(&apos;数据查询失败！&apos;) def info(): sqls = [ &apos;select * from contact&apos;, &apos;select * from mygroup&apos;, &apos;select * from con_gro&apos;, ] try: with MySqliteDb() as db: for sql in sqls: res = db.execute(sql) res = res.fetchall() for r in res: print(r) except: print(&apos;数据查询失败！&apos;) if __name__ == &apos;__main__&apos;: cts = [ &#123;&apos;name&apos;:&apos;Lily&apos;,&apos;home_tel&apos;:&apos;0551-98789233&apos;,&apos;memo&apos;:&quot;安徽&quot;&#125;, &#123;&apos;name&apos;:&apos;Bob&apos;,&apos;office_tel&apos;:&apos;021-94679233&apos;,&apos;memo&apos;:&quot;上海&quot;&#125;, &#123;&apos;name&apos;:&apos;Mike&apos;,&apos;mobile_phone&apos;:&apos;18298781089&apos;&#125;, &#123;&apos;name&apos;:&apos;John&apos;,&apos;home_tel&apos;:&apos;010-57989043&apos;,&apos;memo&apos;:&quot;北京&quot;&#125;, &#123;&apos;name&apos;:&apos;Green&apos;,&apos;mobile_phone&apos;:&apos;13908707652&apos;,&#125;, &#123;&apos;name&apos;:&apos;Tom&apos;,&apos;mobile_phone&apos;:&apos;13109008759&apos;&#125;, ] gps = [ &#123;&quot;name&quot;:&apos;朋友&apos;&#125;, &#123;&apos;name&apos;:&apos;家人&apos;&#125;, &#123;&apos;name&apos;:&quot;学生&quot;&#125;, ] print(&apos;初始化数据库……&apos;) initDb() print(&apos;插入数据……&apos;) for ct in cts: Contact(**ct).save() for gp in gps: Group(**gp).save() print(&apos;从数据库中查询到的数据……&apos;) info() print(&apos;查询联系人，并添加到组2……&apos;) ct = Contact(name=&apos;Tom&apos;) ct.load_from_name() print(ct.id,ct.name) ct.add_to_group(2) cta = Contact() cta.load(1) print(cta.id,cta.name) cta.add_to_group(1) ctb = Contact() ctb.load(3) print(ctb.id,ctb.name) ctb.add_to_group(2) ctb = Contact() ctb.load(5) print(ctb.id,ctb.name) ctb.add_to_group(2) info() print(&quot;查询所有联系人：&quot;) for r in Contact().all(): print(r) print(&quot;查询所有组：&quot;) for r in Group().all(): print(r) print(&quot;查询指定组的成员：&quot;) gp = Group(name=&apos;家人&apos;) gp.load_from_name() print(gp.id,gp.name) print(&quot;通过组添加成员：&quot;) gp.add_contact(4) cts = gp.all_contacts() for ct in cts: print(ct.id,ct.name) print(&quot;删除组成员：&quot;) gp.del_contact(5) info() print(&quot;删除联系人：&quot;) ct = Contact() ct.load(5) ct.delete() info() print(&quot;删除组：&quot;) gp = Group() gp.load(2) gp.delete() info()","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之python DB API 2.0简绍","date":"2017-04-16T04:47:25.830Z","path":"2017/04/16/python/python数据库之python DB API 2.0简绍/","text":"1.目的 统一数据库操作的界面 切换后台数据库时，几乎不用更改数据库操作的有关代码 官网：https://www.python.org/dev/peps/pep-0249/ 2.连接对象 connect()函数成功连接后返回的对象 方法 cursor()返回操作数据库的游标 commit()事务提交 rollback()事务回滚 close()关闭连接 3.游标对象 连接对象的cursor()方法返回游标对象 方法 callproc(pname [,params]) 调用存储过程 execute(sqlstr [,params]) 执行sql查询语句 executemany(sqlstr [,seq-params]) 以序列中的参数执行多次sql查询 fetchone() 获取查询结果中的一个数据行 fetmany(size) 获取查询结果中指定数量的数据行 fetchall()获取查询结果的所有行 nextset() 获取所有查询结果集当前结果集的下一个结果集 可迭代获取结果 属性 arraysize setinputsize setoutputsize 4.操作异常 StandardError Waring Error InterfaceError DataBaseError 5.参数形式 sql查询语句中的使用参数时占位符的形式由模块级变量paramstyle的值指定 qmark:(?) numeric:(:1) named:(:name) format:(%s) cursor.executemany(‘INSERT INTO t_user values(%s,%s)’,(1,’zhangsan’)) pyformat:(%(name)s) cursor.execute(“insert into student values(%(id)s,%(name)s,%(grade)s)” , {‘id’:6,’name’:’haha’,’grade’:100}) 使用%(name)s作为占位符，以字典的方式提供参数 pyformat:(%(name)d)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之ORM工具SQLAlchemy","date":"2017-04-16T04:47:25.828Z","path":"2017/04/16/python/python数据库之ORM工具SQLAlchemy/","text":"1.ORM简介 一种对象关系映射工具 面向对象编程开发方法发生而产生的 面向对象编程中数据保存在对象中并在对象中交互，运行于内存（不易持久化） 关系是指关系数据库，即数据保存在数据库中（可持久化） 对象可以轻松的从数据库中载入持久化的数据 对象还可以将需要持久化的对象数据保存至数据库 2. ORM特性 提高开发效率，不用直接编写查询数据库载入数据的代码 开发人员不用接触SQL语句，可以完成数据库有关操作 主要缺点是：不易数据库查询优化，并可能带来性能上的损失 3.SQLAlchemy 开源并使用MIT许可证 可兼容切换多种数据库（如：SQLite、MySQL、PostgreSQL、MsSQL、Oracle等） 安装：在管理员方式的命令提示符中：pip install SQLAlchemy 4.使用步骤 构造声明基类 Base = sqlalchemy.ext.declarative.declarative_base() 声明对象关系映射类（继承Base） 创建与数据库的连接 engine = sqlalchemy.create_engine(数据库连接字符串) 创建表（如果表存在，可以省略） Base.metadata.create_all(engine) 使用会话访问数据库 创建会话 Session = sqlalchemy.orm.sessionmaker(bind=engine) (或者先创建Session，后配置：Session.configure(bind=engine)) session = Session() 会话方法 123456789101112131415161718#添加session.add()session.add_all()#查询session.query() filter_by()/filter() #过滤 order_by() #排序 first() #取第一条 all() #取所有 可切片（[1:3]） #取切片 #删除session.delete()#提交事务session.commit()会话提交时，映射类各种改变都会提交 5.定义实例 主键的定义 primary_key = True 主要列类型12345678910Column(用它来构造列对象)IntegerFloatStringTextSequenceDateDateTimeBooleanBinary 6.基本操作（使用步骤的代码实现）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061#!/usr/bin/python from sqlalchemy import create_engine,String,Integer,Columnfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmaker #&apos;数据库类型+数据库驱动名称://用户名:口令@机器地址:端口号/数据库名&apos;db_name = &apos;mysql+mysqldb://root:oldboy123@192.168.0.11/testdb&apos; # 创建对象的基类: Base = declarative_base() class User(Base): #继承基类：相当于定义映射关系（实体对象和数据库字段的映射关系） # 表的名字: __tablename__ = &apos;user&apos; # 表的结构: id = Column(Integer,primary_key=True) name = Column(String(50))# 初始化数据库连接:engine = create_engine(db_name)#有这条语句，如果表不存在，那么就创建表（如上面的user表）Base.metadata.create_all(engine) #创建sessionSession = sessionmaker(bind=engine)session = Session() #add&apos;&apos;&apos;u = User()u.id=1u.name = &apos;Lily&apos;session.add(u) u2 = User(id=2,name=&apos;Bob&apos;)session.add(u2) session.commit()&apos;&apos;&apos; #查询并过滤u3 = session.query(User).filter_by(id=2).first()#u3 = session.query(User).filter(User.id=2).first() print(&quot;u3:id=&#123;0&#125;,name=&#123;1&#125;&quot;.format(str(u3.id),u3.name))#查询并排序#session.query(User).order_by(&apos;id&apos;).all()#查询并删除 #session.query(User).filter_by(id=3).delete() #deletesession.delete(u3)session.commit()# 关闭Session:session.close() 7.一对多映射关系user中的addresses是包含若干个Address对象的list 8.多对多映射关系练习关于：联系人和分组123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117from sqlalchemy import create_engine,String,Integer,Column,ForeignKey,Table,Text,Sequencefrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmaker,relationship db_name = &apos;mysql+mysqlconnector://root:123456@localhost:3306/test&apos; Base = declarative_base() con_gro = Table(&apos;asso&apos;,Base.metadata, Column(&apos;con_id&apos;,Integer,ForeignKey(&apos;contact.id&apos;)), Column(&apos;gro_id&apos;,Integer,ForeignKey(&apos;mygroup.id&apos;))) class Contact(Base): __tablename__ = &apos;contact&apos; id = Column(Integer,Sequence(&apos;contact_id&apos;),primary_key=True) name = Column(String(20)) home_tel = Column(String(20)) office_tel = Column(String(20)) mobile_phone = Column(String(20)) memo = Column(Text) groups = relationship(&quot;Group&quot;,secondary=con_gro,backref=&quot;contacts&quot;) class Group(Base): __tablename__ = &apos;mygroup&apos; id = Column(Integer,Sequence(&apos;mygroup_id&apos;),primary_key=True) name = Column(String(20)) memo = Column(Text) if __name__ == &apos;__main__&apos;: cts = [ &#123;&apos;name&apos;:&apos;Lily&apos;,&apos;home_tel&apos;:&apos;0551-98789233&apos;,&apos;memo&apos;:&quot;安徽&quot;&#125;, &#123;&apos;name&apos;:&apos;Bob&apos;,&apos;office_tel&apos;:&apos;021-94679233&apos;,&apos;memo&apos;:&quot;上海&quot;&#125;, &#123;&apos;name&apos;:&apos;Mike&apos;,&apos;mobile_phone&apos;:&apos;18298781089&apos;&#125;, &#123;&apos;name&apos;:&apos;John&apos;,&apos;home_tel&apos;:&apos;010-57989043&apos;,&apos;memo&apos;:&quot;北京&quot;&#125;, &#123;&apos;name&apos;:&apos;Green&apos;,&apos;mobile_phone&apos;:&apos;13908707652&apos;,&#125;, &#123;&apos;name&apos;:&apos;Tom&apos;,&apos;mobile_phone&apos;:&apos;13109008759&apos;&#125;, ] gps = [ &#123;&quot;name&quot;:&apos;朋友&apos;&#125;, &#123;&apos;name&apos;:&apos;家人&apos;&#125;, &#123;&apos;name&apos;:&quot;学生&quot;&#125;, ] engine = create_engine(db_name) Session = sessionmaker(bind=engine) session = Session() print(&apos;初始化数据库……&apos;) Base.metadata.drop_all(engine) Base.metadata.create_all(engine) print(&apos;插入数据……&apos;) for ct in cts: session.add(Contact(**ct)) for gp in gps: session.add(Group(**gp)) session.commit() print(&apos;从数据库中查询到的数据……&apos;) for c in session.query(Contact).all(): print(c.id,c.name,c.home_tel, c.office_tel,c.mobile_phone,c.memo) for g in session.query(Group).all(): print(g.id,g.name,g.memo) print(&apos;查询联系人，并添加到组2……&apos;) ct = session.query(Contact).filter_by(name=&quot;Tom&quot;).first() print(ct.id,ct.name) gp = session.query(Group).filter_by(id=2).first() gp.contacts.append(ct) ct = session.query(Contact).filter_by(id=1).first() print(ct.id,ct.name) gpo = session.query(Group).filter_by(id=1).first() gpo.contacts.append(ct) ct = session.query(Contact).filter_by(id=3).first() print(ct.id,ct.name) gp.contacts.append(ct) ct = session.query(Contact).filter_by(id=5).first() print(ct.id,ct.name) gp.contacts.append(ct) session.commit() print(&quot;查询指定组的成员：&quot;) gp = session.query(Group).filter_by(name=&quot;家人&quot;).first() print(gp.id,gp.name) for c in gp.contacts: print(c.id,c.name) print(&quot;删除组成员：&quot;) gp.contacts.remove(c) session.commit() for c in gp.contacts: print(c.id,c.name) print(&quot;删除联系人：&quot;) c = session.query(Contact).filter_by(id=5).first() session.delete(c) print(&quot;删除组：&quot;) g = session.query(Group).filter_by(id=2).first() session.delete(g) session.commit() for c in session.query(Contact).all(): print(c.id,c.name,c.home_tel, c.office_tel,c.mobile_phone,c.memo) for g in session.query(Group).all(): print(g.id,g.name,g.memo)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之MySQLdb访问mysql数据库","date":"2017-04-16T04:47:25.827Z","path":"2017/04/16/python/python数据库之MySQLdb访问mysql数据库/","text":"1.python连接mysql的第三方驱动 pymysql MySQL-Python MySQL官方连接库（MySQL connector Python） 2.连接数据库的几种方式123456789101112131415161718import MySQLdb as mdb # 连接数据库conn = mdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;) # 也可以使用关键字参数conn = mdb.connect(host=&apos;127.0.0.1&apos;, port=3306, user=&apos;root&apos;, passwd=&apos;root&apos;, db=&apos;test&apos;, charset=&apos;utf8&apos;) # 也可以使用字典进行连接参数的管理config = &#123; &apos;host&apos;: &apos;127.0.0.1&apos;, &apos;port&apos;: 3306, &apos;user&apos;: &apos;root&apos;, &apos;passwd&apos;: &apos;root&apos;, &apos;db&apos;: &apos;test&apos;, &apos;charset&apos;: &apos;utf8&apos;&#125;conn = mdb.connect(**config) 3.游标（cursor）的方法参见：python数据库之python DB API 2.0简绍 3.1.cursor.rowcount返回查询结果的行数12345678910111213141516#!/usr/bin/python# -*- coding: utf-8 -*- import MySQLdb as mdb con = mdb.connect(&apos;localhost&apos;, &apos;testuser&apos;, &apos;test623&apos;, &apos;testdb&apos;); with con: cur = con.cursor() cur.execute(&quot;SELECT * FROM Writers&quot;) for i in range(cur.rowcount): #通过遍历行 row = cur.fetchone() print row[0], row[1] 3.2.查询列名12345678910In [34]: desc = cursor.description In [35]: descOut[35]:((&apos;id&apos;, 3, 1, 11, 11, 0, 1),(&apos;name&apos;, 253, 12, 20, 20, 0, 1),(&apos;grade&apos;, 3, 3, 11, 11, 0, 1))In [37]: print(desc[0][0], desc[1][0],desc[2][0]) (&apos;id&apos;, &apos;name&apos;, &apos;grade&apos;) 4.简单的CRUD12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#coding=utf-8 import MySQLdb conn = MySQLdb.connect(host=&apos;localhost&apos;,user=&apos;root&apos;,passwd=&apos;123456&apos;,charset=&apos;utf8&apos;)cursor = conn.cursor()try: #创建数据库 DB_NAME = &apos;test&apos; cursor.execute(&apos;DROP DATABASE IF EXISTS %s&apos; %DB_NAME) cursor.execute(&apos;CREATE DATABASE IF NOT EXISTS %s&apos; %DB_NAME) #其实这里的%是python的str的格式化方法 conn.select_db(DB_NAME) #选择数据库 #创建表 TABLE_NAME = &apos;t_user&apos; cursor.execute(&apos;CREATE TABLE %s(id int primary key,name varchar(30))&apos; %TABLE_NAME) #插入单条数据 value = [1,&apos;alexzhou1&apos;] cursor.execute(&apos;INSERT INTO t_user values(%s,%s)&apos;,value) #cursor.execute(&quot;insert into student values(%(id)s,%(name)s,%(grade)s)&quot; , &#123;&apos;id&apos;:6,&apos;name&apos;:&apos;haha&apos;,&apos;grade&apos;:100&#125;) 使用%(name)s作为占位符，以字典的方式提供参数 #批量插入数据 values = [] for i in range(2,10): values.append((i,&apos;alexzhou%s&apos; %(str(i)))) cursor.executemany(&apos;INSERT INTO t_user values(%s,%s)&apos;,values) #查询记录数量 count = cursor.execute(&apos;SELECT * FROM %s&apos; %TABLE_NAME) print &apos;total records: %d&apos;,count #查询一条记录 print &apos;fetch one record:&apos; result = cursor.fetchone() print result print &apos;id: %s,name: %s&apos; %(result[0],result[1]) #查询多条记录 print &apos;fetch five record:&apos; results = cursor.fetchmany(5) for r in results: print r #查询所有记录 #重置游标位置，偏移量:大于0向后移动;小于0向前移动，mode默认是relative #relative:表示从当前所在的行开始移动,absolute:表示从第一行开始移动 cursor.scroll(0,mode=&apos;absolute&apos;) results = cursor.fetchall() for r in results: print r cursor.scroll(-2) results = cursor.fetchall() for r in results: print r #更新记录 cursor.execute(&apos;UPDATE %s SET name = &quot;%s&quot; WHERE id = %s&apos; %(TABLE_NAME,&apos;zhoujianghai&apos;,1)) #删除记录 cursor.execute(&apos;DELETE FROM %s WHERE id = %s&apos; %(TABLE_NAME,2)) #必须提交，否则不会插入数据 conn.commit()except: import traceback traceback.print_exc() # 发生错误时会滚 conn.rollback()finally: # 关闭游标连接 if cursor: cursor.close() # 关闭数据库连接 if conn: conn.close() 5.查询时返回字典结构MySQLdb默认查询结果都是返回tuple，通过使用不同的游标可以改变输出格式，这里传递一个cursors.DictCursor参数。123456789101112131415161718192021222324252627282930313233343536373839#方式一import MySQLdb.cursors conn = MySQLdb.connect(host=&apos;localhost&apos;, user=&apos;root&apos;, passwd=&apos;root&apos;, db=&apos;test&apos;, cursorclass=MySQLdb.cursors.DictCursor) #在连接的时候指定 DictCursorcursor = conn.cursor() cursor.execute(&apos;select * from user&apos;)r = cursor.fetchall()print r# 当使用位置参数或字典管理参数时，必须导入MySQLdb.cursors模块 -------------------------------------------------------#方式二import MySQLdb as mdbconn = mdb.connect(&apos;localhost&apos;, &apos;root&apos;, &apos;root&apos;, &apos;test&apos;)cursor = conn.cursor(cursorclass=mdb.cursors.DictCursor) #在返回游标的时候指定 cursor.execute(&apos;select * from user&apos;)r = cursor.fetchall()print r#-------------------- 执行结果如下 ------------------------In [24]: cursor = conn.cursor(cursorclass=MySQLdb.cursors.DictCursor) In [25]: cursor.execute(&quot;select *from student&quot;) Out[25]: 6L In [26]: for item in cursor: print(item) ....: &#123;&apos;grade&apos;: 99L, &apos;id&apos;: 1L, &apos;name&apos;: &apos;chengyansong&apos;&#125;&#123;&apos;grade&apos;: 99L, &apos;id&apos;: 2L, &apos;name&apos;: &apos;zhangsan&apos;&#125;&#123;&apos;grade&apos;: 99L, &apos;id&apos;: 2L, &apos;name&apos;: &apos;zhangsan&apos;&#125;&#123;&apos;grade&apos;: 88L, &apos;id&apos;: 7L, &apos;name&apos;: &apos;aaaaa&apos;&#125;&#123;&apos;grade&apos;: 0L, &apos;id&apos;: 6L, &apos;name&apos;: &apos;haha&apos;&#125;&#123;&apos;grade&apos;: 100L, &apos;id&apos;: 6L, &apos;name&apos;: &apos;haha&apos;&#125; 6.存入和取出图片数据12345678910111213141516171819202122232425262728293031323334353637383940414243444546#存入#!/usr/bin/python# -*- coding: utf-8 -*- import MySQLdb as mdb def read_image(): fin = open(&quot;woman.jpg&quot;) img = fin.read() return img con = mdb.connect(&apos;localhost&apos;, &apos;testuser&apos;, &apos;test623&apos;, &apos;testdb&apos;) with con: cur = con.cursor() data = read_image() #得到图片数据 cur.execute(&quot;INSERT INTO Images VALUES(1, %s)&quot;, (data, )) #存入存入数据库#取出#!/usr/bin/python# -*- coding: utf-8 -*- import MySQLdb as mdb def writeImage(data): fout = open(&apos;woman2.jpg&apos;, &apos;wb&apos;) with fout: fout.write(data) con = mdb.connect(&apos;localhost&apos;, &apos;testuser&apos;, &apos;test623&apos;, &apos;testdb&apos;) with con: cur = con.cursor() cur.execute(&quot;SELECT Data FROM Images WHERE Id=1&quot;) data = cur.fetchone()[0] writeImage(data)","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python数据库之MySQL-python（相当于驱动）安装","date":"2017-04-16T04:47:25.825Z","path":"2017/04/16/python/python数据库之MySQL-python（相当于驱动）安装/","text":"1.下载MySQL-pythonLinux平台可以访问：https://pypi.python.org/pypi/MySQL-python 从这里可选择适合您的平台的安装包 12345$ gunzip MySQL-python-1.2.2.tar.gz$ tar -xvf MySQL-python-1.2.2.tar$ cd MySQL-python-1.2.2$ python setup.py build #其实一般的python模块，下载下来都是执行下面的两步进行安装$ python setup.py install 2.安装过程中出现的问题1.ImportError: No module named setuptools怎么办？ 解决方法12345wget https://pypi.python.org/packages/source/s/setuptools/setuptools-18.0.1.tar.gz --no-check-certificatetar zxvf setuptools-18.0.1.tar.gzcd setuptools-18.0.1python setup.py buildpython setup.py install 2.EnvironmentError: mysql_config not found ？ 解决方法12yum install python-develyum install mysql-devel","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python函数之函数基础","date":"2017-04-16T04:47:25.824Z","path":"2017/04/16/python/python函数之函数基础/","text":"1.语法格式12345def &lt;fc_name&gt;(args1,args2, args3...argsN): &lt;statements&gt; [return &lt;value&gt;] #可选#return语句可以出现在函数主体中的任何地方，他表示函数调用的结束，并将结果返回到函数的调用者，如果一个函数没有返回值，那么默认是返回none，但是这个值往往被忽略掉 2.def语句是实时执行的在python中def语句实际上是一个可执行的语句，当他运行的时候，他创建了一个新的函数对象，并将其赋值给函数名这个变量，因为def是一个语句，所以可以出现在任何的地方，如下：123456789if test: def func(): ...else: def func(): ........func(): ... 因为函数名代表函数对象的引用，所以如下：12othername = funcothername() #调用函数 函数对象可以有附加的属性，如下：12345def func(): .....func() #调用函数func.attr = value #为函数对象添加属性 3.定义和调用1234567891011def times(x, y): return x * y#调用1tiems(2, 4)8#调用2times(&quot;Ni&quot;, 3) #因为函数的参数类型不是固定的，所以可以传递不同的参数，实现不同的功能，即多态&apos;NiNiNi&apos; 4.函数中的多态就像上面的实例，对于times函数中表达式x*y 的意义完全取决于x和y的对象类型，同样的函数，在一个实例下执行的是乘法，但是在另一个实例下执行的是赋值，这就是“*” 在针对被处理的对象进行了随机应变多态——就是一个操作的意义取决于被操作对象的类型多态举例1234567891011121314151617181920212223#功能：实现两个序列的交集def intersect(seq1, seq2): res = [] for x in seq1: if x in seq2: res.append(x) return res#重访多态##1s1 = &apos;spam&apos;s2 = &apos;sccm&apos;intersect(s1, s2)[&apos;s&apos;, &apos;m&apos;]##2x = intersect([1,2,3], (2,3))[2]##总结：intersect是多态的，也就是说，他支持多种类型，只要其参数支持扩展对象接口（对于intersect函数，这意味着第一个参数必须支持for循环，并且第二个函数支持成员测试，所有满足这两点的对象都能正常工作，与他们的类型无关，这就是多态）##如果传入了不支持这些接口的对象（例如：数字），python会自动检测出不匹配，并抛出异常 5.本地变量 所有在函数内部进行赋值的变量都是本地变量，所有本地变量在函数退出时，会消失，拿intersect函数为例： res 是明显在函数内部进行赋值的，所以是本地变量 参数也是通过赋值被传入的，所以seq1和seq2都是本地变量 for循环将元素赋值给了一个变量，所以变量x也是本地变量 6.函数总结 def是可执行的代码python中的函数是由一个新的语句编写的，即def，def是一个可执行的语句——函数并不存在，直到python运行了def后才存在。 def创建了一个对象并将其赋值给某一个变量名当python运行到def语句的时候，他将会生成一个新的函数对象并将其赋值给这个函数名 lambda创建了一个对象并将其作为一个结果返回 return语句将一个结果对象发送给调用者 yield向调用者发回一个结果对象 global声明了一个模块级的变量并被赋值 nonlocal 声明了一个将要被赋值的一个封闭的函数变量 函数是通过赋值传递的 参数、返回值、变量并不是声明","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"python之生成器(generator)详解(转)","date":"2017-04-16T04:47:25.822Z","path":"2017/04/16/python/python之生成器(generator)详解(转)/","text":"引文编程派前几天推送了一篇文章,叫“Python学习进阶路线(简版)”,生成器(generator)赫然在列.可是我不太会.不会怎么办?学咯.于是上网看了不少教程,又看了官方文档,学到了不少知识.在此,权且做个学习笔记,也与大家分享一下. 正文要理解generator,我们先从迭代(iteration)与迭代器(iterator)讲起.当然,本文的重点是generator,iteration与iterator的知识将点到即止.直接看generator 迭代是重复反馈过程的活动，其目的通常是为了接近并到达所需的目标或结果。每一次对过程的重复被称为一次“迭代”，而每一次迭代得到的结果会被用来作为下一次迭代的初始值。 以上是维基百科对迭代的定义.在python中,迭代通常是通过for … in …来完成的,而且只要是可迭代对象(iterable),都能进行迭代.这里简单讲下iterable与iterator: iterable是实现了iter()方法的对象.更确切的说,是container.iter()方法,该方法返回的是的一个iterator对象,因此iterable是你可以从其获得iterator的对象.使用iterable时,将一次性返回所有结果,都存放在内存中,并且这些值都能重复使用.以上说法严重错误!对于iterable,我们该关注的是,它是一个能一次返回一个成员的对象(iterable is an object capable of returning its members one at a time),一些iterable将所有值都存储在内存中,比如list,而另一些并不是这样,比如我们下面将讲到的iterator. iterator是实现了iterator.iter()和iterator.next()方法的对象.iterator.iter()方法返回的是iterator对象本身.根据官方的说法,正是这个方法,实现了for … in …语句.而iterator.next()是iterator区别于iterable的关键了,它允许我们显式地获取一个元素.当调用next()方法时,实际上产生了2个操作: 更新iterator状态,令其指向后一项,以便下一次调用 返回当前结果 如果你学过C++,它其实跟指针的概念很像(如果你还学过链表的话,或许能更好地理解). 正是next(),使得iterator能在每次被调用时,返回一个单一的值(有些教程里,称为一边循环,一边计算,我觉得这个说法不是太准确.但如果这样的说法有助于你的理解,我建议你就这样记),从而极大的节省了内存资源.另一点需要格外注意的是,iterator是消耗型的,即每一个值被使用过后,就消失了.因此,你可以将以上的操作2理解成pop.对iterator进行遍历之后,其就变成了一个空的容器了,但不等于None哦.因此,若要重复使用iterator,利用list()方法将其结果保存起来是一个不错的选择. 我们通过代码来感受一下.12345678910111213141516171819202122232425262728293031323334353637&gt;&gt;&gt; from collections import Iterable, Iterator&gt;&gt;&gt; a = [1,2,3] # 众所周知,list是一个iterable&gt;&gt;&gt; b = iter(a) # 通过iter()方法,得到iterator,iter()实际上调用了__iter__(),此后不再多说&gt;&gt;&gt; isinstance(a, Iterable)True&gt;&gt;&gt; isinstance(a, Iterator)False&gt;&gt;&gt; isinstance(b, Iterable)True&gt;&gt;&gt; isinstance(b, Iterator)True# 可见,itertor一定是iterable,但iterable不一定是itertor # iterator是消耗型的,用一次少一次.对iterator进行变量,iterator就空了!&gt;&gt;&gt; c = list(b)&gt;&gt;&gt; c[1, 2, 3]&gt;&gt;&gt; d = list(b)&gt;&gt;&gt; d[] # 空的iterator并不等于None.&gt;&gt;&gt; if b:... print(1)...1&gt;&gt;&gt; if b == None:... print(1)... # 再来感受一下next()&gt;&gt;&gt; e = iter(a)&gt;&gt;&gt; next(e) #next()实际调用了__next__()方法,此后不再多说1&gt;&gt;&gt; next(e)2 既然提到了for … in …语句,我们再来简单讲下其工作原理吧,或许能帮助理解以上所讲的内容. 123&gt;&gt;&gt; x = [1, 2, 3]&gt;&gt;&gt; for i in x:... ... 我们对一个iterable用for … in …进行迭代时,实际是先通过调用iter()方法得到一个iterator,假设叫做X.然后循环地调用X的next()方法取得每一次的值,直到iterator为空,返回的StopIteration作为循环结束的标志.for … in … 会自动处理StopIteration异常,从而避免了抛出异常而使程序中断.如图所示 磨刀不误砍柴工,有了前面的知识,我们再来理解generator与yield将会事半功倍. 首先先理清几个概念: 123generator: A function which returns a generator iterator. It looks like a normal function except that it contains yield expressions for producing a series of values usable in a for-loop or that can be retrieved one at a time with the next() function.generator iterator: An object created by a generator funcion.generator expression: An expression that returns an iterator. 以上的定义均来自python官方文档.可见,我们常说的生成器,就是带有yield的函数,而generator iterator则是generator function的返回值,即一个generator对象,而形如(elem for elem in [1, 2, 3])的表达式,称为generator expression,实际使用与generator无异. 12345678910111213&gt;&gt;&gt; a = (elem for elem in [1, 2, 3])&gt;&gt;&gt; a&lt;generator object &lt;genexpr&gt; at 0x7f0d23888048&gt;&gt;&gt;&gt; def fib():... a, b = 0, 1... while True:... yield b... a, b = b, a + b...&gt;&gt;&gt; fib&lt;function fib at 0x7f0d238796a8&gt;&gt;&gt;&gt; b = fib()&lt;generator object fib at 0x7f0d20bbfea0&gt; 其实说白了,generator就是iterator的一种,以更优雅的方式实现的iterator.官方的说法是:1Python’s generators provide a convenient way to implement the iterator protocol. 你完全可以像使用iterator一样使用generator,当然除了定义.定义一个iterator,你需要分别实现iter()方法和next()方法,但generator只需要一个小小的yield(好吧,generator expression的使用比较简单,就不展开讲了.) 前文讲到iterator通过next()方法实现了每次调用,返回一个单一值的功能.而yield就是实现generator的next()方法的关键!先来看一个最简单的例子: 123456789101112131415161718192021222324&gt;&gt;&gt; def g():... print(&quot;1 is&quot;)... yield 1... print(&quot;2 is&quot;)... yield 2... print(&quot;3 is&quot;)... yield 3...&gt;&gt;&gt; z = g()&gt;&gt;&gt; z&lt;generator object g at 0x7f0d2387c8b8&gt;&gt;&gt;&gt; next(z)1 is1&gt;&gt;&gt; next(z)2 is2&gt;&gt;&gt; next(z)3 is3&gt;&gt;&gt; next(z)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;StopIteration 第一次调用next()方法时,函数似乎执行到yield 1,就暂停了.然后再次调用next()时,函数从yield 1之后开始执行的,并再次暂停.第三次调用next(),从第二次暂停的地方开始执行.第四次,抛出StopIteration异常. 事实上,generator确实在遇到yield之后暂停了,确切点说,是先返回了yield表达式的值,再暂停的.当再次调用next()时,从先前暂停的地方开始执行,直到遇到下一个yield.这与上文介绍的对iterator调用next()方法,执行原理一般无二. 有些教程里说generator保存的是算法,而我觉得用中断服务子程序来描述generator或许能更好理解,这样你就能将yield理解成一个中断服务子程序的断点,没错,是中断服务子程序的断点.我们每次对一个generator对象调用next()时,函数内部代码执行到”断点”yield,然后返回这一部分的结果,并保存上下文环境,”中断”返回. 怎么样,是不是瞬间就明白了yield的用法? 我们再来看另一段代码. 12345678910111213&gt;&gt;&gt; def gen():... while True:... s = yield... print(s)...&gt;&gt;&gt; g = gen()&gt;&gt;&gt; g.send(&quot;kissg&quot;)Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: can&apos;t send non-None value to a just-started generator&gt;&gt;&gt; next(g)&gt;&gt;&gt; g.send(&quot;kissg&quot;)kissg 我正是看到这个形式的generator,懵了,才想要深入学习generator与yield的.结合以上的知识,我再告诉你,generator其实有第2种调用方法(恢复执行),即通过send(value)方法将value作为yield表达式的当前值,你可以用该值再对其他变量进行赋值,这一段代码就很好理解了.当我们调用send(value)方法时,generator正由于yield的缘故被暂停了.此时,send(value)方法传入的值作为yield表达式的值,函数中又将该值赋给了变量s,然后print函数打印s,循环再遇到yield,暂停返回. 调用send(value)时要注意,要确保,generator是在yield处被暂停了,如此才能向yield表达式传值,否则将会报错(如上所示),可通过next()方法或send(None)使generator执行到yield. 再来看一段yield更复杂的用法,或许能加深你对generator的next()与send(value)的理解. 123456789101112131415161718&gt;&gt;&gt; def echo(value=None):... while 1:... value = (yield value)... print(&quot;The value is&quot;, value)... if value:... value += 1...&gt;&gt;&gt; g = echo(1)&gt;&gt;&gt; next(g)1&gt;&gt;&gt; g.send(2)The value is 23&gt;&gt;&gt; g.send(5)The value is 56&gt;&gt;&gt; next(g)The value is None 上述代码既有yield value的形式,又有value = yield形式,看起来有点复杂.但以yield分离代码进行解读,就不太难了.第一次调用next()方法,执行到yield value表达式,保存上下文环境暂停返回1.第二次调用send(value)方法,从value = yield开始,打印,再次遇到yield value暂停返回.后续的调用send(value)或next()都不外如是. 但是,这里就引出了另一个问题,yield作为一个暂停恢复的点,代码从yield处恢复,又在下一个yield处暂停.可见,在一次next()(非首次)或send(value)调用过程中,实际上存在2个yield,一个作为恢复点的yield与一个作为暂停点的yield.因此,也就有2个yield表达式.send(value)方法是将值传给恢复点yield;调用next()表达式的值时,其恢复点yield的值总是为None,而将暂停点的yield表达式的值返回.为方便记忆,你可以将此处的恢复点记作当前的(current),而将暂停点记作下一次的(next),这样就与next()方法匹配起来啦. generator还实现了另外两个方法throw(type[, value[, traceback]])与close().前者用于抛出异常,后者用于关闭generator.不过这2个方法似乎很少被直接用到,本文就不再多说了,有兴趣的同学请看这里 小结 可迭代对象(Iterable)是实现了iter()方法的对象,通过调用iter()方法可以获得一个迭代器(Iterator) 迭代器(Iterator)是实现了iter()和next()的对象 for … in …的迭代,实际是将可迭代对象转换成迭代器,再重复调用next()方法实现的 生成器(generator)是一个特殊的迭代器,它的实现更简单优雅. yield是生成器实现next()方法的关键.它作为生成器执行的暂停恢复点,可以对yield表达式进行赋值,也可以将yield表达式的值返回. 转自:python之生成器详解","tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"订阅和发布相关命令及实例","date":"2017-04-16T04:47:25.812Z","path":"2017/04/16/nosql/redis/订阅和发布相关命令及实例/","text":"1.订阅和发布的概念 “发布/订阅”模式中包含两种角色，分别是发布者和订阅者。 订阅者可以订阅一个或若干个频道(channel) 发布者可以向指定的频道发送消息，所有订阅此频道的订阅者都会收到此消息。 2.相关的命令 命令 描述 PUBLISH channel msg 发布消息. 接收到这条消息的订阅者数量. 发出去的消息不会被持久化，也就是说当有客户端订阅channel后只能收到后续发布到该频道的消息，之前发送的就收不到了 SUBSCRIBE channel [channel …] 订阅频道，可以同时订阅多个频道 UNSUBSCRIBE [channel …] 取消订阅指定的频道, 如果不指定频道，则会取消订阅所有频道 PSUBSCRIBE pattern [pattern …] 订阅一个或多个符合给定模式的频道 PUNSUBSCRIBE [pattern [pattern …]] 退订指定的规则, 如果没有参数则会退订所有规则 PUBSUB subcommand [argument [argument …]] 查看订阅与发布系统状态 3.实例1234567891011121314151617181920212223242526272829303132333435#发布PUBLISH channel message127.0.0.1:6379&gt; publish it.message &quot;success submi cys......&quot;#订阅SUBSCRIBE channel [channel ...]127.0.0.1:6379&gt; SUBSCRIBE it.message#模糊订阅PSUBSCRIBE pattern [pattern ...]支持的模式(patterns)有:h?llo subscribes to hello, hallo and hxlloh*llo subscribes to hllo and heeeelloh[ae]llo subscribes to hello and hallo, but not hillo127.0.0.1:6379&gt; PSUBSCRIBE it.*#查看订阅与发布系统状态127.0.0.1:6379&gt; PUBSUB channels1) &quot;it.messages&quot;2) &quot;it.message&quot;&apos;可以模式匹配查询&apos;127.0.0.1:6379&gt; pubsub channels &quot;it.mess*&quot;1) &quot;it.messages&quot;2) &quot;it.message&quot;127.0.0.1:6379&gt; &apos;查看频道的订阅数量&apos;127.0.0.1:6379&gt; pubsub numsub it.message1) &quot;it.message&quot;2) (integer) 2","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"订阅与发布模型 — Redis 设计与实现（转）","date":"2017-04-16T04:47:25.811Z","path":"2017/04/16/nosql/redis/订阅与发布模型 — Redis 设计与实现（转）/","text":"Redis 通过 PUBLISH 、 SUBSCRIBE 等命令实现了订阅与发布模式， 这个功能提供两种信息机制， 分别是订阅/发布到频道和订阅/发布到模式， 下文先讨论订阅/发布到频道的实现， 再讨论订阅/发布到模式的实现。 1.频道的订阅与信息发送Redis 的 SUBSCRIBE 命令可以让客户端订阅任意数量的频道， 每当有新信息发送到被订阅的频道时， 信息就会被发送给所有订阅指定频道的客户端。 作为例子， 下图展示了频道 channel1 ， 以及订阅这个频道的三个客户端 —— client2 、 client5 和 client1 之间的关系： 当有新消息通过 PUBLISH 命令发送给频道 channel1 时， 这个消息就会被发送给订阅它的三个客户端： 在后面的内容中， 我们将探讨 SUBSCRIBE 和 PUBLISH 命令的实现， 以及这套订阅与发布机制的运作原理。 2.订阅频道每个 Redis 服务器进程都维持着一个表示服务器状态的 redis.h/redisServer 结构， 结构的 pubsub_channels 属性是一个字典， 这个字典就用于保存订阅频道的信息：12345struct redisServer &#123; // ... dict *pubsub_channels; // ...&#125;; 其中，字典的键为正在被订阅的频道， 而字典的值则是一个链表， 链表中保存了所有订阅这个频道的客户端。 比如说，在下图展示的这个 pubsub_channels 示例中， client2 、 client5 和 client1 就订阅了 channel1 ， 而其他频道也分别被别的客户端所订阅： 当客户端调用 SUBSCRIBE 命令时， 程序就将客户端和要订阅的频道在 pubsub_channels 字典中关联起来。 举个例子，如果客户端 client10086 执行命令 SUBSCRIBE channel1 channel2 channel3 ，那么前面展示的 pubsub_channels 将变成下面这个样子： SUBSCRIBE 命令的行为可以用伪代码表示如下：1234567def SUBSCRIBE(client, channels): # 遍历所有输入频道 for channel in channels: # 将客户端添加到链表的末尾 redisServer.pubsub_channels[channel].append(client) 通过 pubsub_channels 字典， 程序只要检查某个频道是否为字典的键， 就可以知道该频道是否正在被客户端订阅； 只要取出某个键的值， 就可以得到所有订阅该频道的客户端的信息。 3.发送信息到频道了解了 pubsub_channels 字典的结构之后， 解释 PUBLISH 命令的实现就非常简单了： 当调用 PUBLISH channel message 命令， 程序首先根据 channel 定位到字典的键， 然后将信息发送给字典值链表中的所有客户端。 比如说，对于以下这个 pubsub_channels 实例， 如果某个客户端执行命令 PUBLISH channel1 “hello moto” ，那么 client2 、 client5 和 client1 三个客户端都将接收到 “hello moto” 信息： PUBLISH 命令的实现可以用以下伪代码来描述：1234567def PUBLISH(channel, message): # 遍历所有订阅频道 channel 的客户端 for client in server.pubsub_channels[channel]: # 将信息发送给它们 send_message(client, message) 4.退订频道使用 UNSUBSCRIBE 命令可以退订指定的频道， 这个命令执行的是订阅的反操作： 它从 pubsub_channels 字典的给定频道（键）中， 删除关于当前客户端的信息， 这样被退订频道的信息就不会再发送给这个客户端。 5.模式的订阅与信息发送当使用 PUBLISH 命令发送信息到某个频道时， 不仅所有订阅该频道的客户端会收到信息， 如果有某个/某些模式和这个频道匹配的话， 那么所有订阅这个/这些频道的客户端也同样会收到信息。 下图展示了一个带有频道和模式的例子， 其中 tweet.shop.* 模式匹配了 tweet.shop.kindle 频道和 tweet.shop.ipad 频道， 并且有不同的客户端分别订阅它们三个： 当有信息发送到 tweet.shop.kindle 频道时， 信息除了发送给 clientX 和 clientY 之外， 还会发送给订阅 tweet.shop.* 模式的 client123 和 client256 ： 另一方面， 如果接收到信息的是频道 tweet.shop.ipad ， 那么 client123 和 client256 同样会收到信息： 6.订阅模式redisServer.pubsub_patterns 属性是一个链表，链表中保存着所有和模式相关的信息： 12345struct redisServer &#123; // ... list *pubsub_patterns; // ...&#125;; 链表中的每个节点都包含一个 redis.h/pubsubPattern 结构： 1234typedef struct pubsubPattern &#123; redisClient *client; robj *pattern;&#125; pubsubPattern; client 属性保存着订阅模式的客户端，而 pattern 属性则保存着被订阅的模式。 每当调用 PSUBSCRIBE 命令订阅一个模式时， 程序就创建一个包含客户端信息和被订阅模式的 pubsubPattern 结构， 并将该结构添加到 redisServer.pubsub_patterns 链表中。 作为例子，下图展示了一个包含两个模式的 pubsub_patterns 链表， 其中 client123 和 client256 都正在订阅 tweet.shop.* 模式： 如果这时客户端 client10086 执行 PSUBSCRIBE broadcast.list.* ， 那么 pubsub_patterns 链表将被更新成这样： 通过遍历整个 pubsub_patterns 链表，程序可以检查所有正在被订阅的模式，以及订阅这些模式的客户端。 7.发送信息到模式发送信息到模式的工作也是由 PUBLISH 命令进行的， 在前面讲解频道的时候， 我们给出了这样一段伪代码， 说它定义了 PUBLISH 命令的行为： 1234567def PUBLISH(channel, message): # 遍历所有订阅频道 channel 的客户端 for client in server.pubsub_channels[channel]: # 将信息发送给它们 send_message(client, message) 但是，这段伪代码并没有完整描述 PUBLISH 命令的行为， 因为 PUBLISH 除了将 message 发送到所有订阅 channel 的客户端之外， 它还会将 channel 和 pubsub_patterns 中的模式进行对比， 如果 channel 和某个模式匹配的话， 那么也将 message 发送到订阅那个模式的客户端。 完整描述 PUBLISH 功能的伪代码定于如下：12345678910111213141516def PUBLISH(channel, message): # 遍历所有订阅频道 channel 的客户端 for client in server.pubsub_channels[channel]: # 将信息发送给它们 send_message(client, message) # 取出所有模式，以及订阅模式的客户端 for pattern, client in server.pubsub_patterns: # 如果 channel 和模式匹配 if match(channel, pattern): # 那么也将信息发给订阅这个模式的客户端 send_message(client, message) 举个例子，如果 Redis 服务器的 pubsub_patterns 状态如下： 那么当某个客户端发送信息 “Amazon Kindle, $69.” 到 tweet.shop.kindle 频道时， 除了所有订阅了 tweet.shop.kindle 频道的客户端会收到信息之外， 客户端 client123 和 client256 也同样会收到信息， 因为这两个客户端订阅的 tweet.shop.* 模式和 tweet.shop.kindle 频道匹配。 8.退订模式使用 PUNSUBSCRIBE 命令可以退订指定的模式， 这个命令执行的是订阅模式的反操作： 程序会删除 redisServer.pubsub_patterns 链表中， 所有和被退订模式相关联的 pubsubPattern 结构， 这样客户端就不会再收到和模式相匹配的频道发来的信息。 9.小结 订阅信息由服务器进程维持的 redisServer.pubsub_channels 字典保存，字典的键为被订阅的频道，字典的值为订阅频道的所有客户端。 当有新消息发送到频道时，程序遍历频道（键）所对应的（值）所有客户端，然后将消息发送到所有订阅频道的客户端上。 订阅模式的信息由服务器进程维持的 redisServer.pubsub_patterns 链表保存，链表的每个节点都保存着一个 pubsubPattern 结构，结构中保存着被订阅的模式，以及订阅该模式的客户端。程序通过遍历链表来查找某个频道是否和某个模式匹配。 当有新消息发送到频道时，除了订阅频道的客户端会收到消息之外，所有订阅了匹配频道的模式的客户端，也同样会收到消息。 退订频道和退订模式分别是订阅频道和订阅模式的反操作。 转自:订阅与发布","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"浅谈redis数据库的键值设计(转)","date":"2017-04-16T04:47:25.809Z","path":"2017/04/16/nosql/redis/浅谈redis数据库的键值设计/","text":"频繁的查询时需要在频繁的字段上创建key 丰富的数据结构使得redis的设计非常的有趣。不像关系型数据库那样，DEV和DBA需要深度沟通，review每行sql语句，也不像memcached那样，不需要DBA的参与。redis的DBA需要熟悉数据结构，并能了解使用场景。 下面举一些常见适合kv数据库的例子来谈谈键值的设计，并与关系型数据库做一个对比，发现关系型的不足之处。 用户登录系统记录用户登录信息的一个系统， 我们简化业务后只留下一张表。关系型数据库的设计12345678mysql&gt; select * from login;+———+—————-+————-+———————+| user_id | name | login_times | last_login_time |+———+—————-+————-+———————+| 1 | ken thompson | 5 | 2011-01-01 00:00:00 || 2 | dennis ritchie | 1 | 2011-02-01 00:00:00 || 3 | Joe Armstrong | 2 | 2011-03-01 00:00:00 |+———+—————-+————-+———————+ user_id表的主键，name表示用户名，login_times表示该用户的登录次数，每次用户登录后，login_times会自增，而last_login_time更新为当前时间。 REDIS的设计关系型数据转化为KV数据库，我的方法如下：key 表名：主键值：列名value 列值 一般使用冒号做分割符，这是不成文的规矩。比如在php-admin for redis系统里，就是默认以冒号分割，于是user:1 user:2等key会分成一组。于是以上的关系数据转化成kv数据后记录如下： 1234567891011Set login:1:login_times 5Set login:2:login_times 1Set login:3:login_times 2 Set login:1:last_login_time 2011-1-1Set login:2:last_login_time 2011-2-1Set login:3:last_login_time 2011-3-1 set login:1:name ”ken thompson“set login:2:name “dennis ritchie”set login:3:name ”Joe Armstrong“ 这样在已知主键的情况下，通过get、set就可以获得或者修改用户的登录次数和最后登录时间和姓名。一般用户是无法知道自己的id的，只知道自己的用户名，所以还必须有一个从name到id的映射关系，这里的设计与上面的有所不同。123set &quot;login:ken thompson:id&quot; 1set &quot;login:dennis ritchie:id&quot; 2set &quot;login: Joe Armstrong:id&quot; 3 这样每次用户登录的时候业务逻辑如下（python版），r是redis对象，name是已经获知的用户名。 123#获得用户的iduid = r.get(&quot;login:%s:id&quot; % name)#%s 会被name替换,最后获取的是name映射的uid 12#自增用户的登录次数ret = r.incr(&quot;login:%s:login_times&quot; % uid) 12#更新该用户的最后登录时间ret = r.set(&quot;login:%s:last_login_time&quot; % uid, datetime.datetime.now()) 如果需求仅仅是已知id，更新或者获取某个用户的最后登录时间，登录次数，关系型和kv数据库无啥区别。一个通过btree pk，一个通过hash，效果都很好。 假设有如下需求，查找最近登录的N个用户。开发人员看看，还是比较简单的，一个sql搞定。1select * from login order by last_login_time desc limit N DBA了解需求后，考虑到以后表如果比较大，所以在last_login_time上建个索引。执行计划从索引leafblock 的最右边开始访问N条记录，再回表N次，效果很好。 过了两天，又来一个需求，需要知道登录次数最多的人是谁。同样的关系型如何处理？DEV说简单1select * from login order by login_times desc limit N DBA一看，又要在login_time上建立一个索引。有没有觉得有点问题呢，表上每个字段上都有素引。 关系型数据库的数据存储的的不灵活是问题的源头，数据仅有一种储存方法，那就是按行排列的堆表。统一的数据结构意味着你必须使用索引来改变sql的访问路径来快速访问某个列的，而访问路径的增加又意味着你必须使用统计信息来辅助，于是一大堆的问题就出现了。 没有索引，没有统计计划，没有执行计划，这就是kv数据库。 redis里如何满足以上的需求呢？ 对于求最新的N条数据的需求，链表的后进后出的特点非常适合。我们在上面的登录代码之后添加一段代码，维护一个登录的链表，控制他的长度，使得里面永远保存的是最近的N个登录用户。1234#把当前登录人添加到链表里ret = r.lpush(&quot;login:last_login_times&quot;, uid)#保持链表只有N位ret = redis.ltrim(&quot;login:last_login_times&quot;, 0, N-1) 这样需要获得最新登录人的id，如下的代码即可1last_login_list = r.lrange(&quot;login:last_login_times&quot;, 0, N-1) 另外，求登录次数最多的人，对于排序，积分榜这类需求，sorted set非常的适合，我们把用户和登录次数统一存储在一个sorted set里。123zadd login:login_times 5 1zadd login:login_times 1 2zadd login:login_times 2 3 这样假如某个用户登录，额外维护一个sorted set，代码如此12#对该用户的登录次数自增1ret = r.zincrby(&quot;login:login_times&quot;, 1, uid) 那么如何获得登录次数最多的用户呢，逆序排列取的排名第N的用户即可1ret = r.zrevrange(&quot;login:login_times&quot;, 0, N-1) 可以看出，DEV需要添加2行代码，而DBA不需要考虑索引什么的。 TAG系统tag在互联网应用里尤其多见，如果以传统的关系型数据库来设计有点不伦不类。我们以查找书的例子来看看redis在这方面的优势。 关系型数据库的设计 两张表，一张book的明细，一张tag表，表示每本的tag，一本书存在多个tag。123456789101112131415161718mysql&gt; select * from book;+------+-------------------------------+----------------+| id | name | author |+------+-------------------------------+----------------+| 1 | The Ruby Programming Language | Mark Pilgrim || 2 | Ruby on rail | David Flanagan || 3 | Programming Erlang | Joe Armstrong |+------+-------------------------------+----------------+ mysql&gt; select * from tag;+---------+---------+| tagname | book_id |+---------+---------+| ruby | 1 || ruby | 2 || web | 2 || erlang | 3 |+---------+---------+ 假如有如此需求，查找即是ruby又是web方面的书籍，如果以关系型数据库会怎么处理？12select b.name, b.author from tag t1, tag t2, book bwhere t1.tagname = &apos;web&apos; and t2.tagname = &apos;ruby&apos; and t1.book_id = t2.book_id and b.id = t1.book_id tag表自关联2次再与book关联，这个sql还是比较复杂的，如果要求即ruby，但不是web方面的书籍呢？ 关系型数据其实并不太适合这些集合操作。 REDIS的设计 首先book的数据肯定要存储的，和上面一样。1234567set book:1:name ”The Ruby Programming Language”Set book:2:name ”Ruby on rail”Set book:3:name ”Programming Erlang” set book:1:author ”Mark Pilgrim”Set book:2:author ”David Flanagan”Set book:3:author ”Joe Armstrong” tag表我们使用集合来存储数据，因为集合擅长求交集、并集1234sadd tag:ruby 1sadd tag:ruby 2sadd tag:web 2sadd tag:erlang 3 那么，即属于ruby又属于web的书？1inter_list = redis.sinter(&quot;tag.web&quot;, &quot;tag:ruby&quot;) 即属于ruby，但不属于web的书？1inter_list = redis.sdiff(&quot;tag.ruby&quot;, &quot;tag:web&quot;) 属于ruby和属于web的书的合集？1inter_list = redis.sunion(&quot;tag.ruby&quot;, &quot;tag:web&quot;) 简单到不行阿。 从以上2个例子可以看出在某些场景里，关系型数据库是不太适合的，你可能能够设计出满足需求的系统，但总是感觉的怪怪的，有种生搬硬套的感觉。 尤其登录系统这个例子，频繁的为业务建立索引。放在一个复杂的系统里，ddl（创建索引）有可能改变执行计划。导致其它的sql采用不同的执行计划，业务复杂的老系统，这个问题是很难预估的，sql千奇百怪。要求DBA对这个系统里所有的sql都了解，这点太难了。这个问题在oracle里尤其严重，每个DBA估计都碰到过。对于MySQL这类系统，ddl又不方便（虽然现在有online ddl的方法）。碰到大表，DBA凌晨爬起来在业务低峰期操作，这事我没少干过。而这种需求放到redis里就很好处理，DBA仅仅对容量进行预估即可。 未来的OLTP系统应该是kv和关系型的紧密结合。 转自","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"sentinel运维监控","date":"2017-04-16T04:47:25.808Z","path":"2017/04/16/nosql/redis/sentinel运维监控/","text":"运行时手动修改master-slave 修改一台slave为master 执行slaveof no one #执行该机不是一台slave了redis&gt; slaveof no one 修改readonly no #因为要转为master，所以要改成可以写的redis&gt; config set slave-read-only no 其他的slave再指向这台机器 redis&gt; slaveof IP PORT 自动切换自动切换服务器sentinel.conf文件1234567891011121314151617sentinel monitor def_master 127.0.0.1 6379 2 sentinel auth-pass def_master 012_345^678-90 ##master被当前sentinel实例认定为“失效”的间隔时间 ##如果当前sentinel与master直接的通讯中，在指定时间内没有响应或者响应错误代码，那么 ##当前sentinel就认为master失效(SDOWN，“主观”失效) ##&lt;mastername&gt; &lt;millseconds&gt; ##默认为30秒 sentinel down-after-milliseconds def_master 30000 ##当前sentinel实例是否允许实施“failover”(故障转移) ##no表示当前sentinel为“观察者”(只参与&quot;投票&quot;.不参与实施failover)， ##全局中至少有一个为yes sentinel can-failover def_master yes ##sentinel notification-script mymaster /var/redis/notify.sh 参看:redis第九步（sentinel监控主从服务器）","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"sentinel(redis高可用解决方案)","date":"2017-04-16T04:47:25.807Z","path":"2017/04/16/nosql/redis/sentinel(redis高可用解决方案)/","text":"由一个或多个sentinel实例组成的sentinel系统可以监视任意多个主服务器，以及这些主服务器属下的所有从服务器；并在被监视的主服务器进入下线状态时，自动将下线主服务器属下的某个从服务器升级为新的主服务器，然后由新的主服务器代替已下线的主服务器继续处理命令请求。如图所示： 它是如何工作的？ 启动 12redis-sentinel sentinel.conf 或者redis-server sentinel.conf --sentinel 初始化服务器 sentinel本质上只是一个运行在特殊模式下的redis服务器，但它不使用数据库，所以并不会载入RDB文件或者AOF文件来还原数据库状态。初始化过程： 使用sentinel专用代码初始化sentinel状态 创建连向主服务器的网络连接 连接建立之后，sentinel将成为主服务器的客户端，它可以向主服务器发送命令，并从命令回复中获取相关的信息。对于每个被sentinel监视的主服务器来说，sentinel会创建两个连向主服务器的异步网络连接： 命令连接，向主服务发送命令，并接收命令回复 订阅连接，订阅主服务器的sentinel：hello频道 注：如果发现主服务器下有从服务器，也会为从服务器创建命令连接和订阅连接。而sentinel与sentinel之间则只创建命令连接。 获取服务器信息 sentinel默认会以每十秒一次的频率，通过命令连接向被监视的主从服务器发送INFO命令，并通过分析INFO命令的回复来获取主服务器的当前信息。 发送和接收消息 在默认情况下，sentinel会以每两秒一次的频率，通过命令连接向所有被监视的主服务器和从服务器发送以下格式的命令：1PUBLISH __sentinel__:hello &quot;&lt;s_ip&gt;,&lt;s_port&gt;,&lt;s_runid&gt;,&lt;s_epoch&gt;,&lt;m_name&gt;,&lt;m_ip&gt;,&lt;m_port&gt;,&lt;m_epoch&gt;&quot; 当sentinel与一个主服务器或者从服务器建立起订阅连接之后，sentinel就会通过订阅连接，向服务器发送以下命令： 1SUBSCRIBE __sentinel__:hello sentinel对sentinel:hello频道的订阅会一直持续到sentinel与服务器的连接断开为止。 也就是说，对于每个与sentinel连接的服务器，既通过命令连接向服务器的sentinel:hello频道发送信息，又通过订阅连接从服务器的sentinel:hello频道接收信息。对于监视同一个服务器的多个sentinel来说，一个sentinel发送的信息会被其他sentinel接收到，这些信息会被用于更新其他sentinel对发送信息sentinel的认知，也会被用于更新其他sentinel对被监视服务器的认知。 检测服务器状态 检测主观下线状态 sentinel以每秒一次的频率向实例(包括主服务器、从服务器、其他sentinel)发送PING命令，并根据实例对PING命令的回复来判断实例是否在线，当一个实例在指定的时长中连续向sentinel发送无效回复时，sentinel会将这个实例判断为主观下线。 检测客观下线状态，进行故障转移 当sentinel将一个主服务器判断为主观下线时，它会向同样监视这个主服务器的其他sentinel进行询问，看它们是否同意这个主服务器已经进入主观下线状态；当sentinel收集到足够多的主观下线投票之后，它会将主服务器判断为客观下线，并选取一个领头sentinel发起一次针对主服务器的故障转移操作。注：更多细节请查阅&lt;&gt; 第16章内容… 转自:sentinel(redis高可用解决方案) 可以参考:Redis Sentinel机制与用法（一）Redis Sentinel 机制与用法（二）","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis运维常用命令","date":"2017-04-16T04:47:25.806Z","path":"2017/04/16/nosql/redis/redis运维常用命令/","text":"1.启动1.1.启动redis12345678910111213$ redis-server redis.conf 常见选项： ./redis-server (run the server with default conf) ./redis-server /etc/redis/6379.conf ./redis-server --port 7777 ./redis-server --port 7777 --slaveof 127.0.0.1 8888 ./redis-server /etc/myredis.conf --loglevel verbose 1.2 启动redis-sentinel12345./redis-server /etc/sentinel.conf –sentinel ./redis-sentinel /etc/sentinel.conf 部署后可以使用sstart对redis 和sentinel进行拉起，使用sctl进行supervisorctl的控制。（两个alias） 2.停止1234127.0.0.1:6379&gt; shutdownnot connected&gt; sentinel方法一样，只是需要执行sentinel的连接端口 3.检测服务是否可用12345678910127.0.0.1:6379&gt; pingPONG127.0.0.1:6379&gt;&apos;返回PONG说明正常&apos; &apos;如果将服务shutdown&apos;127.0.0.1:6379&gt; pingCould not connect to Redis at 127.0.0.1:6379: Connection refusednot connected&gt; 4.查看redis数据库统计信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191Mrds:6379&gt; info # Server redis_version:2.8.19 ###redis版本号 redis_git_sha1:00000000 ###git SHA1 redis_git_dirty:0 ###git dirty flag redis_build_id:78796c63e58b72dc redis_mode:standalone ###redis运行模式 os:Linux 2.6.32-431.el6.x86_64 x86_64 ###os版本号 arch_bits:64 ###64位架构 multiplexing_api:epoll ###调用epoll算法 gcc_version:4.4.7 ###gcc版本号 process_id:25899 ###服务器进程PID run_id:eae356ac1098c13b68f2b00fd7e1c9f93b1c6a2c ###Redis的随机标识符(用于sentinel和集群) tcp_port:6379 ###Redis监听的端口号 uptime_in_seconds:6419 ###Redis运行时长(s为单位) uptime_in_days:0 ###Redis运行时长(天为单位) hz:10 lru_clock:10737922 ###以分钟为单位的自增时钟,用于LRU管理 config_file:/etc/redis/redis.conf ###redis配置文件 # Clients connected_clients:1 ###已连接客户端的数量（不包括通过从属服务器连接的客户端） client_longest_output_list:0 ###当前连接的客户端中最长的输出列表 client_biggest_input_buf:0 ###当前连接的客户端中最大的输出缓存 blocked_clients:0 ###正在等待阻塞命令（BLPOP、BRPOP、BRPOPLPUSH）的客户端的数量 需监控 # Memory used_memory:2281560 ###由 Redis 分配器分配的内存总量，以字节（byte）为单位 used_memory_human:2.18M ###以更友好的格式输出redis占用的内存 used_memory_rss:2699264 ###从操作系统的角度，返回 Redis 已分配的内存总量（俗称常驻集大小）。这个值和 top 、 ps 等命令的输出一致 used_memory_peak:22141272 ### Redis 的内存消耗峰值（以字节为单位） used_memory_peak_human:21.12M ###以更友好的格式输出redis峰值内存占用 used_memory_lua:35840 ###LUA引擎所使用的内存大小 mem_fragmentation_ratio:1.18 ###used_memory_rss 和 used_memory 之间的比率 mem_allocator:jemalloc-3.6.0 ###在理想情况下， used_memory_rss 的值应该只比 used_memory 稍微高一点儿。当 rss &gt; used ，且两者的值相差较大时，表示存在（内部或外部的）内存碎片。内存碎片的比率可以通过 mem_fragmentation_ratio 的值看出。 当 used &gt; rss 时，表示 Redis 的部分内存被操作系统换出到交换空间了，在这种情况下，操作可能会产生明显的延迟。 # Persistence loading:0 ###记录服务器是否正在载入持久化文件 rdb_changes_since_last_save:0 ###距离最近一次成功创建持久化文件之后，经过了多少秒 rdb_bgsave_in_progress:0 ###记录了服务器是否正在创建 RDB 文件 rdb_last_save_time:1420023749 ###最近一次成功创建 RDB 文件的 UNIX 时间戳 rdb_last_bgsave_status:ok ###最近一次创建 RDB 文件的结果是成功还是失败 rdb_last_bgsave_time_sec:0 ###最近一次创建 RDB 文件耗费的秒数 rdb_current_bgsave_time_sec:-1 ###如果服务器正在创建 RDB 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数 aof_enabled:1 ###AOF 是否处于打开状态 aof_rewrite_in_progress:0 ###服务器是否正在创建 AOF 文件 aof_rewrite_scheduled:0 ###RDB 文件创建完毕之后，是否需要执行预约的 AOF 重写操作 aof_last_rewrite_time_sec:-1 ###最近一次创建 AOF 文件耗费的时长 aof_current_rewrite_time_sec:-1 ###如果服务器正在创建 AOF 文件，那么这个域记录的就是当前的创建操作已经耗费的秒数 aof_last_bgrewrite_status:ok ###最近一次创建 AOF 文件的结果是成功还是失败 aof_last_write_status:ok aof_current_size:176265 ###AOF 文件目前的大小 aof_base_size:176265 ###服务器启动时或者 AOF 重写最近一次执行之后，AOF 文件的大小 aof_pending_rewrite:0 ###是否有 AOF 重写操作在等待 RDB 文件创建完毕之后执行 aof_buffer_length:0 ###AOF 缓冲区的大小 aof_rewrite_buffer_length:0 ###AOF 重写缓冲区的大小 aof_pending_bio_fsync:0 ###后台 I/O 队列里面，等待执行的 fsync 调用数量 aof_delayed_fsync:0 ###被延迟的 fsync 调用数量 # Stats total_connections_received:8466 ###服务器已接受的连接请求数量 total_commands_processed:900668 ###服务器已执行的命令数量 instantaneous_ops_per_sec:1 ###服务器每秒钟执行的命令数量 total_net_input_bytes:82724170 total_net_output_bytes:39509080 instantaneous_input_kbps:0.07 instantaneous_output_kbps:0.02 rejected_connections:0 ###因为最大客户端数量限制而被拒绝的连接请求数量 sync_full:2 sync_partial_ok:0 sync_partial_err:0 expired_keys:0 ###因为过期而被自动删除的数据库键数量 evicted_keys:0 ###因为最大内存容量限制而被驱逐（evict）的键数量。 keyspace_hits:0 ###查找数据库键成功的次数。 keyspace_misses:500000 ###查找数据库键失败的次数。 pubsub_channels:0 ###目前被订阅的频道数量 pubsub_patterns:0 ###目前被订阅的模式数量 latest_fork_usec:402 ###最近一次 fork() 操作耗费的毫秒数 # Replication role:master ###如果当前服务器没有在复制任何其他服务器，那么这个域的值就是 master ；否则的话，这个域的值就是 slave 。注意，在创建复制链的时候，一个从服务器也可能是另一个服务器的主服务器 connected_slaves:2 ###2个slaves slave0:ip=192.168.65.130,port=6379,state=online,offset=1639,lag=1 slave1:ip=192.168.65.129,port=6379,state=online,offset=1639,lag=0 master_repl_offset:1639 repl_backlog_active:1 repl_backlog_size:1048576 repl_backlog_first_byte_offset:2 repl_backlog_histlen:1638 # CPU used_cpu_sys:41.87 ###Redis 服务器耗费的系统 CPU used_cpu_user:17.82 ###Redis 服务器耗费的用户 CPU used_cpu_sys_children:0.01 ###后台进程耗费的系统 CPU used_cpu_user_children:0.01 ###后台进程耗费的用户 CPU # Keyspace db0:keys=3101,expires=0,avg_ttl=0 ###keyspace 部分记录了数据库相关的统计信息，比如数据库的键数量、数据库已经被删除的过期键数量等。对于每个数据库，这个部分都会添加一行以下格式的信息&apos;只看其中一部分：info Replication 重新统计：config resetstat&apos; 5.查看和修改配置12345678910111213141516171819202122查看： config get ：获取服务器配置信息。 redis 127.0.0.1:6379&gt; config get dir config get *：查看所有配置127.0.0.1:6379&gt; config get app*1) &quot;appendfsync&quot;2) &quot;everysec&quot;3) &quot;appendonly&quot;4) &quot;no&quot;127.0.0.1:6379&gt; config get appendonly1) &quot;appendonly&quot;2) &quot;no&quot; 修改： 临时设置：config set 永久设置：config rewrite，将目前服务器的参数配置写入redis conf. 6.批量执行操作6.1.nc命令12345678910 gnuhpc@gnuhpc:~$ (echo -en &quot;ping\\r\\nset key abc\\r\\nget key\\r\\n&quot;;sleep 1) | nc 127.0.0.1 6379 +PONG +OK $3 abc 6.2.pipeline命令12345678910111213在一个脚本中批量执行多个写入操作: 先把插入操作放入操作文本insert.dat： set a b set 1 2 set h w set f u &apos;然后执行命令:cat insert.bat | ./redis-cli --pipe&apos; 7.选择数据库1select db-index，默认连接的数据库所有是0,默认数据库数是16个。返回1表示成功，0失败 8.清空数据库123 flushdb：删除当前选择数据库中的所有 key。生产上已经禁止。 flushall: 删除所有的数据库。生产上已经禁止。 9.模拟宕机1redis-cli debug segfault 10.模拟hang1redis-cli -p 6379 DEBUG sleep 30 11.重命名命令1rename-command，例如：rename-command FLUSHALL &quot;&quot;。必须重启。 12.执行lua脚本123 - -eval 。例如： redis-cli --eval myscript.lua key1 key2 , arg1 arg2 arg3 13.设置密码1config set requirepass [passw0rd] 14.验证密码1auth passw0rd 15.性能测试命令1redis-benchmark -n 100000 16.获取慢查询12345678910111213141516171819127.0.0.1:6379&gt; slowlog get 101) 1) (integer) 1 2) (integer) 1476779945 3) (integer) 65593 4) 1) &quot;bgrewriteaof&quot;2) 1) (integer) 0 2) (integer) 1476779879 3) (integer) 127857 4) 1) &quot;bgsave&quot;127.0.0.1:6379&gt; 结果为查询ID、发生时间、运行时长和原命令&apos;查看慢查询的设置时间&apos;127.0.0.1:6379&gt; config get slow*1) &quot;slowlog-log-slower-than&quot; #多慢才会被记录（单位微妙）2) &quot;10000&quot;3) &quot;slowlog-max-len&quot; #服务器存储多少条慢查询的记录4) &quot;128&quot; 17.查看日志1日志位置在/redis/log下，redis.log为redis主日志，sentinel.log为sentinel监控日志。 18.Redis-cli命令行其他操作12345678910111213141516171819202122232425262728293031323334353637383940411. echo ：在命令行打印一些内容 redis 127.0.0.1:6379&gt; echo HongWan &quot;HongWan&quot; 2. quit ：退出连接。 redis 127.0.0.1:6379&gt; quit 3. -x选项从标准输入（stdin）读取最后一个参数。 比如从管道中读取输入： echo -en &quot;chen.qun&quot; | redis-cli -x set name 4. -r -i -r 选项重复执行一个命令指定的次数。 -i 设置命令执行的间隔。 比如查看redis每秒执行的commands（qps） redis-cli -r 100 -i 1 info stats | grep instantaneous_ops_per_sec 5. -c：开启reidis cluster模式，连接redis cluster节点时候使用。 6. --rdb：获取指定redis实例的rdb文件,保存到本地。 redis-cli -h 192.168.44.16 -p 6379 --rdb 6379.rdb 7. --slave 模拟slave从master上接收到的commands。slave上接收到的commands都是update操作，记录数据的更新行为。 8. - -pipe 这个一个非常有用的参数。发送原始的redis protocl格式数据到服务器端执行。比如下面的形式的数据（linux服务器上需要用unix2dos转化成dos文件）。 linux下默认的换行是\\n,windows系统的换行符是\\r\\n，redis使用的是\\r\\n. echo -en &apos;*3\\r\\n$3\\r\\nSET\\r\\n$3\\r\\nkey\\r\\n$5\\r\\nvalue\\r\\n&apos; | redis-cli --pipe 19.查看时间戳与微妙数123127.0.0.1:6379&gt; time1) &quot;1476779679&quot;2) &quot;313469&quot; 20.查看当前库中的key数量12127.0.0.1:6379&gt; dbsize(integer) 1 21.手动保存rdb12345127.0.0.1:6379&gt; save #阻塞式OK127.0.0.1:6379&gt; bgsave #后台进行Background saving started127.0.0.1:6379&gt; 22.手动重写aof123127.0.0.1:6379&gt; bgrewriteaofBackground append only file rewriting started127.0.0.1:6379&gt; 23.上次保存时间123127.0.0.1:6379&gt; lastsave(integer) 1476779879 #24.设置为slave1127.0.0.1:6379&gt; slaveof host port 25.sync主从同步26.客户端列表1234127.0.0.1:6379&gt; client listid=3 addr=127.0.0.1:59050 fd=6 name= age=1653 idle=1442 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=0 obl=0 oll=0 omem=0 events=r cmd=configid=6 addr=127.0.0.1:59053 fd=5 name= age=512 idle=0 flags=N db=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768 obl=0 oll=0 omem=0 events=r cmd=client127.0.0.1:6379&gt; 27.客户端名字12345678910111213141516171819202122232425262728293031323334# 新连接默认没有名字 redis 127.0.0.1:6379&gt; CLIENT GETNAME(nil) # 设置名字 redis 127.0.0.1:6379&gt; CLIENT SETNAME hello-world-connectionOK # 返回名字 redis 127.0.0.1:6379&gt; CLIENT GETNAME&quot;hello-world-connection&quot; # 在客户端列表中查看 redis 127.0.0.1:6379&gt; CLIENT LISTaddr=127.0.0.1:36851fd=5name=hello-world-connection # &lt;- 名字age=51... # 清除名字 redis 127.0.0.1:6379&gt; CLIENT SETNAME # 只用空格是不行的！(error) ERR Syntax error, try CLIENT (LIST | KILL ip:port) redis 127.0.0.1:6379&gt; CLIENT SETNAME &quot;&quot; # 必须双引号显示包围OK redis 127.0.0.1:6379&gt; CLIENT GETNAME # 清除完毕(nil)","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis的位图","date":"2017-04-16T04:47:25.804Z","path":"2017/04/16/nosql/redis/redis的位图/","text":"setbitSETBIT key offset value对某一个位（bit）进行设置 12345678910111213141516171819202122232425262728293031&apos;A 65 0100 0001 +32---------------------------------------------a 97 0110 0001&apos;127.0.0.1:6379&gt; set char &quot;a&quot;OK127.0.0.1:6379&gt; setbit char 2 0 #将第二bit设置为0(integer) 1127.0.0.1:6379&gt; get char&quot;A&quot; redis&gt; SETBIT mykey 7 1(integer) 0redis&gt; SETBIT mykey 7 0(integer) 1redis&gt; GET mykey&quot;\\u0000&quot;redis&gt; &apos;如果offset过大，则会在中间填充0&apos;127.0.0.1:6379&gt; get char&quot;A&quot;127.0.0.1:6379&gt; setbit char 20 1(integer) 0127.0.0.1:6379&gt; get char&quot;A\\x00\\b&quot; getbit对 key 所储存的字符串值，获取指定偏移量上的位(bit)。当 offset 比字符串值的长度大，或者 key 不存在时，返回 0123456789101112131415161718返回值：字符串值指定偏移量上的位(bit)。# 对不存在的 key 或者不存在的 offset 进行 GETBIT， 返回 0 redis&gt; EXISTS bit(integer) 0 redis&gt; GETBIT bit 10086(integer) 0 # 对已存在的 offset 进行 GETBIT redis&gt; SETBIT bit 10086 1(integer) 0 redis&gt; GETBIT bit 10086(integer) 1 bitcount计算给定字符串中，被设置为 1 的比特位的数量。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。start 和 end 参数的设置和 GETRANGE 命令类似，都可以使用负数值： 比如 -1 表示最后一个字节， -2 表示倒数第二个字节，以此类推。不存在的 key 被当成是空字符串来处理，因此对一个不存在的 key 进行 BITCOUNT 操作，结果为 0 。1234567891011121314redis&gt; BITCOUNT bits(integer) 0 redis&gt; SETBIT bits 0 1 # 0001(integer) 0 redis&gt; BITCOUNT bits(integer) 1 redis&gt; SETBIT bits 3 1 # 1001(integer) 0 redis&gt; BITCOUNT bits(integer) 2 bitopBITOP operation destkey key [key …]对key1,key2,keyN作operation，并将结果保存到destkey上operation可以是AND , OR , NOT , XOR123456789redis&gt; SET key1 &quot;foobar&quot;OKredis&gt; SET key2 &quot;abcdef&quot;OKredis&gt; BITOP AND dest key1 key2(integer) 6redis&gt; GET dest&quot;`bc`ab&quot;redis&gt; 案例参见：redis setbit(bitmaps)的妙用实现节省存储，快速查询，操作，统计(转)","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis的shell启动脚本","date":"2017-04-16T04:47:25.803Z","path":"2017/04/16/nosql/redis/redis的shell启动脚本/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#!/usr/bin/env bash## redis start up the redis server daemon## chkconfig: 345 99 99# description: redis service in /etc/init.d/redis \\# chkconfig --add redis or chkconfig --list redis \\# service redis start or service redis stop# processname: redis-server# config: /main/redis/redis.conf PATH=/usr/local/bin:/sbin:/usr/bin:/bin REDISPORT=6379EXEC=/main/redis/src/redis-serverREDIS_CLI=/main/redis/src/redis-cli PIDFILE=/var/run/redis.pidCONF=&quot;/main/redis/redis.conf&quot; #make sure some dir exist#if [ ! -d /var/lib/redis ] ;then# mkdir -p /var/lib/redis# mkdir -p /var/log/redis#fi case &quot;$1&quot; in status) ps -A|grep redis ;; start) if [ -f $PIDFILE ] then echo &quot;$PIDFILE exists, process is already running or crashed&quot; else echo &quot;Starting Redis server...&quot; $EXEC $CONF fi if [ &quot;$?&quot;=&quot;0&quot; ] then echo &quot;Redis is running...&quot; fi ;; stop) if [ ! -f $PIDFILE ] then echo &quot;$PIDFILE does not exist, process is not running&quot; else PID=$(cat $PIDFILE) echo &quot;Stopping ...&quot; $REDIS_CLI -p $REDISPORT SHUTDOWN while [ -x $&#123;PIDFILE&#125; ] do echo &quot;Waiting for Redis to shutdown ...&quot; sleep 1 done echo &quot;Redis stopped&quot; fi ;; restart|force-reload) $&#123;0&#125; stop $&#123;0&#125; start ;; *) echo &quot;Usage: /etc/init.d/redis &#123;start|stop|restart|force-reload&#125;&quot; &gt;&amp;2 exit 1esac","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis的set结构","date":"2017-04-16T04:47:25.801Z","path":"2017/04/16/nosql/redis/redis的set结构/","text":"官网http://redis.io/commands#set 集合的特点 无序性 唯一性 sadd 添加SADD key member [member …] 12345678910redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SADD myset &quot;World&quot; #不能重复添加(integer) 0redis&gt; SMEMBERS myset1) &quot;Hello&quot;2) &quot;World&quot;redis&gt; smemberes 查看SMEMBERS keyReturn value all elements of the set. 12345678redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SMEMBERS myset1) &quot;Hello&quot;2) &quot;World&quot;redis&gt; srem 删除SREM key member [member …]Return value： the number of members that were removed from the set, not including non existing members1234567891011121314 redis&gt; SADD myset &quot;one&quot;(integer) 1redis&gt; SADD myset &quot;two&quot;(integer) 1redis&gt; SADD myset &quot;three&quot;(integer) 1redis&gt; SREM myset &quot;one&quot;(integer) 1redis&gt; SREM myset &quot;four&quot; #没有就返回0(integer) 0redis&gt; SMEMBERS myset1) &quot;three&quot;2) &quot;two&quot;redis&gt; spop 随机删除元素SPOP key [count]Removes and returns one or more random elements from the set value store at key.Return value： the removed element, or nil when key does not exist. 123456789101112131415161718192021 redis&gt; SADD myset &quot;one&quot;(integer) 1redis&gt; SADD myset &quot;two&quot;(integer) 1redis&gt; SADD myset &quot;three&quot;(integer) 1redis&gt; SPOP myset #随机删除一个&quot;three&quot;redis&gt; SMEMBERS myset1) &quot;two&quot;2) &quot;one&quot;redis&gt; SADD myset &quot;four&quot;(integer) 1redis&gt; SADD myset &quot;five&quot;(integer) 1redis&gt; SPOP myset 3 #随机删除3个1) &quot;five&quot;2) &quot;four&quot;3) &quot;two&quot;redis&gt; SMEMBERS myset1) &quot;one&quot; srandommember 随机得到一个或多个元素SRANDMEMBER key [count]count&gt;0 返回一个元素不重复的数组count&lt;0 返回的数组中元素可能重复如果不指定count，那么随机的得到一个元素 1234567891011121314 redis&gt; SADD myset one two three(integer) 3redis&gt; SRANDMEMBER myset&quot;one&quot;redis&gt; SRANDMEMBER myset 21) &quot;two&quot;2) &quot;three&quot;redis&gt; SRANDMEMBER myset -5 #返回了随机的重复元素1) &quot;two&quot;2) &quot;three&quot;3) &quot;one&quot;4) &quot;two&quot;5) &quot;one&quot;redis&gt; sismember 是否在集合中SISMEMBER key member存在返回1，否返回01234567 redis&gt; SADD myset &quot;one&quot;(integer) 1redis&gt; SISMEMBER myset &quot;one&quot;(integer) 1redis&gt; SISMEMBER myset &quot;two&quot;(integer) 0redis&gt; scard 集合元素的个数SCARD key返回集合中元素的个数 1234567redis&gt; SADD myset &quot;Hello&quot;(integer) 1redis&gt; SADD myset &quot;World&quot;(integer) 1redis&gt; SCARD myset(integer) 2redis&gt; smove 移动集合中的某个元素SMOVE source destination member 从源集合中移动元素到目标集合中，这个操作是原子性的如果源集合不存在或者是源集合中没有指定的元素，什么也不操作，返回0如果元素在目标集合中存在，那么只是删除源集合中的元素，目标集合不动 1234567891011121314redis&gt; SADD myset &quot;one&quot;(integer) 1redis&gt; SADD myset &quot;two&quot;(integer) 1redis&gt; SADD myotherset &quot;three&quot;(integer) 1redis&gt; SMOVE myset myotherset &quot;two&quot;(integer) 1redis&gt; SMEMBERS myset1) &quot;one&quot;redis&gt; SMEMBERS myotherset1) &quot;two&quot;2) &quot;three&quot;redis&gt; sinter 求交集SINTER key [key …]如果一个集合的为空，或者改集合不存在（会被当做空集合），那么求的并集为空（因为任何集合和一个空集合求交集都是空集合） 123456789101112131415161718192021key1 = &#123;a,b,c,d&#125;key2 = &#123;c&#125;key3 = &#123;a,c,e&#125;SINTER key1 key2 key3 = &#123;c&#125;redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SINTER key1 key21) &quot;c&quot;redis&gt; sunion 求并集SUNION key [key …] return : list with members of the resulting set.123456789101112131415161718192021222324252627 key1 = &#123;a,b,c,d&#125;key2 = &#123;c&#125;key3 = &#123;a,c,e&#125;SUNION key1 key2 key3 = &#123;a,b,c,d,e&#125;&apos;Keys that do not exist are considered to be empty sets.&apos;redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SUNION key1 key21) &quot;b&quot;2) &quot;a&quot;3) &quot;c&quot;4) &quot;e&quot;5) &quot;d&quot;redis&gt; sdiff 求集合的差SDIFF key [key …]返回：list with members of the resulting set. 12345678910111213141516171819202122 key1 = &#123;a,b,c,d&#125;key2 = &#123;c&#125;key3 = &#123;a,c,e&#125;SDIFF key1 key2 key3 = &#123;b,d&#125;redis&gt; SADD key1 &quot;a&quot;(integer) 1redis&gt; SADD key1 &quot;b&quot;(integer) 1redis&gt; SADD key1 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;c&quot;(integer) 1redis&gt; SADD key2 &quot;d&quot;(integer) 1redis&gt; SADD key2 &quot;e&quot;(integer) 1redis&gt; SDIFF key1 key21) &quot;b&quot;2) &quot;a&quot;redis&gt; sinterstore/sunionstore/sdiffstoreSINTERSTORE destination key [key …]SUNIONSTORE destination key [key …]SDIFFSTORE destination key [key …] 只是将结果存储在了destination 中，使用方法见上面","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis的list结构","date":"2017-04-16T04:47:25.799Z","path":"2017/04/16/nosql/redis/redis的list结构/","text":"lpush/rpushRPUSH key value [value …] #从右侧 插入数据LPUSH key value [value …] #从左侧插入数据123456789101112131415161718192021222324252627127.0.0.1:6379&gt; lpush str_list &quot;aa&quot; &quot;bb&quot;(integer) 2127.0.0.1:6379&gt; lrange str_list 0 111) &quot;bb&quot;2) &quot;aa&quot;127.0.0.1:6379&gt; rpush str_list &quot;r_cc&quot;(integer) 3127.0.0.1:6379&gt; lrange str_list 0 111) &quot;bb&quot;2) &quot;aa&quot;3) &quot;r_cc&quot;127.0.0.1:6379&gt; rpush str_list &quot;r_dd&quot;(integer) 4127.0.0.1:6379&gt; lrange str_list 0 111) &quot;bb&quot;2) &quot;aa&quot;3) &quot;r_cc&quot;4) &quot;r_dd&quot;127.0.0.1:6379&gt; lpush str_list &quot;r_ee&quot;(integer) 5127.0.0.1:6379&gt; lrange str_list 0 111) &quot;r_ee&quot;2) &quot;bb&quot;3) &quot;aa&quot;4) &quot;r_cc&quot;5) &quot;r_dd&quot;127.0.0.1:6379&gt; lrangeLRANGE key start stop0表示第一个元素，1表示第二个元素。。。。-1表示最后一个元素，-2表示倒数第二个元素。。。。。。。12345678910111213141516171819202122redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; LRANGE mylist 0 01) &quot;one&quot;redis&gt; LRANGE mylist -3 21) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; LRANGE mylist -100 1001) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; LRANGE mylist 5 10(empty list or set)redis&gt;&apos;取所有的元素，0表示第一个，-1表示最后一个&apos;127.0.0.1:6379&gt; lrange str_list 0 -1 rpop/lpopRPOP key删除link中的元素 123456789101112redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; RPOP mylist #从最右侧删除&quot;three&quot;redis&gt; LRANGE mylist 0 -1 #查询所有1) &quot;one&quot;2) &quot;two&quot;redis&gt; lremLREM key count valuecount &gt; 0:从开头到结尾移除值为value的元素countcount &lt; 0: 从结尾到开头移除值为value的元素countcount = 0: 移除所有的value元素 1234567891011121314redis&gt; RPUSH mylist &quot;hello&quot;(integer) 1redis&gt; RPUSH mylist &quot;hello&quot;(integer) 2redis&gt; RPUSH mylist &quot;foo&quot;(integer) 3redis&gt; RPUSH mylist &quot;hello&quot;(integer) 4redis&gt; LREM mylist -2 &quot;hello&quot; #从结尾开始移除，移除两个(integer) 2redis&gt; LRANGE mylist 0 -11) &quot;hello&quot;2) &quot;foo&quot;redis&gt; ltrimLTRIM key start stop剪切key对应的链接，切【start , stop】一段，并将该段重新赋值给key 123456789101112redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; LTRIM mylist 1 -1 #从第二个元素开始截取，到最后一个元素OKredis&gt; LRANGE mylist 0 -11) &quot;two&quot;2) &quot;three&quot;redis&gt; lindexLINDEX key index取某个索引的值 1234567891011redis&gt; LPUSH mylist &quot;World&quot;(integer) 1redis&gt; LPUSH mylist &quot;Hello&quot;(integer) 2redis&gt; LINDEX mylist 0 #取第一个索引的值&quot;Hello&quot; redis&gt; LINDEX mylist -1 #取最后一个索引的值&quot;World&quot;redis&gt; LINDEX mylist 3 #如果超出range，返回nil(nil)redis&gt; llen返回链表的元素个数1234567redis&gt; LPUSH mylist &quot;World&quot;(integer) 1redis&gt; LPUSH mylist &quot;Hello&quot;(integer) 2redis&gt; LLEN mylist(integer) 2redis&gt; linsertLINSERT key BEFORE|AFTER pivot value在指定的值前/后添加 1234567891011121314redis&gt; RPUSH mylist &quot;Hello&quot;(integer) 1redis&gt; RPUSH mylist &quot;World&quot;(integer) 2redis&gt; LINSERT mylist BEFORE &quot;World&quot; &quot;There&quot; #在“World” 前面插入“There”(integer) 3redis&gt; LRANGE mylist 0 -11) &quot;Hello&quot;2) &quot;There&quot;3) &quot;World&quot;redis&gt; &apos;在 a b c d a e d 中的链表中，在a后面插入f&apos;&apos;则：a f b c d a e d &apos; #只会在找到的第一个元素后插入，不会插入多个 rpoplpushRPOPLPUSH source destinationsource右侧弹出元素，同时在destination 左侧添加 场景：task+bak双向链表完成安全队列 业务逻辑： rpoplpush task bak 接受返回值，并做业务处理 如果业务处理成功，那么rpop bak 清除任务，如果不成功下次从bak表中执行任务（因为该任务没有执行成功） 1234567891011121314151617181920212223&apos;source = [a,b,c]destination=[ x,y,z]RPOPLPUSH source destination # results in source holding a,b and destination holding c,x,y,z.source = [a,b]destination=[ c,x,y,z]&apos;redis&gt; RPUSH mylist &quot;one&quot;(integer) 1redis&gt; RPUSH mylist &quot;two&quot;(integer) 2redis&gt; RPUSH mylist &quot;three&quot;(integer) 3redis&gt; RPOPLPUSH mylist myotherlist&quot;three&quot;redis&gt; LRANGE mylist 0 -11) &quot;one&quot;2) &quot;two&quot;redis&gt; LRANGE myotherlist 0 -11) &quot;three&quot;redis&gt; brpop/blpopBRPOP key [key …] timeoutbrpop其实和rpop类似，只是当链表中没有元素的时候，他会等待timeout(指定的时间) ，0表示一直等待 1234127.0.0.1:6379&gt; brpop waitstr 01) &quot;waitstr&quot; #从key2) &quot;aaa&quot; #pop出的元素(27.33s)","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis的hash结构及命令详解","date":"2017-04-16T04:47:25.798Z","path":"2017/04/16/nosql/redis/redis的hash结构及命令详解/","text":"redis的hash结构相当于map的key-value的结构 hset设置新值HSET key field value 设置 key 指定的哈希集中指定字段的值。 如果 key 指定的哈希集不存在，会创建一个新的哈希集并与 key 关联。 如果字段在哈希集中存在，它将被重写。 12345 redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HGET myhash field1&quot;Hello&quot;redis&gt; hget/hgetall 获取值HGET key fieldHGETALL key 12345678910111213141516171819 redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HGET myhash field1&quot;foo&quot;redis&gt; HGET myhash field2(nil)redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HGETALL myhash1) &quot;field1&quot;2) &quot;Hello&quot;3) &quot;field2&quot;4) &quot;World&quot;redis&gt; hmset同时设置多个值HMSET key field value [field value …] 1234567redis&gt; HMSET myhash field1 &quot;Hello&quot; field2 &quot;World&quot;OKredis&gt; HGET myhash field1&quot;Hello&quot;redis&gt; HGET myhash field2&quot;World&quot;redis&gt; 同时获取多个值HMGET key field [field …] 123456789 redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HMGET myhash field1 field2 nofield1) &quot;Hello&quot;2) &quot;World&quot;3) (nil)redis&gt; 删除hdelHDEL key field [field …] 123456redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HDEL myhash field1(integer) 1redis&gt; HDEL myhash field2(integer) 0 hlen指定key的长度HLEN key 1234567redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HLEN myhash(integer) 2redis&gt; 判断指定key中是否存在指定的域HEXISTS key field1 if the hash contains field.0 if the hash does not contain field, or key does not exist.123456redis&gt; HSET myhash field1 &quot;foo&quot;(integer) 1redis&gt; HEXISTS myhash field1(integer) 1redis&gt; HEXISTS myhash field2(integer) 0 对指定的域进行增长hincrbyHINCRBY key field increment 123456789 redis&gt; HSET myhash field 5(integer) 1redis&gt; HINCRBY myhash field 1(integer) 6redis&gt; HINCRBY myhash field -1(integer) 5redis&gt; HINCRBY myhash field -10(integer) -5redis&gt; 增长浮点数：hincrbyfloatHINCRBYFLOAT key field increment 1234567891011 redis&gt; HSET mykey field 10.50(integer) 1redis&gt; HINCRBYFLOAT mykey field 0.1&quot;10.60000000000000001&quot;redis&gt; HINCRBYFLOAT mykey field -5&quot;5.59999999999999964&quot;redis&gt; HSET mykey field 5.0e3(integer) 0redis&gt; HINCRBYFLOAT mykey field 2.0e2&quot;5200&quot;redis&gt; 返回所有的域:hkeysHKEYS key 12345678 redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HKEYS myhash1) &quot;field1&quot;2) &quot;field2&quot;redis&gt; 返回所有域对应的值：hvalsHVALS key 12345678 redis&gt; HSET myhash field1 &quot;Hello&quot;(integer) 1redis&gt; HSET myhash field2 &quot;World&quot;(integer) 1redis&gt; HVALS myhash1) &quot;Hello&quot;2) &quot;World&quot;redis&gt;","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis安装","date":"2017-04-16T04:47:25.796Z","path":"2017/04/16/nosql/redis/redis安装/","text":"1.下载12wget http://download.redis.io/releases/redis-3.2.4.tar.gz或者到官网下载：http://redis.io/download 2.编译1234tar -zxvf redis-3.2.4.tar.gzcd redis-3.2.4/&gt;make &amp;&amp; make install&apos;这时Redis 的可执行文件被放到了/usr/local/bin 目录下&apos; 3.下载配置文件和启动脚本123456wget https://github.com/ijonas/dotfiles/raw/master/etc/init.d/redis-serverwget https://github.com/ijonas/dotfiles/raw/master/etc/redis.conf#下载这2个配置文件是为了能更好的启动和关闭redis。#在/usr/local/bin 目录下&apos;，自带的redis-server也可以用默认的(二进制编译)不能打开，下载来的同名文件是一个shell脚本（自己写的一个启动脚本），调用启动/usr/local/bin 目录下&apos;的redis-server，可以打开查看修改 4.把配置文件放到合适的位置123cp redis.conf /etc/ #配置文件cp redis-server /etc/init.d/ #启动脚本文件chmod +x /etc/init.d/redis-server #添加执行权限 5.第一次启动Redis前，做一些准备工作1 建立redis专用的用户：useradd redis。2 建立数据目录：mkdir -p /var/lib/redis3 建立日志目录：mkdir -p /var/log/redis4 设置这些目录的权限：chown redis.redis /var/lib/redischown redis.redis /var/log/redis上面的这些目录的创建位子是根据配置文件的设置来确认的 6.redis加入开机自启动123[root@originalOS redis-3.2.4]# chkconfig redis-server on[root@originalOS redis-3.2.4]# chkconfig redis-server --listredis-server 0:off 1:off 2:on 3:on 4:on 5:on 6:off 7.重启123456/etc/init.d/redis-server start/etc/init.d/redis-server stop/etc/init.d/redis-server restart启动脚本已经指定了配置文件，所以在启动的时候不需要再显性的指定配置文件了。要是用自己的配置文件，不用下载来的，则需要：/usr/local/bin/redis-server /etc/redis.conf #指定配置文件 8.加入环境变量12345cp /usr/local/bin/redis* /usr/bin/ #因为/usr/bin默认是在环境变量的PATH中的，所以将/usr/local/bin/redis*拷贝到/usr/bin下也是可以找到的#或者是将/usr/local/bin/ 的路径加入环境变量：vim /etc/profileexport PATH = /usr/local/bin:$PATH","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis字符串类型操作","date":"2017-04-16T04:47:25.795Z","path":"2017/04/16/nosql/redis/redis字符串类型操作/","text":"1.官网http://redis.io/commands#string 2.常用操作2.1.setSET key value [EX seconds] | [PX milliseconds] [NX|XX] EX seconds – Set the specified expire time, in seconds. PX milliseconds – Set the specified expire time, in milliseconds. NX – Only set the key if it does not already exist. key 不存在就设置 XX – Only set the key if it already exist key存在就设置 1234567891011121314127.0.0.1:6379&gt; set address beijing ex 22OK127.0.0.1:6379&gt; ttl address #ttl会返回离过期的时间还有多少秒(integer) 6127.0.0.1:6379&gt; ttl address(integer) 3127.0.0.1:6379&gt; ttl address(integer) 2127.0.0.1:6379&gt; ttl address(integer) 2127.0.0.1:6379&gt; ttl address(integer) 0127.0.0.1:6379&gt; ttl address(integer) -2 #已经过期 2.2.msetMSET key value [key value …]一次性设置多个键值 123456127.0.0.1:6379&gt; mset name zhangsan age 55 address guangshuiOK127.0.0.1:6379&gt; keys *1) &quot;address&quot;2) &quot;age&quot;3) &quot;name&quot; 2.3.getGET key 12redis&gt; SET key1 &quot;Hello&quot;OK 2.4.mgetMGET key [key …]12345127.0.0.1:6379&gt; mget name age address1) &quot;zhangsan&quot;2) &quot;55&quot;3) &quot;guangshui&quot;127.0.0.1:6379&gt; 2.5.setrangeSETRANGE key offset value将指定的key从offset（从0开始数）的位置替换成value123456789101112131415161718192021127.0.0.1:6379&gt; set key1 &quot;hello world&quot;OK127.0.0.1:6379&gt; setrange key1 6 &quot;redis&quot;(integer) 11127.0.0.1:6379&gt; get key1&quot;hello redis&quot;127.0.0.1:6379&gt; setrange key2 6 &quot;redis&quot; #从6的offset开始替换key2(integer) 11127.0.0.1:6379&gt; get key2 #因为key2不存在，所以前面使用\\x00填充，一共填充6次&quot;\\x00\\x00\\x00\\x00\\x00\\x00redis&quot;127.0.0.1:6379&gt; 127.0.0.1:6379&gt; set key3 &quot;hello world&quot;OK127.0.0.1:6379&gt; get key3&quot;hello world&quot;127.0.0.1:6379&gt; setrange key3 13 &quot;redis&quot;(integer) 18127.0.0.1:6379&gt; get key3 &quot;hello world\\x00\\x00redis&quot; #不存在的位置会用\\x00进行填充127.0.0.1:6379&gt; 2.6.APPENDAPPEND key value如果key存在并且是一个字符串，则在字符串的末尾添加value如果key不存在，将会创建一个空的字符串，然后在末尾添加123456789redis&gt; EXISTS mykey(integer) 0redis&gt; APPEND mykey &quot;Hello&quot;(integer) 5redis&gt; APPEND mykey &quot; World&quot;(integer) 11redis&gt; GET mykey&quot;Hello World&quot;redis&gt; 2.7.getrangeGETRANGE key start end1234567891011redis&gt; SET mykey &quot;This is a string&quot;OKredis&gt; GETRANGE mykey 0 3 &quot;This&quot;redis&gt; GETRANGE mykey -3 -1 #从后往前获取：-1表示最后一个字符&quot;ing&quot;redis&gt; GETRANGE mykey 0 -1 #所有&quot;This is a string&quot; redis&gt; GETRANGE mykey 10 100 #100超过str的长度，那么将获取指定的位置后的所有&quot;string&quot;redis&gt; 2.8.getsetGETSET key value获取并返回旧值，并设置新值 123456789101112131415161718redis&gt; SET mykey &quot;Hello&quot;OKredis&gt; GETSET mykey &quot;World&quot;&quot;Hello&quot;redis&gt; GET mykey&quot;World&quot;redis&gt;&apos;如果key不存在就返回nil，并在该key上设置新的值&apos; 127.0.0.1:6379&gt; EXISTS run(integer) 0127.0.0.1:6379&gt; getset run &quot;running...&quot;(nil)127.0.0.1:6379&gt; get run&quot;running...&quot;127.0.0.1:6379&gt; exists run(integer) 1127.0.0.1:6379&gt; 2.9.ince/decINCR key自增操作如果key不存在，就创建key=0，然后再自增如果key的type不是number，那么将会抛出异常 12345678910111213141516171819202122232425262728293031redis&gt; SET mykey &quot;10&quot;OKredis&gt; INCR mykey(integer) 11redis&gt; GET mykey&quot;11&quot;redis&gt;&apos;不存在就创建&apos;127.0.0.1:6379&gt; exists number(integer) 0127.0.0.1:6379&gt; incr number(integer) 1127.0.0.1:6379&gt; get number&quot;1&quot;127.0.0.1:6379&gt; decr number(integer) 0127.0.0.1:6379&gt; decr number(integer) -1 #可以是一个负数的redis&gt; SET mykey &quot;10&quot;OKredis&gt; DECR mykey(integer) 9redis&gt; SET mykey &quot;234293482390480948029348230948&quot;OKredis&gt; DECR mykeyERR value is not an integer or out of rangeredis&gt; 2.10.incrby/decrbyINCRBY key increment 1234567891011redis&gt; SET mykey &quot;10&quot;OKredis&gt; INCRBY mykey 5(integer) 15redis&gt;redis&gt; SET mykey &quot;10&quot;OKredis&gt; DECRBY mykey 3(integer) 7redis&gt; 2.11.incrbyfloatINCRBYFLOAT key increment添加浮点数 1234567891011redis&gt; SET mykey 10.50OKredis&gt; INCRBYFLOAT mykey 0.1&quot;10.6&quot;redis&gt; INCRBYFLOAT mykey -5&quot;5.6&quot;redis&gt; SET mykey 5.0e3OKredis&gt; INCRBYFLOAT mykey 2.0e2&quot;5200&quot;redis&gt; 2.12.setbitSETBIT key offset value对某一个位（bit）进行设置 12345678910111213141516171819202122232425262728293031&apos;A 65 0100 0001 +32---------------------------------------------a 97 0110 0001&apos;127.0.0.1:6379&gt; set char &quot;a&quot;OK127.0.0.1:6379&gt; setbit char 2 0 #将第二bit设置为0(integer) 1127.0.0.1:6379&gt; get char&quot;A&quot; redis&gt; SETBIT mykey 7 1(integer) 0redis&gt; SETBIT mykey 7 0(integer) 1redis&gt; GET mykey&quot;\\u0000&quot;redis&gt; &apos;如果offset过大，则会在中间填充0&apos;127.0.0.1:6379&gt; get char&quot;A&quot;127.0.0.1:6379&gt; setbit char 20 1(integer) 0127.0.0.1:6379&gt; get char&quot;A\\x00\\b&quot; 2.13.bitopBITOP operation destkey key [key …]对key1,key2,keyN作operation，并将结果保存到destkey上operation可以是AND , OR , NOT , XOR123456789redis&gt; SET key1 &quot;foobar&quot;OKredis&gt; SET key2 &quot;abcdef&quot;OKredis&gt; BITOP AND dest key1 key2(integer) 6redis&gt; GET dest&quot;`bc`ab&quot;redis&gt;","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis命令——通用key操作命令","date":"2017-04-16T04:47:25.794Z","path":"2017/04/16/nosql/redis/redis命令——通用key操作命令/","text":"1.官网http://redis.io/commands#generic 2.实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123&apos;连接客户端&apos;[root@originalOS bin]# redis-cli&apos; keys * ：查询所有的key 可以使用正则去匹配&apos;127.0.0.1:6379&gt; keys * 1) &quot;site&quot;127.0.0.1:6379&gt; keys site1) &quot;site&quot;127.0.0.1:6379&gt; keys s* #*去匹配所有字符1) &quot;site&quot;127.0.0.1:6379&gt; keys sit[t|e] #匹配【】中的字符1) &quot;site&quot;127.0.0.1:6379&gt;127.0.0.1:6379&gt; keys si?e #？匹配到一个字符1) &quot;site&quot;&apos;randomkey：返回随机key&apos;127.0.0.1:6379&gt; RANDOMKEY&quot;site&quot; &apos;type&apos;127.0.0.1:6379&gt; type sitestring127.0.0.1:6379&gt; type ssnone&apos;exists：是否存在&apos;127.0.0.1:6379&gt; EXISTS site(integer) 1127.0.0.1:6379&gt; EXISTS aaa(integer) 0127.0.0.1:6379&gt; &apos;del&apos;127.0.0.1:6379&gt; del site(integer) 1127.0.0.1:6379&gt; EXISTS site(integer) 0&apos;rename key&apos;127.0.0.1:6379&gt; get name&quot;zhangsan&quot;127.0.0.1:6379&gt; rename name newnameOK127.0.0.1:6379&gt; get newname&quot;zhangsan&quot;127.0.0.1:6379&gt; &apos;renamenx:如果修改后的名字已经存在，不会执行&apos;127.0.0.1:6379&gt; KEYS *1) &quot;newname&quot;127.0.0.1:6379&gt; set name lisiOK127.0.0.1:6379&gt; renamenx newname name(integer) 0 #修改失败127.0.0.1:6379&gt; &apos;rename newname name #将会直接覆盖原来的名字&apos;&apos;select：选择进入的数据库（数据库从0到n）&apos;127.0.0.1:6379&gt; SELECT 1OK127.0.0.1:6379[1]&gt; keys *(empty list or set)127.0.0.1:6379[1]&gt; select 0OK127.0.0.1:6379&gt; keys *1) &quot;name&quot;2) &quot;newname&quot;&apos;move：移动某一个key到指定的数据库&apos;127.0.0.1:6379&gt; move name 1(integer) 1127.0.0.1:6379&gt; select 1OK127.0.0.1:6379[1]&gt; keys *1) &quot;name&quot;127.0.0.1:6379[1]&gt;&apos;ttl：查看key的生命周期&apos;127.0.0.1:6379&gt; keys *1) &quot;newname&quot;127.0.0.1:6379&gt; ttl newname (integer) -1 #不过期的key返回-1127.0.0.1:6379&gt; ttl cc(integer) -2 #不存在的key127.0.0.1:6379&gt; &apos;expire：设置key的生命周期&apos; 127.0.0.1:6379&gt; expire newname 10 #设置10s的生命周期(integer) 1127.0.0.1:6379&gt; get newname(nil)&apos;pexpire：设置毫秒失效&apos;127.0.0.1:6379&gt; pexpire serache 9000(integer) 1127.0.0.1:6379&gt; pttl serache&apos;persist：设置永久有效设置&apos; 127.0.0.1:6379&gt; set age 11OK127.0.0.1:6379&gt; expire age 22 #设置22s后失效(integer) 1127.0.0.1:6379&gt; PERSIST age #设置永久有效(integer) 1127.0.0.1:6379&gt; 127.0.0.1:6379&gt; ttl age #查看生命周期(integer) -1 #永久有效127.0.0.1:6379&gt;","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"Redis事务介绍 （转）","date":"2017-04-16T04:47:25.793Z","path":"2017/04/16/nosql/redis/Redis事务介绍 （转）/","text":"1.Redis事务介绍相信学过Mysql等其他数据库的同学对事务这个词都不陌生，事务表示的是一组动作，这组动作要么全部执行，要么全部不执行。为什么会有这样的需求呢？看看下面的场景： 微博是一个弱关系型社交网络，用户之间有关注和被关注两种关系，比如两个用户A和B，如果A关注B，则B的粉丝中就应该有A。关注这个动作需要两个步骤完成：在A的关注者中添加B；在B的粉丝中添加A。 这两个动作要么都执行成功，要么都不执行。否则就可能会出现A关注了B，但是B的粉丝中没有A的不可容忍的情况。 转账汇款，假设现在有两个账户A和B，现在需要将A中的一万块大洋转到B的账户中，这个动作也需要两个步骤完成：从A的账户中划走一万块；在B的账户中增加一万块。这两个动作要么全部执行成功，要么全部不执行，否则自会有人问候你的！！！ Redis作为一种高效的分布式数据库，同样支持事务。 2.Redis事务&emsp;Redis中的事务(transaction)是一组命令的集合。事务同命令一样都是Redis最小的执行单位，一个事务中的命令要么都执行，要么都不执行。Redis事务的实现需要用到 MULTI 和 EXEC 两个命令，事务开始的时候先向Redis服务器发送 MULTI 命令，然后依次发送需要在本次事务中处理的命令，最后再发送 EXEC 命令表示事务命令结束。 举个例子，使用redis-cli连接redis，然后在命令行工具中输入如下命令：123456789101112131415161718192021127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set url http://qifuguang.meQUEUED127.0.0.1:6379&gt; set title winwill2012QUEUED127.0.0.1:6379&gt; set desc javaQUEUED127.0.0.1:6379&gt; EXEC1) OK2) OK3) OK127.0.0.1:6379&gt;127.0.0.1:6379&gt; get url&quot;http://qifuguang.me&quot;127.0.0.1:6379&gt; get title&quot;winwill2012&quot;127.0.0.1:6379&gt; get desc&quot;java&quot;127.0.0.1:6379&gt; 从输出中可以看到，当输入MULTI命令后，服务器返回OK表示事务开始成功，然后依次输入需要在本次事务中执行的所有命令，每次输入一个命令服务器并不会马上执行，而是返回”QUEUED”，这表示命令已经被服务器接受并且暂时保存起来，最后输入EXEC命令后，本次事务中的所有命令才会被依次执行，可以看到最后服务器一次性返回了三个OK，这里返回的结果与发送的命令是按顺序一一对应的，这说明这次事务中的命令全都执行成功了。 再举个例子，在命令行工具中输入如下命令：1234567891011121314151617127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set a aQUEUED127.0.0.1:6379&gt; sett b b(error) ERR unknown command &apos;sett&apos;127.0.0.1:6379&gt; set c cQUEUED127.0.0.1:6379&gt; EXEC(error) EXECABORT Transaction discarded because of previous errors.127.0.0.1:6379&gt; get a(nil)127.0.0.1:6379&gt; get b(nil)127.0.0.1:6379&gt; get c(nil)127.0.0.1:6379&gt; 和前面的例子一样，先输入MULTI最后输入EXEC表示中间的命令属于一个事务，不同的是中间输入的命令有一个错误(set写成了sett)，这样因为有一个错误的命令导致事务中的其他命令都不执行了(通过后续的get命令可以验证)，可见事务中的所有命令式同呼吸共命运的。 如果客户端在发送EXEC命令之前断线了，则服务器会清空事务队列，事务中的所有命令都不会被执行。而一旦客户端发送了EXEC命令之后，事务中的所有命令都会被执行，即使此后客户端断线也没关系，因为服务器已经保存了事务中的所有命令。 除了保证事务中的所有命令要么全执行要么全不执行外，Redis的事务还能保证一个事务中的命令依次执行而不会被其他命令插入。试想一个客户端A需要执行几条命令，同时客户端B发送了几条命令，如果不使用事务，则客户端B的命令有可能会插入到客户端A的几条命令中，如果想避免这种情况发生，也可以使用事务。 3.Redis事务错误处理如果一个事务中的某个命令执行出错，Redis会怎样处理呢？要回答这个问题，首先要搞清楚是什么原因导致命令执行出错： 语法错误 就像上面的例子一样，语法错误表示命令不存在或者参数错误这种情况需要区分Redis的版本，Redis 2.6.5之前的版本会忽略错误的命令，执行其他正确的命令，2.6.5之后的版本会忽略这个事务中的所有命令，都不执行，就比如上面的例子(使用的Redis版本是2.8的) 运行错误 运行错误表示命令在执行过程中出现错误，比如用GET命令获取一个散列表类型的键值。这种错误在命令执行之前Redis是无法发现的，所以在事务里这样的命令会被Redis接受并执行。如果食物里有一条命令执行错误，其他命令依旧会执行（包括出错之后的命令）。比如下例： 1234567891011121314127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set key 1QUEUED127.0.0.1:6379&gt; SADD key 2QUEUED127.0.0.1:6379&gt; set key 3QUEUED127.0.0.1:6379&gt; EXEC1) OK2) (error) WRONGTYPE Operation against a key holding the wrong kind of value3) OK127.0.0.1:6379&gt; get key&quot;3&quot; Redis中的事务并没有关系型数据库中的事务回滚(rollback)功能，因此使用者必须自己收拾剩下的烂摊子。不过由于Redis不支持事务回滚功能，这也使得Redis的事务简洁快速。 回顾上面两种类型的错误，语法错误完全可以在开发的时候发现并作出处理，另外如果能很好地规划Redis数据的键的使用，也是不会出现命令和键不匹配的问题的。 4.WATCH命令从上面的例子我们可以看到，事务中的命令要全部执行完之后才能获取每个命令的结果，但是如果一个事务中的命令B依赖于他上一个命令A的结果的话该怎么办呢？就比如说实现类似Java中的i++的功能，先要获取当前值，才能在当前值的基础上做加一操作。这种场合仅仅使用上面介绍的MULTI和EXEC是不能实现的，因为MULTI和EXEC中的命令是一起执行的，并不能将其中一条命令的执行结果作为另一条命令的执行参数，所以这个时候就需要引进Redis事务家族中的另一成员：WATCH命令 换个角度思考上面说到的实现i++的方法，可以这样实现： 监控i的值，保证i的值不被修改 获取i的原值 如果过程中i的值没有被修改，则将当前的i值+1，否则不执行 这样就能够避免竞态条件，保证i++能够正确执行。 WATCH命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令（事务中的命令是在EXEC之后才执行的，EXEC命令执行完之后被监控的键会自动被UNWATCH） 举个例子：123456789101112131415127.0.0.1:6379&gt; set mykey 1OK127.0.0.1:6379&gt; WATCH mykeyOK127.0.0.1:6379&gt; set mykey 2OK127.0.0.1:6379&gt; MULTIOK127.0.0.1:6379&gt; set mykey 3QUEUED127.0.0.1:6379&gt; EXEC(nil)127.0.0.1:6379&gt; get mykey&quot;2&quot;127.0.0.1:6379&gt; 上面的例子中，首先设置mykey的键值为1，然后使用WATCH命令监控mykey，随后更改mykey的值为2，然后进入事务，事务中设置mykey的值为3，然后执行EXEC运行事务中的命令，最后使用get命令查看mykey的值，发现mykey的值还是2，也就是说事务中的命令根本没有执行（因为WATCH监控mykey的过程中，mykey被修改了，所以随后的事务便会被取消）。 有了WATCH命令，我们就可以自己实现i++功能了，伪代码如下：1234567891011def incr($key): WATCH $key $value = GET $key if not $value $value = 0 $value = $value + 1 MULTI SET $key $value result = EXEC return result[0] 因为EXEC返回的是多行字符串，使用result[0]表示返回值的第一个字符串。 注意：由于WATCH命令的作用只是当被监控的键被修改后取消之后的事务，并不能保证其他客户端不修改监控的值，所以当EXEC命令执行失败之后需要手动重新执行整个事务。 执行EXEC命令之后会取消监控使用WATCH命令监控的键，如果不想执行事务中的命令，也可以使用UNWATCH命令来取消监控。 转自:Redis事务介绍","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis主从同步及切换主从配置示例(转)","date":"2017-04-16T04:47:25.791Z","path":"2017/04/16/nosql/redis/redis主从同步及切换主从配置示例(转)/","text":"redis主从原理：当一个从数据库启动时，会向主数据库发送SYNC命令，主数据库收到命令后会开始在后台保存快照（即RDB持久化过程），并将保存快照期间接收到的命令缓存起来。当快照完成后，Redis会将快照文件和缓存的命令发给从数据库，从数据库收到数据后，会载入快照文件并执行缓存的命令。以上过程称为复制初始化。复制初始化之结束后，主数据库每收到写命令时就会将命令同步给从数据库，从而保证主从数据库数据一致，这一过程称为复制同步阶段。 注意：redis主从同步版本必须一致，不一致的话同步过程中会出现各种奇葩问题！ 操作系统环境如下：12345[root@a47b0619d2ad ~]# cat /etc/issueCentOS release 6.7 (Final)Kernel \\r on an \\m[root@a47b0619d2ad ~]# getconf LONG_BIT64 1.主从规划如下 redis port dir config master 7001 /opt/7001 redis.conf slave 7002 /opt/7002 redis.conf 2.主从同步 1：拷贝redis编译安装后的配置文件以及redis-server 123456789101112[root@a47b0619d2ad ~]# cp /usr/local/src/redis-3.0.6/redis.conf /usr/local/src/redis-3.0.6/src/redis-server /opt/7001/[root@a47b0619d2ad ~]# cp /usr/local/src/redis-3.0.6/redis.conf /usr/local/src/redis-3.0.6/src/redis-server /opt/7002/#目录结构[root@a47b0619d2ad ~]# tree /opt/700*/opt/7001├── redis.conf└── redis-server/opt/7002├── redis.conf└── redis-server0 directories, 4 files 2：redis 主服务配置文件 1234567[root@a47b0619d2ad ~]# vim /opt/7001/redis.confpidfile /var/run/redis-7001.pidport 7001logfile &quot;/var/log/redis/redis-7001.log&quot;dir /opt/7001/#注：以上只是修改redis主配置文件，其他默认 启动redis master主服务 1234[root@a47b0619d2ad ~]# /opt/7001/redis-server /opt/7001/redis.conf[root@a47b0619d2ad ~]# netstat -ntpl|grep 7001tcp 0 0 0.0.0.0:7001 0.0.0.0:* LISTEN 5595/redis-server *tcp 0 0 :::7001 :::* LISTEN 5595/redis-server * 3：redis 从服务配置文件 1234567[root@a47b0619d2ad ~]# vim /opt/7002/redis.confpidfile /var/run/redis-7002.pidport 7002logfile &quot;/var/log/redis/redis-7002.log&quot;dir /opt/7002/slaveof 127.0.0.1 7001 //redis主服务的ip地址以及端口号注：如果redis master设置了验证密码，还需配置masterauth xxx，指redis master上认证密码即可。 启动redis slave从服务 1234[root@a47b0619d2ad ~]# /opt/7002/redis-server /opt/7002/redis.conf[root@a47b0619d2ad ~]# netstat -ntpl|grep 7002tcp 0 0 0.0.0.0:7002 0.0.0.0:* LISTEN 5605/redis-server *tcp 0 0 :::7002 :::* LISTEN 5605/redis-server * 4：登录redis master 1234567891011121314[root@a47b0619d2ad ~]# redis-cli -p 7001127.0.0.1:7001&gt; info........................# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=7002,state=online,offset=71,lag=1master_repl_offset:71repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:2repl_backlog_histlen:70注：通过info命令，可以查看当然redis服务的角色是master，以及slave相关信息等。 设置一些数据…. 123456789127.0.0.1:7001&gt; set name zxlOK127.0.0.1:7001&gt; set age 33OK127.0.0.1:7001&gt; get name&quot;zxl&quot;127.0.0.1:7001&gt; get age&quot;33&quot;127.0.0.1:7001&gt; 5：登录redis slave服务器 1234567891011121314151617181920212223[root@a47b0619d2ad ~]# redis-cli -p 7002127.0.0.1:7002&gt; info......................# Replicationrole:slavemaster_host:127.0.0.1master_port:7001master_link_status:upmaster_last_io_seconds_ago:6master_sync_in_progress:0slave_repl_offset:212slave_priority:100slave_read_only:1connected_slaves:0master_repl_offset:0repl_backlog_active:0repl_backlog_size:1048576repl_backlog_first_byte_offset:0repl_backlog_histlen:0# Keyspacedb0:keys=2,expires=0,avg_ttl=0#注：通过info命令，可以查看当然redis服务的角色是slave，以及master相关信息等。 redis slave查看数据 123456789101112127.0.0.1:7002&gt; KEYS *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:7002&gt; get age&quot;33&quot;127.0.0.1:7002&gt; get name&quot;zxl&quot;#注：redis slave上可以查看到来自redis master上同步的数据。127.0.0.1:7002&gt; set ox xxoo(error) READONLY You can&apos;t write against a read only slave.127.0.0.1:7002&gt; 注：redis slave只是只读数据库，而不能插入数据。如需要写，把redis slave配置文件中slave-read-only yes改为slave-read-only no，重启服务即可。 如果不想通过配置文件来更改的话，也可以使用config更改，实例如下：12345678127.0.0.1:7002&gt; CONFIG GET slave-read-only1) &quot;slave-read-only&quot;2) &quot;yes&quot;127.0.0.1:7002&gt; CONFIG set slave-read-only noOK127.0.0.1:7002&gt; CONFIG GET slave-read-only1) &quot;slave-read-only&quot;2) &quot;no&quot; 3.主从在线切换把redis的7002端口切换为主服务12345127.0.0.1:7002&gt; CONFIG GET slaveof //查看1) &quot;slaveof&quot;2) &quot;127.0.0.1 7001&quot;127.0.0.1:7002&gt; SLAVEOF no one //设置为主服务OK 插入数据 1234567891011127.0.0.1:7002&gt; KEYS *1) &quot;age&quot;2) &quot;name&quot;127.0.0.1:7002&gt; get age&quot;33&quot;127.0.0.1:7002&gt; set a 11OK127.0.0.1:7002&gt; KEYS *1) &quot;age&quot;2) &quot;name&quot;3) &quot;a&quot; 原来redis主节点端口为7001设置为从服务节点 12127.0.0.1:7001&gt; SLAVEOF 127.0.0.1 7002OK 注：以上就是主从在线切换redis主节点@，主从在线切换后可以通过info命令查看相关信息 转自:redis主从同步及切换主从配置示例","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis-java-API使用","date":"2017-04-16T04:47:25.790Z","path":"2017/04/16/nosql/redis/redis-java-API使用/","text":"官网：https://redis.io/commands 1.string使用1.1.set、get、mset、mget、setex、expire1234567891011121314151617181920212223242526272829303132333435363738public class StringMain &#123; public static void main(String[] args) throws InterruptedException &#123; //创建Jedis客户端 Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); //操作一个String字符串 jedis.set(&quot;name&quot;, &quot;liudehua&quot;); //插入一个名字，叫做刘德华 System.out.println(jedis.get(&quot;name&quot;)); //读取一个名字 //对string类型数据进行增减,前提是kv对应的值是数字 jedis.set(&quot;age&quot;, &quot;17&quot;);//给用户刘德华设置年龄，17岁 jedis.incr(&quot;age&quot;);//让用户刘德华年龄增加一岁 System.out.println(jedis.get(&quot;age&quot;)); //打印结果 18 jedis.decr(&quot;age&quot;);//让刘德华年轻一岁 System.out.println(jedis.get(&quot;age&quot;));//在18的基础上，减一岁，变回17 //一次性插入多条数据 。为江湖大侠设置绝杀技能 jedis.mset(&quot;AAA&quot;, &quot;Mysql数据库的操作&quot; , &quot;BBB&quot;, &quot;熟悉LINXU操作系统&quot; , &quot;CCC&quot;, &quot;熟悉SSH、SSM框架及配置&quot; , &quot;DDD&quot;, &quot;熟悉Spring框架，mybatis框架，Spring IOC MVC的整合，Spring和Mybatis的整合&quot;); List&lt;String&gt; results = jedis.mget(&quot;AAA&quot;, &quot;BBB&quot;, &quot;CCC&quot;, &quot;DDD&quot;); for (String value : results) &#123; System.out.println(value); &#125; //设置字段的自动过期 jedis.setex(&quot;wumai&quot;, 10, &quot;我们活在仙境中&quot;); //让仙境保持10秒钟 while (jedis.exists(&quot;wumai&quot;)) &#123; System.out.println(&quot;真是天上人间呀！&quot;); Thread.sleep(1000); &#125; System.out.println(); //对已经存在的字段设置过期时间 jedis.set(&quot;wumai&quot;, &quot;我们活在仙境中&quot;); jedis.expire(&quot;wumai&quot;, 10); //让天上人间的感觉保持更长的时间 while (jedis.exists(&quot;wumai&quot;)) &#123; System.out.println(&quot;真是天上人间呀！&quot;); Thread.sleep(1000); &#125; &#125;&#125; 1.2.多线程操作Counter计数器1234567891011121314151617181920212223242526272829package redis.string;import redis.clients.jedis.Jedis;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;/** * Describe: 擂台比武 */public class Counter &#123; /** * 计算 武林大会 三个擂台的比武次数 * * @param args */ public static void main(String[] args) &#123; //创建一个固定大小的线程池，3个擂台 ExecutorService executorService = Executors.newFixedThreadPool(10); //擂台1：天龙八部 executorService.submit(new Arena(&quot;biwu:totalNum&quot;,&quot;天龙八部&quot;)); //擂台2：神雕侠侣 executorService.submit(new Arena(&quot;biwu:totalNum&quot;,&quot;神雕侠侣&quot;)); //擂台3：倚天屠龙记 executorService.submit(new Arena(&quot;biwu:totalNum&quot;,&quot;倚天屠龙记&quot;)); //报幕人员，一秒统计一次总共比了多少场 executorService.submit(new BaoMu(&quot;biwu:totalNum&quot;)); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738package redis.string;import redis.clients.jedis.Jedis;import java.util.Random;/** * Describe: 擂台 */public class Arena implements Runnable &#123; private Random random = new Random(); private String redisKey; private Jedis jedis; private String arenaName; public Arena(String redisKey, String arenaName) &#123; this.redisKey = redisKey; this.arenaName = arenaName; &#125; public void run() &#123; jedis = new Jedis(&quot;127.0.0.1&quot;,6379); String[] daxias = new String[]&#123;&quot;郭靖&quot;, &quot;黄蓉&quot;, &quot;令狐冲&quot;, &quot;杨过&quot;, &quot;林冲&quot;, &quot;鲁智深&quot;, &quot;小女龙&quot;, &quot;虚竹&quot;, &quot;独孤求败&quot;, &quot;张三丰&quot;, &quot;王重阳&quot;, &quot;张无忌&quot; , &quot;王重阳&quot;, &quot;东方不败&quot;, &quot;逍遥子&quot;, &quot;乔峰&quot;, &quot;虚竹&quot;, &quot;段誉&quot; , &quot;韦小宝&quot;, &quot;王语嫣&quot;, &quot;周芷若&quot;, &quot;峨眉师太&quot;, &quot;慕容复&quot;&#125;; while (true) &#123; try &#123; Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; int p1 = random.nextInt(daxias.length); int p2 = random.nextInt(daxias.length); while (p1 == p2) &#123; //如果两个大侠出场名字一样，换一个人 p2 = random.nextInt(daxias.length); &#125; System.out.println(&quot;在擂台&quot; + arenaName + &quot;上 大侠&quot; + daxias[p1] + &quot; VS &quot; + daxias[p2]); //多个线程，将调用redis取出同一个变量，不需要加锁，因为redis底层的increase操作时原子性的 jedis.incr(redisKey); &#125; &#125;&#125; 1.3.保存对象到redis中1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package redis.string;import com.google.gson.Gson;import org.junit.Test;import redis.clients.jedis.Jedis;import java.io.ByteArrayInputStream;import java.io.ByteArrayOutputStream;import java.io.ObjectInputStream;import java.io.ObjectOutputStream;import java.util.Map;/** * Describe: 保存Product对象到redis中 */public class ProductService &#123; @Test public void saveProduct2Redis() throws Exception &#123; //初始化刘德华的基本信息 Person person = new Person(&quot;刘德华&quot;, 17); //将刘德华的信息保存到Redis中 Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); //直接保存对象的toString方法，这种方法不反序列化对象 jedis.set(&quot;user:liudehua:str&quot;, person.toString()); System.out.println(jedis.get(&quot;user:liudehua:str&quot;)); //保存序列化之后的对象 jedis.set(&quot;user:liudehua:obj&quot;.getBytes(), getBytesByProduct(person)); byte[] productBytes = jedis.get(&quot;user:liudehua:obj&quot;.getBytes()); Person pByte = getProductByBytes(productBytes); System.out.println(pByte.getName()+&quot; &quot; +pByte.getAge()); //保存Json化之后的对象 jedis.set(&quot;user:liudehua:json&quot;, new Gson().toJson(person)); String personJson = jedis.get(&quot;user:liudehua:json&quot;); Person pjson = new Gson().fromJson(personJson, Person.class); System.out.println(pjson.getName()+&quot; &quot;+ pjson.getAge()); &#125; /** * 从字节数组中读取Java对象 * * @param productBytes * @return * @throws Exception */ public Person getProductByBytes(byte[] productBytes) throws Exception &#123; ByteArrayInputStream byteInputStream = new ByteArrayInputStream(productBytes); //读对象的流 ObjectInputStream objectInputStream = new ObjectInputStream(byteInputStream); return (Person) objectInputStream.readObject(); &#125; /** * 将对象转化成Byte数组 * * @param product * @return * @throws Exception */ public byte[] getBytesByProduct(Person product) throws Exception &#123; ByteArrayOutputStream ba = new ByteArrayOutputStream(); //能够输出对象的流 ObjectOutputStream oos = new ObjectOutputStream(ba); //写对象 oos.writeObject(product); oos.flush(); return ba.toByteArray(); &#125;&#125; Person1234567891011121314151617181920212223242526272829303132333435package redis.string;import java.io.Serializable;/** */public class Person implements Serializable&#123; private static final long serialVersionUID = -9012113097419111583L; private String name;//姓名 private int age;//年龄 public Person() &#123; &#125; public Person(String name, int age) &#123; this.name = name; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; @Override public String toString() &#123; return &quot;Product&#123;&quot; + &quot;name=&apos;&quot; + name + &apos;\\&apos;&apos; + &quot;, age=&quot; + age + &apos;&#125;&apos;; &#125;&#125; 2.map2.1.map基本操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package redis.map;import redis.clients.jedis.Jedis;import java.util.List;import java.util.Map;import java.util.Set;/** * Describe: 请补充类描述 */public class MapMain &#123; public static void main(String[] args) &#123; Jedis jedis = new Jedis(&quot;127.0.0.1&quot;, 6379); jedis.del(&quot;daxia:jingzhongyue&quot;); //创建一个对象 jedis.hset(&quot;daxia:jingzhongyue&quot;, &quot;姓名&quot;, &quot;不为人知&quot;); jedis.hset(&quot;daxia:jingzhongyue&quot;, &quot;年龄&quot;, &quot;18&quot;); jedis.hset(&quot;daxia:jingzhongyue&quot;, &quot;技能&quot;, &quot;杀人于无形&quot;); //打印对象 Map&lt;String, String&gt; jingzhongyue = jedis.hgetAll(&quot;daxia:jingzhongyue&quot;); System.out.println(&quot;hgetAll 大侠的基本信息：&quot;); for (Map.Entry entry : jingzhongyue.entrySet()) &#123; System.out.println(entry.getKey() + &quot;：-----------------&quot; + entry.getValue()); &#125; System.out.println(); //获取大侠的所有字段信息 Set&lt;String&gt; fields = jedis.hkeys(&quot;daxia:jingzhongyue&quot;); System.out.println(&quot;hkeys &quot;); for (String field : fields) &#123; System.out.print(field + &quot; &quot;); &#125; System.out.println(); //获取大侠的所有值的信息 List&lt;String&gt; values = jedis.hvals(&quot;daxia:jingzhongyue&quot;); System.out.println(&quot;hvals &quot; ); for (String value : values) &#123; System.out.print(value + &quot; &quot;); &#125; System.out.println(); //值获取大侠的年龄，进行研究 String age = jedis.hget(&quot;daxia:jingzhongyue&quot;, &quot;年龄&quot;); System.out.println(&quot;对大侠的年龄有质疑：&quot; + age); //给大侠的年龄增加十岁 jedis.hincrBy(&quot;daxia:jingzhongyue&quot;, &quot;年龄&quot;, 10); System.out.println(&quot;经过验核，大侠的实际年龄为：&quot; + jedis.hget(&quot;daxia:jingzhongyue&quot;, &quot;年龄&quot;)); System.out.println(); //删除大侠的姓名 jedis.hdel(&quot;daxia:jingzhongyue&quot;, &quot;姓名&quot;); for (Map.Entry entry : jedis.hgetAll(&quot;daxia:jingzhongyue&quot;).entrySet()) &#123; System.out.println(entry.getKey() + &quot;:&quot; + entry.getValue()); &#125; &#125;&#125; 2.2.购物车redis","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"redis setbit(bitmaps)的妙用实现节省存储，快速查询，操作，统计(转)","date":"2017-04-16T04:47:25.789Z","path":"2017/04/16/nosql/redis/redis setbit(bitmaps)的妙用实现节省存储，快速查询，操作，统计(转)/","text":"redis提供了二进制的操作，分别是：12345678 setbit getbit bitcount bitop 看完具体的使用后，可以衍生出两个经典案例 1.存储布尔性质的值，节约内存，快速实现业务如下需求，在用户系统中(user),ID往往是唯一且递增生成的，如果用redis实现一套用户系统，则可以这样123set user:1:name &quot;begin man&quot; # 注意空格必须加引号以区分set user:1:age 24set user:1:sex 1 # 性别 1表示男，0表示女 这样如果有1w个用户则对应1w个user:ID:sex键值对，如果我们想要节省内存，何不尝试用bit呢，如下面二进制，2个字节(16位二进制组成)，其中是1的二进制位共9个 那么我们可以用user_id对应每个二进制位，这里设置键名为”users:sex” 比如我们总共有1亿个用户，userID从1到一亿，当然数字不一定是连贯性的，那么userID为0则表示offset(偏移量)为0， userID为15则表示offset为15,(如果你的uid不是从1开始的，比如从100000开始，实际上你也可以相应的用uid减去初始值来表示其位数，比如1000000用户对应到bitmap的第一位)如下命令我们创建了该键值：12127.0.0.1:6379&gt; SETBIT users:sex 100000000 0(integer) 0 需要注意的是redis的bitmap只支持2^32大小,即使用512M内存。结合你的业务规划好内存的分配。 接下来做个测试，这里初始化userID用户性别都是0，这里看下userID为0和15和1亿的性别 123456127.0.0.1:6379&gt; GETBIT users:sex 0(integer) 0127.0.0.1:6379&gt; GETBIT users:sex 15(integer) 0127.0.0.1:6379&gt; GETBIT users:sex 100000000(integer) 0 某一天UserID为15的哥们设置了自己的性别为男，则 1234127.0.0.1:6379&gt; setbit users:sex 15 1(integer) 0127.0.0.1:6379&gt; GETBIT users:sex 15(integer) 1 后台管理员为了统计男女比例，则可以： 12127.0.0.1:6379&gt; BITCOUNT users:sex(integer) 1 哇，亿里才有1个男人啊，赶紧抢啊。。 别看有1亿多数据，其实整个操作耗时基本是毫秒级别的，时间复杂度为O(1).迅速吧。 想到这里我们还可以扩展常见的需求： 对用户标识一条消息的已读未读 用户是否某某某等，只要涉及布尔形式的都可以用bitmaps 2.日活跃用户参考了这篇译文: 使用Redis bitmaps进行快速、简单、实时统计为了统计今日登录的用户数，我们建立了一个bitmap,每一位标识一个用户ID。当某个用户访问我们的网页或执行了某个操作，就在bitmap中把标识此用户的位置为1。 这个简单的例子中，每次用户登录时会执行一次redis.setbit(daily_active_users, user_id, 1)。将bitmap中对应位置的位置为1，时间复杂度是O(1)。统计bitmap结果显示有今天有9个用户登录。Bitmap的key是daily_active_users，它的值是1011110100100101。 因为日活跃用户每天都变化，所以需要每天创建一个新的bitmap。我们简单地把日期添加到key后面，实现了这个功能。例如，要统计某一天有多少个用户至少听了一个音乐app中的一首歌曲，可以把这个bitmap的redis key设计为play:yyyy-mm-dd-hh。当用户听了一首歌曲，我们只是简单地在bitmap中把标识这个用户的位置为1，时间复杂度是O(1)。 setbit play:yyyy-mm-dd user_id 1 今天听过歌曲的用户就是key是play:yyyy-mm-dd的bitmap的位图计数。如果要按周或月统计，只要对这周或这个月的所有bitmap求并集，得出新的bitmap，在对它做位图计数。 利用这些bitmap做其它复杂的统计也非常容易。例如，统计11月听过歌曲的高级用户(premium user),只需通过bitop命令进行AND,OR,XOR,NOT操作即可，如下伪代码： (play:2011-11-01 ∪ play:2011-11-02 ∪…∪play:2011-11-30) ∩ premium:2011-11 下面的表格显示了在1亿2千8百万用户上完成的时间粒度为1天，一周，一个月的用户统计的时间消耗比较。","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"RDB快照持久化","date":"2017-04-16T04:47:25.787Z","path":"2017/04/16/nosql/redis/RDB快照持久化/","text":"1.简介因为Redis是内存数据库，他将自己的数据库状态存储在内存里面，所以如果不想办法将存储在内存中的数据库状态保存到磁盘里面，那么一旦服务器进程退出，服务器中的数据库状态也会消失不见，Redis提供了RDB持久化功能 RDB持久化既可以手动执行，也可以根据配置文件（redis.conf）配置选项定期执行，该功能可以将某个时间点上的数据库状态保存到一个RDB文件中，如下： 2.RDB文件的创建2.1.save、bgsavesave命令会阻塞服务器进程，知道rdb文件创建完毕为止，在服务器进程阻塞期间，服务器不能处理任何命令请求12127.0.0.1:6379&gt; save OK bgsave 命令会派生出一个子进程，然后由子进程负责创建rdb文件，服务器进程（父进程）继续处理命令请求 12127.0.0.1:6379&gt; bgsave #派生子进程，然后由子进程创建rdb文件Background saving started 创建rdb文件的实际工作由rdb.c/rdbSave函数完成，save命令和bgsave命令会以不同的方式调用这个函数，以下通过伪代码可以明显看出这两个命令之间的区别： 123456789101112131415161718def SAVE(): #创建rdb文件 rdbSave()def BGSAVE(); #创建子进程 pid = fork() if pid ==0: #子进程负责创建rdb文件 rdbSave() #完成工作后向父进程发送信号 elif pid &gt; 0: #父进程继续处理命令请求，并通过轮询等待子进程的信号 handle_request_and_wait_singal() else: #处理出错情况 handle_fork_error() 3.自动载入rdb文件的载入工作是在服务器启动时自动执行的，Redis没有专门用于载入rdb文件的命令，只要Redis服务器在启动时检测到rdb文件的存在，他就会自动载入rdb文件，如下是启动过程：DB loaded from disk : 0.018 seconds就是服务器在成功载入rdb文件之后打印的 因为aof文件的更新频率通常比rdb文件的更新频率高，也就是说aof的中对数据的记录时效性相对于rdb来说更加接近真实数据 如果服务器开启了aof持久化功能，那么服务器会优先使用aof文件来还原数据库状态 只有在aof持久化功能处于关闭状态时，服务器才会使用rdb文件来还原数据库状态，如下图： 4.自动间隔性保存因为bgsave命令可以在不阻塞服务器进程的情况下执行，所以redis允许用户通过设置服务器配置文件，让服务器每隔一段时间自动执行一次bgsave命令：vim /etc/redis.conf1234567891011121314#3种保存时间，满足一个就会触发保存save 900 1 #900秒内，对数据库进行了至少1次修改save 300 10 #300秒内，对数据库进行了至少10次修改save 60 10000 #60秒内，对数据库进行了至少1000次修改stop-writes-on-bgsave-error yes #后台存储错误停止写rdbcompression yes #使用LZF压缩rdb文件rdbchecksum yes #存储和加载rdb文件时校验dbfilename dump.rdb #设置rdb文件名dir ./ #设置rdb文件的保存目录 参考书籍:《Redis设计与实现》","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"RDB 文件结构（转）","date":"2017-04-16T04:47:25.785Z","path":"2017/04/16/nosql/redis/RDB 文件结构（转）/","text":"在本章之前的内容中， 我们介绍了 Redis 服务器保存和载入 RDB 文件的方法， 在这一节， 我们将对 RDB 文件本身进行介绍， 并详细说明文件各个部分的结构和意义。 图 IMAGE_RDB_STRUCT_OVERVIEW 展示了一个完整 RDB 文件所包含的各个部分。 注意为了方便区分变量、数据、常量， 图 IMAGE_RDB_STRUCT_OVERVIEW 中用全大写单词标示常量， 用全小写单词标示变量和数据。本章展示的所有 RDB 文件结构图都遵循这一规则。 RDB 文件的最开头是 REDIS 部分， 这个部分的长度为 5 字节， 保存着 “REDIS” 五个字符。 通过这五个字符， 程序可以在载入文件时， 快速检查所载入的文件是否 RDB 文件。 注意因为 RDB 文件保存的是二进制数据， 而不是 C 字符串， 为了简便起见， 我们用 “REDIS” 符号代表 ‘R’ 、 ‘E’ 、 ‘D’ 、 ‘I’ 、 ‘S’ 五个字符， 而不是带 ‘\\0’ 结尾符号的 C 字符串 ‘R’ 、 ‘E’ 、 ‘D’ 、 ‘I’ 、 ‘S’ 、 ‘\\0’ 。本章介绍的所有内容，以及展示的所有 RDB 文件结构图都遵循这一规则。 db_version 长度为 4 字节， 它的值是一个字符串表示的整数， 这个整数记录了 RDB 文件的版本号， 比如 “0006” 就代表 RDB 文件的版本为第六版。 本章只介绍第六版 RDB 文件的结构。 databases 部分包含着零个或任意多个数据库， 以及各个数据库中的键值对数据：如果服务器的数据库状态为空（所有数据库都是空的）， 那么这个部分也为空， 长度为 0 字节。如果服务器的数据库状态为非空（有至少一个数据库非空）， 那么这个部分也为非空， 根据数据库所保存键值对的数量、类型和内容不同， 这个部分的长度也会有所不同 EOF 常量的长度为 1 字节， 这个常量标志着 RDB 文件正文内容的结束， 当读入程序遇到这个值的时候， 它知道所有数据库的所有键值对都已经载入完毕了。 check_sum 是一个 8 字节长的无符号整数， 保存着一个校验和， 这个校验和是程序通过对 REDIS 、 db_version 、 databases 、 EOF 四个部分的内容进行计算得出的。 服务器在载入 RDB 文件时， 会将载入数据所计算出的校验和与 check_sum 所记录的校验和进行对比， 以此来检查 RDB 文件是否有出错或者损坏的情况出现。 作为例子， 图 IMAGE_RDB_WITH_EMPTY_DATABASE 展示了一个 databases 部分为空的 RDB 文件： 文件开头的 “REDIS” 表示这是一个 RDB 文件， 之后的 “0006” 表示这是第六版的 RDB 文件， 因为 databases 为空， 所以版本号之后直接跟着 EOF 常量， 最后的 6265312314761917404 是文件的校验和。 1.databases 部分一个 RDB 文件的 databases 部分可以保存任意多个非空数据库。 比如说， 如果服务器的 0 号数据库和 3 号数据库非空， 那么服务器将创建一个如图 IMAGE_RDB_WITH_TWO_DB 所示的 RDB 文件， 图中的 database 0 代表 0 号数据库中的所有键值对数据， 而 database 3 则代表 3 号数据库中的所有键值对数据。 每个非空数据库在 RDB 文件中都可以保存为 SELECTDB 、 db_number 、 key_value_pairs 三个部分， 如图 IMAGE_DATABASE_STRUCT_OF_RDB 所示。 SELECTDB 常量的长度为 1 字节， 当读入程序遇到这个值的时候， 它知道接下来要读入的将是一个数据库号码。 db_number 保存着一个数据库号码， 根据号码的大小不同， 这个部分的长度可以是 1 字节、 2 字节或者 5 字节。 当程序读入 db_number 部分之后， 服务器会调用 SELECT 命令， 根据读入的数据库号码进行数据库切换， 使得之后读入的键值对可以载入到正确的数据库中。 key_value_pairs 部分保存了数据库中的所有键值对数据， 如果键值对带有过期时间， 那么过期时间也会和键值对保存在一起。 根据键值对的数量、类型、内容、以及是否有过期时间等条件的不同， key_value_pairs 部分的长度也会有所不同。 作为例子， 图 IMAGE_EXAMPLE_OF_DB 展示了 RDB 文件中， 0 号数据库的结构。 另外， 图 IMAGE_RDB_WITH_DB_0_AND_DB_3 则展示了一个完整的 RDB 文件， 文件中包含了 0 号数据库和 3 号数据库。 2.key_value_pairs 部分RDB 文件中的每个 key_value_pairs 部分都保存了一个或以上数量的键值对， 如果键值对带有过期时间的话， 那么键值对的过期时间也会被保存在内。 不带过期时间的键值对在 RDB 文件中对由 TYPE 、 key 、 value 三部分组成， 如图 IMAGE_KEY_WITHOUT_EXPIRE_TIME 所示。 12345678910# TYPE 记录了 value 的类型， 长度为 1 字节， 值可以是以下常量的其中一个：REDIS_RDB_TYPE_STRINGREDIS_RDB_TYPE_LISTREDIS_RDB_TYPE_SETREDIS_RDB_TYPE_ZSETREDIS_RDB_TYPE_HASHREDIS_RDB_TYPE_LIST_ZIPLISTREDIS_RDB_TYPE_SET_INTSETREDIS_RDB_TYPE_ZSET_ZIPLISTREDIS_RDB_TYPE_HASH_ZIPLIST 以上列出的每个 TYPE 常量都代表了一种对象类型或者底层编码， 当服务器读入 RDB 文件中的键值对数据时， 程序会根据 TYPE 的值来决定如何读入和解释 value 的数据。 key 和 value 分别保存了键值对的键对象和值对象： 其中 key 总是一个字符串对象， 它的编码方式和 REDIS_RDB_TYPE_STRING 类型的 value 一样。 根据内容长度的不同， key 的长度也会有所不同。 根据 TYPE 类型的不同， 以及保存内容长度的不同， 保存 value 的结构和长度也会有所不同， 本节稍后会详细说明每种 TYPE 类型的 value 结构保存方式。 带有过期时间的键值对在 RDB 文件中的结构如图 IMAGE_KEY_WITH_EXPIRE_TIME 所示。 带有过期时间的键值对中的 TYPE 、 key 、 value 三个部分的意义， 和前面介绍的不带过期时间的键值对的 TYPE 、 key 、 value 三个部分的意义完全相同， 至于新增的 EXPIRETIME_MS 和 ms ， 它们的意义如下： EXPIRETIME_MS 常量的长度为 1 字节， 它告知读入程序， 接下来要读入的将是一个以毫秒为单位的过期时间。 ms 是一个 8 字节长的带符号整数， 记录着一个以毫秒为单位的 UNIX 时间戳， 这个时间戳就是键值对的过期时间。 作为例子， 图 IMAGE_EXAMPLE_OF_KEY_WITHOUT_EXPIRE_TIME 展示了一个没有过期时间的字符串键值对。 图 IMAGE_EXAMPLE_OF_KEY_WITH_EXPIRE_TIME 展示了一个带有过期时间的集合键值对， 其中键的过期时间为 1388556000000 （2014 年 1 月 1 日零时）。 3.value 的编码RDB 文件中的每个 value 部分都保存了一个值对象， 每个值对象的类型都由与之对应的 TYPE 记录， 根据类型的不同， value 部分的结构、长度也会有所不同。在接下来的各个小节中， 我们将分别介绍各种不同类型的值对象在 RDB 文件中的保存结构。 3.1.字符串对象如果 TYPE 的值为 REDIS_RDB_TYPE_STRING ， 那么 value 保存的就是一个字符串对象， 字符串对象的编码可以是 REDIS_ENCODING_INT 或者 REDIS_ENCODING_RAW 。 如果字符串对象的编码为 REDIS_ENCODING_INT ， 那么说明对象中保存的是长度不超过 32 位的整数， 这种编码的对象将以图 IMAGE_INT_ENCODING_STRING 所示的结构保存。 其中， ENCODING 的值可以是 REDIS_RDB_ENC_INT8 、 REDIS_RDB_ENC_INT16 或者 REDIS_RDB_ENC_INT32 三个常量的其中一个， 它们分别代表 RDB 文件使用 8 位（bit）、 16 位或者 32 位来保存整数值 integer 。举个例子， 如果字符串对象中保存的是可以用 8 位来保存的整数 123 ， 那么这个对象在 RDB 文件中保存的结构将如图 IMAGE_EXAMPLE_OF_INT_ENCODING_STRING 所示。 如果字符串对象的编码为 REDIS_ENCODING_RAW ， 那么说明对象所保存的是一个字符串值， 根据字符串长度的不同， 有压缩和不压缩两种方法来保存这个字符串： 如果字符串的长度小于等于 20 字节， 那么这个字符串会直接被原样保存。 如果字符串的长度大于 20 字节， 那么这个字符串会被压缩之后再保存。 注意以上两个条件是在假设服务器打开了 RDB 文件压缩功能的情况下进行的， 如果服务器关闭了 RDB 文件压缩功能， 那么 RDB 程序总以无压缩的方式保存字符串值。具体信息可以参考 redis.conf 文件中关于 rdbcompression 选项的说明。 对于没有被压缩的字符串， RDB 程序会以图 IMAGE_NON_COMPRESS_STRING 所示的结构来保存该字符串。 其中， string 部分保存了字符串值本身，而 len 保存了字符串值的长度。 对于压缩后的字符串， RDB 程序会以图 IMAGE_COMPRESSED_STRING 所示的结构来保存该字符串。 其中， REDIS_RDB_ENC_LZF 常量标志着字符串已经被 LZF 算法（http://liblzf.plan9.de）压缩过了， 读入程序在碰到这个常量时， 会根据之后的 compressed_len 、 origin_len 和 compressed_string 三部分， 对字符串进行解压缩： 其中 compressed_len 记录的是字符串被压缩之后的长度， 而 origin_len 记录的是字符串原来的长度， compressed_string 记录的则是被压缩之后的字符串。 图 IMAGE_EXAMPLE_OF_NON_COMPRESS_STRING 展示了一个保存无压缩字符串的例子， 其中字符串的长度为 5 ， 字符串的值为 “hello” 。 图 IMAGE_EXAMPLE_OF_COMPRESS_STRING 展示了一个压缩后的字符串示例， 从图中可以看出， 字符串原本的长度为 21 ， 压缩之后的长度为 6 ， 压缩之后的字符串内容为 “?aa???” ， 其中 ? 代表的是无法用字符串形式打印出来的字节。 3.2.列表对象如果 TYPE 的值为 REDIS_RDB_TYPE_LIST ， 那么 value 保存的就是一个 REDIS_ENCODING_LINKEDLIST 编码的列表对象， RDB 文件保存这种对象的结构如图 IMAGE_LINKEDLIST_ENCODING_LIST 所示。 list_length 记录了列表的长度， 它记录列表保存了多少个项（item）， 读入程序可以通过这个长度知道自己应该读入多少个列表项。图中以 item 开头的部分代表列表的项， 因为每个列表项都是一个字符串对象， 所以程序会以处理字符串对象的方式来保存和读入列表项。作为示例， 图 IMAGE_EXAMPLE_OF_LINKEDLIST_ENCODING_LIST 展示了一个包含三个元素的列表。 结构中的第一个数字 3 是列表的长度， 之后跟着的分别是第一个列表项、第二个列表项和第三个列表项， 其中： 第一个列表项的长度为 5 ， 内容为字符串 “hello” 。 第二个列表项的长度也为 5 ， 内容为字符串 “world” 。 第三个列表项的长度为 1 ， 内容为字符串 “!” 。 3.3.集合对象如果 TYPE 的值为 REDIS_RDB_TYPE_SET ， 那么 value 保存的就是一个 REDIS_ENCODING_HT 编码的集合对象， RDB 文件保存这种对象的结构如图 IMAGE_HT_ENCODING_SET 所示。 其中， set_size 是集合的大小， 它记录集合保存了多少个元素， 读入程序可以通过这个大小知道自己应该读入多少个集合元素。图中以 elem 开头的部分代表集合的元素， 因为每个集合元素都是一个字符串对象， 所以程序会以处理字符串对象的方式来保存和读入集合元素。作为示例， 图 IMAGE_EXAMPLE_OF_HT_SET 展示了一个包含四个元素的集合。 结构中的第一个数字 4 记录了集合的大小， 之后跟着的是集合的四个元素： 第一个元素的长度为 5 ，值为 “apple” 。 第二个元素的长度为 6 ，值为 “banana” 。 第三个元素的长度为 3 ，值为 “cat” 。 第四个元素的长度为 3 ，值为 “dog” 。 3.4.哈希表对象如果 TYPE 的值为 REDIS_RDB_TYPE_HASH ， 那么 value 保存的就是一个 REDIS_ENCODING_HT 编码的集合对象， RDB 文件保存这种对象的结构如图 IMAGE_HT_HASH 所示： hash_size 记录了哈希表的大小， 也即是这个哈希表保存了多少键值对， 读入程序可以通过这个大小知道自己应该读入多少个键值对。 以 key_value_pair 开头的部分代表哈希表中的键值对， 键值对的键和值都是字符串对象， 所以程序会以处理字符串对象的方式来保存和读入键值对。 结构中的每个键值对都以键紧挨着值的方式排列在一起， 如图 IMAGE_KEY_VALUE_PAIR_OF_HT_HASH 所示。 因此， 从更详细的角度看， 图 IMAGE_HT_HASH 所展示的结构可以进一步修改为图 IMAGE_DETIAL_HT_HASH 。 作为示例， 图 IMAGE_EXAMPLE_OF_HT_HASH 展示了一个包含两个键值对的哈希表。 在这个示例结构中， 第一个数字 2 记录了哈希表的键值对数量， 之后跟着的是两个键值对： 第一个键值对的键是长度为 1 的字符串 “a” ， 值是长度为 5 的字符串 “apple” 。 第二个键值对的键是长度为 1 的字符串 “b” ， 值是长度为 6 的字符串 “banana” 3.5.有序集合对象如果 TYPE 的值为 REDIS_RDB_TYPE_ZSET ， 那么 value 保存的就是一个 REDIS_ENCODING_SKIPLIST 编码的有序集合对象， RDB 文件保存这种对象的结构如图 IMAGE_SKIPLIST_ZSET 所示。 sorted_set_size 记录了有序集合的大小， 也即是这个有序集合保存了多少元素， 读入程序需要根据这个值来决定应该读入多少有序集合元素。以 element 开头的部分代表有序集合中的元素， 每个元素又分为成员（member）和分值（score）两部分， 成员是一个字符串对象， 分值则是一个 double 类型的浮点数， 程序在保存 RDB 文件时会先将分值转换成字符串对象， 然后再用保存字符串对象的方法将分值保存起来。有序集合中的每个元素都以成员紧挨着分值的方式排列， 如图 IMAGE_MEMBER_AND_SCORE_OF_ZSET 所示。 因此， 从更详细的角度看， 图 IMAGE_SKIPLIST_ZSET 所展示的结构可以进一步修改为图 IMAGE_DETIAL_SKIPLIST_ZSET 。 作为示例， 图 IMAGE_EXAMPLE_OF_SKIPLIST_ZSET 展示了一个带有两个元素的有序集合。 在这个示例结构中， 第一个数字 2 记录了有序集合的元素数量， 之后跟着的是两个有序集合元素： 第一个元素的成员是长度为 2 的字符串 “pi” ， 分值被转换成字符串之后变成了长度为 4 的字符串 “3.14” 。 第二个元素的成员是长度为 1 的字符串 “e” ， 分值被转换成字符串之后变成了长度为 3 的字符串 “2.7” 。 3.6.INTSET 编码的集合如果 TYPE 的值为 REDIS_RDB_TYPE_SET_INTSET ， 那么 value 保存的就是一个整数集合对象， RDB 文件保存这种对象的方法是， 先将整数集合转换为字符串对象， 然后将这个字符串对象保存到 RDB 文件里面。 如果程序在读入 RDB 文件的过程中， 碰到由整数集合对象转换成的字符串对象， 那么程序会根据 TYPE 值的指示， 先读入字符串对象， 再将这个字符串对象转换成原来的整数集合对象。 3.7.ZIPLIST 编码的列表、哈希表或者有序集合如果 TYPE 的值为 REDIS_RDB_TYPE_LIST_ZIPLIST 、 REDIS_RDB_TYPE_HASH_ZIPLIST 或者 REDIS_RDB_TYPE_ZSET_ZIPLIST ， 那么 value 保存的就是一个压缩列表对象， RDB 文件保存这种对象的方法是： 将压缩列表转换成一个字符串对象。 将转换所得的字符串对象保存到 RDB 文件。 如果程序在读入 RDB 文件的过程中， 碰到由压缩列表对象转换成的字符串对象， 那么程序会根据 TYPE 值的指示， 执行以下操作： 读入字符串对象，并将它转换成原来的压缩列表对象。 根据 TYPE 的值，设置压缩列表对象的类型： 如果 TYPE 的值为 REDIS_RDB_TYPE_LIST_ZIPLIST ， 那么压缩列表对象的类型为列表； 如果 TYPE 的值为 REDIS_RDB_TYPE_HASH_ZIPLIST ， 那么压缩列表对象的类型为哈希表； 如果TYPE 的值为 REDIS_RDB_TYPE_ZSET_ZIPLIST ， 那么压缩列表对象的类型为有序集合。 从步骤 2 可以看出， 由于 TYPE 的存在， 即使列表、哈希表和有序集合三种类型都使用压缩列表来保存， RDB 读入程序也总可以将读入并转换之后得出的压缩列表设置成原来的类型。 转自:RDB 文件结构","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"order set(有序set)结构及命令详解","date":"2017-04-16T04:47:25.784Z","path":"2017/04/16/nosql/redis/order set(有序set)结构及命令详解/","text":"zadd 添加ZADD key [NX|XX] [CH] [INCR] score member [score member …]如果指定添加的成员已经是有序集合里面的成员，则会更新改成员的分数（scrore）并更新到正确的排序位置如果key不存在，将会创建一个新的有序集合（sorted set）并将分数/成员（score/member）对添加到有序集合，就像原来存在一个空的有序集合一样。如果key存在，但是类型不是有序集合，将会返回一个错误应答历史>= 2.4: 接受多个成员。 在Redis 2.4以前，命令只能添加或者更新一个成员 1234567891011121314151617181920212223XX: 仅仅更新存在的成员，不添加新成员。NX: 不更新存在的成员。只添加新成员。CH: 修改返回值为发生变化的成员总数，原始是返回新添加成员的总数 (CH 是 changed 的意思)。更改的元素是新添加的成员，已经存在的成员更新分数。 所以在命令中指定的成员有相同的分数将不被计算在内。注：在通常情况下，ZADD返回值只计算新添加成员的数量。INCR: 当ZADD指定这个选项时，成员的操作就等同ZINCRBY命令，对成员的分数进行递增操作。--------------------------------------------------------------------------------------------------------------------------------------------------redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 1 &quot;uno&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot; 3 &quot;three&quot;(integer) 2redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;1&quot;3) &quot;uno&quot;4) &quot;1&quot;5) &quot;two&quot;6) &quot;2&quot;7) &quot;three&quot;8) &quot;3&quot;redis&gt; zrange 查询一定范围的元素ZRANGE key start stop [WITHSCORES]把集合排序后，返回名次【start，stop】的元素默认是升序排列withscores 是把score也打印出来1234567891011121314151617181920212223 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZRANGE myzset 0 -11) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; ZRANGE myzset 2 31) &quot;three&quot;redis&gt; ZRANGE myzset -2 -11) &quot;two&quot;2) &quot;three&quot;redis&gt;redis&gt; ZRANGE myzset 0 1 WITHSCORES #打印分数1) &quot;one&quot;2) &quot;1&quot;3) &quot;two&quot;4) &quot;2&quot;redis&gt; zrangebyscore 指定分数的最大与最小值查询ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]作用：集合（升序）排序后，取score在【start，max】内的元素返回分数在min和max之间（包含）的有序的元素如果有limit， 则跳过offset，取出count个withscores 是打印分数和元素 12345678910111213141516171819202122232425262728&apos;区间的控制&apos;min和max可以是-inf和+inf，这样一来，你就可以在不知道有序集的最低和最高score值的情况下，使用ZRANGEBYSCORE这类命令。默认情况下，区间的取值使用闭区间(小于等于或大于等于)，你也可以通过给参数前增加(符号来使用可选的开区间(小于或大于)。举个例子：ZRANGEBYSCORE zset (1 5返回所有符合条件1 &lt; score &lt;= 5的成员；ZRANGEBYSCORE zset (5 (10返回所有符合条件5 &lt; score &lt; 10 的成员。redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZRANGEBYSCORE myzset -inf +inf1) &quot;one&quot;2) &quot;two&quot;3) &quot;three&quot;redis&gt; ZRANGEBYSCORE myzset 1 21) &quot;one&quot;2) &quot;two&quot;redis&gt; ZRANGEBYSCORE myzset (1 21) &quot;two&quot;redis&gt; ZRANGEBYSCORE myzset (1 (2(empty list or set)redis&gt; zrank 升序时查看排名ZRANK key member从低到高排序，第一个元素排名是0 1234567891011 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZRANK myzset &quot;three&quot;(integer) 2redis&gt; ZRANK myzset &quot;four&quot; #元素不存在就返回nil(nil)redis&gt; zrevrank 降序时查看排名ZREVRANK key member从高到低排序，第一个元素排名是0 1234567891011 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZREVRANK myzset &quot;one&quot;(integer) 2redis&gt; ZREVRANK myzset &quot;four&quot;(nil)redis&gt; zremrangebyscore删除：通过分数来删ZREMRANGEBYSCORE key min max 1234567891011121314redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZREMRANGEBYSCORE myzset -inf (2 #（-inf，2） 不包含2(integer) 1redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;two&quot;2) &quot;2&quot;3) &quot;three&quot;4) &quot;3&quot;redis&gt; zremrangebyrank删除：通过排名来删ZREMRANGEBYRANK key start stop 排名是从0开始的123456789101112 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZREMRANGEBYRANK myzset 0 1(integer) 2redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;three&quot;2) &quot;3&quot;redis&gt; zrem删除；指定元素ZREM key member [member …] 1234567891011121314 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZREM myzset &quot;two&quot; #删除指定值的元素(integer) 1redis&gt; ZRANGE myzset 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;1&quot;3) &quot;three&quot;4) &quot;3&quot;redis&gt; zcard 统计集合的数量ZCARD key 1234567 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZCARD myzset #统计(integer) 2redis&gt; zcount：统计在特殊分数段的元素的个数ZCOUNT key min max 1234567891011 redis&gt; ZADD myzset 1 &quot;one&quot;(integer) 1redis&gt; ZADD myzset 2 &quot;two&quot;(integer) 1redis&gt; ZADD myzset 3 &quot;three&quot;(integer) 1redis&gt; ZCOUNT myzset -inf +inf #【-inf , +inf 】(integer) 3 redis&gt; ZCOUNT myzset (1 3 #（1,3】 的个数(integer) 2redis&gt; zunionstore求并集：ZUNIONSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 计算给定的numkeys个有序集合的并集，并且把结果放到destination中。在给定要计算的key和其它参数之前，必须先给定key个数(numberkeys)。 默认情况下，结果集中某个成员的score值是所有给定集下该成员score值之和。 使用WEIGHTS选项，你可以为每个给定的有序集指定一个乘法因子，意思就是，每个给定有序集的所有成员的score值在传递给聚合函数之前都要先乘以该因子。如果WEIGHTS没有给定，默认就是1。 使用AGGREGATE选项，你可以指定并集的结果集的聚合方式。默认使用的参数SUM，可以将所有集合中某个成员的score值之和作为结果集中该成员的score值。如果使用参数MIN或者MAX，结果集就是所有集合中元素最小或最大的元素。1234567891011121314151617181920 redis&gt; ZADD zset1 1 &quot;one&quot;(integer) 1redis&gt; ZADD zset1 2 &quot;two&quot;(integer) 1redis&gt; ZADD zset2 1 &quot;one&quot;(integer) 1redis&gt; ZADD zset2 2 &quot;two&quot;(integer) 1redis&gt; ZADD zset2 3 &quot;three&quot;(integer) 1redis&gt; ZUNIONSTORE out 2 zset1 zset2 WEIGHTS 2 3(integer) 3redis&gt; ZRANGE out 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;5&quot;3) &quot;three&quot;4) &quot;9&quot;5) &quot;two&quot;6) &quot;10&quot;redis&gt; ZINTERSTORE求交集：ZINTERSTORE destination numkeys key [key …] [WEIGHTS weight [weight …]] [AGGREGATE SUM|MIN|MAX] 语法参见zunionstore12345678910111213141516171819 redis&gt; ZADD zset1 1 &quot;one&quot;(integer) 1redis&gt; ZADD zset1 2 &quot;two&quot;(integer) 1redis&gt; ZADD zset2 1 &quot;one&quot;(integer) 1redis&gt; ZADD zset2 2 &quot;two&quot;(integer) 1redis&gt; ZADD zset2 3 &quot;three&quot;(integer) 1redis&gt; ZINTERSTORE out 2 zset1 zset2 WEIGHTS 2 3(integer) 2redis&gt; ZRANGE out 0 -1 WITHSCORES1) &quot;one&quot;2) &quot;5&quot;3) &quot;two&quot;4) &quot;10&quot;redis&gt;#1.将单个集合中的所有元素乘以weights, 2.将每个集合间key相同的score相加(aggregate默认是求sum),然后求交集","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"flushall故障处理和rdb服务器间的迁移","date":"2017-04-16T04:47:25.782Z","path":"2017/04/16/nosql/redis/flushall故障处理和rdb服务器间的迁移/","text":"1.flushall故障处理12flushdb #清空当前库所有键flushall #清空所有库所有键 如果不小心运行了flushall, 立即 shutdown nosave ,关闭服务器, 然后 手工编辑aof文件, 去掉文件中的 “flushall ”相关行, 然后开启服务器,就可以导入回原来数据.如果,flushall之后,系统恰好bgrewriteaof了,那么aof就清空了,数据丢失. 2.rdb服务器间的迁移将rdb文件拿到，然后在配置文件中指定rdb的路径，redis会自动读入rdb文件，实现迁移","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"aof日志持久化","date":"2017-04-16T04:47:25.780Z","path":"2017/04/16/nosql/redis/aof日志持久化/","text":"1.简介除了Redis的持久化功能外，Redis还提供了AOF（Append Only File ）持久化功能，AOF持久化是通过保存Redis服务器所执行的写命令来记录数据库状态的，如下图： aof持久化功能的实现可以分为命令追加（append），文件写入、文件同步（sync）三个步骤 2.命令追加服务器在执行完一个写命令之后，会以协议格式将被执行的写命令追加到服务器状态的aof_buf缓冲区的末尾 1234567struct redisServer&#123; //.... //aof缓冲区 sds aof_buf; //.....&#125; 如：如果客户端向服务器发送以下命令：12redis&gt; set key valueok 那么在aof_buf缓冲区的末尾： 3.文件写入与同步Redis的服务器进程就是一个事件循环（loop），这个循环中的文件事件负责接收客户端的命令请求，以及向客户端发送命令回复，而时间事件负责执行向serverCron函数这样需要定时运行的函数, 在服务器每次结束一个事件循环之前，他都会调用flushAppendOnlyFile函数，考虑是否需要将aof_buf缓冲区中的内容写入和保存到AOF文件里面，这个过程可以用下面的伪代码表示： 1234567891011def eventLoop(): while True: #处理文件事件，接受命令请求以及发送命令回复 #处理命令请求时可能会有新内容被追加到aof_buf缓冲区中 processFileEvents() #处理时间事件（定时任务） processTimeEvents() #考虑是否要将aof_buf中的内容写入和保存到aof文件里面 flushAppendOnlyFile() flushAppendOnlyFile函数的行为有配置文件中的appendfsync选项的值来确定 appendfsync选项的值 flushAppendOnlyFile函数的行为 always 将aof_buf缓冲区中你的所有内容写入到AOF文件中 everysec 将aof_buf缓冲区中的所有内容写入到AOF文件中，如果上次同步aof文件的时间距离现在超过一秒，那么再次对aof文件进行同步，并且这个同步操作是由一个线程专门负责执行的 no 将aof_buf缓冲区中你的所有内容写入到aof文件中，但并不对aof文件进行同步，何时同步由操作系统决定 4.配置文件中和aof相关的参数12345678910111213appendonly no #是否打开aof日志功能appendfilename &quot;appendonly.aof&quot; #aof文件名# appendfsync always #每一个命令都立即同步到aof，安全，速度慢appendfsync everysec #每秒写1次aof# appendfsync no #由操作系统判断缓冲区的大小统一写入aofno-appendfsync-on-rewrite no #正在导出rdb快照的过程中要不要停止同步aofauto-aof-rewrite-percentage 100 #aof文件大小比上次重写时的大小，增长率100%时重写auto-aof-rewrite-min-size 64mb #aof文件至少超过64M时重写 5.aof文件的载入与数据还原因为aof文件里面包含了重建数据库状态所需的所有写命令，所以只要服务器读入并重新执行一遍aof文件里面保存的写命令，就可以还原服务器关闭之前的数据库状态，步骤如下（以下步骤都是在服务端启动时完成的）： 创建一个不带网络连接的伪客户端 因为要执行aof中的命令，而命令只能在客户端中执行 从aof文件中分析并读取一行一条写命令 使用伪客户端执行被读出的写命令 循环执行步骤2和步骤3，知道aof文件中的所有写命令都被处理完毕为止 6.aof重写随着服务器运行时间的流逝，aof文件中的内容会越来越多，文件的体积会越来越大，并且aof文件的体积越大，使用aof文件来进行数据还原所需的时间就越多 举例：123456789101112131415redis&gt;push list &quot;a&quot; &quot;b&quot; //[&quot;a&apos;, &quot;b&quot;]redis&gt;push list &quot;c&quot; //[&quot;a&apos;, &quot;b&quot;, &quot;c&quot;]redis&gt;push list &quot;d&quot; &quot;e&quot; //[&quot;a&apos;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot; ]redis&gt;lpop listredis&gt;lpoplistredis&gt;rpush list &quot;f&quot; &quot;g&quot; //[c, d ,e ,f, g]&apos;那么光是为了记录这个list键的状态，aof文件就需要保存6条命令，在实际应用中，写命令执行的次数和频率会比上面的简单示例要高得多，所以造成的问题也会严重得多，所以就有了aof重写的功能&apos; 通过重写，Redis服务器可以创建一个新的aof文件来替代现有的aof文件，新旧两个aof文件所保存的数据库状态想听，但是新aof文件不会包含任何浪费空间的冗余命令，所以以新aof文件的体积通常会比旧aof文件的体积要小得多 7.aof文件重写的实现aof 文件重写并不需要对现有的aof文件进行任何操作、分析或者写入操作，这个功能是通过读取服务器,当前的数据库状态来实现的 123456789101112131415161718redis&gt;push list &quot;a&quot; &quot;b&quot; //[&quot;a&apos;, &quot;b&quot;] redis&gt;push list &quot;c&quot; //[&quot;a&apos;, &quot;b&quot;, &quot;c&quot;] redis&gt;push list &quot;d&quot; &quot;e&quot; //[&quot;a&apos;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot; ] redis&gt;lpop list redis&gt;lpoplist redis&gt;rpush list &quot;f&quot; &quot;g&quot; //[c, d ,e ,f, g]&apos;如果没有使用重写，那么aof中将被记录6条命令重写：直接从数据库中读取键list的值，然后用一条rpush list &quot;c&quot; &quot;d&quot; &quot;e&quot; &quot;f&quot; &quot;g&quot;命令来替代保存在aof文件中你的6条命令，这样就可以将保存list键所需的命令从6条减少为1条&apos; 重写的伪代码实现12345678910111213141516171819202122232425262728293031323334def aof_rewrite(new_aof_file_name): #创建新aof文件 f = create_file(new_aof_file_name) #遍历数据库 for db in redisServer.db: #忽略空数据库 if db.is_empty(): continue #写入select命令，指定数据库号码 f.write_command(&quot;select&quot;+db.id) #遍历数据库中的所有的键 for key in db: #忽略已过期的键 if key.is_expired(): continue #根据键的类型对键进行重写 if key.type == string: rewrite_string(key) elif key.type == List: rewrite_list(key) elif key.type == Hash: rewrite_hash(key) elif key.type == Set: rewrite_set(key) elif key.type == SortedSet: rewrite_sorted_set(key) #如果键带有过期时间，那么过期时间也要被重写 if key.have_expire_time(): rewrite_expire_time(key) #写入完毕，关闭文件 f.close() 8.aof后台重写aof_rewrite函数在重写的时候会大量的写入操作，所以调用这个函数的线程将被长时间的阻塞，因为Redis服务器使用单个线程来处理命令请求，所以如果由服务器直接调用aof_rewrite函数的话那么在重写aof文件期间，服务器将无法处理客户端发来的请求命令，所以将重写的任务放到子进程里执行 子进程进行aof重写期间，服务器进程（父进程）可以继续处理命令请求 子进程带有服务器进程的数据副本，使用子进程而不是线程，可以避免使用锁的情况下，保证数据的安全性 需要解决的问题：在子进程进行aof重写期间，服务器进程还需要继续处理命令请求，而新的命令可能对现有的数据库状态进行修改，从而使得服务器当前的数据库状态和重写后的aof文件所保存的数据库状态不一致 如下： 时间 服务器进程 子进程 T1 执行命令: set k1 v1 T2 执行命令: set k1 v2 T3 执行命令: set k1 v3 T4 创建子进程,执行aof文件重写 开始aof文件重写 T5 执行命令: set k2 10086 执行重写操作 T6 执行命令: set k3 123456 执行重写操作 T7 执行命令: set k4 2222 完成aof文件重写 当子进程进行重写时，数据库中只有一个k1键，但是当子进程完成aof文件重写之后，服务器进程的数据库中已经新设置了k2 , k3 ,k4三个键，因此，重写后的aof文件和服务器当前的数据库状态并不一致新的aof文件只保存了k1一个键的数据，而服务器数据库现在却有k1, k2, k3 , k4四个键 为了解决数据不一致的问题，Redis服务器设置了一个aof重写缓冲区，这个缓冲区在服务器创建子进程之后开始使用，当Redis服务器执行完一个写命令之后，他会同时将这个写命令发送给aof缓冲区和aof重写缓冲区 在子进程执行aof重写期间，服务器进程需要执行以下三个工作： 执行客户端发送来的命令 将执行后的写命令追加到aof缓冲区 将执行后的写命令追加到aof重写缓冲区 当子进程完成aof重写工作之后，他会向父进程发送一个信号，父进程在接到信号之后，会调用一个信号处理函数，并执行以下工作： 将aof重写缓冲区中的所有内容写入到aof文件中，这时新aof文件所保存的数据库状态将和服务器当前的数据库状态一致 对新的aof文件进行改名，原子地覆盖现有的aof文件，完成新旧两个aof文件的替换 在整个aof后台重写过程中，只有信号处理函数执行时会对服务器进程（父进程）造成阻塞，在其他时候 aof后台重写都不会阻塞父进程，这将aof重写对服务器性能造成的影响降到了最低 时间 服务器进程 子进程 T1 执行命令: set k1 v1 T2 执行命令: set k1 v2 T3 执行命令: set k1 v3 T4 创建子进程,执行aof文件重写 开始aof文件重写 T5 执行命令: set k2 10086 执行重写操作 T6 执行命令: set k3 123456 执行重写操作 T7 执行命令: set k4 2222 完成aof文件重写,向父进程发送信号 T8 接收到子进程发来的信号，将命令： set k2 10086 set k3 123456 set k4 2222 追加到新aof文件的末尾 T9 用新aof文件覆盖旧aof文件","tags":[{"name":"redis","slug":"redis","permalink":"http://yoursite.com/tags/redis/"}]},{"title":"聚集运算之group与aggregate(转)","date":"2017-04-16T04:47:25.777Z","path":"2017/04/16/nosql/mongodb/聚集运算之group与aggregate(转)/","text":"1.group1.1.语法123456789db.collection.group(&#123; key:&#123;field:1&#125;,//按什么字段进行分组 initial:&#123;count:0&#125;,//进行分组前变量初始化,该处声明的变量可以在以下回调函数中作为result的属性使用 cond:&#123;&#125;,//类似mysql中的having,分组后的查询返回 reduce: function ( curr, result ) &#123; &#125;, //The function takes two arguments: the current document and an aggregation result document for that group. #先迭代出分组,然后再迭代分组中的文档,即curr变量就代表当前分组中此刻迭代到的文档,result变量就代表当前分组。 keyf：function(doc)&#123;&#125;,//keyf和key二选一,传入的参数doc代表当前文档,如果分组的字段是经过运算后的字段用到,作用类似mysql中的group by left(&apos;2015-09-12 14:05:22&apos;,10); finalize:function(result) &#123;&#125;//该result也就是reduce的result,都是代表当前分组,这个函数是在走完当前分组结束后回调;&#125;) 除了分组的key字段外,就只返回有result参数的回调函数中的操作的属性字段; 1.2.实例12345678910# 表结构如下&#123; _id: ObjectId(&quot;5085a95c8fada716c89d0021&quot;), ord_dt: ISODate(&quot;2012-07-01T04:00:00Z&quot;), ship_dt: ISODate(&quot;2012-07-02T04:00:00Z&quot;), item: &#123; sku: &quot;abc123&quot;, price: 1.99, uom: &quot;pcs&quot;, qty: 25 &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#Example1SELECT ord_dt, item_skuFROM ordersWHERE ord_dt &gt; &apos;01/01/2012&apos;GROUP BY ord_dt, item_sku↓↓↓↓db.orders.group( &#123; key: &#123; ord_dt: 1, &apos;item.sku&apos;: 1 &#125;, cond: &#123; ord_dt: &#123; $gt: new Date( &apos;01/01/2012&apos; ) &#125; &#125;, reduce: function ( curr, result ) &#123; &#125;, initial: &#123; &#125; &#125;) #Example2SELECT ord_dt, item_sku, SUM(item_qty) as totalFROM ordersWHERE ord_dt &gt; &apos;01/01/2012&apos;GROUP BY ord_dt, item_sku↓↓↓↓db.orders.group( &#123; key: &#123; ord_dt: 1, &apos;item.sku&apos;: 1 &#125;, cond: &#123; ord_dt: &#123; $gt: new Date( &apos;01/01/2012&apos; ) &#125; &#125;, reduce: function( curr, result ) &#123; result.total += curr.item.qty; &#125;, initial: &#123; total : 0 &#125; &#125;) #Example3db.orders.group( &#123; keyf: function(doc) &#123; #计算的结果进行分组 return &#123; day_of_week: doc.ord_dt.getDay() &#125;; &#125;, cond: &#123; ord_dt: &#123; $gt: new Date( &apos;01/01/2012&apos; ) &#125; &#125;, reduce: function( curr, result ) &#123; result.total += curr.item.qty; result.count++; &#125;, initial: &#123; total : 0, count: 0 &#125;, finalize: function(result) &#123; var weekdays = [ &quot;Sunday&quot;, &quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;, &quot;Saturday&quot; ]; result.day_of_week = weekdays[result.day_of_week]; result.avg = Math.round(result.total / result.count); &#125; &#125;) [ &#123; &quot;day_of_week&quot; : &quot;Sunday&quot;, &quot;total&quot; : 70, &quot;count&quot; : 4, &quot;avg&quot; : 18 &#125;, &#123; &quot;day_of_week&quot; : &quot;Friday&quot;, &quot;total&quot; : 110, &quot;count&quot; : 6, &quot;avg&quot; : 18 &#125;, &#123; &quot;day_of_week&quot; : &quot;Tuesday&quot;, &quot;total&quot; : 70, &quot;count&quot; : 3, &quot;avg&quot; : 23 &#125;] 1.3.工作中用到的实例123456789101112131415161718192021222324252627#查询每个栏目最贵的商品价格, max()操作&#123; key:&#123;cat_id:1&#125;, cond:&#123;&#125;, reduce:function(curr , result) &#123; if(curr.shop_price &gt; result.max) &#123; result.max = curr.shop_price; &#125; &#125;, initial:&#123;max:0&#125;&#125; #查询每个栏目下商品的平均价格&#123; key:&#123;cat_id:1&#125;, cond:&#123;&#125;, reduce:function(curr , result) &#123; result.cnt += 1; result.sum += curr.shop_price; &#125;, initial:&#123;sum:0,cnt:0&#125;, finalize:function(result) &#123; result.avg = result.sum/result.cnt; //在每次分组完毕后进行运算 &#125;&#125; group其实略微有点鸡肋,因为既然用到了mongodb,那复制集和分片是避无可免的,而group是不支持分片的运算 2.Aggregation聚合管道是一个基于数据处理管道概念的框架。通过使用一个多阶段的管道，将一组文档转换为最终的聚合结果。 2.1.语法参考手册: http://docs.mongoing.com/manual-zh/core/aggregation-pipeline.html 123456789101112131415161718192021222324db.collection.aggregate(pipeline, options); pipeline Array # 与mysql中的字段对比说明$project # 返回哪些字段,select,说它像select其实是不太准确的,因为aggregate是一个阶段性管道操作符,$project是取出哪些数据进入下一个阶段管道操作,真正的最终数据返回还是在group等操作中; $match # 放在group前相当于where使用,放在group后面相当于having使用 $sort # 排序1升-1降 sort一般放在group后,也就是说得到结果后再排序,如果先排序再分组没什么意义; $limit # 相当于limit m,不能设置偏移量 $skip # 跳过第几个文档 $unwind # 把文档中的数组元素打开,并形成多个文档,参考Example1 $group: &#123; _id: &lt;expression&gt;, &lt;field1&gt;: &#123; &lt;accumulator1&gt; : &lt;expression1&gt; &#125;, ... # 按什么字段分组,注意所有字段名前面都要加$,否则mongodb就为以为不加$的是普通常量,其中accumulator又包括以下几个操作符# $sum,$avg,$first,$last,$max,$min,$push,$addToSet#如果group by null就是 count(*)的效果 $geoNear # 取某一点的最近或最远,在LBS地理位置中有用 $out # 把结果写进新的集合中。注意1,不能写进一个分片集合中。注意2,不能写进 2.2.实例Example1: unwind 1234567891011121314151617&gt; db.test.insert(&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, sizes: [ &quot;S&quot;, &quot;M&quot;, &quot;L&quot;] &#125;);WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.test.aggregate( [ &#123; $unwind : &quot;$sizes&quot; &#125; ] )&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;S&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;M&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;L&quot; &#125; db.test.insert(&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;ABC1&quot;, sizes: [ &quot;S&quot;, &quot;M&quot;, &quot;L&quot;,[&quot;XXL&quot;,&apos;XL&apos;]] &#125;);WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.test.aggregate( [ &#123; $unwind : &quot;$sizes&quot; &#125; ] )&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;S&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;M&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;L&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;S&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;M&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : &quot;L&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;ABC1&quot;, &quot;sizes&quot; : [ &quot;XXL&quot;, &quot;XL&quot; ] &#125; # 只能打散一维数组 Example2 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#数据源&#123; &quot;_id&quot; : 1, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : 10, &quot;quantity&quot; : 2, &quot;date&quot; : ISODate(&quot;2014-03-01T08:00:00Z&quot;) &#125;&#123; &quot;_id&quot; : 2, &quot;item&quot; : &quot;jkl&quot;, &quot;price&quot; : 20, &quot;quantity&quot; : 1, &quot;date&quot; : ISODate(&quot;2014-03-01T09:00:00Z&quot;) &#125;&#123; &quot;_id&quot; : 3, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : 5, &quot;quantity&quot; : 10, &quot;date&quot; : ISODate(&quot;2014-03-15T09:00:00Z&quot;) &#125;&#123; &quot;_id&quot; : 4, &quot;item&quot; : &quot;xyz&quot;, &quot;price&quot; : 5, &quot;quantity&quot; : 20, &quot;date&quot; : ISODate(&quot;2014-04-04T11:21:39.736Z&quot;) &#125;&#123; &quot;_id&quot; : 5, &quot;item&quot; : &quot;abc&quot;, &quot;price&quot; : 10, &quot;quantity&quot; : 10, &quot;date&quot; : ISODate(&quot;2014-04-04T21:23:13.331Z&quot;) &#125; # 综合示例db.sales.aggregate([ # 由上到下,分阶段的进行,注意该数组中的顺序是有意义的 &#123; $project:&#123;item:1,price:1,quantity:1&#125; # 1.取出什么元素待操作; &#125;, &#123; $group:&#123; # 2. 对已取出的元素进行聚合运算; _id:&quot;$item&quot;, # 根据什么来分组 quantityCount:&#123;$sum:&apos;$quantity&apos;&#125;, priceTotal:&#123;$sum:&apos;$price&apos;&#125; &#125; &#125;, &#123; $sort:&#123; quantityCount:1 #3.升序 &#125; &#125;, # 4.基于上面的结果,取倒数第二名 &#123; $skip: 2 &#125;, &#123; $limit:1 &#125;, # 5.然后把结果写到result集合中 &#123; $out:&apos;result&apos; &#125;]) #表达式$month,$dayOfMonth,$year,$sum,$avgdb.sales.aggregate( [ &#123; $group : &#123; _id : &#123; month: &#123; $month: &quot;$date&quot; &#125;, day: &#123; $dayOfMonth: &quot;$date&quot; &#125;, year: &#123; $year: &quot;$date&quot; &#125; &#125;, #按月日年分组 totalPrice: &#123; $sum: &#123; $multiply: [ &quot;$price&quot;, &quot;$quantity&quot; ] &#125; &#125;, averageQuantity: &#123; $avg: &quot;$quantity&quot; &#125;, count: &#123; $sum: 1 &#125; &#125; &#125; ]) #结果&#123; &quot;_id&quot; : &#123; &quot;month&quot; : 3, &quot;day&quot; : 15, &quot;year&quot; : 2014 &#125;, &quot;totalPrice&quot; : 50, &quot;averageQuantity&quot; : 10, &quot;count&quot; : 1 &#125;&#123; &quot;_id&quot; : &#123; &quot;month&quot; : 4, &quot;day&quot; : 4, &quot;year&quot; : 2014 &#125;, &quot;totalPrice&quot; : 200, &quot;averageQuantity&quot; : 15, &quot;count&quot; : 2 &#125;&#123; &quot;_id&quot; : &#123; &quot;month&quot; : 3, &quot;day&quot; : 1, &quot;year&quot; : 2014 &#125;, &quot;totalPrice&quot; : 40, &quot;averageQuantity&quot; : 1.5, &quot;count&quot; : 2 &#125; ### 表达式$pushdb.sales.aggregate( [ &#123; $group: &#123; _id: &#123; day: &#123; $dayOfYear: &quot;$date&quot;&#125;, year: &#123; $year: &quot;$date&quot; &#125; &#125;, itemsSold: &#123; $push: &#123; item: &quot;$item&quot;, quantity: &quot;$quantity&quot; &#125; &#125; &#125; &#125; ]) # result&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 46, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &#123; &quot;item&quot; : &quot;abc&quot;, &quot;quantity&quot; : 10 &#125;, &#123; &quot;item&quot; : &quot;xyz&quot;, &quot;quantity&quot; : 10 &#125;, &#123; &quot;item&quot; : &quot;xyz&quot;, &quot;quantity&quot; : 5 &#125;, &#123; &quot;item&quot; : &quot;xyz&quot;, &quot;quantity&quot; : 10 &#125; ]&#125;&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 34, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &#123; &quot;item&quot; : &quot;jkl&quot;, &quot;quantity&quot; : 1 &#125;, &#123; &quot;item&quot; : &quot;xyz&quot;, &quot;quantity&quot; : 5 &#125; ]&#125;&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 1, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &#123; &quot;item&quot; : &quot;abc&quot;, &quot;quantity&quot; : 2 &#125; ]&#125; ### 表达式$addToSetdb.sales.aggregate( [ &#123; $group: &#123; _id: &#123; day: &#123; $dayOfYear: &quot;$date&quot;&#125;, year: &#123; $year: &quot;$date&quot; &#125; &#125;, itemsSold: &#123; $addToSet: &quot;$item&quot; &#125; &#125; &#125; ]) #result&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 46, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &quot;xyz&quot;, &quot;abc&quot; ] &#125;&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 34, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &quot;xyz&quot;, &quot;jkl&quot; ] &#125;&#123; &quot;_id&quot; : &#123; &quot;day&quot; : 1, &quot;year&quot; : 2014 &#125;, &quot;itemsSold&quot; : [ &quot;abc&quot; ] &#125; ### 表达式 $firstdb.sales.aggregate( [ &#123; $sort: &#123; item: 1, date: 1 &#125; &#125;, &#123; $group: &#123; _id: &quot;$item&quot;, firstSalesDate: &#123; $first: &quot;$date&quot; &#125; &#125; &#125; ]) # result&#123; &quot;_id&quot; : &quot;xyz&quot;, &quot;firstSalesDate&quot; : ISODate(&quot;2014-02-03T09:05:00Z&quot;) &#125;&#123; &quot;_id&quot; : &quot;jkl&quot;, &quot;firstSalesDate&quot; : ISODate(&quot;2014-02-03T09:00:00Z&quot;) &#125;&#123; &quot;_id&quot; : &quot;abc&quot;, &quot;firstSalesDate&quot; : ISODate(&quot;2014-01-01T08:00:00Z&quot;) &#125; Example3123456789101112db.sales.aggregate( [ &#123; $group : &#123; _id : null, # 如果为null,就统计出全部 totalPrice: &#123; $sum: &#123; $multiply: [ &quot;$price&quot;, &quot;$quantity&quot; ] &#125; &#125;, averageQuantity: &#123; $avg: &quot;$quantity&quot; &#125;, count: &#123; $sum: 1 &#125; &#125; &#125; ]) Example412345678910111213141516171819202122232425262728293031323334353637383940414243# 数据源&#123; &quot;_id&quot; : 8751, &quot;title&quot; : &quot;The Banquet&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 2 &#125;&#123; &quot;_id&quot; : 8752, &quot;title&quot; : &quot;Divine Comedy&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 1 &#125;&#123; &quot;_id&quot; : 8645, &quot;title&quot; : &quot;Eclogues&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 2 &#125;&#123; &quot;_id&quot; : 7000, &quot;title&quot; : &quot;The Odyssey&quot;, &quot;author&quot; : &quot;Homer&quot;, &quot;copies&quot; : 10 &#125;&#123; &quot;_id&quot; : 7020, &quot;title&quot; : &quot;Iliad&quot;, &quot;author&quot; : &quot;Homer&quot;, &quot;copies&quot; : 10 &#125; # 根据作者分组,获得其著多少书籍db.books.aggregate( [ &#123; $group : &#123; _id : &quot;$author&quot;, books: &#123; $push: &quot;$title&quot; &#125; &#125; &#125; ]) # result&#123; &quot;_id&quot; : &quot;Homer&quot;, &quot;books&quot; : [ &quot;The Odyssey&quot;, &quot;Iliad&quot; ] &#125;&#123; &quot;_id&quot; : &quot;Dante&quot;, &quot;books&quot; : [ &quot;The Banquet&quot;, &quot;Divine Comedy&quot;, &quot;Eclogues&quot; ] &#125; # 通过系统变量$$ROOT(当前的根文档)来分组db.books.aggregate( [ &#123; $group : &#123; _id : &quot;$author&quot;, books: &#123; $push: &quot;$$ROOT&quot; &#125; &#125; &#125; ]) # result&#123; &quot;_id&quot; : &quot;Homer&quot;, &quot;books&quot; : [ &#123; &quot;_id&quot; : 7000, &quot;title&quot; : &quot;The Odyssey&quot;, &quot;author&quot; : &quot;Homer&quot;, &quot;copies&quot; : 10 &#125;, &#123; &quot;_id&quot; : 7020, &quot;title&quot; : &quot;Iliad&quot;, &quot;author&quot; : &quot;Homer&quot;, &quot;copies&quot; : 10 &#125; ]&#125;&#123; &quot;_id&quot; : &quot;Dante&quot;, &quot;books&quot; : [ &#123; &quot;_id&quot; : 8751, &quot;title&quot; : &quot;The Banquet&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 2 &#125;, &#123; &quot;_id&quot; : 8752, &quot;title&quot; : &quot;Divine Comedy&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 1 &#125;, &#123; &quot;_id&quot; : 8645, &quot;title&quot; : &quot;Eclogues&quot;, &quot;author&quot; : &quot;Dante&quot;, &quot;copies&quot; : 2 &#125; ]&#125; 邮政编码数据集的聚合实例: http://docs.mongoing.com/manual-zh/tutorial/aggregation-zip-code-data-set.html 对用户爱好数据做聚合实例: http://docs.mongoing.com/manual-zh/tutorial/aggregation-with-user-preference-data.html 转自:聚集运算之group与aggregate","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"索引与expain(转)","date":"2017-04-16T04:47:25.776Z","path":"2017/04/16/nosql/mongodb/索引与expain(转)/","text":"官网：https://docs.mongodb.com/manual/indexes/ 1.索引操作&emsp;数据库百分之八十的工作基本上都是查询,而索引能帮我们更快的查询到想要的数据.但是其降低了数据的写入速度,所以要权衡常用的查询字段,不必在太多字段上建立索引.在mongoDB中默认是用btree来组织索引文件,并且可以按字段升序/降序来创建,便于排序. 1.1.数据准备123for (var i = 1; i &lt;100000; i++) &#123; db.test.insert(&#123;name:&apos;user&apos;+i,num:i,sn:Math.floor(Math.random()*10000000)&#125;)&#125; 1.2.索引常用操作1.2.1.查看当前索引1234567891011&gt; db.test.getIndexes();[ &#123; &quot;v&quot; : 1, &quot;key&quot; : &#123; &quot;_id&quot; : 1 &#125;, &quot;name&quot; : &quot;_id_&quot;, &quot;ns&quot; : &quot;test.test&quot; &#125;] MongoDB有个默认的_id的键，他相当于“主键”的角色。集合创建后系统会自动创建一个索引在_id键上，它是默认索引，索引名叫“_id”，是无法被删除的。 另外, system.indexes集合中包含了每个索引的详细信息，因此可以通过下面的命令查询已经存在的索引1db.system.indexes.find(&#123;&#125;); 1.2.2.创建单列索引1db.collection.ensureIndex(&#123;field:1/-1&#125;) # 1是正序,-1是倒序 1.2.3.创建多列索引(组合索引)1db.collection.ensureIndex(&#123;field1:1/-1, field2:1/-1&#125;) &emsp;在大多数情况下我们创建的索引都是多列索引,因为数据库查询器只会选择最优的索引来进行查询,在多列分别建立索引,查询器只会选择其中一列索引来进行查询,而直接建立一个多列索引的话,该索引由于是作用于多列的,效率更高于单列索引,具体多列索引建立技巧可以查看下文中的 &lt;&lt;新版Explain的分析实例&gt;&gt;，另外，mongoDB的多列索引也遵循着最左前缀的原则1db.test.ensureIndex(&#123;&quot;username&quot;:1, &quot;age&quot;:-1&#125;) &emsp;该索引被创建后，基于username和age的查询将会用到该索引，或者是基于username的查询也会用到该索引，但是只是基于age的查询将不会用到该复合索引。因此可以说，如果想用到复合索引，必须在查询条件中包含复合索引中的前N个索引列。然而如果查询条件中的键值顺序和复合索引中的创建顺序不一致的话，MongoDB可以智能的帮助我们调整该顺序，以便使复合索引可以为查询所用 1db.test.find(&#123;&quot;age&quot;: 30, &quot;username&quot;: &quot;stephen&quot;&#125;) 对于上面示例中的查询条件，MongoDB在检索之前将会动态的调整查询条件文档的顺序，以使该查询可以用到刚刚创建的复合索引。 1.2.4.创建子文档索引1db.collection.ensureIndex(&#123;&apos;filed.subfield&apos;:1/-1&#125;); 1.2.5.创建唯一索引1db.collection.ensureIndex(&#123;filed:1/-1&#125;, &#123;unique:true&#125;); 1.2.6.创建稀疏索引稀疏索引的特点——如果针对field做索引,针对不含field列的文档,将不建立索引.与之相对,普通索引,会把该文档的field列的值认为NULL,并建索引.适宜于: 小部分文档含有某列时.12345678 db.tea.find();&#123; &quot;_id&quot; : ObjectId(&quot;5275f99b87437c610023597b&quot;), &quot;email&quot; : &quot;a@163.com&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5275f99e87437c610023597c&quot;), &quot;email&quot; : &quot;b@163.com&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5275f9e887437c610023597e&quot;), &quot;email&quot; : &quot;c@163.com&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5275fa3887437c6100235980&quot;) &#125;db.collection.ensureIndex(&#123;field:1/-1&#125;,&#123;sparse:true&#125;); 如上内容,最后一行没有email列,如果分别加普通索引,和稀疏索引,对于最后一行的email分别当成null 和 忽略最后一行来处理.根据{email:null}来查询,前者能利用到索引,而后者用不到索引,是一个全表扫描的过程; 1.2.7.创建哈希索引哈希索引速度比普通索引快,但是,不能对范围查询进行优化.适宜于随机性强的散列 1db.collection.ensureIndex(&#123;file:’hashed’&#125;); 1.2.8.重建索引一个表经过很多次修改后,导致表的文件产生空洞,索引文件也如此.可以通过索引的重建,减少索引文件碎片,并提高索引的效率.类似mysql中的optimize table 1db.collection.reIndex() 1.2.9.删除索引12db.collection.dropIndex(&#123;filed:1/-1&#125;); #删除单个索引db.collection.dropIndexes(); #删除所有索引 1.3.正则表达式在索引中的应用正则表达式可以灵活地匹配查询条件，如果希望正则表达式能命中索引，就要注意了：Mongodb能为前缀型的正则表达式命中索引(和mysql一样)，比如：需要查询Mail中user以z开头的：/^z/如果有user索引,这种查询很高效,但其他的即使有索引,也不会命中索引，比说：需要查询Mail中的user中含有z的： 12/.*z.*//^.*z.*/ 这种查询是不会命中到索引的，当数据量很大，速度很慢总之，^后的条件必须明确，不能^.* ^[a-z]之类开头的 2.查询计划explain&emsp;我学习mongodb比较晚,安装的是3.05版本的,发现此版本的explain的使用方法跟教程上有很大不同,究竟是从什么版本开始发生改变的,我也就不去追溯了. 2.1.新版explain介绍新版本的explain有三种模式,作为explain的参数传进去 queryPlanner 默认 executionStats allPlansExecution queryPlanner queryPlanner是现版本explain的默认模式，queryPlanner模式下并不会去真正进行query语句查询，而是针对query语句进行执行计划分析并选出winning plan。123456789101112131415161718192021222324252627282930313233343536373839404142434445&#123; &quot;queryPlanner&quot;: &#123; &quot;plannerVersion&quot;: 1, &quot;namespace&quot;: &quot;game_db.game_user&quot;, &quot;indexFilterSet&quot;: false,//针对该query是否有indexfilter（详见下文） &quot;parsedQuery&quot;: &#123; &quot;w&quot;: &#123; &quot;$eq&quot;: 1 &#125; &#125;, &quot;winningPlan&quot;: &#123; // 查询优化器针对该query所返回的最优执行计划的详细内容。 &quot;stage&quot;: &quot;FETCH&quot;, //最优执行计划的stage，这里返回是FETCH，可以理解为通过返回的index位置去检索具体的文档(详见下文) &quot;inputStage&quot;: &#123; // 上一个stage的child stage，此处是IXSCAN，表示进行的是index scanning。 &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;keyPattern&quot;: &#123; &quot;w&quot;: 1, //所扫描的index内容 &quot;n&quot;: 1 // 返回的条数? &#125;, &quot;indexName&quot;: &quot;w_1_n_1&quot;, //索引名称 &quot;isMultiKey&quot;: false, //是否是Multikey，此处返回是false，如果索引建立在array上，此处将是true &quot;direction&quot;: &quot;forward&quot;, //此query的查询顺序，此处是forward，如果用了.sort(&#123;w:-1&#125;)将显示backward。 &quot;indexBounds&quot;: &#123; //winningplan所扫描的索引范围，此处查询条件是w:1,使用的index是w与n的联合索引，故w是[1.0,1.0]而n没有指定在查询条件中，故是[MinKey,MaxKey]。 &quot;w&quot;: [&quot;[1.0, 1.0]&quot;], &quot;n&quot;: [&quot;[MinKey, MaxKey]&quot;] &#125; &#125; &#125;, &quot;rejectedPlans&quot;: [&#123; //他执行计划（非最优而被查询优化器reject的）的详细返回，其中具体信息与winningPlan的返回中意义相同 &quot;stage&quot;: &quot;FETCH&quot;, &quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;keyPattern&quot;: &#123; &quot;w&quot;: 1, &quot;v&quot;: 1 &#125;, &quot;indexName&quot;: &quot;w_1_v_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;forward&quot;, &quot;indexBounds&quot;: &#123; &quot;w&quot;: [&quot;[1.0, 1.0]&quot;], &quot;v&quot;: [&quot;[MinKey, MaxKey]&quot;] &#125; &#125; &#125;] &#125; indexFilterSet IndexFilter决定了查询优化器对于某一类型的查询将如何使用index，indexFilter仅影响查询优化器对于该类查询可以用尝试哪些index的执行计划分析，查询优化器还是根据分析情况选择最优计划。 如果某一类型的查询设定了IndexFilter，那么执行时通过hint指定了其他的index，查询优化器将会忽略hint所设置index，仍然使用indexfilter中设定的查询计划。 IndexFilter可以通过命令移除，也将在实例重启后清空。 IndexFilter的创建123456789101112131415161718192021db.runCommand( &#123; planCacheSetFilter: &lt;collection&gt;, query: &lt;query&gt;, sort: &lt;sort&gt;, projection: &lt;projection&gt;, indexes: [ &lt;index1&gt;, &lt;index2&gt;, ...] &#125;)db.runCommand( &#123; planCacheSetFilter: &quot;orders&quot;, query: &#123; status: &quot;A&quot; &#125;, indexes: [ &#123; cust_id: 1, status: 1 &#125;, &#123; status: 1, order_date: -1 &#125; ] &#125;) 针对orders表建立了一个indexFilter，indexFilter指定了对于orders表只有status条件（仅对status进行查询，无sort等）的查询的indexes，所以以下的查询语句的查询优化器仅仅会从{cust_id:1,status:1}和{status:1,order_date:-1}中进行winning plan的选择 12db.orders.find( &#123; status: &quot;D&quot; &#125; )db.orders.find( &#123; status: &quot;P&quot; &#125; ) indexFilter的列表可以通过如下命令展示某一个collecton的所有indexFilter1db.runCommand( &#123; planCacheListFilters: &lt;collection&gt; &#125; ) indexFilter的删除 可以通过如下命令对IndexFilter进行删除123456789db.runCommand( &#123; planCacheClearFilters: &lt;collection&gt;, query: &lt;query pattern&gt;, sort: &lt;sort specification&gt;, projection: &lt;projection specification&gt; &#125;) Stage返回参数说明 123456789101112131415COLLSCAN #全表扫描IXSCAN #索引扫描FETCH #根据索引去检索指定documentSHARD_MERGE #将各个分片返回数据进行mergeSORT #表明在内存中进行了排序（与老版本的scanAndOrder:true一致）LIMIT #使用limit限制返回数SKIP #使用skip进行跳过IDHACK #针对_id进行查询SHARDING_FILTER #通过mongos对分片数据进行查询COUNT #利用db.coll.explain().count()之类进行count运算COUNTSCAN #count不使用Index进行count时的stage返回COUNT_SCAN #count使用了Index进行count时的stage返回SUBPLA #未使用到索引的$or查询的stage返回TEXT #使用全文索引进行查询时候的stage返回PROJECTION #限定返回字段时候stage的返回 executionStats 该模式是mongoDB查询的执行状态,类似老版本的explain123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, //是否执行成功 &quot;nReturned&quot;: 29861, //查询的返回条数 &quot;executionTimeMillis&quot;: 23079, //整体执行时间 毫秒 &quot;totalKeysExamined&quot;: 29861, // 索引扫描次数 &quot;totalDocsExamined&quot;: 29861, // document扫描次数 &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;FETCH&quot;, //这里是FETCH去扫描对于documents &quot;nReturned&quot;: 29861, //由于是FETCH，所以这里该值与executionStats.nReturned一致 &quot;executionTimeMillisEstimate&quot;: 22685, &quot;works&quot;: 29862, //查看源码中发现，每次操作会加1，且会把执行时间记录在executionTimeMillis中。 &quot;advanced&quot;: 29861,//而在查询结束EOF，works又会加1，advanced不加。正常的返回works会比nReturned多1，这时候isEOF为true（1）：另外advanced的返回值只有在命中的时候+1，在skip,eof的时候不会增加 &quot;needTime&quot;: 0, &quot;needFetch&quot;: 0, &quot;saveState&quot;: 946, &quot;restoreState&quot;: 946, &quot;isEOF&quot;: 1, &quot;invalidates&quot;: 0, &quot;docsExamined&quot;: 29861, // 与executionStats.totalDocsExamined一致 &quot;alreadyHasObj&quot;: 0, &quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;nReturned&quot;: 29861, &quot;executionTimeMillisEstimate&quot;: 70, &quot;works&quot;: 29862, &quot;advanced&quot;: 29861, &quot;needTime&quot;: 0, &quot;needFetch&quot;: 0, &quot;saveState&quot;: 946, &quot;restoreState&quot;: 946, &quot;isEOF&quot;: 1, &quot;invalidates&quot;: 0, &quot;keyPattern&quot;: &#123; &quot;w&quot;: 1, &quot;n&quot;: 1 &#125;, &quot;indexName&quot;: &quot;w_1_n_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;forward&quot;, &quot;indexBounds&quot;: &#123; &quot;w&quot;: [&quot;[1.0, 1.0]&quot;], &quot;n&quot;: [&quot;[MinKey, MaxKey]&quot;] &#125;, &quot;keysExamined&quot;: 29861, &quot;dupsTested&quot;: 0, &quot;dupsDropped&quot;: 0, &quot;seenInvalidated&quot;: 0, &quot;matchTested&quot;: 0 &#125; &#125;&#125; allPlansExecution 该模式可以看做是以上两个模式加起来; 2.2.如何通过新版explain来分析索引 分析executionTimeMillis 123456789101112131415161718&quot;executionStats&quot; : &#123; &quot;nReturned&quot; : 29861, &quot;totalKeysExamined&quot; : 29861, &quot;totalDocsExamined&quot; : 29861, &quot;executionTimeMillis&quot; : 66948, # 该query的整体查询时间 ... &quot;executionStages&quot; : &#123; ... &quot;executionTimeMillisEstimate&quot; : 66244, # 该查询根据index去检索document获取29861条具体数据的时间 ... &quot;inputStage&quot; : &#123; &quot;stage&quot; : &quot;IXSCAN&quot;, ... &quot;executionTimeMillisEstimate&quot; : 290, #该查询扫描29861行index所用时间 ...&#125; 这三个值我们都希望越少越好，那么是什么影响这这三个返回值呢？ 分析index与document扫描数与查询返回条目数 这里主要谈3个返回项，nReturned，totalKeysExamined与totalDocsExamined，分别代表该条查询返回的条目、索引扫描条目和文档扫描条目。理想状态如下:nReturned=totalKeysExamined &amp; totalDocsExamined=0 （cover index，仅仅使用到了index，无需文档扫描，这是最理想状态。） 或者 nReturned=totalKeysExamined=totalDocsExamined(需要具体情况具体分析)（正常index利用，无多余index扫描与文档扫描。） 如果有sort的时候，为了使得sort不在内存中进行，我们可以在保证nReturned=totalDocsExamined的基础上，totalKeysExamined可以大于totalDocsExamined与nReturned，因为量级较大的时候内存排序非常消耗性能。 分析Stage状态 对于普通查询，我们最希望看到的组合有这些：1234567891011Fetch+IDHACK Fetch+ixscan Limit+（Fetch+ixscan） PROJECTION+ixscan SHARDING_FILTER+ixscan 等 不希望看到包含如下的stage： 12COLLSCAN（全表扫），SORT（使用sort但是无index），不合理的SKIP，SUBPLA（未用到index的$or） 对于count查询，希望看到的有：1COUNT_SCAN 不希望看到的有:1COUNTSCAN 2.3.新版Explain的分析实例表中数据如下(简单测试用例，仅10条数据，主要是对explain分析的逻辑进行解析)：12345678910&#123; &quot;_id&quot; : ObjectId(&quot;55b86d6bd7e3f4ccaaf20d70&quot;), &quot;a&quot; : 1, &quot;b&quot; : 1, &quot;c&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b86d6fd7e3f4ccaaf20d71&quot;), &quot;a&quot; : 1, &quot;b&quot; : 2, &quot;c&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b86d72d7e3f4ccaaf20d72&quot;), &quot;a&quot; : 1, &quot;b&quot; : 3, &quot;c&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b86d74d7e3f4ccaaf20d73&quot;), &quot;a&quot; : 4, &quot;b&quot; : 2, &quot;c&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b86d75d7e3f4ccaaf20d74&quot;), &quot;a&quot; : 4, &quot;b&quot; : 2, &quot;c&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b86d77d7e3f4ccaaf20d75&quot;), &quot;a&quot; : 4, &quot;b&quot; : 2, &quot;c&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b879b442bfd1a462bd8990&quot;), &quot;a&quot; : 2, &quot;b&quot; : 1, &quot;c&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b87fe842bfd1a462bd8991&quot;), &quot;a&quot; : 1, &quot;b&quot; : 9, &quot;c&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b87fe942bfd1a462bd8992&quot;), &quot;a&quot; : 1, &quot;b&quot; : 9, &quot;c&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55b87fe942bfd1a462bd8993&quot;), &quot;a&quot; : 1, &quot;b&quot; : 9, &quot;c&quot; : 1 &#125; 查询语句 1db.test.find(&#123;a:1,b:&#123;$lt:3&#125;&#125;).sort(&#123;c:-1&#125;).explain(); 未加索引前1234567891011121314151617181920212223242526272829303132&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, &quot;nReturned&quot;: 2, &quot;executionTimeMillis&quot;: 0, &quot;totalKeysExamined&quot;: 0, // 为0表示没有使用索引 &quot;totalDocsExamined&quot;: 10, // 扫描了所有记录 &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;SORT&quot;, //为SORT,未使用index的sort &quot;nReturned&quot;: 2, ...&quot;sortPattern&quot;: &#123; &quot;c&quot;: -1 &#125;, &quot;memUsage&quot;: 126, //占用的内存 &quot;memLimit&quot;: 33554432, //内存限制 &quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;COLLSCAN&quot;, //全表扫描 &quot;filter&quot;: &#123; &quot;$and&quot;: [&#123; &quot;a&quot;: &#123; &quot;$eq&quot;: 1 &#125; &#125;, &#123; &quot;b&quot;: &#123; &quot;$lt&quot;: 3 &#125; &#125;] &#125;, &quot;nReturned&quot;: 2, ...&quot;direction&quot;: &quot;forward&quot;, &quot;docsExamined&quot;: 10 &#125; 很明显，没有index的时候，进行了全表扫描，在内存中sort，数据量达百万级以后就会有明显的慢 接着我们对c加一个正序索引 1db.d.ensureIndex(&#123;c:1&#125;) 再来看一下123456789101112131415161718192021222324252627282930313233&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, &quot;nReturned&quot;: 2, &quot;executionTimeMillis&quot;: 1, &quot;totalKeysExamined&quot;: 10, &quot;totalDocsExamined&quot;: 10, &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;FETCH&quot;, #变成了fetch &quot;filter&quot;: &#123; &quot;$and&quot;: [&#123; &quot;a&quot;: &#123; &quot;$eq&quot;: 1 &#125; &#125;, &#123; &quot;b&quot;: &#123; &quot;$lt&quot;: 3 &#125; &#125;] &#125;, &quot;nReturned&quot;: 2, ...&quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, #索引扫描 &quot;nReturned&quot;: 10, ...&quot;keyPattern&quot;: &#123; &quot;c&quot;: 1 &#125;, &quot;indexName&quot;: &quot;c_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;backward&quot;, &quot;indexBounds&quot;: &#123; &quot;c&quot;: [&quot;[MaxKey, MinKey]&quot;] &#125; 我们发现，Stage没有了SORT，因为我们sort字段有了index，但是由于查询还是没有index，故totalDocsExamined还是10，但是由于sort用了index，totalKeysExamined也是10，但是仅对sort排序做了优化，查询性能还是一样的低效。 接下来， 我们对查询条件做index1234567891011121314151617181920212223242526272829303132333435db.test.ensureIndex(&#123;b:1,a:1,c:1&#125;)&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, &quot;nReturned&quot;: 2, &quot;executionTimeMillis&quot;: 0, &quot;totalKeysExamined&quot;: 4, &quot;totalDocsExamined&quot;: 2, &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;SORT&quot;, &quot;nReturned&quot;: 2, ...&quot;sortPattern&quot;: &#123; &quot;c&quot;: -1 &#125;, &quot;memUsage&quot;: 126, &quot;memLimit&quot;: 33554432, &quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;FETCH&quot;, &quot;nReturned&quot;: 2, ...&quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;nReturned&quot;: 2, ...&quot;keyPattern&quot;: &#123; &quot;b&quot;: 1, &quot;a&quot;: 1, &quot;c&quot;: 1 &#125;, &quot;indexName&quot;: &quot;b_1_a_1_c_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;forward&quot;, &quot;indexBounds&quot;: &#123; &quot;b&quot;: [&quot;[-inf.0, 3.0)&quot;], &quot;a&quot;: [&quot;[1.0, 1.0]&quot;], &quot;c&quot;: [&quot;[MinKey, MaxKey]&quot;] &#125;, nReturned为2，返回2条记录totalKeysExamined为4，扫描了4个indextotalDocsExamined为2，扫描了2个docs 此时nReturned=totalDocsExamined&lt;totalKeysExamined，不符合我们的期望。且executionStages.Stage为Sort，在内存中进行排序了，也不符合我们的期望 1db.test.ensureIndex(&#123;a:1,b:1,c:1&#125;) 123456789101112131415161718192021222324252627282930313233&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, &quot;nReturned&quot;: 2, &quot;executionTimeMillis&quot;: 0, &quot;totalKeysExamined&quot;: 2, &quot;totalDocsExamined&quot;: 2, &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;SORT&quot;, &quot;nReturned&quot;: 2, ...&quot;sortPattern&quot;: &#123; &quot;c&quot;: -1 &#125;, &quot;memUsage&quot;: 126, &quot;memLimit&quot;: 33554432, &quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;FETCH&quot;, &quot;nReturned&quot;: 2, ...&quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;nReturned&quot;: 2, ...&quot;keyPattern&quot;: &#123; &quot;a&quot;: 1, &quot;b&quot;: 1, &quot;c&quot;: 1 &#125;, &quot;indexName&quot;: &quot;a_1_b_1_c_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;forward&quot;, &quot;indexBounds&quot;: &#123; &quot;a&quot;: [&quot;[1.0, 1.0]&quot;], &quot;b&quot;: [&quot;[-inf.0, 3.0)&quot;], &quot;c&quot;: [&quot;[MinKey, MaxKey]&quot;] &#125;, nReturned为2，返回2条记录totalKeysExamined为2，扫描了2个indextotalDocsExamined为2，扫描了2个docs此时nReturned=totalDocsExamined=totalKeysExamined，符合我们的期望。但是！executionStages.Stage为Sort，在内存中进行排序了，这个在生产环境中尤其是在数据量较大的时候，是非常消耗性能的，这个千万不能忽视了，我们需要改进这个点。 1db.test.ensureIndex(&#123;a:1,c:1,b:1&#125;) 123456789101112131415161718192021222324252627282930&quot;executionStats&quot;: &#123; &quot;executionSuccess&quot;: true, &quot;nReturned&quot;: 2, &quot;executionTimeMillis&quot;: 0, &quot;totalKeysExamined&quot;: 4, &quot;totalDocsExamined&quot;: 2, &quot;executionStages&quot;: &#123; &quot;stage&quot;: &quot;FETCH&quot;, &quot;nReturned&quot;: 2, ...&quot;inputStage&quot;: &#123; &quot;stage&quot;: &quot;IXSCAN&quot;, &quot;nReturned&quot;: 2, ...&quot;keyPattern&quot;: &#123; &quot;a&quot;: 1, &quot;c&quot;: 1, &quot;b&quot;: 1 &#125;, &quot;indexName&quot;: &quot;a_1_c_1_b_1&quot;, &quot;isMultiKey&quot;: false, &quot;direction&quot;: &quot;backward&quot;, &quot;indexBounds&quot;: &#123; &quot;a&quot;: [&quot;[1.0, 1.0]&quot;], &quot;c&quot;: [&quot;[MaxKey, MinKey]&quot;], &quot;b&quot;: [&quot;(3.0, -inf.0]&quot;] &#125;, &quot;keysExamined&quot;: 4, &quot;dupsTested&quot;: 0, &quot;dupsDropped&quot;: 0, &quot;seenInvalidated&quot;: 0, &quot;matchTested&quot;: 0 我们可以看到 nReturned为2，返回2条记录totalKeysExamined为4，扫描了4个indextotalDocsExamined为2，扫描了2个docs 虽然不是nReturned=totalKeysExamined=totalDocsExamined，但是Stage无Sort，即利用了index进行排序，而非内存，这个性能的提升高于多扫几个index的代价。综上可以有一个小结论，当查询覆盖精确匹配，范围查询与排序的时候，{精确匹配字段,排序字段,范围查询字段}这样的索引排序会更为高效 2.4.旧版本的explain1234567891011121314151617181920&gt; db.blogs.find(&#123;&quot;comment.author&quot;:&quot;joe&quot;&#125;).explain(); &#123; &quot;cursor&quot; : &quot;BtreeCursor comment.author_1&quot;, &quot;nscanned&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;n&quot; : 1, &quot;millis&quot; : 70, &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;isMultiKey&quot; : true, &quot;indexOnly&quot; : false, &quot;indexBounds&quot; : &#123; &quot;comment.author&quot; : [ [ &quot;joe&quot;, &quot;joe&quot; ] ] &#125; &#125; 参数说明: cursor：因为这个查询使用了索引，MongoDB中索引存储在B树结构中，所以这是也使用了BtreeCursor类型的游标。如果没有使用索引，游标的类型是BasicCursor。这个键还会给出你所使用的索引的名称，你通过这个名称可以查看当前数据库下的system.indexes集合（系统自动创建，由于存储索引信息，这个稍微会提到）来得到索引的详细信息。 nscanned/nscannedObjects：表明当前这次查询一共扫描了集合中多少个文档，我们的目的是，让这个数值和返回文档的数量越接近越好。 n：当前查询返回的文档数量。 millis：当前查询所需时间，毫秒数。 indexBounds：当前查询具体使用的索引 2.5.hint强制使用某个索引12345678910111213141516171819202122232425262728&gt; db.user.ensureIndex(&#123;&quot;name&quot;:1,&quot;age&quot;:1&#125;);&gt; db.user.ensureIndex(&#123;&quot;age&quot;:1,&quot;name&quot;:1&#125;);&gt; db.user.find(&#123;&quot;age&quot;:40, &quot;name&quot;:&quot;tim&quot;&#125;).explain();&#123; &quot;cursor&quot; : &quot;BtreeCursor name_1_age_1&quot;, &quot;nscanned&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;n&quot; : 1, &quot;millis&quot; : 0, &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;isMultiKey&quot; : false, &quot;indexOnly&quot; : false, &quot;indexBounds&quot; : &#123; &quot;name&quot; : [ [ &quot;tim&quot;, &quot;tim&quot; ] ], &quot;age&quot; : [ [ 40, 40 ] ] &#125;&#125; 返回文档的键没有区别，其默认使用了索引”name_1_age_1”，这是查询优化器为我们使用的索引！我们此处可以通过hint进行更行，即强制这个查询使用我们定义的“age_1_name_1”索引，如下 123456789101112131415161718192021222324252627&gt; var cursor = db.user.find(&#123;&quot;age&quot;:40, &quot;name&quot;:&quot;tim&quot;&#125;).hint(&#123;&quot;age&quot;:1,&quot;name&quot;:1&#125;);&gt; cursor.explain();&#123; &quot;cursor&quot; : &quot;BtreeCursor age_1_name_1&quot;, &quot;nscanned&quot; : 1, &quot;nscannedObjects&quot; : 1, &quot;n&quot; : 1, &quot;millis&quot; : 0, &quot;nYields&quot; : 0, &quot;nChunkSkips&quot; : 0, &quot;isMultiKey&quot; : false, &quot;indexOnly&quot; : false, &quot;indexBounds&quot; : &#123; &quot;age&quot; : [ [ 40, 40 ] ], &quot;name&quot; : [ [ &quot;tim&quot;, &quot;tim&quot; ] ] &#125;&#125; hint函数会返回游标，我们可以在游标上调用explain查看索引的使用情况！99%的情况，我们没有必要通过hint去强制使用某个索引，MongoDB的查询优化器非常智能，绝对能帮助我们使用最佳的索引去进行查询！ 转自","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"用户管理(转)","date":"2017-04-16T04:47:25.774Z","path":"2017/04/16/nosql/mongodb/用户管理(转)/","text":"1. 3.0版本以前在mongodb3.0版本以前中,有一个admin数据库, 牵涉到服务器配置层面的操作,需要先切换到admin数据库.即 use admin , 相当于进入超级用户管理模式,mongo的用户是以数据库为单位来建立的, 每个数据库有自己的管理员.我们在设置用户时,需要先在admin数据库下建立管理员—这个管理员登陆后,相当于超级管理员. 123#命令:db.addUser();#简单参数: db.addUser(用户名,密码,是否只读) 注意: 添加用户后,我们再次退出并登陆,发现依然可以直接读数据库?原因: mongodb服务器启动时, 默认不是需要认证的.要让用户生效, 需要启动服务器时,就指定–auth选项.这样, 操作时,就需要认证了. 1234567# 添加用户&gt; use admin&gt; db.addUser(&apos;admin&apos;,&apos;admin&apos;,false); # 3.0版本更改为createUser(); # 删除用户&gt; use test&gt; db.removeUser(用户名); # 3.0版本更改为dropUser(); 2. 3.0版本以后2.1. 创建管理员在3.0版本以后,mongodb默认是没有admin这个数据库的,并且创建管理员不再用addUser,而用createUser; 语法说明 12345678&#123; user: &quot;&lt;name&gt;&quot;, pwd: &quot;&lt;cleartext password&gt;&quot;, customData: &#123; &lt;any information&gt; &#125;, # 任意的数据,一般是用于描述用户管理员的信息 roles: [ &#123; role: &quot;&lt;role&gt;&quot;, db: &quot;&lt;database&gt;&quot; &#125; | &quot;&lt;role&gt;&quot;, # 如果是role就是直接指定了角色,并作用于当前的数据库 ... ] # roles是必传项,但是可以指定空数组,为空就是不指定任何权限&#125; Built-In Roles（内置角色）： 数据库用户角色：read、readWrite; 数据库管理角色：dbAdmin、dbOwner、userAdmin； 集群管理角色：clusterAdmin、clusterManager、clusterMonitor、hostManager； 备份恢复角色：backup、restore； 所有数据库角色：readAnyDatabase、readWriteAnyDatabase、userAdminAnyDatabase、dbAdminAnyDatabase 超级用户角色：root 这里还有几个角色间接或直接提供了系统超级用户的访问（dbOwner 、userAdmin、userAdminAnyDatabase） 内部角色：__systemPS：关于每个角色所拥有的操作权限可以点击上面的内置角色链接查看详情。 官方Example 12345678910use products # mongoDB的权限设置是以库为单位的,必选要先选择库db.createUser(&#123; &quot;user&quot; : &quot;accountAdmin01&quot;,&quot;pwd&quot;: &quot;cleartext password&quot;,&quot;customData&quot; : &#123; employeeId: 12345 &#125;,&quot;roles&quot; : [ &#123; role: &quot;clusterAdmin&quot;, db: &quot;admin&quot; &#125;, &#123; role: &quot;readAnyDatabase&quot;, db: &quot;admin&quot; &#125;, &quot;readWrite&quot; ] &#125;,&#123; w: &quot;majority&quot; , wtimeout: 5000 &#125; ) # readWrite 适用于products库,clusterAdmin与readAnyDatabase角色适用于admin库 writeConcern文档（官方说明） w选项：允许的值分别是 1、0、大于1的值、”majority”、； j选项：确保mongod实例写数据到磁盘上的journal（日志），这可以确保mongd以外关闭不会丢失数据。设置true启用。 wtimeout：指定一个时间限制,以毫秒为单位。wtimeout只适用于w值大于1。 123456use shop;db.createUser(&#123; user:&apos;admin&apos;, pwd:&apos;zhouzhou123&apos;, roles:[&apos;dbOwner&apos;]&#125;) 只要新加了一个用户,admin数据库就会重新存在; 1234567891011121314151617181920212223mongo --host xxx -u admin -p zhouzhou123 --authenticationDatabase shop # 用新创建的用户登录 # 查看当前用户在shop数据库的权限use shop;db.runCommand( &#123; usersInfo:&quot;shopzhouzhou&quot;, showPrivileges:true &#125;) # 查看用户信息db.runCommand(&#123;usersInfo:&quot;userName&quot;&#125;) # 创建一个不受访问限制的超级用户use admindb.createUser( &#123; user:&quot;superuser&quot;, pwd:&quot;pwd&quot;, roles:[&quot;root&quot;] &#125;) 2.2. 认证用户12&gt; use test&gt; db.auth(用户名,密码); #注意是以库为单位,必须先选择库; 2.3. 删除用户123# 删除用户&gt; use test&gt; db.dropUser(&apos;用户名&apos;); 2.4. 修改用户密码1234567891011&gt; use test&gt; db.changeUserPassword(用户名, 新密码); # 修改密码和用户信息db.runCommand( &#123; updateUser:&quot;username&quot;, pwd:&quot;xxx&quot;, customData:&#123;title:&quot;xxx&quot;&#125; &#125;) 3. 权限规则参考: http://blog.csdn.net/kk185800961/article/details/45619863 转自","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"游标操作(转)","date":"2017-04-16T04:47:25.773Z","path":"2017/04/16/nosql/mongodb/游标操作(转)/","text":"官网：https://docs.mongodb.com/manual/reference/method/js-cursor/ 1.简述通俗的说,游标不是查询结果,可以理解为数据在遍历过程中的内部指针,其返回的是一个资源,或者说数据读取接口. 客户端通过对游标进行一些设置就能对查询结果进行有效地控制，如可以限制查询得到的结果数量、跳过部分结果、或对结果集按任意键进行排序等！ 直接对一个集合调用find()方法时，我们会发现，如果查询结果超过二十条，只会返回二十条的结果，这是因为Mongodb会自动递归find() 返回的游标。 2.基本操作&emsp;当我们使用一个变量来保存 find()的返回值时，其将不会自动进行遍历显示查询结果的操作,并没有真正的去查询数据库，只要当用到的时候（也就是遍历游标的时候）才会到数据库中将数据取出来,和PHP链接mysql资源一样:12345678910111213141516171819 // while循环var cursor = db.goods.find(&#123;goods_id:&#123;$lte:20&#125;&#125;,&#123;_id:0,goods_id:1&#125;); //使用变量来保存游标while(cursor.hasNext())&#123; //cursor.hasNext()判断游标是否取到尽头 printjson(cursor.next()); //cursor.next()取出游标的下1个单元(从0开始游),注意print打印的是二进制对象,printjson打印出来的才是可阅读的json格式数据&#125; // for循环var cursor = db.goods.find(&#123;goods_id:&#123;$lte:100&#125;&#125;,&#123;_id:0,goods_id:1&#125;); //使用变量来保存游标for (;cursor.hasNext();) &#123; //由于cursor.hasNext()自动判断的特性这里的for循环可以很简单 printjson(cursor.next());&#125; //forEach循环var cursor = db.goods.find(&#123;goods_id:&#123;$lte:100&#125;&#125;,&#123;_id:0,goods_id:1,goods_name:1&#125;);var callback = function(obj)&#123; //obj就是查出的文档对象 printjson(obj.goods_id) //直接取出goods_id的值;&#125;cursor.forEach(callback); 3.游标在分页中的应用 limit，skip，sort比如查到10000行,跳过100页,取10行.一般地,我们假设每页N行, 当前是page页,就需要跳过前 (page-1)*N 行, 再取N行, 在mysql中, limit offset,N来实现 在mongo中,用skip(), limit()函数来实现的,当获得游标后，我们可以先对游标进行处理后，再让访问数据库的动作按照我们的意愿发生。在这里我们就可以使用limit，skip，sort三个函数来处理游标。同时这三个函数可以组成方法链式调用的形式。 limit：限制游标返回的数量，指定了上限 skip：忽略前面的部分文档，如果文档总数量小于忽略的数量，则返回空集合 sort：得到的子集合进行排序，可以按照多个键进行正反排序！ 1234567891011121314151617181920212223242526272829# 查询5条&gt; var cursor = db.goods.find(&#123;&#125;,&#123;_id:0,goods_id:1&#125;).limit(5); //注意,再次使用游标的时候,游标得重置,因为使用过一次就游到最后了;&gt; cursor.forEach(function(obj)&#123;print(obj.goods_id);&#125;) # 查询5条,并按照shop_price的降序排列&gt; var cursor = db.goods.find(&#123;&#125;,&#123;_id:0,goods_id:1,shop_price:1&#125;).limit(5).sort(&#123;shop_price:-1&#125;)&gt; cursor.forEach(function(obj)&#123; print(obj.goods_id+&apos; &apos;+obj.shop_price) &#125;)22 599923 370032 301018 287814 2625 # 每页10条取第二页,并且升序排列var cursor = db.goods.find(&#123;&#125;,&#123;_id:0,goods_id:1,shop_price:1&#125;).limit(10).sort(&#123;goods_id:1&#125;).skip(10)cursor.forEach(function(obj)&#123; print(obj.goods_id+&apos; &apos;+obj.shop_price)&#125;) # 一次性打印所有的行,以易读的模式var cursor = db.goods.find(&#123;&#125;,&#123;_id:0,goods_id:1,shop_price:1&#125;);printjson(cursor.toArray()); /*注意: 不要随意使用toArray()原因: 会把所有的行立即以对象形式组织在内存里.可以在取出少数几行时,用此功能.*/ 转自","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"查询表达式","date":"2017-04-16T04:47:25.772Z","path":"2017/04/16/nosql/mongodb/查询表达式/","text":"db.goods.find().count(); 1.比较查询操作符对于官网：$eq $gt $gte $lt $lte $ne $in $ninhttps://docs.mongodb.com/manual/reference/operator/query/in/ 1234567891011121314151617181920212223242526272829303132#$eq&#123; _id: 1, item: &#123; name: &quot;ab&quot;, code: &quot;123&quot; &#125;, qty: 15, tags: [ &quot;A&quot;, &quot;B&quot;, &quot;C&quot; ] &#125;&#123; _id: 2, item: &#123; name: &quot;cd&quot;, code: &quot;123&quot; &#125;, qty: 20, tags: [ &quot;B&quot; ] &#125;&#123; _id: 3, item: &#123; name: &quot;ij&quot;, code: &quot;456&quot; &#125;, qty: 25, tags: [ &quot;A&quot;, &quot;B&quot; ] &#125;&#123; _id: 4, item: &#123; name: &quot;xy&quot;, code: &quot;456&quot; &#125;, qty: 30, tags: [ &quot;B&quot;, &quot;A&quot; ] &#125;&#123; _id: 5, item: &#123; name: &quot;mn&quot;, code: &quot;000&quot; &#125;, qty: 20, tags: [ [ &quot;A&quot;, &quot;B&quot; ], &quot;C&quot; ] &#125;db.inventory.find( &#123; qty: &#123; $eq: 20 &#125; &#125; )#结果如下:&#123; _id: 2, item: &#123; name: &quot;cd&quot;, code: &quot;123&quot; &#125;, qty: 20, tags: [ &quot;B&quot; ] &#125;&#123; _id: 5, item: &#123; name: &quot;mn&quot;, code: &quot;000&quot; &#125;, qty: 20, tags: [ [ &quot;A&quot;, &quot;B&quot; ], &quot;C&quot; ] &#125;db.inventory.find( &#123; &quot;item.name&quot;: &#123; $eq: &quot;ab&quot; &#125; &#125; ) #因为是文档类型,所以可以使用点来取属性#结果如下:&#123; _id: 1, item: &#123; name: &quot;ab&quot;, code: &quot;123&quot; &#125;, qty: 15, tags: [ &quot;A&quot;, &quot;B&quot;, &quot;C&quot; ] &#125;#等值查询db.collection.find(&#123;filed:value&#125;)#指定返回的列db.user.find(&#123;name:&quot;user0&quot;&#125;,&#123;age:1&#125;) &#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;age&quot; : 0 &#125; &#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;age&quot; : 20 &#125;#_id是默认显示的,可以传入_id:0来隐藏它#不等于 $nedb.collection.find(&#123;filed:&#123;$ne:value&#125;&#125;)#not in $nindb.collection.find(&#123;filed:&#123;$nin:[value1,value2,value3]&#125;&#125;) 2.逻辑查询操作符https://docs.mongodb.com/manual/reference/operator/query-logical/ 12345678910111213141516171819202122232425262728293031323334 #满足一个 $or&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843ba&quot;), &quot;name&quot; : &quot;user8&quot;, &quot;age&quot; : 8 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bb&quot;), &quot;name&quot; : &quot;user9&quot;, &quot;age&quot; : 9 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 20 &#125; &gt; db.user.find(&#123;$or:[&#123;name:&quot;user1&quot;&#125;,&#123;age:20&#125;]&#125;) # 由此可见or的键值为一个数组&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 20 &#125; #都不满足(排除) $nor&gt; db.user.find(&#123;$nor:[&#123;name:&quot;user1&quot;&#125;,&#123;age:20&#125;]&#125;) # name不等于user1,以及age不等于20,可以理解为排除;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843ba&quot;), &quot;name&quot; : &quot;user8&quot;, &quot;age&quot; : 8 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bb&quot;), &quot;name&quot; : &quot;user9&quot;, &quot;age&quot; : 9 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125; 3.元素查询操作符https://docs.mongodb.com/manual/reference/operator/query-element/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103取模操作 $mod&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843ba&quot;), &quot;name&quot; : &quot;user8&quot;, &quot;age&quot; : 8 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bb&quot;), &quot;name&quot; : &quot;user9&quot;, &quot;age&quot; : 9 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 20 &#125; &gt; db.user.find(&#123;age:&#123;$mod:[3,1]&#125;&#125;) # 模三余一&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125; #查找包含该字段的文档 $exists&gt; db.phone.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e51a20c9b0dc40419388&quot;), &quot;state&quot; : 1 &#125; &gt; db.phone.find(&#123;state:&#123;$exists:1&#125;&#125;) # 存在state字段的&#123; &quot;_id&quot; : ObjectId(&quot;5198e51a20c9b0dc40419388&quot;), &quot;state&quot; : 1 &#125; &gt; db.phone.find(&#123;state:&#123;$exists:0&#125;&#125;) # 不存在state字段的文档&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125; #根据数据类型查询 $type#在mongodb中每一种数据类型都有对应的数字,我们在使用$type的时候需要使用这些数字,文档中给出如下的表示|类型|编号||:-:|:-:||双精度|1||字符串|2||对象|3||数组|4||二进制数据|5||对象 ID|7||布尔值|8||日期|9||空|10||正则表达式|11||JavaScript|13||符号|14||JavaScript（带范围）|15||32 位整数|16||时间戳|17||64 位整数|18||最小键|255||最大键|127|&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843ba&quot;), &quot;name&quot; : &quot;user8&quot;, &quot;age&quot; : 8 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bb&quot;), &quot;name&quot; : &quot;user9&quot;, &quot;age&quot; : 9 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 20 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;51996ef22b76790566165e47&quot;), &quot;name&quot; : 23, &quot;age&quot; : 33 &#125;&gt; db.user.find(&#123;name:&#123;$type:1&#125;&#125;) # 查找name为双精度的文档&#123; &quot;_id&quot; : ObjectId(&quot;51996ef22b76790566165e47&quot;), &quot;name&quot; : 23, &quot;age&quot; : 33 &#125;#$andSyntax: &#123; $and: [ &#123; &lt;expression1&gt; &#125;, &#123; &lt;expression2&gt; &#125; , ... , &#123; &lt;expressionN&gt; &#125; ] &#125;db.inventory.find( &#123; $and : [ &#123; $or : [ &#123; price : 0.99 &#125;, &#123; price : 1.99 &#125; ] &#125;, &#123; $or : [ &#123; sale : true &#125;, &#123; qty : &#123; $lt : 20 &#125; &#125; ] &#125; ]&#125; )/*the price field value equals 0.99 or 1.99, andthe sale field value is equal to true or the qty field value is less than 20.*/#$or&#123; $or: [ &#123; &lt;expression1&gt; &#125;, &#123; &lt;expression2&gt; &#125;, ... , &#123; &lt;expressionN&gt; &#125; ] &#125;#数量小于20或者价格等于10 db.inventory.find( &#123; $or: [ &#123; quantity: &#123; $lt: 20 &#125; &#125;, &#123; price: 10 &#125; ] &#125; ) 4.数组操作1234567891011121314151617181920212223242526272829303132333435#数组查询$all $in$all 数组中必须包含所有给定的查询的元素$in 数组中只要包含给定的查询元素就可以&gt; db.phone.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125;&gt; db.phone.find(&#123;num:&#123;$all:[1,2]&#125;&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125;&gt; db.phone.find(&#123;num:&#123;$all:[1,4]&#125;&#125;) # 同时包含１，４的没有数据&gt; db.phone.find(&#123;num:&#123;$in:[1,4]&#125;&#125;) # 包含１或４的数据&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125;#查询数组的长度等于给定数组长度的文档 $size&gt; db.phone.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e51a20c9b0dc40419388&quot;), &quot;state&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;519969952b76790566165de2&quot;), &quot;num&quot; : [ 2, 3 ] &#125; &gt; db.phone.find(&#123;num:&#123;$size:4&#125;&#125;) # num数组长度为4的结果没有 &gt; db.phone.find(&#123;num:&#123;$size:3&#125;&#125;) # 长度为3的有三个&#123; &quot;_id&quot; : ObjectId(&quot;5198e20220c9b0dc40419385&quot;), &quot;num&quot; : [ 1, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e21820c9b0dc40419386&quot;), &quot;num&quot; : [ 4, 2, 3 ] &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198e22120c9b0dc40419387&quot;), &quot;num&quot; : [ 1, 2, 5 ] &#125; 5.自定义的查询 $where由bson转换为json,然后再通过回调函数去判断,性能很差,能不用尽量别用1234567891011121314151617181920&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843ba&quot;), &quot;name&quot; : &quot;user8&quot;, &quot;age&quot; : 8 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bb&quot;), &quot;name&quot; : &quot;user9&quot;, &quot;age&quot; : 9 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843bc&quot;), &quot;name&quot; : &quot;user10&quot;, &quot;age&quot; : 10 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c3cac686eb50e2c843bd&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 20 &#125; &gt; db.user.find(&#123;$where:function()&#123;return this.age == 3 || this.age == 4&#125;&#125;) # 回调,进入了隐式迭代,然后符合条件的才返回;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125; # 如今的新版本也可以直接写where条件db.goods.find(&#123;$where:&apos;this.cat_id != 3 &amp;&amp; this.cat_id != 11&apos;&#125;); 6.正则表达式正则的效率都知道的,得一一解析后再查找,所以效率也是很低; 12&gt; db.user.find(&#123;name:/user.*/i&#125;) # 查询name以user开头不区分大小写的文档&gt; db.goods.find(&#123;goods_name:/诺基亚.*/&#125;,&#123;goods_name:1&#125;); # 以诺基亚开头的商品 7.limit1234&gt; db.user.find(&#123;age:&#123;$gte:5&#125;&#125;).limit(3) # 限制返回的是三条数据&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b8&quot;), &quot;name&quot; : &quot;user6&quot;, &quot;age&quot; : 6 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b9&quot;), &quot;name&quot; : &quot;user7&quot;, &quot;age&quot; : 7 &#125; 8.分页查询使用到skip 和limit方法.skip表示跳过前面的几个文档,limit表示显示几个文档. 123456789101112131415&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125;&gt; db.user.find().skip(2).limit(3) # 跳过前两个文档查询后面的三个文档,经过测试这两个方法的使用顺序没有影响&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125;&gt; db.user.find().limit(3).skip(2)&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125; 9.sort 排序在mongodb中排序很简单,使用sort方法,传递给它你想按照哪个字段的哪种方式排序即可.这里1代表升序,-1代表降序.123456789101112131415161718192021&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&gt; db.user.find().sort(&#123;age:1&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&gt; db.user.find().sort(&#123;age:-1&#125;)&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 5 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 4 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 3 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125; 10.group 分组查询&emsp;mongodb中的group可以实现类似关系型数据库中的分组的功能,但是mongodb中的group远比关系型数据库中的group强大,可以实现map-reduce功能,关于什么是map-reduce,会在后续大数据专题里面说明,这里先略过,感兴趣的朋友可以百度group中的json参数类似这样{key:{字段:1},initial:{变量:初始值},\\$reduce:function(doc,prev){函数代码}}.其中的字段代表,需要按哪个字段分组.变量表示这一个分组中会使用的变量,并且给一个初始值.可以在后面的$reduce函数中使用.$reduce的两个参数,分别代表当前的文档和上个文档执行完函数后的结果.如下我们按年龄分组,同级不同年龄的用户的多少: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125; &gt; db.user.group(&#123;key:&#123;age:1&#125;,initial:&#123;count:0&#125;,$reduce:function(doc,prev)&#123;prev.count++&#125;&#125;)[ &#123; &quot;age&quot; : 0, &quot;count&quot; : 1 &#125;, &#123; &quot;age&quot; : 1, &quot;count&quot; : 3 &#125;, &#123; &quot;age&quot; : 2, &quot;count&quot; : 2 &#125;] &gt; db.user.group(&#123;key:&#123;age:1&#125;,initial:&#123;users:[]&#125;,$reduce:function(doc,prev)&#123;prev.users.push(doc.name)&#125;&#125;); #由于内部是使用js引擎来解析的,所以完全可以通过js语法来操作(操作数组用push),这使得虽然mongodb的分组很麻烦但却很灵活[ &#123; &quot;age&quot; : 0, &quot;users&quot; : [ &quot;user0&quot; ] &#125;, &#123; &quot;age&quot; : 1, &quot;users&quot; : [ &quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot; ] &#125;, &#123; &quot;age&quot; : 2, &quot;users&quot; : [ &quot;user2&quot;, &quot;user5&quot; ] &#125;] # 另外本函数还有两个可选参数 condition 和 finalize # condition就是分组的条件筛选类似mysql中的having&gt; db.user.group(&#123;key:&#123;age:1&#125;,initial:&#123;users:[]&#125;,$reduce:function(doc,prev)&#123;prev.users.push(doc.name)&#125;,condition:&#123;age:&#123;$gt:0&#125;&#125;&#125;) # 筛选出age大于0的;[ &#123; &quot;age&quot; : 1, &quot;users&quot; : [ &quot;user1&quot;, &quot;user3&quot;, &quot;user4&quot; ] &#125;, &#123; &quot;age&quot; : 2, &quot;users&quot; : [ &quot;user2&quot;, &quot;user5&quot; ] &#125;] 11.count 统计1234&gt; db.goods.count() #不传参数就统计该集合的总数31&gt; db.goods.count(&#123;cat_id:3&#125;) # 统计cat_id=3的总数15 12.distinct 排重123456789&gt; db.user.find()&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b2&quot;), &quot;name&quot; : &quot;user0&quot;, &quot;age&quot; : 0 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b3&quot;), &quot;name&quot; : &quot;user1&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b4&quot;), &quot;name&quot; : &quot;user2&quot;, &quot;age&quot; : 2 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b5&quot;), &quot;name&quot; : &quot;user3&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b6&quot;), &quot;name&quot; : &quot;user4&quot;, &quot;age&quot; : 1 &#125;&#123; &quot;_id&quot; : ObjectId(&quot;5198c286c686eb50e2c843b7&quot;), &quot;name&quot; : &quot;user5&quot;, &quot;age&quot; : 2 &#125;&gt; db.user.distinct(&quot;age&quot;) # 略微有点特殊,传入的参数直接是字符串,而不是对象;[ 0, 1, 2 ] 13.子文档查询 $elemMatch与对象中的属性1234567891011121314151617181920212223&#123; _id: 1, results: [ 82, 85, 88 ] &#125;&#123; _id: 2, results: [ 75, 88, 89 ] &#125; db.scores.find( &#123; results: &#123; $elemMatch: &#123; $gte: 80, $lt: 85 &#125; &#125; &#125; #查询results文档中的元素同时满足即大于80并且又小于85的,注意此处只要其中一个元素满足这个查询就会返回) &#123; &quot;_id&quot; : 1, &quot;results&quot; : [ 82, 85, 88 ] &#125;&gt; db.user.find();&#123; &quot;_id&quot; : ObjectId(&quot;55c070a02cc8cec37073a1d9&quot;), &quot;name&quot; : &quot;zxg&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : &#123; &quot;life&quot; : [ &quot;电影&quot;, &quot;小说&quot;, &quot;漫画&quot; ], &quot;work&quot; : [ &quot;发呆&quot;, &quot;发呆2&quot; ], &quot;home&quot; : &quot;玩耍&quot; &#125; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55c070a52cc8cec37073a1da&quot;), &quot;name&quot; : &quot;jyh&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : &#123; &quot;life&quot; : [ &quot;卖萌&quot;, &quot;养兔子&quot;, &quot;做家务&quot; ], &quot;work&quot; : [ &quot;郁闷&quot;, &quot;郁闷2&quot; ], &quot;home&quot; : &quot;卖萌&quot; &#125; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55c072db2cc8cec37073a1db&quot;), &quot;name&quot; : &quot;jyh&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : [ &#123; &quot;life&quot; : [ &quot;卖萌&quot;, &quot;养兔子&quot;, &quot;做家务&quot; ] &#125;, &#123; &quot;work&quot; : [ &quot;郁闷&quot;, &quot;郁闷2&quot; ] &#125;, &#123; &quot;home&quot; : &quot;卖萌&quot; &#125; ] &#125; &gt; db.user.find(&#123;hobby:&#123;$elemMatch:&#123;home:&apos;卖萌&apos;&#125;&#125;&#125;) # 注意上文的结构,必须是要在数组中才可以查出&#123; &quot;_id&quot; : ObjectId(&quot;55c072db2cc8cec37073a1db&quot;), &quot;name&quot; : &quot;jyh&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : [ &#123; &quot;life&quot; : [ &quot;卖萌&quot;, &quot;养兔子&quot;, &quot;做家务&quot; ] &#125;, &#123; &quot;work&quot; : [ &quot;郁闷&quot;, &quot;郁闷2&quot; ] &#125;, &#123; &quot;home&quot; : &quot;卖萌&quot; &#125; ] &#125; &gt; db.user.find(&#123;&apos;hobby.home&apos;:&apos;卖萌&apos;&#125;) # 注意,hobby.home类似js中对象与属性的操作方式,但是要加上引号,否则会报错&#123; &quot;_id&quot; : ObjectId(&quot;55c070a52cc8cec37073a1da&quot;), &quot;name&quot; : &quot;jyh&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : &#123; &quot;life&quot; : [ &quot;卖萌&quot;, &quot;养兔子&quot;, &quot;做家务&quot; ], &quot;work&quot; : [ &quot;郁闷&quot;, &quot;郁闷2&quot; ], &quot;home&quot; : &quot;卖萌&quot; &#125; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;55c072db2cc8cec37073a1db&quot;), &quot;name&quot; : &quot;jyh&quot;, &quot;age&quot; : 28, &quot;hobby&quot; : [ &#123; &quot;life&quot; : [ &quot;卖萌&quot;, &quot;养兔子&quot;, &quot;做家务&quot; ] &#125;, &#123; &quot;work&quot; : [ &quot;郁闷&quot;, &quot;郁闷2&quot; ] &#125;, &#123; &quot;home&quot; : &quot;卖萌&quot; &#125; ] &#125; 14.查询实例以下查询基于ecshop网站的数据查询12345678910111213141516171819202122232425262728293031323334353637383940# 本店价格低于或等于100元的商品($lte)db.goods.find(&#123;shop_price:&#123;$lte:100&#125;&#125;,&#123;goods_name:1,shop_price:1&#125;); # 取出第4栏目或第11栏目的商品($in)db.goods.find(&#123;cat_id:&#123;$in:[4,11]&#125;&#125;,&#123;goods_name:1,shop_price:1&#125;); # 取出100&lt;=价格&lt;=500的商品($and)db.goods.find(&#123;$and:[&#123;&apos;shop_price&apos;:&#123;$gte:100&#125;&#125;,&#123;&apos;shop_price&apos;:&#123;$lte:500&#125;&#125;]&#125;,&#123;_id:0,shop_price:1&#125;) # 取出不属于第3栏目且不属于第11栏目的商品($and $nin和$nor分别实现)db.goods.find(&#123;$and:[&#123;cat_id:&#123;$ne:3&#125;&#125;,&#123;cat_id:&#123;$ne:11&#125;&#125;]&#125;,&#123;_id:0,cat_id:1&#125;)db.goods.find(&#123;cat_id:&#123;$nin:[3,11]&#125;&#125;,&#123;_id:0,cat_id:1&#125;)db.goods.find(&#123;$nor:[&#123;cat_id:3&#125;,&#123;cat_id:11&#125;]&#125;,&#123;_id:0,cat_id:1&#125;) # 取出价格大于100且小于300,或者大于2000且小于5000的商品()db.goods.find(&#123;$or:[&#123;$and:[&#123;shop_price:&#123;$gt:100&#125;&#125;, &#123;shop_price:&#123;$lt:300&#125; &#125;]&#125;, &#123;$and:[&#123;shop_price:&#123;$gt:2000&#125;&#125;, &#123;shop_price:&#123;$lt:5000&#125; &#125;] &#125; ] &#125;,&#123;_id:0,shop_price:1&#125; ) # 取出所有goods_id为偶数的商品;db.goods.find(&#123;goods_id:&#123;$mod:[2,0]&#125;&#125;,&#123;_id:0,goods_id:1&#125;)#主键为32的商品db.goods.find(&#123;goods_id:32&#125;);#不属第3栏目的所有商品($ne)db.goods.find(&#123;cat_id:&#123;$ne:3&#125;&#125;,&#123;goods_id:1,cat_id:1,goods_name:1&#125;); #本店价格高于3000元的商品&#123;$gt&#125;db.goods.find(&#123;shop_price:&#123;$gt:3000&#125;&#125;,&#123;goods_name:1,shop_price:1&#125;);#本店价格低于或等于100元的商品($lte)db.goods.find(&#123;shop_price:&#123;$lte:100&#125;&#125;,&#123;goods_name:1,shop_price:1&#125;); #取出100&lt;=价格&lt;=500的商品($and)db.goods.find(&#123;$and:[&#123;price:&#123;$gt:100&#125;,&#123;$price:&#123;$lt:500&#125;&#125;&#125;]); #取出价格大于100且小于300,或者大于4000且小于5000的商品()db.goods.find(&#123;$or:[&#123;$and:[&#123;shop_price:&#123;$gt:100&#125;&#125;,&#123;shop_price:&#123;$lt:300&#125;&#125;]&#125;,&#123;$and:[&#123;shop_price:&#123;$gt:4000&#125;&#125;,&#123;shop_price:&#123;$lt:5000&#125;&#125;]&#125;]&#125;,&#123;goods_name:1,shop_price:1&#125;);","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"帮助说明","date":"2017-04-16T04:47:25.770Z","path":"2017/04/16/nosql/mongodb/帮助说明/","text":"help12345678910111213141516171819202122help db.help() help on db methods db.mycoll.help() help on collection methods sh.help() sharding helpers rs.help() replica set helpers help admin administrative help help connect connecting to a db help help keys key shortcuts help misc misc things to know help mr mapreduce show dbs show database names show collections show collections in current database show users show users in current database show profile show most recent system.profile entries with time &gt;= 1ms show logs show the accessible logger names show log [name] prints out the last segment of log in memory, &apos;global&apos; is default use &lt;db_name&gt; set current database db.foo.find() list objects in collection foo db.foo.find( &#123; a : 1 &#125; ) list objects in foo where a == 1 it result of the last line evaluated; use to further iterate DBQuery.shellBatchSize = x set default number of items to display on shell exit quit the mongo shell 官方文档:12#很多操作文档上都有详细的示例https://docs.mongodb.com/manual/","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"备份还原与导出导入(转)","date":"2017-04-16T04:47:25.769Z","path":"2017/04/16/nosql/mongodb/备份还原与导出导入/","text":"官方文档:https://docs.mongodb.com/manual/core/backups/原本使用操作系统的快照进行备份还原，备份成功后，还原没有成功（参考：Backup and Restore with Filesystem Snapshots）,所以这个方法就先不记录到这里了。 当前测试以下几种备份还原方法（个人初学理解）： 使用拷贝和替换数据库文件进行备份还原 使用mongodump和mongorestore 使用mongoimport 和 mongoexport 1.使用拷贝和替换数据库文件进行备份还原（有些危险又不好）1.1.备份12345678#在MongoDB中执行db.fsyncLock()，刷新数据写入磁盘并锁住整个实例：&gt;db.fsyncLock()#打包压缩整个数据库目录（也可以打包部分数据库），作为备份：[root@localhost ~]# tar -cvzf /root/mongodb_20150505.tar.gz /var/lib/mongo #在mongodb中执行db.fsyncUnlock()解锁，备份步骤完成&gt;db.fsyncUnlock() 1.2.还原12345678910111213141516#在mongodb中关闭服务（或者在操作系统层面关闭mongod服务）：&gt;use admin &gt;db.shutdownServer() # 将mongo数据文件删除！注意确认备份存在且正常！~否则回天无力！[root@localhost ~]# rm -rf /var/lib/mongo/* #解压备份的文件到根目录下，相当于还原：[root@localhost ~]# tar -xvzf /root/mongodb_20150505.tar.gz -C / #如果启动不了服务，先删除文件mongod.lock：[root@localhost ~]# rm -f /var/lib/mongo/mongod.lock #启动服务，正常访问[root@localhost ~]# service mongod start 2. 使用mongodump和mongorestore2.1.简单备份还原本地数据库的的方法，备份所有及还原所有12mongodump mongorestore /root/dump 备份完成后在当前目录将生成一个文件夹”dump“： 2.1.还原单个数据库时，指定要还原的数据库及其备份目录12mongorestore --db test /root/dump/test 还可以指定输出路径，使用计算机名： 12mongodump --host localhost.localdomain --port 27017 --out /root/mongodump-2015-05-05 mongorestore --port 27017 --db test /root/mongodump-2015-05-05/test 远程备份时指定用户名密码（这个没尝试，参考官方例子）：12mongodump --host mongodb1.example.net --port 3017 --username user --password pass --out /opt/backup/mongodump-2013-10-24 mongorestore --host mongodb1.example.net --port 3017 --username user --password pass /opt/backup/mongodump-2013-10-24 更多参考：执行 #mongorestore –help123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# mongorestore --help general options: --help print usage --version print the tool version and exit verbosity options: -v, --verbose more detailed log output (include multiple times for more verbosity, e.g. -vvvvv) --quiet hide all log output connection options: -h, --host= mongodb host to connect to (setname/host1,host2 for replica sets) --port= server port (can also use --host hostname:port) ssl options: --ssl connect to a mongod or mongos that has ssl enabled --sslCAFile= the .pem file containing the root certificate chain from the certificate authority --sslPEMKeyFile= the .pem file containing the certificate and key --sslPEMKeyPassword= the password to decrypt the sslPEMKeyFile, if necessary --sslCRLFile= the .pem file containing the certificate revocation list --sslAllowInvalidCertificates bypass the validation for server certificates --sslAllowInvalidHostnames bypass the validation for server name --sslFIPSMode use FIPS mode of the installed openssl library authentication options: -u, --username= username for authentication -p, --password= password for authentication --authenticationDatabase= database that holds the user&apos;s credentials --authenticationMechanism= authentication mechanism to use namespace options: -d, --db= database to use -c, --collection= collection to use input options: --objcheck validate all objects before inserting --oplogReplay replay oplog for point-in-time restore --oplogLimit= only include oplog entries before the provided Timestamp(seconds[:ordinal]) --restoreDbUsersAndRoles restore user and role definitions for the given database --dir= input directory, use &apos;-&apos; for stdin restore options: --drop drop each collection before import --writeConcern= write concern options e.g. --writeConcern majority, --writeConcern &apos;&#123;w: 3, wtimeout: 500, fsync: true, j:true&#125;&apos; (defaults to &apos;majority&apos;) --noIndexRestore don&apos;t restore indexes --noOptionsRestore don&apos;t restore collection options --keepIndexVersion don&apos;t update index version --maintainInsertionOrder preserve order of documents during restoration -j, --numParallelCollections= number of collections to restore in parallel (4 by default) --numInsertionWorkersPerCollection= number of insert operations to run concurrently per collection (1 by default) --stopOnError stop restoring if an error is encountered on insert (off bydefault) 3.使用mongoimport 和 mongoexport3.1.mongoexport导出12345678910111213141516171819202122--type: 为json 或 csv --fields: 选择导出的列 #导出列&#123;_id,id,size&#125;为csv的格式 mongoexport --db test --collection tab --type=csv --fields _id,id,size --out /root/test_tab.csv #导出json格式 mongoexport --db test --collection tab --type=json --out /root/test_tab.json #输出到shell(命令行)中，查询id=2 并按name升序输出 mongoexport --db test --collection tab --query &apos;&#123;&quot;id&quot;: 2&#125;&apos; --sort &apos;&#123;&quot;name&quot;: 1&#125;&apos; #查询导出 mongoexport --db test --collection tab --type=csv --query &apos;&#123;&quot;id&quot;: 2&#125;&apos; --fields _id,id --out /root/test_tab.csv #简写选项[--db]和[--collection]，使用跳过和限制函数输出 mongoexport -d test -c tab --sort &apos;&#123;&quot;name&quot;: -1&#125;&apos; --limit 2 --skip 2 --out /root/test_tab.json #若是远程，需要添加参数：host，port，username，password --host servername_or_ip --port 37017 --username user --password pass 3.2.查看帮助：mongoexport –help官网:https://docs.mongodb.com/manual/reference/program/mongoexport/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647general options: --help print usage --version print the tool version and exit verbosity options: -v, --verbose more detailed log output (include multiple times for more verbosity, e.g. -vvvvv) --quiet hide all log output connection options: -h, --host= mongodb host to connect to (setname/host1,host2 for replica sets) --port= server port (can also use --host hostname:port) ssl options: --ssl connect to a mongod or mongos that has ssl enabled --sslCAFile= the .pem file containing the root certificate chain from the certificate authority --sslPEMKeyFile= the .pem file containing the certificate and key --sslPEMKeyPassword= the password to decrypt the sslPEMKeyFile, if necessary --sslCRLFile= the .pem file containing the certificate revocation list --sslAllowInvalidCertificates bypass the validation for server certificates --sslAllowInvalidHostnames bypass the validation for server name --sslFIPSMode use FIPS mode of the installed openssl library authentication options: -u, --username= username for authentication -p, --password= password for authentication --authenticationDatabase= database that holds the user&apos;s credentials --authenticationMechanism= authentication mechanism to use namespace options: -d, --db= database to use -c, --collection= collection to use output options: -f, --fields= comma separated list of field names (required for exporting CSV) e.g. -f &quot;name,age&quot; --fieldFile= file with field names - 1 per line --type= the output format, either json or csv (defaults to &apos;json&apos;) -o, --out= output file; if not specified, stdout is used --jsonArray output to a JSON array rather than one object per line --pretty output JSON formatted to be human-readable querying options: -q, --query= query filter, as a JSON string, e.g., &apos;&#123;x:&#123;$gt:1&#125;&#125;&apos; -k, --slaveOk allow secondary reads if available (default true) --forceTableScan force a table scan (do not use $snapshot) --skip= number of documents to skip --limit= limit the number of documents to export --sort= sort order, as a JSON string, e.g. &apos;&#123;x:1&#125;&apos; 3.3.mongoimport 导入12345678#列&quot;_id&quot;也会导入，注意重复键 mongoimport --db mydb --collection tab --file /root/test_tab.json mongoimport --db mydb --collection tab --type csv --headerline --file /root/test_tab.csv #若是远程，需要添加参数：host，port，username，password --host servername_or_ip --port 37017 --username user --password pass 3.4.查看帮助： mongoimport –help12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152general options: --help print usage --version print the tool version and exit verbosity options: -v, --verbose more detailed log output (include multiple times for more verbosity, e.g. -vvvvv) --quiet hide all log output connection options: -h, --host= mongodb host to connect to (setname/host1,host2 for replica sets) --port= server port (can also use --host hostname:port) ssl options: --ssl connect to a mongod or mongos that has ssl enabled --sslCAFile= the .pem file containing the root certificate chain from the certificate authority --sslPEMKeyFile= the .pem file containing the certificate and key --sslPEMKeyPassword= the password to decrypt the sslPEMKeyFile, if necessary --sslCRLFile= the .pem file containing the certificate revocation list --sslAllowInvalidCertificates bypass the validation for server certificates --sslAllowInvalidHostnames bypass the validation for server name --sslFIPSMode use FIPS mode of the installed openssl library authentication options: -u, --username= username for authentication -p, --password= password for authentication --authenticationDatabase= database that holds the user&apos;s credentials --authenticationMechanism= authentication mechanism to use namespace options: -d, --db= database to use -c, --collection= collection to use input options: -f, --fields= comma separated list of field names, e.g. -f name,age --fieldFile= file with field names - 1 per line --file= file to import from; if not specified, stdin is used --headerline use first line in input source as the field list (CSV and TSV only) --jsonArray treat input source as a JSON array --type= input format to import: json, csv, or tsv (defaults to &apos;json&apos;) ingest options: --drop drop collection before inserting documents --ignoreBlanks ignore fields with empty values in CSV and TSV --maintainInsertionOrder insert documents in the order of their appearance in the input source -j, --numInsertionWorkers= number of insert operations to run concurrently (defaults to 1) --stopOnError stop importing at first insert/upsert error --upsert insert or update objects that already exist --upsertFields= comma-separated fields for the query part of the upsert --writeConcern= write concern options e.g. --writeConcern majority, --writeConcern &apos;&#123;w: 3, wtimeout: 500, fsync: true, j: true&#125;&apos; (defaults to &apos;majority&apos;) 转自","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"shard分片(转)","date":"2017-04-16T04:47:25.768Z","path":"2017/04/16/nosql/mongodb/shard分片/","text":"1.简述1.1.为何要分片 减少单机请求数，降低单机负载，提高总负载 减少单机的存储空间，提高总存空间。 1.2.常见的mongodb sharding服务器架构 要构建一个 MongoDB Sharding Cluster，需要三种角色： Shard Server即存储实际数据的分片，每个Shard可以是一个mongod实例，也可以是一组mongod实例构成的Replication Set。为了实现每个Shard内部的auto-failover(自动故障切换)，MongoDB官方建议每个Shard为一组Replica Set。 Config Server为了将一个特定的collection存储在多个shard中，需要为该collection指定一个shard key(片键)，例如{age: 1} ，shard key可以决定该条记录属于哪个chunk(分片是以chunk为单位,后续会介绍)。Config Servers就是用来存储：所有shard节点的配置信息、每个chunk的shard key范围、chunk在各shard的分布情况、该集群中所有DB和collection的sharding配置信息。 Route Process这是一个前端路由，客户端由此接入，然后询问Config Servers需要到哪个Shard上查询或保存记录，再连接相应的Shard进行操作，最后将结果返回给客户端。客户端只需要将原本发给mongod的查询或更新请求原封不动地发给Routing Process，而不必关心所操作的记录存储在哪个Shard上。（所有操作在mongos上操作即可） 2.配置分片服务器下面我们在同一台物理机器上构建一个简单的 Sharding Cluster： 1234Shard Server 1：27017Shard Server 2：27018Config Server ：27027Route Process：40000 2.1.步骤一:启动Shard Server12345678mkdir -p ./data/shard/s0 ./data/shard/s1 #创建数据目录mkdir -p ./data/shard/log # 创建日志目录./bin/mongod --port 27017 --dbpath /usr/local/mongodb/data/shard/s0 --fork --logpath /usr/local/mongodb/data/shard/log/s0.log # 启动Shard Server实例1./bin/mongod --port 27018 --dbpath /usr/local/mongodb/data/shard/s1 --fork --logpath /usr/local/mongodb/data/shard/log/s1.log # 启动Shard Server实例2 2.2.步骤二:启动Config Server12345mkdir -p ./data/shard/config #创建数据目录 ./bin/mongod --port 27027 --dbpath /usr/local/mongodb/data/shard/config --fork --logpath /usr/local/mongodb/data/shard/log/config.log #启动Config Server实例 注意，这里我们完全可以像启动普通mongodb服务一样启动，不需要添加—shardsvr和configsvr参数。因为这两个参数的作用就是改变启动端口的，所以我们自行指定了端口就可以 2.3.启动Route Process12./bin/mongos --port 4000 --configdb localhost:27027 --fork --logpath /usr/local/mongodb/data/shard/log/route.log --chunkSize=1 # 启动Route Server实例 mongos启动参数中，chunkSize这一项是用来指定chunk的大小的，单位是MB，默认大小为200MB，为了方便测试Sharding效果，我们把chunkSize指定为 1MB。意思是当这个分片中插入的数据大于1M时开始进行数据转移 2.4.配置Sharding12345678910111213141516# 我们使用MongoDB Shell登录到mongos，添加Shard节点./bin/mongo admin --port 40000 #此操作需要连接admin库&gt; db.runCommand(&#123; addshard:&quot;localhost:27017&quot; &#125;) #添加 Shard Server 或者用 sh.addshard()命令来添加,下同;&#123; &quot;shardAdded&quot; : &quot;shard0000&quot;, &quot;ok&quot; : 1 &#125;&gt; db.runCommand(&#123; addshard:&quot;localhost:27018&quot; &#125;)&#123; &quot;shardAdded&quot; : &quot;shard0001&quot;, &quot;ok&quot; : 1 &#125; &gt; db.runCommand(&#123; enablesharding:&quot;test&quot; &#125;) #设置分片存储的数据库&#123; &quot;ok&quot; : 1 &#125; &gt; db.runCommand(&#123; shardcollection: &quot;test.users&quot;, key: &#123; id:1 &#125;&#125;) # 设置分片的集合名称。且必须指定Shard Key，系统会自动创建索引，然后根据这个shard Key来计算&#123; &quot;collectionsharded&quot; : &quot;test.users&quot;, &quot;ok&quot; : 1 &#125; &gt; sh.status(); #查看片的状态&gt; printShardingStatus(db.getSisterDB(&quot;config&quot;),1); # 查看片状态(完整版);&gt; db.stats(); # 查看所有的分片服务器状态 注意这里我们要注意片键的选择，选择片键时需要根据具体业务的数据形态来选择，切不可随意选择，实际中尤其不要轻易选择自增_id作为片键，除非你很清楚你这么做的目的，具体原因我不在此分析，根据经验推荐一种较合理的片键方式，“自增字段+查询字段”，没错，片键可以是多个字段的组合。 另外这里说明一点，分片的机制：mongodb不是从单篇文档的级别,绝对平均的散落在各个片上, 而是N篇文档,形成一个块”chunk”,优先放在某个片上, 当这片上的chunk,比另一个片的chunk区别比较大时(&gt;=3) ,会把本片上的chunk,移到另一个片上, 以chunk为单位,维护片之间的数据均衡。 也就是说，一开始插入数据时，数据是只插入到其中一块分片上的，插入完毕后，mongodb内部开始在各片之间进行数据的移动，这个过程可能不是立即的，mongodb足够智能会根据当前负载决定是立即进行移动还是稍后移动。在插入数据后，立马执行db.users.stats();两次可以验证如上所说。 这种分片机制,节省了人工维护成本,但是由于其是优先往某个片上插入,等到chunk失衡时,再移动chunk,并且随着数据的增多,shard的实例之间,有chunk来回移动的现象,这将会为服务器带来很大的IO开销,解决这种开销的方法,就是手动预先分片; 3. 手动预先分片以shop.user表为例12345sh.shardCollection(‘shop.user’,&#123;userid:1&#125;); # user表用userid做shard key for(var i=1;i&lt;=40;i++) &#123; sh.splitAt(&apos;shop.user&apos;,&#123;userid:i*1000&#125;) &#125; # 预先在1K 2K...40K这样的界限切好chunk(虽然chunk是空的), 这些chunk将会均匀移动到各片上. 通过mongos添加user数据. 数据会添加到预先分配好的chunk上, chunk就不会来回移动了. 4.replication set and shard12sh.addShard( host ) server:port OR setname/server:port # 如果是复制集的片服务器,我们应该复制集的名称写在前面比如sh.addShard(&apos;ras/192.168.42.168:27017&apos;); # 27017也就是复制集中的primary 另外在启动本机的mongod服务的时候,最好把ip也给写进去,否则有可能会有不可预知的错误; 转载:Shard 分片集群 参考:MongoDB 基础（九）分片","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"replication set复制集","date":"2017-04-16T04:47:25.767Z","path":"2017/04/16/nosql/mongodb/replication set复制集/","text":"1.介绍replicattion set 多台服务器维护相同的数据副本,提高服务器的可用性,总结下来有以下好处: 数据备份与恢复 读写分离 1.1.Mongodb复制集的结构及基本概念 正如上图所示，MongoDB 复制集的架构中，主要分为两部分：主节点（Primary）和从节点（Secondary）。 主节点：在一个复制集中只有并且必须有一个主节点，主节点也是众多实例中唯一可以接收客户端写操作的节点，当然也可以进行读操作； 从节点：从节点会复制主节点的操作，以获取完全一致的数据集。客户端不能够直接对从节点进行写操作，但是可以进行读操作，这个需要通过复制集选项进行设置。 投票节点：投票节点 并不含有 复制集中的数据集副本，且也 无法 升职为主节点。投票节点的存在是为了使复制集中的节点数量为奇数，这样保证在进行投票的时候不会出现票数相同的情况。如果添加了一个节点后，总节点数为偶数，那么就需要相应的增加一个投票节点。 注：MongoDB 3.0 把复制集中的成员数量从原来的12个提升到了50个，但是投票节点的数量仍然保持不变，还是7个。 1.2.最基本的复制集架构 一个主节点，两个从节点，自动化故障切换的特性最基本的复制集架构是有3个节点的形式。这样在主节点不可用以后，从节点会进行投票选出一个节点成为主节点，继续工作。如下图所示： 三个节点的复制集架构，还有另外一种形式：一个主节点，一个从节点，一个投票节点。如下图所示： 一个主节点，一个从节点，一个投票节点在这种架构中，当主节点不可用时，只有从节点可以升为主节点，而投票节点是不可以成为主节点的。投票节点仅仅在选举中进行投票。如下图所示： 1.3.从节点无法升职为主节点的情况其他概念从节点还有集中特殊的设置情况，不同的设置有不同的需求： 优先级为0：设置 priority:0 ，那么该结点将不能成为主节点，但是其数据仍是与主节点保持一致的,而且应用程序也可以进行读操作。这样可以在某些特殊的情况下，保证其他特定节点优先成为主节点。 隐藏节点：隐藏节点与主节点的数据集一致，但是对于应用程序来说是不可见的。隐藏节点可以很好的与 复制集 中的其他节点隔离，并应对特殊的需求，比如进行报表或者数据备份。隐藏节点也应该是一个不能升职为主节点的优先级为0的节点。 延时节点：延时节点也将从 复制集 中主节点复制数据，然而延时节点中的数据集将会比复制集中主节点的数据延后。举个例子，现在是09：52，如果延时节点延后了1小时，那么延时节点的数据集中将不会有08：52之后的操作。 由于延时节点的数据集是延时的，因此它可以帮助我们在人为误操作或是其他意外情况下恢复数据。举个例子，当应用升级失败，或是误操作删除了表和数据库时，我们可以通过延时节点进行数据恢复。 oplog：全拼 oprations log，它保存有数据库的所有的操作的记录。在复制集中，主节点产生 oplog，然后从节点复制主节点的 oplog 进行相应的操作，这样达到保持数据集一致的要求。因此从节点的数据与主节点的数据相比是有延迟的。 2.配置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 创建数据存储目录mkdir -p /data/r0 /data/r1 /data/r2 # 创建日志文件touch /var/log/mongo17.log /var/log/mongo18.log /var/log/mongo19.log #启动3个实例,且声明实例属于某复制集 rsa./bin/mongod --port 27017 --dbpath /data/r0 --smallfiles --replSet rsa --fork --logpath /var/log/mongo17.log./bin/mongod --port 27018 --dbpath /data/r1 --smallfiles --replSet rsa --fork --logpath /var/log/mongo18.log./bin/mongod --port 27019 --dbpath /data/r2 --smallfiles --replSet rsa --fork --logpath /var/log/mongo19.log # 进入27017进行配置初始化./bin/mongo --port 27017rsconf = &#123; _id:&apos;rsa&apos;, members: [ &#123;_id:0, host:&apos;192.168.42.168:27017&apos; &#125; ]&#125;rs.initiate(rsconf); # 如果以后需要再重载一下config的话,用rs.reconfig(rsconf); # 添加节点rs.add(&apos;192.168.42.168:27018&apos;);rs.add(&apos;192.168.42.168:27019&apos;); # 查看状态rs.status(); # 删除节点rs.remove(&apos;192.168.1.201:27019&apos;); # 主节点插入数据&gt;use test&gt;db.user.insert(&#123;uid:1,name:&apos;lily&apos;&#125;); #连接secondary查询同步情况./bin/mongo --port 27019&gt;show dbs rsa:SECONDARY&gt; show dbs;2015-08-27T11:39:00.638+0800 E QUERY Error: listDatabases failed:&#123; &quot;note&quot; : &quot;from execCommand&quot;, &quot;ok&quot; : 0, &quot;errmsg&quot; : &quot;not master&quot; &#125; # 还可以通过isMaster()命令来查看信息;rsa:PRIMARY&gt; db.isMaster();&#123; &quot;setName&quot; : &quot;rsa&quot;, &quot;setVersion&quot; : 5, &quot;ismaster&quot; : true, &quot;secondary&quot; : false, &quot;hosts&quot; : [ &quot;192.168.42.168:27018&quot;, &quot;192.168.42.168:27019&quot;, &quot;192.168.42.168:27017&quot; ], &quot;primary&quot; : &quot;192.168.42.168:27018&quot;, &quot;me&quot; : &quot;192.168.42.168:27018&quot;, &quot;electionId&quot; : ObjectId(&quot;55dea0cffa0c638625a82486&quot;), &quot;maxBsonObjectSize&quot; : 16777216, &quot;maxMessageSizeBytes&quot; : 48000000, &quot;maxWriteBatchSize&quot; : 1000, &quot;localTime&quot; : ISODate(&quot;2015-08-27T05:49:13.740Z&quot;), &quot;maxWireVersion&quot; : 3, &quot;minWireVersion&quot; : 0, &quot;ok&quot; : 1&#125; # 出现上述错误,是因为slave默认不许读写&gt;rs.slaveOk();&gt;show dbs; # 执行上面一个语句就可以看到和primary一致的数据,并且可以把读和写分离开来; 以上便是一个最简单的复制集架构,其中如果27017的主节点崩溃,那27018的节点就由从节点变为主节点;注意,如果再添加原来的27017节点进来,那主节点还是27018; 2.1.自动化配置脚本12345678910111213141516171819202122232425262728293031323334353637383940414243#!/bin/bashIP=&apos;192.168.1.202&apos;NA=&apos;rsb&apos; if [ &quot;$1&quot; = &quot;reset&quot; ]then pkill -9 mongo rm -rf /home/m* exitfi if [ &quot;$1&quot; = &quot;install&quot; ]then mkdir -p /home/m0 /home/m1 /home/m2 /home/mlog /usr/local/mongodb/bin/mongod --dbpath /home/m0 --logpath /home/mlog/m17.log --logappend --port 27017 --fork--replSet $&#123;NA&#125;/usr/local/mongodb/bin/mongod --dbpath /home/m1 --logpath /home/mlog/m18.log --logappend --port 27018 --fork--replSet $&#123;NA&#125;/usr/local/mongodb/bin/mongod --dbpath /home/m2 --logpath /home/mlog/m19.log --logappend --port 27019 --fork--replSet $&#123;NA&#125; exitfi if [ &quot;$1&quot; = &quot;repl&quot; ]then/usr/local/mongodb/bin/mongo &lt;&lt;EOF use adminrsconf = &#123; _id:&apos;$&#123;NA&#125;&apos;, members:[ &#123;_id:0,host:&apos;$&#123;IP&#125;:27017&apos;&#125;, &#123;_id:1,host:&apos;$&#123;IP&#125;:27018&apos;&#125;, &#123;_id:2,host:&apos;$&#123;IP&#125;:27019&apos;&#125;, ]&#125;rs.initiate(rsconf)EOFfi 转自:replication set复制集 参考:MongoDB 基础（八）复制Ⅱ—部署仲裁节点 MongoDB 基础（七）复制Ⅰ—部署复制集","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"Mongodb简介","date":"2017-04-16T04:47:25.765Z","path":"2017/04/16/nosql/mongodb/Mongodb简介/","text":"mongodb文档数据库，存储的是文档（Bson-&gt;json的二进制化） 内部执行引擎为JS解释器，把文档存储成bson结构，在查询时，转换为JS对象，并可以通过熟悉的js语法来操作 和传统数据库的比较：表下的每篇文档都可以有自己的结构（json对象可以有自己独特的属性和值） 一部电影，有影评，影评下面有回复，这样的多表的关系（在传统数据库中肯定是几张表），mongodb可以使用多个层级的json来存储 这样的存储结构，就像一棵树，也像一个文档（JavaScript的document）","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"mongodb的CRUD","date":"2017-04-16T04:47:25.764Z","path":"2017/04/16/nosql/mongodb/mongodb的CRUD/","text":"mongodb的官方文档12#在文档上都有详细的示例https://docs.mongodb.com/manual/ 1.增加12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#语法db.collectionName.insert(document)#增加单篇文档&gt; db.student.insert(&#123;sn:&apos;001&apos;,name:&apos;xiaoming&apos;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;57ff1837abf4c31796f90449&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&gt;#增加单篇文档,并指定_id&gt; db.student.insert(&#123;_id:2,sn:&apos;002&apos;,name:&apos;xiaogang&apos;&#125;)WriteResult(&#123; &quot;nInserted&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;57ff1837abf4c31796f90449&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaogang&quot; &#125;&gt;#增加多个文档添加：一个数组对象&gt; db.student.insert([&#123;_id:3,sn:&apos;003&apos;,name:&apos;zhangfei&apos;&#125;,&#123;sn:&apos;004&apos;,name:&apos;guanyu&apos;&#125;])BulkWriteResult(&#123; &quot;writeErrors&quot; : [ ], &quot;writeConcernErrors&quot; : [ ], &quot;nInserted&quot; : 2, &quot;nUpserted&quot; : 0, &quot;nMatched&quot; : 0, &quot;nModified&quot; : 0, &quot;nRemoved&quot; : 0, &quot;upserted&quot; : [ ]&#125;)&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;57ff1837abf4c31796f90449&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaogang&quot; &#125;&#123; &quot;_id&quot; : 3, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&gt; #插入复杂的结构&gt; db.student.insert([&#123;_id:4,sn:&apos;003&apos;,name:&apos;zhangfei&apos;,class:[&apos;math&apos;,&apos;chinese&apos;]&#125;])&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;57ff1837abf4c31796f90449&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaogang&quot; &#125;&#123; &quot;_id&quot; : 3, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&#123; &quot;_id&quot; : 4, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot;, &quot;class&quot; : [ &quot;math&quot;, &quot;chinese&quot; ] &#125;&gt; 2.删除123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#语法db.collection.remove(查询表达式, 选项)#选项是指: &#123;justOne: true/ false&#125; 是否只删一行,默认是false&gt; db.student.find()&#123; &quot;_id&quot; : ObjectId(&quot;57ff1837abf4c31796f90449&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&#123; &quot;_id&quot; : 2, &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaogang&quot; &#125;&#123; &quot;_id&quot; : 3, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&#123; &quot;_id&quot; : 4, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot;, &quot;class&quot; : [ &quot;math&quot;, &quot;chinese&quot; ] &#125;&gt;&gt; db.student.remove(&#123;sn:&apos;002&apos;&#125;) #删除student表中sn属性值为&quot;002&quot;的文档WriteResult(&#123; &quot;nRemoved&quot; : 1 &#125;)&gt; db.student.find()&#123; &quot;_id&quot; : 3, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&#123; &quot;_id&quot; : 4, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot;, &quot;class&quot; : [ &quot;math&quot;, &quot;chinese&quot; ] &#125;&gt;#删除student表中gender属性为m的文档,只删除1行&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088ab1ba35a73d3b4275&quot;), &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaoming2&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; db.student.remove(&#123;gender:&quot;m&quot;&#125;,true)WriteResult(&#123; &quot;nRemoved&quot; : 1 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; #删除所有的记录&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a8fe12b1ba35a73d3b426e&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a903a9b1ba35a73d3b426f&quot;), &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaoming2&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a903c3b1ba35a73d3b4270&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a903ccb1ba35a73d3b4271&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming4&quot;, &quot;gender&quot; : &quot;m&quot; &#125; &gt; db.student.remove(&#123;&#125;);WriteResult(&#123; &quot;nRemoved&quot; : 4 &#125;)&gt; db.student.find();&gt;#drop是删除整个表,而remove是删除表中的记录&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a90683b1ba35a73d3b4272&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming4&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9068db1ba35a73d3b4273&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a90693b1ba35a73d3b4274&quot;), &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaoming2&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&gt; show collections;studentuser&gt; db.student.drop();true&gt; db.student.find();&gt; show collections;user&gt; /*To delete all records from a table, uses db.tablename.remove().To drop the table, uses db.tablename.drop().*/ 注意: 查询表达式依然是个json对象 查询表达式匹配的行,将被删除 如果不写查询表达式,collections中的所有文档将被删除 3.更新123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159#语法db.collection.update(查询表达式, 新值, Option) &gt; db.student.find()&#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;zhangfei2222&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&#123; &quot;_id&quot; : 4, &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;zhangfei&quot;, &quot;class&quot; : [ &quot;math&quot;, &quot;chinese&quot; ] &#125; &gt; db.student.update(&#123;name:&apos;zhangfei&apos;&#125;,&#123;name:&apos;zhangfei2222&apos;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;) &gt; db.student.find()&#123; &quot;_id&quot; : 3, &quot;name&quot; : &quot;zhangfei2222&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;57ff18d2abf4c31796f9044a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;guanyu&quot; &#125;&#123; &quot;_id&quot; : 4, &quot;name&quot; : &quot;zhangfei2222&quot; &#125; #文档中除了_id没有变之外，其他字段都被覆盖,即新文档直接替换了旧文档,而不是修改#如果是想修改文档的某列, 使用$set关键字&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; db.student.update(&#123;name:&quot;xiaoming3&quot;&#125;,&#123;$set:&#123;name:&quot;QQ&quot;&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; /*除了$set还有如下的选项:$inc #增长$rename #重命名$setOnInsert #当upsert时,设置字段的值$set #设置字段的新值$unset #删除指定的列$push # 将一个数字存入一个数组,分为三种情况,如果该字段存在,则直接将数字存入数组.如果该字段不存在,创建字段并且将数字插入该数组.如果更新的字段不是数组,会报错的.$pop #删除数组最后一个元素$pull # 删除数组中的指定的元素,如果删除的字段不是数组,会报错*/&gt; db.stu.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a90e08b1ba35a73d3b4278&quot;), &quot;name&quot; : &quot;wukong&quot;, &quot;jingu&quot; : true, &quot;sex&quot; : &quot;m&quot;, &quot;age&quot; : 500 &#125; &gt; db.stu.update(&#123;name:&quot;wukong&quot;&#125;,&#123;$set:&#123;name:&quot;shitou&quot;&#125;,$unset:&#123;jingu:1&#125;,$rename:&#123;sex:&quot;gender&quot;&#125;,$inc:&#123;age:16&#125;&#125;);&gt; db.stu.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a90e08b1ba35a73d3b4278&quot;), &quot;name&quot; : &quot;shitou&quot;, &quot;age&quot; : 516, &quot;gender&quot; : &quot;m&quot; &#125;&gt; #数组操作# $pushdb.test.find()&#123; &quot;_id&quot; : 1, &quot;ary&quot; : [ 1, 2, 3, 4 ] &#125;&#123; &quot;_id&quot; : 2, &quot;text&quot; : &quot;test&quot; &#125; db.test.update(&#123;_id:1&#125;,&#123;$push:&#123;ary:5&#125;&#125;) # 数组存在 直接压入，但是这个地方如果是数组的话就压入一个数组，并非是合并数组中的元素 db.test.update(&#123;_id:1&#125;,&#123;$push:&#123;ary:[8,9,10]&#125;&#125;) db.test.find()&#123; &quot;_id&quot; : 2, &quot;text&quot; : &quot;test&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;ary&quot; : [ 1, 2, 3, 4, 5,[8,9,10] ] &#125; # 由此可见push一次只能插入一个字段,如果想要批量插入的话就缓存pushAll; db.test.update(&#123;_id:2&#125;,&#123;$push:&#123;ary:6&#125;&#125;) # 数组不存在,创建数组并存入 db.test.find()&#123; &quot;_id&quot; : 2, &quot;ary&quot; : [ 6 ], &quot;text&quot; : &quot;test&quot; &#125;&#123; &quot;_id&quot; : 1, &quot;ary&quot; : [ 1, 2, 3, 4, 5 ] &#125; db.test.update(&#123;_id:2&#125;,&#123;$push:&#123;text:6&#125;&#125;) # 更新字段存在但不是数组报错Cannot apply $push/$pushAll modifier to non-array # popdb.user.update(&#123;_id:9&#125;,&#123;$pop:&#123;test:0&#125;&#125;) # 这里的test无论传入什么值,都是删掉test数组的最后一个 # $pulldb.user.update(&#123;_id:9&#125;,&#123;$pull:&#123;test:2&#125;&#125;) #这里的test传什么值就删掉什么值#Option的作用&#123;upsert:true/false, multi:true/false&#125;#upsert是指没有匹配的行就直接插入该行#multi: 是指修改多行,默认情况下,即使查询表达式命中多行,默认也只是修改一行#upsert&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&gt; db.student.update(&#123;_id:99&#125;,&#123;x:123,y:456&#125;,&#123;upsert:true&#125;)WriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : 99 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : 99, &quot;x&quot; : 123, &quot;y&quot; : 456 &#125;#没有multi的情况下:&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&gt; db.student.update(&#123;sn:&quot;004&quot;&#125;,&#123;$set:&#123;name:&quot;xxxxx&quot;&#125;&#125;)WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125; #因为sn=004的有多个,但是update之后只是修改一条记录,这是默认的情况下&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;#有multi的情况下:&gt; db.student.update(&#123;gender:&quot;m&quot;&#125;,&#123;$set:&#123;sn:&quot;888&quot;&#125;&#125;,&#123;multi:true&#125;)WriteResult(&#123; &quot;nMatched&quot; : 3, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 3 &#125;)&gt; db.student.find(); #update所有的gender=m的,并修改所有查询到的记录(将sn=888)&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&gt;# $setOnInsert 当upsert为true时,并且发生了insert操作时,可以补充的字段:&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125; &gt; db.student.update(&#123;_id:88&#125;,&#123;$set:&#123;name:&quot;AAAAAA&quot;&#125;,$setOnInsert:&#123;job:&quot;java-hadoop&quot;&#125;&#125;,&#123;upsert:true&#125;); #此时更新的记录不存在,那么就insertWriteResult(&#123; &quot;nMatched&quot; : 0, &quot;nUpserted&quot; : 1, &quot;nModified&quot; : 0, &quot;_id&quot; : 88 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;QQ&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : 88, &quot;name&quot; : &quot;AAAAAA&quot;, &quot;job&quot; : &quot;java-hadoop&quot; &#125;#此时更新的记录存在,那么setOnInsert就不会起作用&gt; db.student.update(&#123;sn:&quot;003&quot;&#125;,&#123;$set:&#123;name:&quot;AAAAAA&quot;&#125;,$setOnInsert:&#123;job:&quot;java-hadoop&quot;&#125;&#125;,&#123;upsert:true&#125;);WriteResult(&#123; &quot;nMatched&quot; : 1, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 1 &#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;AAAAAA&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : 88, &quot;name&quot; : &quot;AAAAAA&quot;, &quot;job&quot; : &quot;java-hadoop&quot; &#125;&gt; 4.查询123456789101112131415161718192021222324252627282930313233343536373839#语法db.collection.find(查询表达式, 查询的列)#查询所有&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a90683b1ba35a73d3b4272&quot;), &quot;sn&quot; : &quot;004&quot;, &quot;name&quot; : &quot;xiaoming4&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9068db1ba35a73d3b4273&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a90693b1ba35a73d3b4274&quot;), &quot;sn&quot; : &quot;002&quot;, &quot;name&quot; : &quot;xiaoming2&quot;, &quot;gender&quot; : &quot;m&quot; &#125;#指定查询表达式db.collection.find(查询表达式, &#123;列1:1, 列2:1&#125;)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;001&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; db.student.find(&#123;name:&quot;xiaoming3&quot;&#125;);&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;xiaoming3&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&gt; #指定要显示的列(0表示不显示,1表示显示)&gt; db.student.find();&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming1&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;sn&quot; : &quot;003&quot;, &quot;name&quot; : &quot;AAAAAA&quot;, &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xxxxx&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;sn&quot; : &quot;888&quot;, &quot;name&quot; : &quot;xiaoming&quot;, &quot;gender&quot; : &quot;m&quot; &#125;&gt; db.student.find(&#123;&#125;,&#123;gender:1&#125;); #_id属性默认总是显示出来&#123; &quot;_id&quot; : ObjectId(&quot;58a9088db1ba35a73d3b4276&quot;), &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a9089db1ba35a73d3b4277&quot;), &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91258b1ba35a73d3b4279&quot;), &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;_id&quot; : ObjectId(&quot;58a91327b1ba35a73d3b427a&quot;), &quot;gender&quot; : &quot;m&quot; &#125;&gt; db.student.find(&#123;&#125;,&#123;gender:1,_id:0&#125;);&#123; &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;gender&quot; : &quot;f&quot; &#125;&#123; &quot;gender&quot; : &quot;m&quot; &#125;&#123; &quot;gender&quot; : &quot;m&quot; &#125;&gt;","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"mapreduce","date":"2017-04-16T04:47:25.762Z","path":"2017/04/16/nosql/mongodb/mapreduce/","text":"官网https://docs.mongodb.com/manual/reference/method/db.collection.mapReduce/ https://docs.mongodb.com/manual/core/map-reduce/ 1.语法123456789101112131415db.collection.mapReduce( &lt;map&gt;, #map函数 &lt;reduce&gt;, #reduce函数 &#123; out: &lt;collection&gt;, #指定输出结果到那张表上 query: &lt;document&gt;, #在map之前的过滤 sort: &lt;document&gt;, limit: &lt;number&gt;, finalize: &lt;function&gt;, #最终处理函数 scope: &lt;document&gt;, jsMode: &lt;boolean&gt;, verbose: &lt;boolean&gt;, bypassDocumentValidation: &lt;boolean&gt; &#125;) 2.mapreduce的处理过程 3.参数说明3.1.map函数12345678910111213141516171819202122232425262728293031323334function() &#123; ... emit(key, value);&#125;#1.当遍历每一篇文档的时候，都会调用map函数#2.map函数中维护着一个this，他就是指向文档的指针，可以通过this来引用文档的属性#3.在map函数中调用：emit(key,value) ==========》key相当于一个分组，而value相当于一个组中的一个值#4.map函数的输入就是每一篇文档，输出就是一个key-value键值对，一个key对应多个value（即一个数组）#下面是一个订单的一条记录（即文档document）的格式&#123; _id: ObjectId(&quot;50a8240b927d5d8b5891743c&quot;), cust_id: &quot;abc123&quot;, #订单号 ord_date: new Date(&quot;Oct 04, 2012&quot;), #订单日期 status: &apos;A&apos;, #状态 price: 25, #总价格 items: [ &#123; sku: &quot;mmm&quot;, qty: 5, price: 2.5 &#125;, #具体的订单项 &#123; sku: &quot;nnn&quot;, qty: 5, price: 2.5 &#125; ]&#125;#Example_1function() &#123; if (this.status == &apos;A&apos;) #在遍历所有文档（记录）的时候，都会调用一次map函数，this.status就是代表的一个文档对象的属性 emit(this.cust_id, 1); #如果条件成立就向组中添加一个1（即统计一个组中的数量）&#125;#Example_2（统计某种商品的购买次数）function() &#123; this.items.forEach(function(item)&#123; emit(item.sku, 1); &#125;);&#125; 3.2.reduce函数12345#格式function(key, values) &#123; ... return result; #合并每一个key对应的分组&#125; 3.3.finalize 最后处理函数12345#格式function(key, reducedValue) &#123; &apos;输入是reduce处理之后的分组的key，和reduce数组之后的value&apos; ... return modifiedObject; &apos;最后的输出结果&apos;&#125; 3.4.选项out1234567891011121314151617181920#格式1（字符串）out: &lt;collectionName&gt;#格式2（文档对象）out: &#123; &lt;action&gt;: &lt;collectionName&gt; [, db: &lt;dbName&gt;] [, sharded: &lt;boolean&gt; ] [, nonAtomic: &lt;boolean&gt; ] &#125;&lt;action&gt;: Specify one of the following actions:replace &apos;替换已经存在的&apos; Replace the contents of the &lt;collectionName&gt; if the collection with the &lt;collectionName&gt; exists.merge &apos;合并&apos; Merge the new result with the existing result if the output collection already exists. If an existing document has the same key as the new result, overwrite that existing document.reducedb：&apos;指定存入哪个库中，默认是当前库&apos;sharded:&apos;是否分片&apos; 3.5.选项query 在map之前的过滤 4.实例1123456789101112131415161718192021222324252627282930313233343536#orders表的一条记录的格式（document）&#123; _id: ObjectId(&quot;50a8240b927d5d8b5891743c&quot;), cust_id: &quot;abc123&quot;, ord_date: new Date(&quot;Oct 04, 2012&quot;), status: &apos;A&apos;, price: 25, items: [ &#123; sku: &quot;mmm&quot;, qty: 5, price: 2.5 &#125;, &#123; sku: &quot;nnn&quot;, qty: 5, price: 2.5 &#125; ]&#125;#map函数var mapFunction1 = function() &#123; emit(this.cust_id, this.price); #以cust_id为key进行分组 &#125;;#reduce函数var reduceFunction1 = function(keyCustId, valuesPrices) &#123; #计算每组key对应的总的price return Array.sum(valuesPrices); &#125;;#db.orders.mapReducedb.orders.mapReduce( mapFunction1, reduceFunction1, &#123; out: &quot;map_reduce_example&quot; &#125; #对应的结果输出到map_reduce_example表中 )#结果&gt; db.map_reduce_example.find();&#123; &quot;_id&quot; : &quot;abc123&quot;, &quot;value&quot; : 25 &#125;&gt; 5.实例2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#orders表的一条记录的格式（document）&#123; _id: ObjectId(&quot;50a8240b927d5d8b5891743c&quot;), cust_id: &quot;abc123&quot;, ord_date: new Date(&quot;Oct 04, 2012&quot;), status: &apos;A&apos;, price: 25, items: [ &#123; sku: &quot;mmm&quot;, qty: 5, price: 2.5 &#125;, &#123; sku: &quot;nnn&quot;, qty: 5, price: 2.5 &#125; ]&#125; #map函数var mapFunction2 = function() &#123; for (var idx = 0; idx &lt; this.items.length; idx++) &#123; var key = this.items[idx].sku; var value = &#123; count: 1, qty: this.items[idx].qty &#125;; emit(key, value); #这里的key，value都不是文档的属性，而是通过计算得到的，key是商品的名称，value是一个文档 &#125; &#125;;&apos;map函数最后输出的是key，values (是一个文档对象数组)&apos;#reduce函数var reduceFunction2 = function(keySKU, countObjVals) &#123; &apos;输入的是key，values (是一个文档对象数组)&apos; reducedVal = &#123; count: 0, qty: 0 &#125;; for (var idx = 0; idx &lt; countObjVals.length; idx++) &#123; reducedVal.count += countObjVals[idx].count; reducedVal.qty += countObjVals[idx].qty; &#125; return reducedVal; &apos;输出时一个文档对象&apos; &#125;;#最后处理函数var finalizeFunction2 = function (key, reducedVal) &#123; &apos;对reduce函数的输出的文档对象进行最后的处理&apos; reducedVal.avg = reducedVal.qty/reducedVal.count; #给reducedVal文档对象添加了一个avg属性 return reducedVal; #返回文档对象 &#125;;#db.orders.mapReducedb.orders.mapReduce( mapFunction2, reduceFunction2, &#123; out: &#123; merge: &quot;map_reduce_example&quot; &#125;, query: &#123; ord_date: &#123; $gt: new Date(&apos;01/01/2012&apos;) &#125; &#125;, finalize: finalizeFunction2 &#125; )#结果&gt; db.map_reduce_example.find();&#123; &quot;_id&quot; : &quot;mmm&quot;, &quot;value&quot; : &#123; &quot;count&quot; : 1, &quot;qty&quot; : 5, &quot;avg&quot; : 5 &#125; &#125;&#123; &quot;_id&quot; : &quot;nnn&quot;, &quot;value&quot; : &#123; &quot;count&quot; : 1, &quot;qty&quot; : 5, &quot;avg&quot; : 5 &#125; &#125;#如果没有：finalizeFunction2&#123; &quot;_id&quot; : &quot;mmm&quot;, &quot;value&quot; : &#123; &quot;count&quot; : 1, &quot;qty&quot; : 5 &#125; &#125;&#123; &quot;_id&quot; : &quot;nnn&quot;, &quot;value&quot; : &#123; &quot;count&quot; : 1, &quot;qty&quot; : 5 &#125; &#125;","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"Centos 安装MongoDB 详细教程(转).md","date":"2017-04-16T04:47:25.761Z","path":"2017/04/16/nosql/mongodb/Centos 安装MongoDB 详细教程(转)/","text":"1.环境准备1234Centos 6.7MongoDB 3.2.7 #官网: https://www.mongodb.com 2.安装&emsp;这里我们在官网下载源码进行安装. 下载地址: https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.2.7.tgz1234cd /usr/localwget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel62-3.2.7.tgztar -xvf mongodb-linux-x86_64-rhel62-3.2.7.tgzmv mongodb-linux-x86_64-rhel62-3.2.7 mongodb 配置环境变量 12345#vim /etc/profileexport MONGODB_HOME=/usr/local/mongodbexport PATH=$MONGODB_HOME/bin:$PATHsource /etc/profile 查看mongodb版本信息 mongod -v1234567891011121314151617mongod -v 2016-07-09T22:01:18.546+0800 I CONTROL [initandlisten] MongoDB starting : pid=1314 port=27017 dbpath=/data/db 64-bit host=iZ28lgwrrtqZ2016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] db version v3.2.72016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] git version: 4249c1d2b5999ebbf1fdf3bc0e0e3b3ff5c0aaf22016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 20132016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] allocator: tcmalloc2016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] modules: none2016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] build environment:2016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] distmod: rhel622016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] distarch: x86_642016-07-09T22:01:18.553+0800 I CONTROL [initandlisten] target_arch: x86_642016-07-09T22:01:18.555+0800 I CONTROL [initandlisten] options: &#123; systemLog: &#123; verbosity: 1 &#125; &#125;2016-07-09T22:01:18.555+0800 D NETWORK [initandlisten] fd limit hard:65535 soft:65535 max conn: 524282016-07-09T22:01:18.616+0800 D - [initandlisten] User Assertion: 29:Data directory /data/db not found.2016-07-09T22:01:18.635+0800 I STORAGE [initandlisten] exception in initAndListen: 29 Data directory /data/db not found., terminating2016-07-09T22:01:18.635+0800 I CONTROL [initandlisten] dbexit: rc: 100 3.添加配置文件创建数据库目录MongoDB需要自建数据库文件夹. 123mkdir -p /data/mongodbmkdir -p /data/mongodb/logtouch /data/mongodb/log/mongodb.log 添加配置文件新建mongodb.conf配置文件, 通过这个配置文件进行启动. 12345678#vim /etc/mongodb.confdbpath=/data/mongodblogpath=/data/mongodb/log/mongodb.loglogappend=trueport=27017fork=true##auth = true # 先关闭, 创建好用户在启动 配置文件参数说明123456789101112131415mongodb的参数说明：--dbpath 数据库路径(数据文件)--logpath 日志文件路径--master 指定为主机器--slave 指定为从机器--source 指定主机器的IP地址--pologSize 指定日志文件大小不超过64M.因为resync是非常操作量大且耗时，最好通过设置一个足够大的oplogSize来避免resync(默认的 oplog大小是空闲磁盘大小的5%)。--logappend 日志文件末尾添加--port 启用端口号--fork 在后台运行--only 指定只复制哪一个数据库--slavedelay 指从复制检测的时间间隔--auth 是否需要验证权限登录(用户名和密码) 注：mongodb配置文件里面的参数很多，定制特定的需求，请参考官方文档 4.启动通过配置文件启动1234mongod -f /etc/mongodb.confabout to fork child process, waiting until server is ready for connections.forked process: 2814child process started successfully, parent exiting 出现successfully表示启动成功了. 5.客户端测试进入 MongoDB后台管理 Shell123456789101112131415161718cd /usr/local/mongodb/bin./mongo#创建数据库use testswitched to db test#创建用户, 设置权限db.createUser( &#123; user: &quot;test&quot;, pwd: &quot;test&quot;, roles: [ &#123; role: &quot;readWrite&quot;, db: &quot;test&quot; &#125; ] &#125;)#详细权限配置参考网址: [MongoDB 3.0 用户创建](http://www.cnblogs.com/zhoujinyi/p/4610050.html) 配置防火墙将27017端口添加到防火墙中123vi /etc/sysconfig/iptables -A INPUT -m state --state NEW -m tcp -p tcp --dport 27017 -j ACCEPT/etc/init.d/iptables reload 注意 我们创建了用户, 这个时候要开启权限启动, 在配置文件中添加auth=true, 然后重启一下 MongoDB 默认没有用户权限的, 建议大家一定要设置, 这样数据才安全. BIN目录说明12345678910111213bsondump 导出BSON结构mongo 客户端mongod 服务端mongodump 整体数据库二进制导出mongoexport 导出易识别的json文档或csv文档mongorestore 数据库整体导入mongos 路由器(分片用)mongofiles GridFS工具，内建的分布式文件系统mongoimport 数据导入程序mongotop 运维监控mongooplogmongoperfmongostat","tags":[{"name":"mongodb","slug":"mongodb","permalink":"http://yoursite.com/tags/mongodb/"}]},{"title":"永久数据被踢现象","date":"2017-04-16T04:47:25.758Z","path":"2017/04/16/nosql/memcached/永久数据被踢现象/","text":"1.数据永久被踢现象网上有人反馈为”memcached 数据丢失”,明明设为永久有效,却莫名其妙的丢失了.其实,这要从 2 个方面来找原因: 即前面介绍的 惰性删除,与 LRU 最近最少使用记录删除. 2.分析 如果 slab 里的很多 chunk,已经过期,但过期后没有被 get 过, 系统不知他们已经过期 永久数据很久没 get 了,不活跃,如果新增 item，如果永久数据的活跃度比过期的数据低，则永久数据被踢了. 当然,如果那些非永久数据被 get,也会被标识为 expire,从而不会再踢掉永久数据 3.解决方案永久数据和非永久数据分开放 4.举例如果数据过期失效了，但是没有访问的话，是不会知道该数据是失效的，因为memcached的惰性失效原理1234567891011121314151617#举例/*场景：memcached中只能存储3个数据A（最近最少使用） B C（过期失效） #如果此时又添加数据D：会踢掉A，因为A最近最少使用#如果get(C)此时会清除C，因为惰性失效原理，此时C的存储位置空出#如果get(C)之后，又添加D此时D会占据原来C的位置（因为惰性失效，C已经被清除）*/ 5.stats查看被踢数据的数量123456stats.............STAT evictions 0 #被踢数据的数量.................","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"一致性哈希算法及其在分布式系统中的应用(转)","date":"2017-04-16T04:47:25.756Z","path":"2017/04/16/nosql/memcached/一致性哈希算法及其在分布式系统中的应用(转)/","text":"1.摘要&emsp;本文将会从实际应用场景出发，介绍一致性哈希算法（Consistent Hashing）及其在分布式系统中的应用。首先本文会描述一个在日常开发中经常会遇到的问题场景，借此介绍一致性哈希算法以及这个算法如何解决此问题；接下来会对这个算法进行相对详细的描述，并讨论一些如虚拟节点等与此算法应用相关的话题。 2.分布式缓存问题&emsp;假设我们有一个网站，最近发现随着流量增加，服务器压力越来越大，之前直接读写数据库的方式不太给力了，于是我们想引入Memcached作为缓存机制。现在我们一共有三台机器可以作为Memcached服务器，如下图所示。 &emsp;很显然，最简单的策略是将每一次Memcached请求随机发送到一台Memcached服务器，但是这种策略可能会带来两个问题：一是同一份数据可能被存在不同的机器上而造成数据冗余，二是有可能某数据已经被缓存但是访问却没有命中，因为无法保证对相同key的所有访问都被发送到相同的服务器。因此，随机策略无论是时间效率还是空间效率都非常不好 &emsp;要解决上述问题只需做到如下一点：保证对相同key的访问会被发送到相同的服务器。很多方法可以实现这一点，最常用的方法是计算哈希。例如对于每次访问，可以按如下算法计算其哈希值：h = Hash(key) % 3 &emsp;其中Hash是一个从字符串到正整数的哈希映射函数。这样，如果我们将Memcached Server分别编号为0、1、2，那么就可以根据上式和key计算出服务器编号h，然后去访问。&emsp;这个方法虽然解决了上面提到的两个问题，但是存在一些其它的问题。如果将上述方法抽象，可以认为通过：h = Hash(key) % N&emsp;这个算式计算每个key的请求应该被发送到哪台服务器，其中N为服务器的台数，并且服务器按照0 – (N-1)编号。&emsp;这个算法的问题在于容错性和扩展性不好。所谓容错性是指当系统中某一个或几个服务器变得不可用时，整个系统是否可以正确高效运行；而扩展性是指当加入新的服务器后，整个系统是否可以正确高效运行。&emsp;现假设有一台服务器宕机了，那么为了填补空缺，要将宕机的服务器从编号列表中移除，后面的服务器按顺序前移一位并将其编号值减一，此时每个key就要按h = Hash(key) % (N-1)重新计算；同样，如果新增了一台服务器，虽然原有服务器编号不用改变，但是要按h = Hash(key) % (N+1)重新计算哈希值。因此系统中一旦有服务器变更，大量的key会被重定位到不同的服务器从而造成大量的缓存不命中。而这种情况在分布式系统中是非常糟糕的。 &emsp;一个设计良好的分布式哈希方案应该具有良好的单调性，即服务节点的增减不会造成大量哈希重定位。一致性哈希算法就是这样一种哈希方案。 3.一致性哈希算法3.1.算法简述&emsp;一致性哈希算法（Consistent Hashing）最早在论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》中被提出。简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0 - 2的32次方-1（即哈希值是一个32位无符号整形），整个哈希空间环如下： 整个空间按顺时针方向组织。0和2的32次方-1在零点中方向重合。 &emsp;下一步将各个服务器使用H进行一个哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中三台服务器使用ip地址哈希后在环空间的位置如下： &emsp;接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数H计算出哈希值h，通根据h确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。&emsp;例如我们有A、B、C、D四个数据对象，经过哈希计算后，在环空间上的位置如下： 根据一致性哈希算法，数据A会被定为到Server 1上，D被定为到Server 3上，而B、C分别被定为到Server 2上。 3.2.容错性与可扩展性分析下面分析一致性哈希算法的容错性和可扩展性。现假设Server 3宕机了： &emsp;可以看到此时A、C、B不会受到影响，只有D节点被重定位到Server 2。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。&emsp;下面考虑另外一种情况，如果我们在系统中增加一台服务器Memcached Server 4： &emsp;此时A、D、C不受影响，只有B需要重定位到新的Server 4。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即顺着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。 综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。 3.3.虚拟节点&emsp;一致性哈希算法在服务节点太少时，容易因为节点分部不均匀而造成数据倾斜问题。例如我们的系统中有两台服务器，其环分布如下： &emsp;此时必然造成大量数据集中到Server 1上，而只有极少量会定位到Server 2上。为了解决这种数据倾斜问题，一致性哈希算法引入了虚拟节点机制，即对每一个服务节点计算多个哈希，每个计算结果位置都放置一个此服务节点，称为虚拟节点。具体做法可以在服务器ip或主机名的后面增加编号来实现。例如上面的情况，我们决定为每台服务器计算三个虚拟节点，于是可以分别计算“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”、“Memcached Server 2#1”、“Memcached Server 2#2”、“Memcached Server 2#3”的哈希值，于是形成六个虚拟节点： &emsp;同时数据定位算法不变，只是多了一步虚拟节点到实际节点的映射，例如定位到“Memcached Server 1#1”、“Memcached Server 1#2”、“Memcached Server 1#3”三个虚拟节点的数据均定位到Server 1上。这样就解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为32甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。 4.总结&emsp;目前一致性哈希基本成为了分布式系统组件的标准配置，例如Memcached的各种客户端都提供内置的一致性哈希支持。本文只是简要介绍了这个算法，更深入的内容可以参看论文《Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web》，同时提供一个C语言版本的实现供参考。 转自","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"Windows下安装memcached","date":"2017-04-16T04:47:25.754Z","path":"2017/04/16/nosql/memcached/Windows下安装memcached/","text":"1.windows下安装只需要将memcached.exe 放在相应的目录下，然后在cmd下，进入目录，执行： 1.1.查看帮助文档12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455D:\\installed_soft\\memcached&gt;memcached.exe -h D:\\installed_soft\\memcached&gt;memcached.exe -hmemcached 1.4.4-14-g9c660c0-p &lt;num&gt; TCP port number to listen on (default: 11211) #监听的端口-U &lt;num&gt; UDP port number to listen on (default: 11211, 0 is off)-s &lt;file&gt; UNIX socket path to listen on (disables network support)-a &lt;mask&gt; access mask for UNIX socket, in octal (default: 0700)-l &lt;ip_addr&gt; interface to listen on (default: INADDR_ANY, all addresses)-s &lt;file&gt; unix socket path to listen on (disables network support)-a &lt;mask&gt; access mask for unix socket, in octal (default 0700)-l &lt;ip_addr&gt; interface to listen on, default is INADDR_ANY-d start tell memcached to start #启动-d restart tell running memcached to do a graceful restart-d stop|shutdown tell running memcached to shutdown-d install install memcached service-d uninstall uninstall memcached service-r maximize core file limit-u &lt;username&gt; assume identity of &lt;username&gt; (only when run as root) #指定用户-m &lt;num&gt; max memory to use for items in megabytes (default: 64 MB) #指定内存-M return error on memory exhausted (rather than removing items)-c &lt;num&gt; max simultaneous connections (default: 1024)-k lock down all paged memory. Note that there is a limit on how much memory you may lock. Trying to allocate more than that would fail, so be sure you set the limit correctly for the user you started the daemon with (not for -u &lt;username&gt; user; under sh this is done with &apos;ulimit -S -l NUM_KB&apos;).-v verbose (print errors/warnings while in event loop)-vv very verbose (also print client commands/reponses)-vvv extremely verbose (also print internal state transitions) #指定打印详细信息-h print this help and exit-i print memcached and libevent license-P &lt;file&gt; save PID in &lt;file&gt;, only used with -d option-f &lt;factor&gt; chunk size growth factor (default: 1.25) #指定增长因子-n &lt;bytes&gt; minimum space allocated for key+value+flags (default: 48)-L Try to use large memory pages (if available). Increasing the memory page size could reduce the number of TLB misses and improve the performance. In order to get large pages from the OS, memcached will allocate the total item-cache in one large chunk.-D &lt;char&gt; Use &lt;char&gt; as the delimiter between key prefixes and IDs. This is used for per-prefix stats reporting. The default is &quot;:&quot; (colon). If this option is specified, stats collection is turned on automatically; if not, then it may be turned on by sending the &quot;stats detail on&quot; command to the server.-t &lt;num&gt; number of threads to use (default: 4)-R Maximum number of requests per event, limits the number of requests process for a given connection to prevent starvation (default: 20)-C Disable use of CAS-b Set the backlog queue limit (default: 1024)-B Binding protocol - one of ascii, binary, or auto (default)-I Override the size of each slab page. Adjusts max item size (default: 1mb, min: 1k, max: 128m) 1.2.启动1D:\\installed_soft\\memcached&gt;memcached.exe -m 64 -p 11211 -vv #指定启动时监听的端口，开64M内存，打印详细信息（vvv ) 1.3.测试1234567#另外开启一个cmd窗口，使用Telnet去连接memcached：C:\\Users\\Administrator&gt;telnet 127.0.0.1 11211 #添加一条数据add new 0 0 8zhangsan #值STORED #表示添加成功","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"MemCache访问模型（转）","date":"2017-04-16T04:47:25.752Z","path":"2017/04/16/nosql/memcached/MemCache访问模型（转）/","text":"MemCache访问模型 &emsp;为了加深理解，我模仿着原阿里技术专家李智慧老师《大型网站技术架构 核心原理与案例分析》一书MemCache部分，自己画了一张图： &emsp;特别澄清一个问题，MemCache虽然被称为”分布式缓存”，但是MemCache本身完全不具备分布式的功能，MemCache集群之间不会相互通信（与之形成对比的，比如JBoss Cache，某台服务器有缓存数据更新时，会通知集群中其他机器更新缓存或清除缓存数据），所谓的”分布式”，完全依赖于客户端程序的实现，就像上面这张图的流程一样。 &emsp;同时基于这张图，理一下MemCache一次写缓存的流程： 应用程序输入需要写缓存的数据 API将Key输入路由算法模块，路由算法根据Key和MemCache集群服务器列表得到一台服务器编号 由服务器编号得到MemCache及其的ip地址和端口号 API调用通信模块和指定编号的服务器通信，将数据写入该服务器，完成一次分布式缓存的写操作 &emsp;读缓存和写缓存一样，只要使用相同的路由算法和服务器列表，只要应用程序查询的是相同的Key，MemCache客户端总是访问相同的客户端去读取数据，只要服务器中还缓存着该数据，就能保证缓存命中。 &emsp;这种MemCache集群的方式也是从分区容错性的方面考虑的，假如Node2宕机了，那么Node2上面存储的数据都不可用了，此时由于集群中Node0和Node1还存在，下一次请求Node2中存储的Key值的时候，肯定是没有命中的，这时先从数据库中拿到要缓存的数据，然后路由算法模块根据Key值在Node0和Node1中选取一个节点，把对应的数据放进去，这样下一次就又可以走缓存了，这种集群的做法很好，但是缺点是成本比较大。 转自","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"Memcached雪崩和穿透","date":"2017-04-16T04:47:25.750Z","path":"2017/04/16/nosql/memcached/Memcached雪崩和穿透/","text":"1.缓存穿透1.1.什么是缓存穿透？&emsp;一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。如果key对应的value是一定不存在的，并且对该key并发请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。 1.2.如何避免？ 对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert了之后清理缓存。 对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该bitmap过滤。【感觉应该用的不多吧】 2.缓存雪崩2.1.什么是缓存雪崩？&emsp;当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。 2.2.如何避免？ 在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"memcached缓存的无底洞现象","date":"2017-04-16T04:47:25.749Z","path":"2017/04/16/nosql/memcached/memcached缓存的无底洞现象/","text":"1.Memcached 缓存无底洞现象&emsp;facebook的工作人员反应的，facebook在2010年左右,memcached节点就已经达到了3000个,存储的数据进千G的数据存储。&emsp;他们发现一个问题，memecached连接频率，效率都下降了，于是加了memcached节点.添加节点后发现因为连接频率导致的问题并没有好转。称之为“无底洞现象”。 2.问题分析：以用户为例：user-133-age, usr-133-name, usr-133-height ….共有N个KEY。&emsp;当服务器增多，133号的信息也被散落在更多的节点上。所以同样访问个人主页，要得到的个人主页信息,则节点越多，要连接的节点也越多。对于memcached的连接数，并没有随着节点的增多，而降低。导致问题出现…。&emsp;事实上：NoSQL和传统的RDBMS，并不是水火不容，而两者在某些设计上,是可以互相参考的,对于memcached,redis这种key value存储，key的设计可以参考MySql中表/列的设计，比如user表下，有age列，name列,身高列,对应的key,可以用user:1333:age=23,user:133:name=’lisi’,suer:133:height=’168’ 3.问题的解决方案：&emsp;把某一组key，按照公同的前缀,来分布，比如 user-133-age,user-133-name,user-133-height这个3个key，在用分布式算法求其节点时,应该可以用user-133来计算而不是以 user-133-nage来计算,这样3个关于个人的信息的key都落到了同一个节点上，点击的访问个人主页的时,值需要连接1个节点。","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"memcached的内存分配机制与惰性失效机制","date":"2017-04-16T04:47:25.747Z","path":"2017/04/16/nosql/memcached/memcached的内存分配机制与惰性失效机制/","text":"1.内存的碎片化如果用C语言直接 malloc free来向操作系统申请和释放内存时，在不断的申请和释放过程中，形成了一些很小的内存片段，无法再利用，这就是内存碎片 2.memcached的内存分配机制2.1.Slab Allocator分配机制memcached用slab allocator来管理内存的， 其基本原理：把内存划分成数个slab仓库，各个仓库切分不同的尺寸的小块（chunk)，需要存储内容时，判断内存的大小，为其选取合理的仓库 ，但是如果有100byte的内容要存储，但距离100byte最近的122大小的仓库中你的chunk满了，此时并不会寻找更大的chunk，而是将122中不常用的数据踢掉 2.2.Slab Allocator缓存原理 memcached根据收到的数据的大小， 选择最适合数据大小的slab。memcached中保存着slab内空闲chunk的列表， 根据该列表选择chunk， 然后将数据缓存于其中。 2.3.lab Allocator的缺点chunks为固定大小,造成浪费. 这个问题 不能克服,只能缓解 2.4.grow factor(增长因子)memcached在启动时指定Growth Factor因子（通过­f选项）， 就可以在某种程度上控制slab之间的差异。默认值为1.25。 但是，在该选项出现之前，这个因子曾经固定为2，称为“powers of 2”策略1#如下图：112/88=1.25 也就是增长因子，一般而言，观察缓存数据大小的变化规律，设置合理的生长因子，可以根据自己网站的缓存数据的大小来调整增长因子 3.惰性失效机制（Lazy Expiration）memcached内部不会监视记录是否过期，而是在get时会查看记录的时间戳（和设定的时间比较），检查记录是否过期。这种技术被称为lazy（惰性）expiration。 因此，memcached不会在过期监视上耗费CPU时间 当某个值过期后，并没有从内存删除，因此stats统计的时候，curr_item有其信息 当某个新值去占用他的位置的时候，当成空chunk来占用 当get值时，判断是否过期，如果过期返回空，并且清空，此时在stats，curr_item就减少了 即：这个过期只是让用户看不到这个数据而已并没有真正删除数据 为什么说节省了CPU？实际上设置5s之后失效，如果是主动的使数据失效的话，就会在5s的时候去主动的触发数据失效，这样系统本身还要去检查，5s到了吗，哦，没到，那等一下再去检查，系统会一直这样检查，这样就会消耗CPU的时间，但是惰性机制就是，在get的时候，直接取检查，当前时间是否和数据建立的时间之间的时间差，然后，超过了我设定的过期时间间隔，此时才正式的将数据删除，新增数据的时候也是这样去检查， 4.memcached的删除机制Least Recently Used（LRU）——最近最少使用，memcached会优先使用已超时的记录的空间，但即使如此，也会发生追加新记录时空间不 足的情况，此时就要使用名为 Least Recently Used（LRU）机制来分配空间如果chunk满了，又有新的加入，旧数据会被踢掉。踢掉可以设置两种FIFO（先进先出）、LRU(最近最少使用)。下面是一个LRU的demo: 注意：get、inrc、decr等可以使数据变成最近使用","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"Memcached参数含义及CRUD、自增、stats、flush_all等语句","date":"2017-04-16T04:47:25.745Z","path":"2017/04/16/nosql/memcached/Memcached参数含义及CRUD、自增、stats、flush_all等语句/","text":"1.参数含义1add &lt;key&gt; &lt;flag&gt; &lt;expires&gt; &lt;byte&gt; key 每个缓存有一个独特的名字和存储空间. key是操作数据的唯一标识，key可以250个字节以内,(不能有空格和控制字符) 注:在新版开发计划中提到key可能会扩充到65535个字节 flag 用来表示存入的是字符串、数组、还是对象等 expires 缓存的有效期，一种方式是秒数，另一种是使用Unix时间戳，0为有效期无限 byte 保留值的字节数 2.参数expires 举例1234567891011121314151617#设置秒数，从设定开始数第N秒后失效set web 0 10 5 #设置key=web，然后10s之后数据失效，存储数据的大小为5个字节zixue#时间戳，到指定的时间戳后失效，比如在团购网站缓存的某团到中午12:00失效set web2 0 1379209940 5 #1379209940 就是一个时间戳（就是某一个时间点）abcde#设置为0，表示不自动失效set web3 0 0 5abcde##有种误会，设为0，永久有效，是错误的#1.编译memcached时，指定一个最长的有效期，默认是30天，所以即使设置为0,30天后也会失效#2.可能等不到30天，就会被新数据挤出去#3.memcached重启） 3.增删改查语句12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#增加（add)add class 00 0 4erqiSTOREDadd class 0 0 4eri9NOT_STORED #为何会添加失败？因为在add只能是新增一个内存中没有的key，如果想要重复的对一个key进行操作，只能用replace#删除（delete）add class 00 0 4erqiSTOREDdelete classDELETED#delete key [time seconds] #加秒之后，是指被删除的key，N秒内不能再用，目的是让网站上的页面缓存也代谢完毕#替换（replace改）get classVALUE class 0 4eqi3ENDreplace class 0 0 4 #重新设置eri8STOREDget classVALUE class 0 4eri8END#replace只能修改存在的keyget abdENDreplace abd 0 0 4 #因为abd是不存在的，所以用replace是不能修改的eqi9NOT_STORED #查找（get）get classVALUE class 0 4eri8END#set#其实set是add和replace的结合，如果有对应的key就修改，如果没有对应的key就添加set aa 0 0 4 #第一次seteqi5STOREDget aaVALUE aa 0 4eqi5ENDset aa 0 0 4 #对同样的key，第二次seteqi0STOREDget aaVALUE aa 0 4eqi0END 4.自增、自减操作1incr/decr key num 123456789101112131415161718192021222324252627282930313233343536373839404142434445set age 00 0 224STOREDincr age 1 #增加125get ageVALUE age 0 225ENDdecr age 1 #减少124get ageVALUE age 0 224END#注意：incre/decr操作是将值当做32位无符号来操作的，值的范围：【0 - 2的32次-1】set num 0 0 12STOREDdecr num 11decr num 1 #因为是无符号的，所以一直减下去，还会是00decr num 10decr num 10get numVALUE num 0 10END/**应用场景：秒杀功能每个人的抢单主要在内存中操作，速度非常的快，这里的秒杀只是每人发一个订单号，标明你能够购买了，下面就是自己去购买页面下单即可，而秒杀只是负责前面的抢单部分*/ 5.统计命令12345678910111213141516171819202122232425262728293031323334353637statsSTAT pid 8816 #进程号 STAT uptime 3054547022 #服务器自运行以来的秒数STAT time 234797177 #当前服务器上的UNIX时间STAT version 1.4.4-14-g9c660c0 #服务器的版本字符串STAT pointer_size 64STAT curr_connections 10 #当前存在的连接数STAT total_connections 11 #历史上的总的连接数STAT connection_structures 11STAT cmd_get 21 #get了多少次STAT cmd_set 14STAT cmd_flush 0STAT get_hits 12 #get成功返回(关键字获取命中的次数)STAT get_misses 9 #get没有取到值 ，这里可以计算出缓存命中率STAT delete_misses 0STAT delete_hits 1STAT incr_misses 0STAT incr_hits 1STAT decr_misses 1STAT decr_hits 5STAT cas_misses 0STAT cas_hits 0STAT cas_badval 0STAT auth_cmds 0STAT auth_errors 0STAT bytes_read 702STAT bytes_written 669STAT limit_maxbytes 20971520STAT accepting_conns 1STAT listen_disabled_num 0STAT threads 4STAT conn_yields 0STAT bytes 437STAT curr_items 6 #当前存在的key的数量STAT total_items 9 #历史上存在的总的数量STAT evictions 0END 6.flush_all 将删除所有的数据，但是这里是惰性删除的全删: flush_all [time] time参数是指是所有缓存失效,并在time秒内限制使用删除的key","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"memcached内存实现原理（转）","date":"2017-04-16T04:47:25.743Z","path":"2017/04/16/nosql/memcached/memcached内存实现原理（转）/","text":"首先要说明一点，MemCache的数据存放在内存中，存放在内存中个人认为意味着几点： 访问数据的速度比传统的关系型数据库要快，因为Oracle、MySQL这些传统的关系型数据库为了保持数据的持久性，数据存放在硬盘中，IO操作速度慢 MemCache的数据存放在内存中同时意味着只要MemCache重启了，数据就会消失 既然MemCache的数据存放在内存中，那么势必受到机器位数的限制，这个之前的文章写过很多次了，32位机器最多只能使用2GB的内存空间，64位机器可以认为没有上限, 然后我们来看一下MemCache的原理，MemCache最重要的莫不是内存分配的内容了，MemCache采用的内存分配方式是固定空间分配，还是自己画一张图说明： 这张图片里面涉及了slab_class、slab、page、chunk四个概念，它们之间的关系是： MemCache将内存空间分为一组slab 每个slab下又有若干个page，每个page默认是1M，如果一个slab占用100M内存的话，那么这个slab下应该有100个page 每个page里面包含一组chunk，chunk是真正存放数据的地方，同一个slab里面的chunk的大小是固定的 有相同大小chunk的slab被组织在一起，称为slab_class &emsp;MemCache内存分配的方式称为allocator，slab的数量是有限的，几个、十几个或者几十个，这个和启动参数的配置相关。 &emsp;MemCache中的value过来存放的地方是由value的大小决定的，value总是会被存放到与chunk大小最接近的一个slab中，比如slab[1]的chunk大小为80字节、slab[2]的chunk大小为100字节、slab[3]的chunk大小为128字节（相邻slab内的chunk基本以1.25为比例进行增长，MemCache启动时可以用-f指定这个比例），那么过来一个88字节的value，这个value将被放到2号slab中。 &emsp;放slab的时候，首先slab要申请内存，申请内存是以page为单位的，所以在放入第一个数据的时候，无论大小为多少，都会有1M大小的page被分配给该slab。申请到page后，slab会将这个page的内存按chunk的大小进行切分，这样就变成了一个chunk数组，最后从这个chunk数组中选择一个用于存储数据。 &emsp;如果这个slab中没有chunk可以分配了怎么办，如果MemCache启动没有追加-M（禁止LRU，这种情况下内存不够会报Out Of Memory错误），那么MemCache会把这个slab中最近最少使用的chunk中的数据清理掉，然后放上最新的数据。针对MemCache的内存分配及回收算法，总结三点： MemCache的内存分配chunk里面会有内存浪费，88字节的value分配在128字节（紧接着大的用）的chunk中，就损失了30字节，但是这也避免了管理内存碎片的问题 MemCache的LRU算法不是针对全局的，是针对slab的 应该可以理解为什么MemCache存放的value大小是限制的，因为一个新数据过来，slab会先以page为单位申请一块内存，申请的内存最多就只有1M，所以value大小自然不能大于1M了","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"Memcached中一些参数的限制","date":"2017-04-16T04:47:25.741Z","path":"2017/04/16/nosql/memcached/Memcached中一些参数的限制/","text":"1.Key的长度1234#语法: add key flag expire length#key 250个字节 2.Value的限制1M一般都是存储一些文本，1M够了 3.内存限制如果有30G的数据要缓存，一般也不会单实例装30G（不要把鸡蛋放在一个篮子里），一般建议，开启多个实例（可以在不同的机器或同一台机器上的不同端口中启多个memcached） 4.启动多个实例12345678910#可以启动多个实例（指定不同的端口即可）[root@originalOS memcached]# ./bin/memcached -p 11212 -u nobody -d[root@originalOS memcached]# ./bin/memcached -p 11213 -u nobody -d#查看启动的实例[root@originalOS memcached]# ps -ef|grep memcached nobody 8695 1 0 19:19 ? 00:00:00 ./bin/memcached -m 64 -p 11211 -u nobody -dnobody 8711 1 1 19:29 ? 00:00:00 ./bin/memcached -p 11212 -u nobody -dnobody 8721 1 1 19:29 ? 00:00:00 ./bin/memcached -p 11213 -u nobody -droot 8729 1022 3 19:29 pts/0 00:00:00 grep memcached","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"Linux下编译安装memcached","date":"2017-04-16T04:47:25.740Z","path":"2017/04/16/nosql/memcached/Linux下编译安装memcached/","text":"1.准备编译环境1yum install gcc make cmake autoconf libtool #gcc make 都是编译器 2.安装依赖：libevent123456去http://libevent.org/ 下载稳定版本tar zxvf libevent-2.0.21-stable.tar.gzcd libevent-2.0.21-stable./configure --prefix=/usr/local/libevent #指定libevent的安装位置make &amp;&amp; make install 3.编译memcached123cd memcached-1.4.5./configure--prefix=/usr/local/memcached --with-libevent=/usr/local/libevent #指定上面安装libevent的安装位置，因为memcached依赖libeventmake &amp;&amp; make install 4.启动，测试12345678910111213141516171819#查看帮助./bin/memcached -h#启动（不指定用户）不允许使用root去启动，因为一般不用root，安全问题[root@originalOS memcached]# ./bin/memcached -m 64 -p 11211 -vvcan\\&apos;t run as root without the -u switch[root@originalOS memcached]##启动（指定用户：nobody用户）[root@originalOS memcached]# ./bin/memcached -m 64 -u nobody -p 11211 -vv #-vv表示前台直接启动，会打印详细信息#以守护进程的方式运行(-d)[root@originalOS memcached]# ./bin/memcached -m 64 -p 11211 -u nobody -d[root@originalOS memcached]# ps -ef|grep memcachednobody 8695 1 3 19:19 ? 00:00:00 ./bin/memcached -m 64 -p 11211 -u nobody -d 5.编译出现问题，解决注意: 在虚拟机下练习编译,一个容易碰到的问题—虚拟机的时间不对,导致的 gcc 编译过程中,检测时间通不过,一直处于编译过程，错误的原因在于系统时间比文件修改时间早（可能是虚拟机长时间没有用了，没有设置时间同步）12# date -s &apos;yyyy-mm-dd hh:mm:ss&apos;# clock -w # 把时间写入 cmos 6.启动多个实例12345678910#可以启动多个实例（指定不同的端口即可）[root@originalOS memcached]# ./bin/memcached -p 11212 -u nobody -d[root@originalOS memcached]# ./bin/memcached -p 11213 -u nobody -d #查看启动的实例[root@originalOS memcached]# ps -ef|grep memcached nobody 8695 1 0 19:19 ? 00:00:00 ./bin/memcached -m 64 -p 11211 -u nobody -dnobody 8711 1 1 19:29 ? 00:00:00 ./bin/memcached -p 11212 -u nobody -dnobody 8721 1 1 19:29 ? 00:00:00 ./bin/memcached -p 11213 -u nobody -droot 8729 1022 3 19:29 pts/0 00:00:00 grep memcached","tags":[{"name":"memcached","slug":"memcached","permalink":"http://yoursite.com/tags/memcached/"}]},{"title":"配置mysql从库级联复制和主主复制(转)","date":"2017-04-16T04:47:25.734Z","path":"2017/04/16/mysql/配置mysql从库级联复制和主主复制(转)/","text":"1.配置mysql从库级联复制环境是：3306主库 3307从库 3308从库由于已经做了主库3306到从库3307，所以现在我们要实现的需求是，当主库3306产生bin_log，发给从库，从库3307产生的bin_log文件发送给其他从库3308。相当于下图中第三个图 开启从库3307的log-bin日志文件 1sed -i &apos;s@#log-bin = /data/3307/mysql-bin@log-bin = /data/3307/mysql-bin@g&apos; /data/3307/my.cnf 在3307从库配置文件my.cnf，[mysqld]模块添加 如下内容 123log-bin = /data/3307/mysql-binlog-slave-updates = 1expire_logs_days = 7 重启数据库33071234[root@lixiang data]# /data/3307/mysql stop Stoping MySQL...[root@lixiang data]# /data/3307/mysql startStarting MySQL... 如果现下面的错误的时候1234[root@lixiang data]# /data/3307/mysql stopStoping MySQL.../application/mysql/bin/mysqladmin: connect to server at &apos;localhost&apos; failederror: &apos;Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: YES)&apos; 那是因为我们在做单台主从复制的时候，是将主服务器整个包导入到从库3307的，所以修改从库3307的启动文件mysqld1sed -i &apos;s/lx3307/lx3306/g&apos; /data/3307/mysql 查看log_slave_updates状态是否开启 1234567mysql&gt; show variables like &quot;log_slave_updates&quot;;+-------------------+-------+| Variable_name | Value |+-------------------+-------+| log_slave_updates | ON |+-------------------+-------+1 row in set (0.00 sec) 通过mysqldump导出从库3307数据文件 1234mysqldump -uroot -plx3306 -S /data/3307/mysql.sock -A --events -B -F -x --master-data=1|gzip &gt; /opt/lx.sql.gz #--master-data=1,表示在lx.sql文件中将取消注释“CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=107;”[root@lixiang opt]# cat lx.sql |grep &quot;mysql-bin.000003&quot;CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, MASTER_LOG_POS=107; 解压数据库，并导入从库3308 123cd /opt/3308gzip -d lx.sql.gzmysql -uroot -plx3308 -S /data/3308/mysql.sock &lt;lx.sql 登录从数据库3308 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869mysql -uroot -plx3308 -S /data/3308/mysql.sockmysql&gt; CHANGE MASTER TO MASTER_HOST=&apos;192.168.10.102&apos;, MASTER_PORT=3307, MASTER_USER=&apos;rep&apos;, MASTER_PASSWORD=&apos;lx123&apos;;#此时就不用指定binlog的日志文件和pos,因为在mysqldump的时候,指定了--master-data=1mysql&gt; start slave; #开启从库3307到从库3308同步开关mysql&gt; show slave status\\G; #查看从库3308状态*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.10.102 Master_User: rep Master_Port: 3307 Connect_Retry: 60 Master_Log_File: mysql-bin.000003 Read_Master_Log_Pos: 107 Relay_Log_File: relay-bin.000005 Relay_Log_Pos: 253 Relay_Master_Log_File: mysql-bin.000003 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: mysql Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 107 Relay_Log_Space: 446 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 31 row in set (0.00 sec)ERROR:No query specifiedmysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || lx || mysql || performance_schema || test |+--------------------+5 rows in set (0.00 sec) #使用此种方法也能快速查看从库mysql的状态[root@lixiang data]# mysql -uroot -p&apos;lx3306&apos; -S /data/3307/mysql.sock -e &quot;show slave status\\G;&quot;|egrep -i &quot;_running|_Behind&quot; Slave_IO_Running: Yes Slave_SQL_Running: Yes Seconds_Behind_Master: 0 这个时候我们看见从库3308已经多了一个数据库名为 “lx”的数据库，出现了这个则表示创建成功 登录主库3306，删除测试数据库 12345678910111213[root@lixiang opt]# mysql -uroot -plx3306 -S /data/3306/mysql.sockmysql&gt; drop database test; #删除主库名称为 &quot;test&quot;数据库Query OK, 0 rows affected (0.13 sec)mysql&gt; show databases; #查看数据库+--------------------+| Database |+--------------------+| information_schema || lx || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec) 分别登录从库3306和从库3307查看 12345678910111213141516171819202122[root@lixiang opt]# mysql -uroot -plx3307 -S /data/3307/mysql.sockmysql&gt; show databases; #查看数据库+--------------------+| Database |+--------------------+| information_schema || lx || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec)[root@lixiang opt]# mysql -uroot -plx3308 -S /data/3308/mysql.sockmysql&gt; show databases; #查看数据库+--------------------+| Database |+--------------------+| information_schema || lx || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec) 发现test数据库都被删除了，至此mysql级联复制配置完毕 2.mysql主主复制 应用场景：高并发场景，使用双主双写，慎用！ 注意： ID会冲突 解决 ID 冲突问题方法一： 表的id自增，让主A 写1，3，5；主B 写2，4，6；方法二：表的id不自增，通过web端程序去seq取id，写入双主 环境：主库3306 ，从库 3307由于我们已经做了主库3306到从库3307，现在我们需要将从库3307变为主库，将3306作为从库 编辑数据库配置文件 12345678910111213141516171819202122232425262728293031323334353637383940414243##3306[root@lixiang 3306]# cd /data/3306[root@lixiang 3306]# vim my.cnf #……省略……[mysqld] # 以下内容加在[mysqld]下面#________m-m m1 start________auto_increment_increment = 2 #自增ID的间隔auto_increment_offset = 1 #ID的初始位置log-slave-updates = 1log-bin = /data/3306/mysql-binexpire_logs_days = 7#________m-m m1 end________……省略……#重启mysql[root@lixiang 3306]# ./mysql stopStoping MySQL...[root@lixiang 3306]# ./mysql startStarting MySQL...##3307[root@lixiang 3306]# cd /data/3307[root@lixiang 3307]# vim my.cnf ……省略……[mysqld] # 以下内容加在[mysqld]下面#________m-m m1 start________auto_increment_increment = 2 #自增ID的间隔auto_increment_offset = 2 #ID的初始位置log-slave-updates = 1log-bin = /data/3307/mysql-binexpire_logs_days = 7#________m-m m1 end________……省略…… #重启mysql[root@lixiang 3307]# ./mysql stopStoping MySQL...[root@lixiang 3307]# ./mysql startStarting MySQL... 导出3307数据库数据 1mysqldump -uroot -plx3306 -S /data/3307/mysql.sock -A --events -B -F -x --master-data=1|gzip &gt; /opt/$(date +%F).sql.gz 解压并将数据导入到3306 12gzip -d 2015-07-27.sql.gzmysql -uroot -plx3306 -S /data/3306/mysql.sock &lt; 2015-07-27.sql 登录主数据库3306 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849mysql -uroot -plx3306 -S /data/3306/mysql.sockmysql&gt; CHANGE MASTER TO MASTER_HOST=&apos;192.168.10.102&apos;, MASTER_USER=&apos;rep&apos;, MASTER_PORT=3307, MASTER_PASSWORD=&apos;lx123&apos;;mysql&gt; start slave;查看从库3306状态mysql&gt; show slave status\\G;*************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.10.102 Master_User: rep Master_Port: 3307 Connect_Retry: 60 Master_Log_File: mysql-bin.000008 Read_Master_Log_Pos: 1561 Relay_Log_File: relay-bin.000011 Relay_Log_Pos: 253 Relay_Master_Log_File: mysql-bin.000008 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: mysql Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 1561 Relay_Log_Space: 446 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 31 row in set (0.00 sec)ERROR:No query specified 在数据库3306创建数据库students 123456789101112131415mysql -uroot -plx3306 -S /data/3306/mysql.sockmysql&gt; create database students;mysql&gt; use students;创建表t1，并插入内容mysql&gt; CREATE TABLE `t1` ( `id` bigint(12) NOT NULL auto_increment, `name` varchar(12) NOT NULL, PRIMARY KEY (`id`) );mysql&gt; insert into t1(name) values(&quot;oldgirl&quot;);mysql&gt; insert into t1(name) values(&quot;oldboy&quot;);mysql&gt; select * from t1; +----+---------+| id | name |+----+---------+| 1 | oldgirl || 3 | oldboy |+----+---------+#结果查看到内容是按照ID号，1 3 ……进行增长 登录到3307数据库 1234567891011121314151617181920212223[root@lixiang opt]# mysql -uroot -plx3306 -S /data/3307/mysql.sockmysql&gt; use students;mysql&gt; select * from t1;+----+---------+| id | name |+----+---------+| 1 | oldgirl || 3 | oldboy |+----+---------+mysql&gt; insert into t1(name) values(&quot;lx&quot;);mysql&gt; insert into t1(name) values(&quot;swj&quot;);mysql&gt; select * from t1; +----+---------+| id | name |+----+---------+| 1 | oldgirl || 3 | oldboy || 4 | lx || 6 | swj |+----+---------+4 rows in set (0.00 sec)#查看到数据库3307的ID是按照偶数进行递增的 主从复制的故障处理&emsp;当从库复制遇到错误时，比如报错“要创建的数据库已存在” 解决方案： 让从库跳过这一步操作，继续执行其它的操作 方法一： 命令行实现，跳过这一步； 123mysql&gt; stop slave;mysql&gt; set global sql_slave_skip_counter =1;mysql&gt; start slave; 方法二： 配置文件中，指定忽略的错误； 12[root@MySQL opt]# grep slave-skip /data/3308/my.cnfslave-skip-errors = 1032,1062 整理自","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"编译安装MySQL-5.5.32","date":"2017-04-16T04:47:25.732Z","path":"2017/04/16/mysql/编译安装MySQL-5.5.32/","text":"卸载掉原有mysql因为mysql数据库在Linux上实在是太流行了，所以目前下载的主流Linux系统版本基本上都集成了mysql数据库在里面，我们可以通过如下命令来查看我们的操作系统上是否已经安装了mysql数据库 1234[root@xiaoluo ~]# rpm -qa | grep mysql // 这个命令就会查看该操作系统上是否已经安装了mysql数据库 [root@xiaoluo ~]# rpm -e mysql // 普通删除模式[root@xiaoluo ~]# rpm -e --nodeps mysql // 强力删除模式，如果使用上面命令删除时，提示有依赖的其它文件，则用该命令可以对其进行强力删除 1.依赖包1.1.ncurses-devel libaio-devel123456yum install ncurses-devel libaio-devel -y//检查[root@MySQL data]# rpm -qa ncurses-devel libaio-develncurses-devel-5.7-4.20090207.el6.i686libaio-devel-0.3.107-10.el6.i686[root@MySQL data]# 1.2.安装cmake123456789cd /home/oldboy/tools/tar zxf cmake-2.8.8.tar.gzcd cmake-2.8.8./configuregmakegmake install//检查[root@MySQL data]# which cmake/usr/local/bin/cmake 2.创建用户和组12 useradd mysql -s /sbin/nologin -Mid mysql 3.解压编译mysql12345678910111213141516171819202122232425262728cd /home/oldboy/tools/tar zxf mysql-5.5.32.tar.gzcd mysql-5.5.32cmake . -DCMAKE_INSTALL_PREFIX=/application/mysql-5.5.32 \\-DMYSQL_DATADIR=/application/mysql-5.5.32/data \\-DMYSQL_UNIX_ADDR=/application/mysql-5.5.32/tmp/mysql.sock \\-DDEFAULT_CHARSET=utf8 \\-DDEFAULT_COLLATION=utf8_general_ci \\-DEXTRA_CHARSETS=gbk,gb2312,utf8,ascii \\-DENABLED_LOCAL_INFILE=ON \\-DWITH_INNOBASE_STORAGE_ENGINE=1 \\-DWITH_FEDERATED_STORAGE_ENGINE=1 \\-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\-DWITHOUT_EXAMPLE_STORAGE_ENGINE=1 \\-DWITHOUT_PARTITION_STORAGE_ENGINE=1 \\-DWITH_FAST_MUTEXES=1 \\-DWITH_ZLIB=bundled \\-DENABLED_LOCAL_INFILE=1 \\-DWITH_READLINE=1 \\-DWITH_EMBEDDED_SERVER=1 \\-DWITH_DEBUG=0#-- Build files have been written to: /home/oldboy/tools/mysql-5.5.32提示，编译时可配置的选项很多，具体可参考结尾附录或官方文档：make &amp;&amp; make install#[100%] Built target my_safe_process 4.创建软链接12ln -s /application/mysql-5.5.32/ /application/mysql#如果上述操作未出现错误，则MySQL5.5.32软件cmake方式的安装就算成功了。 5.加入环境变量1234567 echo “export PATH=/application/mysql/bin/:$PATH”&gt;&gt;/etc/profile[root@MySQL 3306]# source /etc/profile [root@MySQL 3306]# which mysql/application/mysql/bin/mysql 6.初始化数据库1234#初始化数据库,初始化系统表cd /application/mysql/scripts/./mysql_install_db --basedir=/application/mysql/ --datadir=/application/mysql/data/ --user=mysqlcd ../ 7.授权用户及目录1234 #授权用户及/tmp/临时文件目录chown -R mysql.mysql /application/mysql/data/chmod -R 1777 /tmp/ 8.拷贝配置文件123#拷贝配置文件,在support-files下有默认的配置文件cp mysql-5.5.32/support-files/my-small.cnf /etc/my.cnf 9.启动数据库及测试1234567891011121314151617181920212223242526272829303132cp support-files/mysql.server /etc/init.d/mysqld#添加执行权限chmod +x /etc/init.d/mysqld/etc/init.d/mysqld start #检查端口netstat -lntup|grep 3306# 客户端登录[root@lamp01 mysql]# mysql #因为是刚刚安装,所以不需要密码Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.5.51 Source distribution Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&gt; #关闭mysql[root@lamp01 mysql]# /etc/init.d/mysqld stopShutting down MySQL. SUCCESS![root@lamp01 mysql]# lsof -i:3306 [root@lamp01 mysql]#","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"查询性能优化","date":"2017-04-16T04:47:25.730Z","path":"2017/04/16/mysql/查询性能优化/","text":"1.查询执行基础知识1.1.mysql执行查询过程 客户端将查询发送到服务器 服务器检查查询缓存 如果找到了就从缓存返回结果 否则进行下一步 服务器解析，预处理和优化查询，生成执行计划 执行引擎调用存储引擎api执行查询 服务器将结果发送回客户端 mysql客户端/服务器协议 该协议是半双工通信，可以发送或接收数据，但是不能同时发送和接收决定了mysql的沟通简单又快捷； 缺点：无法进行流程控制，一旦一方发送消息，另一方在发送回复之前必须提取完整的消息，就像抛球游戏，任意时间，只有某一方有球，而且有球在手上，否则就不能把球抛出去(发送消息) mysql客户端发送/服务器响应 可以设定max_packet_size这个参数控制客户端发送的数据包(一旦发送数据包，唯一做的就是等待结果) 服务器发送的响应由多个数据包组成， 客户端必须完整接收结果，即使只需要几行数据，也得等到全部接收 然后丢掉，或者强制断开连接。(这两个方法好挫，所以我们使用limit子句呀！！) 也可以理解，客户端从服务器 “拉” 数据 ，实际是服务器产生数据 “推”到客户端， 客户端不能说不要 是必须全部装着！ 常用的Mysql类库 其实是从客户端提取数据 缓存到array(内存)中，然后进行 foreach 处理。 但是对于庞大的结果集装载在内存中需要很长时间，如果不缓存，使用较少的内存并且可以尽快工作，但是应用程序和类库交互时候，服务器端的锁和资源都是被锁定的。 查询状态 每个mysql连接都是mysql服务器的一个线程 任意一个给定的时间都有一个状态来标识正在发生的事情。 使用 show full processlist 命令查看 12345678mysql&gt; show full processlist;+----+------+-------------------+------+-------------+-------+-----------------------------------------------------------------------+-----------------------+| Id | User | Host | db | Command | Time | State | Info |+----+------+-------------------+------+-------------+-------+-----------------------------------------------------------------------+-----------------------+| 13 | root | localhost | test | Query | 0 | NULL | show full processlist |+----+------+-------------------+------+-------------+-------+-----------------------------------------------------------------------+-----------------------+#mysql中一共有12个状态：休眠、查询、锁定、分析和统计、拷贝到磁盘上的临时表、排序结果、发送数据，通过这些状态 知道 &quot;球在谁手上&quot; 查询缓存 解析一个查询，如果开启了缓存，mysql会检查查询缓存，发现缓存匹配，返回缓存之前，检查查询的权限 2.优化数据访问查询性能低下最基本的原因是访问了太多的数据，分析两方面： 查明应用程序是否获取超过需要的数据 通常意味着访问了过多的行或列 查明mysql服务器是否分析了超过需要的行 向服务器请求了不需要的数据 一般请求不需要的数据，再丢掉他们，造成服务器额外的负担，增加网络开销，消耗了内存和cpu。 典型的错误： 提取超过需要的行 =&gt; 添加 limit 10 控制获取行数 多表联接提取所有列 =&gt; select fruit.* from fruit left join fruit_juice where 提取所有的列 =&gt; select id，name… from fruit … (有时提取超过需要的数据便于复用) mysql检查了太多数据 简单的开销指标：执行时间、检查的行数、返回的行数 以上三个指标写入了慢查询日志 可以使用 mysqlsla工具进行日志分析： 执行时间：执行时间只是参考 不可一概而论 因为执行时间 和服务器当时负载有关 检查和返回的行：理想情况下返回的行和检查的行一样，但是显示基本不可能 比如联接查询 检查的行和访问类型： 使用explain sq语句，观察typ列 typ列：(访问速度依次递增) 全表扫描(full table scan) 索引扫描(index scan) 范围扫描(range scan) 唯一索引查找(unique index lookup) 常量(constant) mysql会在3种情况下使用where子句，从最好到最坏依次是： 对索引查找应用where子句来消除不匹配的行 这发生在存储层 使用覆盖索引(extra 列 “using index”) 避免访问行 从索引取得数据过滤不匹配的行 这发生在服务层不需要从表中读取行 从表中检索出数据 过滤不匹配的行(extra:using where) 如果发现访问数据行数很大，尝试以下措施 使用覆盖索引 ，存储了数据 存储引擎不会读取完整的行 更改架构使用汇总表 重写复杂的查询 让mysql优化器优化执行它 3.重构查询的方式优化有问题的查询，其实也可以找到替代方案，提供更高的效率。 复杂查询和多个查询 mysql一般服务器可以每秒50000个查询，常规情况下，使用尽可能少的查询 有时候分解查询得到更高的效率。 缩短查询 分治法，查询本质上不变，每次执行一小部分，以减少受影响的行数。比如清理陈旧的数据，每次清理1000条：1delete from message where create &lt; date_sub(now()，inteval 3 month) limit 1000 防止长时间锁住很多行的数据。 分解联接 把一个多表联接分解成多个单个查询 然后在应用程序实现联接操作123456789select * from teacher join school on teacher.id = school.idjoin course on teacher.id = course.idwhere course.name = &quot;english&quot;#使用以下语句代替select * from course where name = &quot;english&quot;select * from teacher where course_id =1024select * from school where teacher_id in (111,222,333) 第一眼看上去比较浪费，因为增加了查询数量，但是有重大的性能优势： 缓存效率高，应用程序直接缓存了表 类似第一个查询直接跳过 对于myisam表来说 每个表一个查询有效利用表锁 查询锁住表的时间缩短 应用程端进行联接更方便扩展数据库 使用in() 避免联表查询id排序的耗费 减少多余行的访问 ， 意味着每行数据只访问一次 避免联接查询的非正则化的架构带来的反复访问同一行的弊端 分解联接应用场景： ① 可以缓存早期查询的大量的数据② 使用了多个myisam表(mysiam表锁 并发时候 一条sql锁住多个表 所以要分解)③ 数据分布在不同的服务器上④ 对于大表使用in() 替换联接④ 一个联接引用了同一个表很多次","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"字符集","date":"2017-04-16T04:47:25.728Z","path":"2017/04/16/mysql/字符集/","text":"1、字符集简介&emsp;字符集，character set，就是一套表示字符的符号和这些的符号的底层编码；而校验规则，则是在字符集内用于比较字符的一套规则。简单的说，字符集就是一套文字符号及其编码、比较规则的集合，第一个计算机字符集ASC2，MySQL数据库字符集包括字符集和校对规则两个概念，字符集是定义数据库里面的内容字符串的存储方式，而校对规则是定义比较字符串的方式。 建议：中英文环境选择utf8 2、查看设置字符集1234567891011121314# 查看MySQL字符集设置情况show variables like &apos;character_set%&apos;; # 查看库的字符集show create database db; # 查看表的字符集show create table db_tb\\G # 查询所有show collation; # 设置表的字符集set tables utf8; 3.MySQL数据乱码及解决方法1234567891011121314151617181920211&gt;系统方面cat /etc/sysconfig/i18nLANG=&quot;zh_CN.UTF-8&quot; 2&gt;客户端（程序），调整字符集为latin1。mysql&gt; set names latin1; #临时生效Query OK, 0 rows affected (0.00 sec) #更改my.cnf客户端模块的参数，实现set name latin1 的效果，并且永久生效。 [client]default-character-set=latin1#无需重启服务，退出登录就生效，相当于set name latin1。 3&gt;服务端，更改my.cnf参数[mysqld]default-character-set=latin1 #适合5.1及以前版本character-set-server=latin1 #适合5.5 4&gt;库、表、程序#建表指定utf8字符集mysql&gt; create database nick_defailtsss DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;Query OK, 1 row affected (0.00 sec) 4.将utf8字符集修改成GBK字符集的实际过程1234567891011121314151617181920211&gt;导出表结构#以utf8格式导出mysqldump -uroot -p --default-character-set=utf8 -d nick_defailt&gt;alltable.sql--default-character-set=gbk #表示已GBK字符集连接 –d 只表示表结构 2&gt;编辑alltable.sql 将utf8改成gbk。 3&gt;确保数据库不在更新，导出所有数据mysqldump -uroot -p --quick --no-create-info --extended-insert --default-character-set=utf8 nick_defailt&gt;alldata.sql 4&gt;打开alldata.sql将set name utf8 修改成 set names gbk(或者修改系统的服务端和客户端) 5&gt;建库create database oldsuo default charset gbk; 6&gt;创建表，执行alltable.sqlmysql -uroot -p oldsuo 7&gt;导入数据mysql -uroot -p oldsuo","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"利用Percona Xtrabackup快速备份MySQL(转)","date":"2017-04-16T04:47:25.726Z","path":"2017/04/16/mysql/利用Percona Xtrabackup快速备份MySQL/","text":"1.了解备份方式 热备份：读写不受影响（mysqldump–&gt;innodb） 温备份：仅可以执行读操作（mysqldump–&gt;myisam） 冷备份：离线备份，读写都不可用 逻辑备份：将数据导出文本文件中（mysqldump） 物理备份：将数据文件拷贝（xtrabackup、mysqlhotcopy） 完整备份：备份所有数据 增量备份：仅备份上次完整备份或增量备份以来变化的数据 差异备份：仅备份上次完整备份以来变化的数据 2.创建备份用户12mysql&gt; grant reload,lock tables,replication client on *.* to &apos;bak&apos;@&apos;localhost&apos; identified by &apos;bak2015&apos;;mysql&gt; flush privileges; 3.安装1234# rpm -ivh http://www.percona.com/downloads/percona-release/redhat/0.1-3/percona-release-0.1-3.noarch.rpm# yum install percona-xtrabackup#xtrabackup2.2不支持MySQL5.1的Innodb引擎，如需要可安装2.0版本 4.常用参数123456789101112131415–user= #指定数据库备份用户–password= #指定数据库备份用户密码–port= #指定数据库端口–host= #指定备份主机–socket= #指定socket文件路径–databases= #备份指定数据库,多个空格隔开，如–databases=”dbname1 dbname2″，不加备份所有库–defaults-file= #指定my.cnf配置文件–apply-log #日志回滚–incremental= #增量备份，后跟增量备份路径–incremental-basedir= #增量备份，指上次增量备份路径–redo-only #合并全备和增量备份数据文件–copy-back #将备份数据复制到数据库，数据库目录要为空–no-timestamp #生成备份文件不以时间戳为目录名–stream= #指定流的格式做备份，–stream=tar，将备份文件归档–remote-host=user@ip DST_DIR #备份到远程主机 5.完整备份与恢复5.1.完整备份1# innobackupex --user=bak --password=&apos;bak2015&apos; /mysql_backup 5.2.备份恢复1# innobackupex --defaults-file=/etc/mysql/my.cnf --copy-back /home/loongtao/mysql_backup/2015-02-08_11-56-48/ 5.3.备份文件说明12345678# ls 2015-02-08_11-56-48/*backup-my.cnf：记录innobackup使用到mysql参数xtrabackup_binary：备份中用到的可执行文件xtrabackup_checkpoints：记录备份的类型、开始和结束的日志序列号xtrabackup_logfile：备份中会开启一个log copy线程，用来监控innodb日志文件（ib_logfile），如果修改就会复制到这个文件*/ 6.完整备份+增量备份与恢复6.1.完整备份12innobackupex --user=bak --password=&apos;bak2015&apos; /mysql_backup#备份后位置是：/mysql_backup/2015-02-08_11-56-48 6.2.增量备份112# innobackupex --user=bak --password=&apos;bak2015&apos; --incremental /data1/mysql_backup --incremental-basedir=/mysql_backup/2015-02-08_11-56-48 #指定上次完整备份目录 6.3.增量备份212innobackupex --user=bak --password=&apos;bak2015&apos; --incremental /data1/mysql_backup --incremental-basedir=/mysql_backup/2015-02-08_12-16-06 #指定上次增量备份目录 6.4.查看xtrabackup_checkpoints文件一目了然，可以看到根据日志序号来增量备份 6.5.备份恢复 备份恢复思路 将增量备份1、增量备份2…合并到完整备份，加到一起出来一个新的完整备份，将新的完整备份以拷贝的形式到数据库空目录（rm /var/lib/mysql/* -rf） 预备完整备份 xtrabackup把备份过程中可能有尚未提交的事务或已经提交但未同步数据文件的事务，写到xtrabackup_logfile文件，所以要先通过这个日志文件回滚，把未完成的事务同步到备份文件，保证数据文件处于一致性。1# innobackup --apply-log --redo-only 2015-02-08_11-56-48 合并第一个增量备份 1# innobackupex --apply-log --redo-only /mysql_backup/2015-02-08_11-56-48/ --incremental-dir=mysql_backup/2015-02-08_12-16-06 合并第二个增量备份 1# innobackupex --apply-log --redo-only /mysql_backup/2015-02-08_11-56-48/ --incremental-dir=mysql_backup/2015-02-08_16-06-53 恢复完整备份 这时2015-02-08_11-56-48完整备份已经包含所有增量备份，可以通过查看checkpoints来核实1# innobackupex --defaults-file=/etc/mysql/my.cnf --copy-back /mysql_backup/2015-02-08_11-56-48/ 修改恢复数据文件权限 1# chown -R mysql.mysql /var/lib/mysql 启动MySQL,查看数据库恢复情况 1# /etc/init.d/mysqld start 7.备份文件归档压缩7.1.归档并发送到备份服务器123 innobackupex --databases=test --user=bak --password=&apos;bak2015&apos; --stream=tar /mysql_backup 2&gt;/mysql_backup/bak.log |ssh root@192.168.18.251 &quot;cat - &gt; /mysql_backup/`date +%F`.tar&quot;#解压：tar -ixvf `date +%F`.tar 7.2.归档备份123# innobackupex --databases=test --user=bak --password=&apos;bak2015&apos; --stream=tar /mysql_backup &gt; /mysql_backup/`date +%F`.tar#解压：tar -ixvf `date +%F`.tar 7.3.压缩归档备份123# innobackupex --databases=test --user=bak --password=&apos;bak2015&apos; --stream=tar /mysql_backup |gzip &gt;/mysql_backup/`date +%F`.tar.gz#解压：tar -izxvf `date +%F`.tar.gz 整理自","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"navicat for mysql 快捷键","date":"2017-04-16T04:47:25.725Z","path":"2017/04/16/mysql/navicat for mysql 快捷键/","text":"ctrl+q 打开查询窗口 ctrl+/ 注释sql语句 ctrl+shift +/ 解除注释 ctrl+r 运行查询窗口的sql语句 ctrl+shift+r 只运行选中的sql语句 F6 打开一个mysql命令行窗口 ctrl+l 删除一行 ctrl+n 打开一个新的查询窗口 ctrl+w 关闭一个查询窗口","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql的日志","date":"2017-04-16T04:47:25.723Z","path":"2017/04/16/mysql/mysql的日志/","text":"1.错误日志1.1.配置错误日志1234567891011121314151617#1.在配置文件中指定vim /data/3306/my.cnf[mysqld_safe]log-error=/data/3309/mysql_oldboy3309.err#2.在启动命令中加入 mysql_safe --defaults-file=/data/3306/my.cnf --log-error=/data/3306/mysql_oldboy.err #3.在mysql客户端查看error log 的位置mysql&gt; show variables like &apos;log_error%&apos;;+---------------+---------------------------------+| Variable_name | Value |+---------------+---------------------------------+| log_error | /data/3309/mysql_oldboy3309.err |+---------------+---------------------------------+ 2.普通日志记录客户端连接的信息和执行的sql语句的信息12345678910111213141516 mysql&gt; show variables like &apos;general_log%&apos;;+------------------+---------------------------+| Variable_name | Value |+------------------+---------------------------+| general_log | OFF || general_log_file | /data/3309/data/MySQL.log |+------------------+---------------------------+#在生产环境下，我们是关闭的，因为mysql的瓶颈就是磁盘IO，而这样的日志很多，将会产生IO，所以我们是关闭的，开启的方法如下：mysql&gt;set global general_log = on#在配置文件中修改general_log = ongeneral_log_file = /data/3306/data/MySQL_oldboy.log 3.慢查询日志 执行时间超过指定时间的sql语句,利用慢查询来的优化sql 3.1.开启慢查询日志12345#慢查询的设置对数据的sql优化非常重要#vim /data/3306/my.cnflong_query_time=1 //超过1s 的查询语句log-slow-queries=/data/3306/slow.loglog_queries_not_using_indexs //没有使用索引的语句 3.2.慢查询日志切割123456cd /server/scripts/vim cut_slow_log.shcd /data/3309/ &amp;&amp;\\/bin/mv slow.log slow.log.$(date +%F) &amp;&amp;\\mysqladmin -uroot -poldboy123 -S /data/3309/mysql.sock flush-log 3.3.使用工具mysqlsla分析慢查询，定时发送邮件给相关人员参见文档：mysqlsla日志分析工具.md 4.二进制日志4.1.设置开启12[root@lamp01 data]# grep log-bin /data/3306/my.cnflog-bin = /data/3306/mysql-bin 4.2.作用log-bin的作用: 记录更改的SQL语句 主从复制 增量备份 4.3.清除binlog日志mysql&gt;reset master 5.记录日志的三种模式5.1.行模式(Row Level)12345678910日志中会记录每一行数据被修改举例：update teacher set name=”zhangsan”;上述语句在binlog就会被解析成update teacher set name=”zhangsan” where id=1update teacher set name=”zhangsan” where id=2update teacher set name=”zhangsan” where id=3//.................. #缺点：会产生大量的binlog日志 5.2.语句级别（Statement Level）12345#mysql的默认级别#举例：update teacher set name=”zhangsan”;#上述语句在binlog就会被解析成update teacher set name=”zhangsan”; 5.3.mixed是上述两种模式的结合，智能选择其中的一种 6.设置binlog的记录模式1234567891011121314151617181920#在配置文件中修改log-bin=mysql-bin#binlog_format=&quot;STATEMENT&quot;#binlog_format=&quot;ROW&quot;#binlog_format=&quot;MIXED&quot;#运行时在线修改mysql&gt; SET SESSION binlog_format = &apos;STATEMENT&apos;;mysql&gt; SET SESSION binlog_format = &apos;ROW&apos;;mysql&gt; SET SESSION binlog_format = &apos;MIXED&apos;;#查看mysql&gt; show variables like &quot;binlog_format%&quot;;+---------------+-----------+| Variable_name | Value |+---------------+-----------+| binlog_format | STATEMENT |+---------------+-----------+ 7.使用mysqlbinlog提取二进制日志7.1.提取指定的binlog日志123456[root@lamp01 3306]# mysqlbinlog /data/3306/mysql-bin.000002|grep insert/*!40019 SET @@session.max_insert_delayed_threads=0*/;insert into student values(1,&quot;zhangsan2222222&quot;)insert into student values(1,&quot;lisi&quot;)insert into student values(2,&quot;lisi2&quot;)[root@lamp01 3306]# 7.2.提取指定position位置的binlog日志1mysqlbinlog --start-position=&quot;120&quot; --stop-position=&quot;332&quot; /opt/data/APP01bin.000001 7.3.提取指定position位置的binlog日志并输出到压缩文件1mysqlbinlog --start-position=&quot;120&quot; --stop-position=&quot;332&quot; /opt/data/APP01bin.000001 |gzip &gt;extra_01.sql.gz 7.4.提取指定position位置的binlog日志导入数据库1mysqlbinlog --start-position=&quot;120&quot; --stop-position=&quot;332&quot; /opt/data/APP01bin.000001 | mysql -uroot -p 7.5.提取指定开始时间的binlog并输出到sql文件1mysqlbinlog --start-datetime=&quot;2014-12-15 20:15:23&quot; /opt/data/APP01bin.000002 --result-file=extra02.sql 7.6.提取指定位置的多个binlog日志文件1mysqlbinlog --start-position=&quot;120&quot; --stop-position=&quot;332&quot; /opt/data/APP01bin.000001 /opt/data/APP01bin.000002|more 7.7.提取指定数据库binlog并转换字符集到UTF8123 mysqlbinlog --database=test --set-charset=utf8 /opt/data/APP01bin.000001 /opt/data/APP01bin.000002 &gt;test.sql #&gt;test.sql 也是输出到一个sql文件中 7.8.远程提取日志，指定结束时间1mysqlbinlog -urobin -p -h192.168.1.116 -P3306 --stop-datetime=&quot;2014-12-15 20:30:23&quot; --read-from-remote-server mysql-bin.000033 |more 7.9.远程提取使用row格式的binlog日志并输出到本地文件1mysqlbinlog -urobin -p -P3606 -h192.168.1.177 --read-from-remote-server -vv inst3606bin.000005 &gt;row.sql 8.binlog命令的参数8.1.–base64-output&emsp;对于不同的日志模式，生成的binlog有不同的记录方式。对于MIXED(部分SQL语句)和ROW模式是以base-64方式记录，会以BINLOG开头，是一段伪SQL，我们可以用使用base64-output参数来抑制其显示 8.2.–verbose(-v)选项可以获取更多的可读信息，但是并不是一个原始的SQL语句1[root@MySQL 3309]# mysqlbinlog --base64-output=&quot;decode-row&quot; -v mysql-bin.000001 8.3.-d 指定数据库1234567891011121314151617181920212223[root@MySQL 3309]# mysqlbinlog mysql-bin.000002|egrep -v &quot;^$|^#|^\\/\\*|^SET|BEGIN|COMMIT|DELIMIT|ROLL|&apos;&quot; gAyxVw8BAAAAZwAAAGsAAAABAAQANS41LjUxLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACADLFXEzgNAAgAEgAEBAQEEgAAVAAEGggAAAAICAgCAA==use `teacher`/*!*/;DROP TABLE `t1` /* generated by server */DROP TABLE `t2` /* generated by server */insert into t3(id,name) values(33,&quot;chenyansong&quot;)insert into t3(id,name,age) values(333,&quot;chenyansong&quot;,33)drop database tst_t1drop database tst_t2 #指定数据库[root@MySQL 3309]# mysqlbinlog -d teacher mysql-bin.000002|egrep -v &quot;^$|^#|^\\/\\*|^SET|BEGIN|COMMIT|DELIMIT|ROLL|&apos;&quot;gAyxVw8BAAAAZwAAAGsAAAABAAQANS41LjUxLWxvZwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACADLFXEzgNAAgAEgAEBAQEEgAAVAAEGggAAAAICAgCAA==use `teacher`/*!*/;DROP TABLE `t1` /* generated by server */DROP TABLE `t2` /* generated by server */insert into t3(id,name) values(33,&quot;chenyansong&quot;)insert into t3(id,name,age) values(333,&quot;chenyansong&quot;,33) insert into teacher.t3(id,name,age) values(8,&quot;lisi_4&quot;,22)#上面的语句就不会计入分库binlog，只有我们使用use的时候，才会在-d的时候计入分库 9.删除二进制日志&emsp;随着时间的推移，二进制日志也会变得很多很大，因此，有必要执行删除操作，我们会在配置文件中加入下面的参数来实现自动清理二进制日志的工作 9.1.reset master删除所有的binlog日志，新日志编号从头开始 9.2.purge master logs to “mysql-bin.000002”12#删除mysql-bin.000002之前所有日志，不包含000002自身mysql&gt; purge master logs to &quot;mysql-bin.000002&quot;; 9.3.purge master logg before “2015-09-11 24:45:56”;删除指定时间之前的binlog 9.4.配置文件中定义删除指定日期的日志12 grep expire /data/3309/my.cnfexpire_logs_days = 7","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql数据库安装之后的优化操作","date":"2017-04-16T04:47:25.721Z","path":"2017/04/16/mysql/mysql数据库安装之后的优化操作/","text":"1.删除不必要的用户和库1234567891011121314151617#查看用户和主机列，从mysql.user里查看select user,host from mysql.user; #删除用户名为空的用户，并检查delete from mysql.user where user=&apos;&apos;;select user,host from mysql.user; #删除主机名为localhost.localdomain的库，并检查delete from mysql.user where host=&apos;localhost.localdomain&apos;;select user,host from mysql.user; #删除主机名为::1的库，并检查。::1库的作用为IPV6delete from mysql.user where host=&apos;::1&apos;; #删除test库drop database test; 2.添加额外管理员1234567891011121314151617# 添加额外管理员，system作为管理员，oldsuo为密码mysql&gt; delete from mysql.user;Query OK, 2 rows affected (0.00 sec)mysql&gt; grant all privileges on *.* to system@&apos;localhost&apos; identified by &apos;oldsuo&apos; with grant option;Query OK, 0 rows affected (0.00 sec) # 刷新MySQL的系统权限相关表，使配置生效mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; select user,host from mysql.user;+--------+-----------+| user | host |+--------+-----------+| system | localhost |+--------+-----------+1 row in set (0.00 sec)mysql&gt; 3.设置登录密码并开机自启1234567#设置密码，并登陆/usr/local/mysql/bin/mysqladmin -u root password &apos;oldsuo&apos;mysql -usystem -p #开机启动mysqld，并检查chkconfig mysqld onchkconfig --list mysqld","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql忘记密码解决","date":"2017-04-16T04:47:25.719Z","path":"2017/04/16/mysql/mysql忘记密码解决/","text":"mysql忘记密码怎么办? 1234567891011121314151617181920212223242526272829303132333435# 1&gt; 普通方式service mysqld stopmysqld_safe --skip-grant-tables &amp;输入 mysql -uroot -p 回车进入&gt;use mysql;&gt; update user set password=PASSWORD(&quot;newpass&quot;)where user=&quot;root&quot;;更改密码为 newpassord&gt; flush privileges; 更新权限&gt; quit 退出service mysqld restartmysql -uroot -p新密码进入 # 2&gt; 普通方式的简写service mysqld stopmysqld_safe --skip-grant-tables --user=mysql &amp;mysqlupdate mysql.user set password=PASSWORD(&quot;newpass&quot;)where user=&quot;root&quot; and host=&apos;localhost&apos;;flush privileges;mysqladmin -uroot -pnewpass shutdown/etc/init.d/mysqld startmysql -uroot -pnewpass #登陆 # 3&gt;多实例方式killall mysqldmysqld_safe –defaults-file=/data/3306/my.cnf –skip-grant-table &amp;mysql –u root –p –S /data/3306/mysql.sock #指定sock登陆update mysql.user set password=PASSWORD(&quot;newpass&quot;)where user=&quot;root&quot;;flush privileges;mysqladmin -uroot -pnewpass shutdown/etc/init.d/mysqld startmysql -uroot -pnewpass #登陆","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之表操作","date":"2017-04-16T04:47:25.718Z","path":"2017/04/16/mysql/mysql实战之表操作/","text":"1、创建表1234567891011# 基本语法：create table 表名( 列名 类型 是否可以为空 默认值 自增 主键， 列名 类型 是否可以为空)ENGINE=InnoDB DEFAULT CHARSET=utf8 not null # 不可以为空default 1 # 默认值为1auto_increment # 自增primary key # 主键constraint 外键名 foreign key (从表字段’自己‘) references 主表(主键字段) # 外键 是否可空，null表示空，非字符串 12not null - 不可空null - 可空 默认值 12345#创建列时可以指定默认值，当插入数据时如果未主动设置，则自动添加默认值 create table tb1( nid int not null defalut 2, num int not null ) 自增 12345678910111213141516171819#如果为某列设置自增列，插入数据时无需设置此列，默认将自增（表中只能有一个自增列） create table tb1( nid int not null auto_increment primary key, num int null ) 或 create table tb1( nid int not null auto_increment, num int null, index(nid) )/*注意：1、对于自增列，必须是索引（含主键）。 2、对于自增可以设置步长和起始值*/show session variables like &apos;auto_inc%&apos;;set session auto_increment_increment=2;set session auto_increment_offset=10; 主键 123456789101112#主键，一种特殊的唯一索引，不允许有空值，如果主键使用单个列，则它的值必须唯一，如果是多列，则其组合必须唯一。create table tb1( nid int not null auto_increment primary key, num int null)#orcreate table tb1( nid int not null, num int not null, primary key(nid,num)) 外键 123456789101112#一个特殊的索引，只能是指定内容create table color( nid int not null primary key, name char(16) not null)create table fruit( nid int not null primary key, smt char(32) null, color_id int not null, constraint fk_cc foreign key (color_id) references color(nid)) 2、删除表1drop table 表名 3、清空表123# 表还存在，表内容清空delete from 表名truncate table 表名 4、修改表1234567891011121314151617181920212223242526# 添加列： alter table 表名 add 列名 类型# 删除列： alter table 表名 drop column 列名# 修改列： alter table 表名 modify column 列名 类型; -- 类型 alter table 表名 change 原列名 新列名 类型; -- 列名，类型# 添加主键： alter table 表名 add primary key(列名);# 删除主键： alter table 表名 drop primary key; alter table 表名 modify 列名 int, drop primary key;# 添加外键： alter table 从表 add constraint 外键名称（形如：FK_从表_主表） foreign key 从表(外键字段) references 主表(主键字段);# 删除外键： alter table 表名 drop foreign key 外键名称# 修改默认值： ALTER TABLE testalter_tbl ALTER i SET DEFAULT 1000;# 删除默认值： ALTER TABLE testalter_tbl ALTER i DROP DEFAULT;# 更改表名 rename table 原表名 to 新表名; 增删改表的字段 123456789101112131415161718192021#增加表字段，altertable法。#1&gt; 语法： altertable 表名 add 字段 类型 其他；#2&gt; 插入列，名为sex。 alter table student add sex char(4)#3&gt; 插入名为suo列在name后面。 alter table student add suo int(4) after name;#4&gt; 插入名为qq列在第一。 alter table student add qq varchar(15) first;#更改表名字，rename法。#1&gt; 语法: rename table 原表名 to 新表名；#2&gt; 更改oldsuo表为oldning。 rename table oldsu to oldning#删除表#1&gt; 语法：drop table &lt;表名&gt;；#2&gt; 删除表名为oldsuo表。 drop table oldsu","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之表内容操作","date":"2017-04-16T04:47:25.716Z","path":"2017/04/16/mysql/mysql实战之表内容操作/","text":"1、增12345678# 语法：insert into 表 (列名,列名...) values (值,值,值...)# 插入单条数据 insert into 表 (列名,列名...) values (值,值,值...)# 插入多条数据 insert into 表 (列名,列名...) values (值,值,值...),(值,值,值...)# 插入另一条语句的查询结果 insert into 表 (列名,列名...) select 列名,列名... from 表 2、删123# 语法：delete from 表delete from 表;delete from 表 where id＝1; 3、改12# 语法：update 表 set name ＝ &apos;nick&apos; where id&gt;1update 表 set name ＝ &apos;nick&apos; where id&gt;1 4、查12345# 语法：select * from 表select * from 表select * from 表 where id &gt; 1select nid,name,gender as gg from 表 where id &gt; 1# as 做别名 5、条件123456789101112# 语法：select * from 表 where id &gt; 1# id在5到16之间 select * from 表 where id between 5 and 16# 多个条件 select * from 表 where id&gt;1 and name != &apos;nick&apos; and num = 12# id在元组中 select * from 表 where id in(11, 22, 33)# id不在元组中 select * from 表 where id not in (11, 22, 33)# id在查询结果中 select * from 表 where id in (select nid from 表); 6、通配符12345# 语法：select * from 表 where name like &apos;_n%&apos;# ni开头的所有(多个字符串) select * from 表 where name like &quot;ni%&quot;;#s开头的所有(一个字符) select * from where name like &quot;s_&quot;; 7、限制1234567# 语法：select * from 表 limit 9,5;# 前5行 select * from 表 limit 5;# 从第9行开始的5行 select * from 表 limit 9,5;# 从第9行开始的5行 select * from 表 limit 5 offset 9; 8、排序1234567# 语法：select * from 表 order by 列1 desc,列2 asc# 根据&quot;列&quot;从小到大排列 select * from 表 order by 列 asc;# 根据&quot;列&quot;从大到小排列 select * from 表 order by 列 desc;# 根据&quot;列1&quot;从大到小排列,如果相同则按&quot;列2&quot;从小到大排序 select * from 表 order by 列1 desc , 列2 asc; 9、分组123456789101112131415161718# 语法：select num from 表 group by num# 根据num分组 select num from 表 group by num;# 根据num和nid分组 select num, nid from 表 group by num,nid;# 内置函数 select num, nid from 表 where nid&gt;10 group by num,nid order nid desc;# 取分组后id大于10的组 select num from 表 group by num having max(id) &gt; 10/* 注：group by 必须在where之后，order by之前count(*) , count(1) #表示个数sum(score) #表示和max(score) #最大数min(score) #最小数having #分组之后用having*/ 10、连表12345678# 语法：inner join . on、left join . on、right join . on# 内连接 select A.num, A.name, B.name from A, B where A.id = B.id select A.num, A.name, B.name from A inner join B on A.id = B.id# 左连接(A 表所有显示,如果B中无对应关系,则值为null) select A.num, A.name, B.name from A left join B on A.id = B.id# 右连接(B 表所有显示,如果B中无对应关系,则值为null select A.num, A.name, B.name from A right join B on A.id = B.id 11、组合123456789# 语法：union、union allselect nickname from Aunionselect name from B#组合,不处理重合select nickname from Aunion allselect name from B 12.表中插入数据12345# 1&gt; 插入单个数据，student为表的名称 insert into student(id, name) values (1, &apos;nick&apos;)# ２&gt; 批量插入数据，student为表的名称。 insert into student(id, name) values (1, &apos;nick&apos;), (3, &apos;kangkan&apos;), (4,&apos;kadd&apos;) 13.表中删除数据123456# 1&gt; 删除所有数据，student为表的名称 delete from student;# 2&gt; 删除表中的某行或某些 delete from student where id=4# 3&gt; 直接清空某张表 truncate table student; 14.查看建表语句1show create table 表名 \\G 15.查看表结构1desc 表名 16.查看是否建索引12345678910111213mysql&gt; explain select * from student where name=&quot;student&quot; \\G;*************************** 1. row *************************** id: 1 select_type: SIMPLE table:student type: ref #有索引possible_keys:index_name key: index_name #表示有 key_len: 20 ref: const rows: 1 #检索了几行 Extra: Using where1 row in set (0.07 sec) 整理自","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之索引","date":"2017-04-16T04:47:25.714Z","path":"2017/04/16/mysql/mysql实战之索引/","text":"1、索引概述 索引是表的索引目录，在查找内容之前先查目录中查找索引位置，从而快速定位查询数据； 可以理解成新华字典中的索引； 索引会保存在额外的文件中。 2、索引种类 普通索引：仅加速查询 唯一索引：加速查询 + 列值唯一（可以有null） 主键索引：加速查询 + 列值唯一 + 表中只有一个（不可以有null） 组合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并 全文索引：对文本的内容进行分词，进行搜索 索引合并：使用多个单列索引组合查询搜索 覆盖索引：select的数据列只用从索引中就能够取得，不必读取数据行，换句话说查询列要被所建的索引覆盖 a、普通索引123456789101112131415161718192021# 创建表 + 索引create table in1&#123; nid int not null auto_increament primary key, name varchar(32) not null, email varchar(64) not null, extra text, index ix_name (name)&#125;# 创建索引create index index_name on table_name(column_name)# 删除索引drop index_name on table_name; # 查看索引show index from table_name;#注意：对于创建索引时如果是BLOB 和 TEXT 类型，必须指定length。create index ix_extra on in1(extra(32)); b、唯一索引1234567891011121314# 创建表 + 唯一索引create table in1&#123; nid int not null auto_increament primary key, name varchar(32) not null, email varchar(64) not null, extra text, unique ix_name (name)&#125;# 创建唯一索引create unique index 索引名 on 表名(列名) # 删除唯一索引drop unique index 索引名 on 表名 c、主键索引1234567891011121314151617181920212223242526# 创建表 + 创建主键create table in1&#123; nid int not null auto_increament primary key, name varchar(32) not null, email varchar(64) not null, extra text, index ix_name (name)&#125;#orcreate table in1&#123; nid int not null auto_increament , name varchar(32) not null, email varchar(64) not null, extra text, primary key(nid), index ix_name (name)&#125;# 创建主键alter table 表名 add primary key(列名); # 删除主键alter table 表名 drop primary key;alter table 表名 modify 列名 int, drop primary key; d、组合索引123456789101112131415161718192021222324/*组合索引是多个列组合成一个索引来查询应用场景：频繁的同时使用多列来进行查询，如：where name = &apos;nick&apos; and age = 18。*/# 创建表create table mess&#123; nid int not null auto_increment primary key, name varchar(32) not null, age int not null&#125;# 创建组合索引create index ix_name_age on mess(name,age);/*如上创建组合索引之后，查询一定要注意： name and email -- &gt;使用索引，name一定要放前面name -- &gt;使用索引email -- &gt;不使用索引 注意：同时搜索多个条件时，组合索引的性能效率好过于多个单一索引合并。*/ 3、相关命令1234567# 查看索引 show index from 表名 # 查看执行时间 set profiling = 1; # 开启profiling SQL... # 执行SQL语句 show profiles; # 查看结果 4、如何正确使用索引123456789101112131415161718192021222324252627282930313233343536# like &apos;%xx&apos;，避免%_写在开头 select * from tb1 where name like &apos;%n&apos;; # 使用函数 select * from tb1 where reverse(name) = &apos;nick&apos;; # or select * from tb1 where nid = 1 or email = &apos;630571017@qq.com&apos;; 注：当or条件中有未建立索引的列才失效，否则会走索引 # 类型不一致 如果列是字符串类型，传入条件是必须用引号引起来。 select * from tb1 where name = 999; # !=，不等于 select * from tb1 where name != &apos;nick&apos; 注：如果是主键，则还是会走索引 select * from tb1 where nid != 123 # &gt;，大于 select * from tb1 where name &gt; &apos;nick&apos; 注：如果是主键或索引是整数类型，则还是会走索引 select * from tb1 where nid &gt; 123 select * from tb1 where num &gt; 123 # order by select email from tb1 order by name desc; 当根据索引排序时候，选择的映射如果不是索引，则不走索引 注：如果对主键排序，则还是走索引： select * from tb1 order by nid desc; # 组合索引最左前缀 如果组合索引为：(name,email)，查询使用： name and email -- 使用索引 name -- 使用索引 email -- 不使用索引 5、注意事项123456789101112131415# 避免使用select *# count(1)或count(列) 代替 count(*)# 创建表时尽量时 char 代替 varchar# 表的字段顺序固定长度的字段优先# 组合索引代替多个单列索引（经常使用多个条件查询时）# 尽量使用短索引# 使用连接（JOIN）来代替子查询(Sub-Queries)# 连表时注意条件类型需一致# 索引散列值（重复少）不适合建索引，例：性别不适合# 索引会加快查询速度，但是也会影响更新的速度，因为更新要维护索引数据。# 索引列并不是越多越好，要在频繁查询的where后的条件列上创建索引# 小表或重复值很多的列可以不建索引，要在大表以及重复值少的条件列上创建索引# 多个列联合索引有前缀生效特性# 当字段内容前n个字符已经接近唯一时，可以对字段的前n个字符创建索引 6、执行计划1234567891011121314151617181920212223242526272829mysql&gt; select *from student;+------+-----------+| id | name |+------+-----------+| 1 | zhangsan || 2 | zhangsan2 || 3 | zhangsan3 || 4 | zhangsan4 |+------+-----------+4 rows in set (0.00 sec) mysql&gt; explain select *from student;+----+-------------+---------+------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+---------+------+---------------+------+---------+------+------+-------+| 1 | SIMPLE | student | ALL | NULL | NULL | NULL | NULL | 4 | |+----+-------------+---------+------+---------------+------+---------+------+------+-------+1 row in set (0.00 sec) #有子查询mysql&gt; explain select * from (select id,name from student where id&lt;4) as B; +----+-------------+------------+------+---------------+------+---------+------+------+-------------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+| 1 | PRIMARY | &lt;derived2&gt; | ALL | NULL | NULL | NULL | NULL | 3 | || 2 | DERIVED | student | ALL | NULL | NULL | NULL | NULL | 4 | Using where |+----+-------------+------------+------+---------------+------+---------+------+------+-------------+2 rows in set (0.08 sec) 整理自","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之用户管理及授权管理","date":"2017-04-16T04:47:25.712Z","path":"2017/04/16/mysql/mysql实战之用户管理及授权管理/","text":"1、用户管理12345678910111213141516171819# 创建用户create user &apos;用户名&apos;@&apos;IP地址&apos; identified by &apos;密码&apos;;# 删除用户drop user &apos;用户名&apos;@&apos;IP地址&apos;;# 修改用户rename user &apos;用户名&apos;@&apos;IP地址&apos;; to &apos;新用户名&apos;@&apos;IP地址&apos;;;# 修改密码set password for &apos;用户名&apos;@&apos;IP地址&apos; = Password(&apos;新密码&apos;)#PS：用户权限相关数据保存在mysql数据库的user表中，所以也可以直接对其进行操作（不建议）# 查看当前用户select user();# 查看所有用户select host,user from mysql.user;# 人性化显示所有用户SELECT DISTINCT CONCAT(&apos;User: &apos;&apos;&apos;,user,&apos;&apos;&apos;@&apos;&apos;&apos;,host,&apos;&apos;&apos;;&apos;) AS query FROM mysql.user;# 查看用户的所有权限show grants for &apos;nick&apos;@&apos;%&apos;; 2、授权管理123456# 查看权限 show grants for &apos;用户&apos;@&apos;IP地址&apos;# 授权 grant 权限 on 数据库.表 to &apos;用户&apos;@&apos;IP地址&apos;# 取消权限 revoke 权限 on 数据库.表 from &apos;用户&apos;@&apos;IP地址&apos; 常用权限： all privileges 除grant外的所有权限 select 仅查权限 select,insert 查和插入权限 usage 无访问权限 对于目标数据库以及内部其他： 数据库名.* #数据库中的所有 数据库名.表 #指定数据库中的某张表 数据库名.存储过程 #指定数据库中的存储过程 *.* #所有数据库中的所有表 对于用户和IP： 用户名@IP地址 #用户只能在该IP下才能访问 用户名@192.168.0.1.% #用户只能在该IP段下才能访问(通配符%表示任意) 用户名@% #用户可以在任意IP下访问(默认IP地址位%) 更多权限 添加额外管理员 简单示例 创建用户一般流程 6、授权局域网内主机远程连接数据库12345678#百分号匹配法 grant all on *.* to &apos;test&apos;@&apos;192.168.200.%&apos; identified by &apos;test123&apos;;#子网掩码配置法 grant all on *.* to &apos;test&apos;@&apos;192.168.200.0/255.255.255.0&apos; identified by &apos;test123&apos;;#刷新权限 flush privileges;#远程登陆连接 mysql -utest -ptest123 -h 192.168.200.96 整理自","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之数据库操作","date":"2017-04-16T04:47:25.710Z","path":"2017/04/16/mysql/mysql实战之数据库操作/","text":"1、查看数据库123456SHOW DATABASES; # 默认数据库： mysql - 用户权限相关数据 test - 用于用户测试数据 information_schema - MySQL本身架构相关数据 2、创建数据库12345# utf-8 编码CREATE DATABASE 数据库名称 DEFAULT CHARSET utf8 COLLATE utf8_general_ci; # gbk 编码CREATE DATABASE 数据库名称 DEFAULT CHARACTER SET gbk COLLATE gbk_chinese_ci; 3、使用数据库123USE db_name; # 可以不使用分号","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql实战之基本语句命令","date":"2017-04-16T04:47:25.709Z","path":"2017/04/16/mysql/mysql实战之基本语句命令/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#1、单实例mysql启动[root@localhost ~]# /etc/init.d/mysqld startStarting MySQL [确定]#mysqld_safe –user=mysql &amp;#2、查看MySQL端口[root@localhost ~]# ss -lntup|grep 3306tcp LISTEN 0 50 *:3306 *:* users:((&quot;mysqld&quot;,19651,10))#3、查看MySQL进程[root@localhost ~]# ps -ef|grep mysql|grep -v greproot 19543 1 0 Oct10 ? 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --datadir=/usr/local/mysql/data --pid-file=/usr/local/mysql/data/localhost.localdomain.pidmysql 19651 19543 0 Oct10 ? 00:05:04 /usr/local/mysql/libexec/mysqld --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql --log-error=/usr/local/mysql/data/localhost.localdomain.err --pid-file=/usr/local/mysql/data/localhost.localdomain.pid --socket=/tmp/mysql.sock --port=3306#4、MySQL启动原理/etc/init.d/mysqld 是一个shell启动脚本，启动后最终会调用mysqld_safe脚本，最后调用mysqld服务启动mysql(是一个二进制文件)。 &quot;$manager&quot; \\ --mysqld-safe-compatible \\ --user=&quot;$user&quot; \\ --pid-file=&quot;$pid_file&quot; &gt;/dev/null 2&gt;&amp;1 &amp;#5、关闭数据库[root@localhost ~]# /etc/init.d/mysqld stopShutting down MySQL.... [确定]#6、查看mysql数据库里操作命令历史cat /root/.mysql_history#7、强制linux不记录敏感历史命令HISTCONTROL=ignorespace# 8、MySQL设置密码/usr/local/mysql/bin/mysqladmin -u root password &apos;oldsuo&apos;#9、MySQL修改密码，与多实例指定sock修改密码mysqladmin -uroot -passwd password &apos;oldsuo&apos;mysqladmin -uroot -passwd password &apos;oldsuo&apos; -S /data/3306/mysql.sock#登陆mysql数据库mysql -uroot –p #查看有哪些库show databases;#删除test库drop database test; #使用test库use test;#查看有哪些表show tables;#查看suoning表的所有内容select * from suoning; #查看当前版本select version();#查看当前用户select user();#查看用户和主机列，从mysql.user里查看select user,host from mysql.user;#删除前为空，后为localhost的库drop user &quot;&quot;@localhost； #刷新权限flush privileges; #跳出数据库执行命令system ls;","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysql多实例配置","date":"2017-04-16T04:47:25.707Z","path":"2017/04/16/mysql/mysql多实例配置/","text":"前提: 在mysql已经安装的完毕,可以参见: &lt;&lt;编译安装MySQL-5.5.32.md&gt;&gt;一文 1.多实例数据文件目录结构&emsp;在该文档中，采用的是/data目录作为mysql多实例总的根目录，然后规则不同的数字（即Mysql实例端口号）作为/data下面的二级目录，不同的二级目录对应的数字就作为Mysql实例的端口号，以区别不同的实例，数字对应的二级目录下包含mysql数据文件、配置文件以及启动文件等。 12345678mkdir -p /data/&#123;3306,3307&#125;/data[root@lamp01 /]# tree /data/data|-- 3306| `-- data`-- 3307 `-- data 2.上传配置和启动文件12345678910111213141516上传配置好的文件（F:\\Linux_NOTE\\MySQL数据库\\data.zip）到 / 目录下cd /rz -yunzip data.zip#解压后的文件目录如下：[root@lamp01 /]# tree /data/data|-- 3306| |-- data| |-- my.cnf| `-- mysql`-- 3307 |-- data |-- my.cnf `-- mysql 3.配置文件/data/3306/my.cnf4.配置文件/data/3306/mysql使用vim进去看，其实这个启动文件就是一个脚本，其中包含：启动、停止、重启的方法 5.每个实例初始化数据123456789101112131415161718192021222324[root@lamp01 /]# cd /application/mysql/scripts/#指定数据生成的目录为: /data/3306/data [root@lamp01 scripts]# ./mysql_install_db --basedir=/application/mysql/ --datadir=/data/3306/data --user=mysql[root@lamp01 scripts]# ./mysql_install_db --basedir=/application/mysql/ --datadir=/data/3307/data --user=mysql[root@lamp01 data]# tree -L 3 /data/data|-- 3306| |-- data| | |-- mysql| | |-- performance_schema| | `-- test| |-- my.cnf| |-- mysql| `-- mysql_oldboy3306.err`-- 3307 |-- data | |-- mysql | |-- performance_schema | `-- test |-- my.cnf |-- mysql `-- mysql_oldboy3307.err 6.授权用户及目录123 #授权用户及/tmp/临时文件目录chown -R mysql.mysql /data 7.给执行脚本执行权限123456789[root@lamp01 3306]# chmod a+x /data/&#123;3306,3307&#125;/mysql[root@lamp01 3306]# lltotal 16drwxr-xr-x 5 mysql mysql 4096 Feb 16 22:33 data-rw-r--r-- 1 mysql mysql 1899 Oct 29 2013 my.cnf-rwxr-xr-x 1 mysql mysql 1307 Jul 15 2013 mysql-rw-r----- 1 mysql mysql 77 Feb 16 22:31 mysql_oldboy3306.err[root@lamp01 3306]# 5.启动多实例123456789[root@lamp01 3306]# /data/3306/mysql startStarting MySQL...[root@lamp01 3306]# /data/3307/mysql startStarting MySQL...#查看[root@lamp01 3306]# netstat -lntup|grep mysqltcp 0 0 0.0.0.0:3306 0.0.0.0:* LISTEN 2527/mysqld tcp 0 0 0.0.0.0:3307 0.0.0.0:* LISTEN 3240/mysqld 6.初始化登录mysql123456789101112131415#要指定对应实例的sock文件[root@lamp01 3306]# mysql -S /data/3306/mysql.sock Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 1Server version: 5.5.51-log Source distribution Copyright (c) 2000, 2016, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners. Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement. mysql&gt; 7.配置密码和修改密码操作12345678910//配置初始化密码[root@MySQL ~]# mysqladmin -uroot password &quot;oldboy123&quot; -S /data/3309/mysql.sock//用初始化密码登录[root@MySQL ~]# mysql -uroot -poldboy123 -S /data/3309/mysql.sock //修改密码//非交互式的：直接写出了密码mysqladmin -uroot -poldboy123 password &quot;123&quot; -S /data/3309/mysql.sock//交互式的mysqladmin -uroot -poldboy123 password -S /data/3309/mysql.sock 8.去掉多余的登录用户和库参见: mysql数据库安装之后的优化操作.md 一文 9.快速配置一个多实例下面让我们来配置mysql 3308的多实例启动方法： 12345678910111213141516171819202122mkdir -p /data/3308/data\\cp /data/3306/my.cnf /data/3308/\\cp /data/3306/mysql /data/3308/sed -i &apos;s/3306/3308/g&apos; /data/3308/my.cnfsed -i &apos;s/server-id = 1/server-id = 9/g&apos; /data/3308/my.cnfsed -i &apos;s/3306/3308/g&apos; /data/3308/mysqlchown -R mysql:mysql /data/3308chmod 700 /data/3308/mysqlcd /application/mysql/scripts./mysql_install_db --datadir=/data/3308/data --basedir=/application/mysql --user=mysqlchown -R mysql:mysql /data/3308egrep &quot;server-id|log-bin&quot; /data/3308/my.cnf/data/3308/mysql startsleep 5netstat -lnt|grep 3308mysqladmin -u root password &apos;lx3308&apos; -S /data/3308/mysql.sock #初始化3308数据库密码#查看mysql 3306 3307 3308各个服务是否开启[root@lixiang scripts]# netstat -lntup|grep 3308tcp 0 0 0.0.0.0:3308 0.0.0.0:* LISTEN 5251/mysqld","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"MySQL主从复制","date":"2017-04-16T04:47:25.705Z","path":"2017/04/16/mysql/MySQL主从复制/","text":"1.目的我们为什么要用主从复制？ 可以做数据库的实时备份，保证数据的完整性； 可做读写分离，主服务器只管写，从服务器只管读，这样可以提升整体性能。 2.原理 Mysql的 Replication 是一个异步的复制过程，从一个 Mysql instace(我们称之为 Master)复制到另一个 Mysql instance(我们称之 Slave)。在 Master 与 Slave 之间的实现整个复制过程主要由三个线程来完成，其中两个线程(Sql线程和IO线程)在 Slave 端，另外一个线程(IO线程)在 Master 端。 要实现 MySQL 的 Replication ，首先必须打开 Master 端的Binary Log(mysql-bin.xxxxxx)功能，否则无法实现。因为整个复制过程实际上就是Slave从Master端获取该日志然后再在自己身上完全 顺序的执行日志中所记录的各种操作。打开 MySQL 的 Binary Log 可以通过在启动 MySQL Server 的过程中使用 “—log-bin” 参数选项，或者在 my.cnf 配置文件中的 mysqld 参数组([mysqld]标识后的参数部分)增加 “log-bin” 参数项。 MySQL 复制的基本过程如下： Slave 上面的IO线程连接上 Master，并请求从指定日志文件的指定位置(或者从最开始的日志)之后的日志内容; Master 接收到来自 Slave 的 IO 线程的请求后，通过负责复制的 IO 线程根据请求信息读取指定日志指定位置之后的日志信息，返回给 Slave 端的 IO 线程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息在 Master 端的 Binary Log 文件的名称以及在 Binary Log 中的位置; Slave 的 IO 线程接收到信息后，将接收到的日志内容依次写入到 Slave 端的Relay Log文件(mysql-relay-bin.xxxxxx)的最末端，并将读取到的Master端的bin-log的文件名和位置记录到master- info文件中，以便在下一次读取的时候能够清楚的高速Master“我需要从某个bin-log的哪个位置开始往后的日志内容，请发给我” Slave 的 SQL 线程检测到 Relay Log 中新增加了内容后，会马上解析该 Log 文件中的内容成为在 Master 端真实执行时候的那些可执行的 Query 语句，并在自身执行这些 Query。这样，实际上就是在 Master 端和 Slave 端执行了同样的 Query，所以两端的数据是完全一样的。 3.更改配置文件1234567891011121314151617181920212223242526# 3306和3307分别代表2台机器# 打开log-bin,并使server-id不一样#vim /data/3306/my.cnflog-bin = /data/3306/mysql-binserver-id = 1#vim /data/3307/my.cnflog-bin = /data/3307/mysql-binserver-id = 3#检查#1、[root@bogon ~]# egrep &quot;log-bin|server-id&quot; /data/3306/my.cnflog-bin = /data/3306/mysql-binserver-id = 1[root@bogon ~]# egrep &quot;log-bin|server-id&quot; /data/3307/my.cnflog-bin = /data/3307/mysql-binserver-id = 3#2、[root@localhost ~]# mysql -uroot -p -S /data/3306/mysql.sock -e &quot;show variables like &apos;log_bin&apos;;&quot;Enter password:+--------+--------+| Variable_name | Value |+--------+--------+| log_bin | ON | # ON 为开始开启成功+--------+--------+ 4.建立用于从库复制的账号rep通常会创建一个用于主从复制的专用账户，不要忘记授权。12345678910111213141516171819# 主库授权，允许从库来连接我取日志[root@localhost ~]# mysql -uroot -p -S /data/3306/mysql.sockEnter password:# 允许从库192.168.200网段连接，账号rep，密码nick。mysql&gt; grant replication slave on *.* to &apos;rep&apos;@&apos;192.168.200.%&apos; identified by &apos;nick&apos;;Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)# 检查创建的rep账号：mysql&gt; select user,host from mysql.user;+-----+-------------+| user | host |+-----+--------------+| root | 127.0.0.1 || rep | 192.168.200.% || root | localhost || root | localhost.localdomain |+-----+------------------+7 rows in set (0.00 sec) 5.备份主库，及恢复到从库把主库现有数据备份下来，再恢复到从库，此时两个主机的数据一致如果事先有数据的话，这不不能忘。12345678910111213141516171819202122232425262728293031323334353637383940414243444546#1)在主库上加锁，使只有只读权限。mysql&gt; flush table with read lock;Query OK, 0 rows affected (0.00 sec)#5.1、5.5锁表命令略有不同。# 5.1锁表：flush tables with read lock;# 5.5锁表：flush table with read lock;#2)记住就是这个点备份的。mysql&gt; show master status;+-------+------+--------+---------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------+------+--------+---------+| mysql-bin.000013 | 410 | | |+-------+------+--------+---------+1 row in set (0.00 sec)#3)克隆窗口，备份数据。[root@bogon ~]# mysqldump -uroot -p -S /data/3306/mysql.sock -A -B --events --master-data=2|gzip &gt;/opt/rep.sql.gzEnter password:参数： -A：备份所有的#看rep.sql.gz参数vim /opt/rep.sql.gz-- CHANGE MASTER TO MASTER_LOG_FILE=&apos;mysql-bin.000013&apos;, MASTER_LOG_POS=410;#4)查看master status；数值是否正常。mysql&gt; show master status;+------+------+---------+-------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+-------+-----+---------+--------+| mysql-bin.000013 | 410 | | |+--------+----+---------+--------+1 row in set (0.00 sec)#5)解锁库mysql&gt; unlock tables;Query OK, 0 rows affected (0.00 sec)#6)恢复到从库[root@bogon ~]# gunzip &lt; /opt/rep.sql.gz | mysql -uroot -p -S /data/3307/mysql.sockEnter password: 6.配置从库及生效更改从库和主库的连接参数，配置生效。检查就成功了！123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990#1)进入从库。[root@bogon ~]# mysql -uroot -p -S /data/3307/mysql.sockEnter password:#2)更改从属服务器用于与主服务器进行连接和通讯的参数。mysql&gt; CHANGE MASTER TO MASTER_HOST=&apos;192.168.200.98&apos;, MASTER_PORT=3306, MASTER_USER=&apos;rep&apos;, MASTER_PASSWORD=&apos;nick&apos;, MASTER_LOG_FILE=&apos;mysql-bin.000013&apos;, MASTER_LOG_POS=410;Query OK, 0 rows affected (0.01 sec)#3)查看更改的参数。[root@localhost ~]# cd /data/3307/data/[root@localhost data]# cat master.info18mysql-bin.000013410192.168.200.98REPnick330660001800.0000#4)生效！mysql&gt; start slave;Query OK, 0 rows affected (0.01 sec)#5)检查下列参数，符合则正常！mysql&gt; show slave status\\GRelay_Master_Log_File: mysql-bin.000013 Slave_IO_Running: Yes #取logo。 Slave_SQL_Running: Yes #读relay-bin、logo,写数据。Seconds_Behind_Master: 0 #落后主库的秒数。#6)查看relay-bin.logo(因为同步的过程中会有中继日志,即relay-bin.xxx日志的生成,所以查看是否已经生成)[root@localhost 3307]# cd /data/3307[root@localhost 3307]# ll总用量 48drwxr-xr-x. 9 mysql mysql 4096 10月 29 18:52 data-rw-r--r--. 1 mysql mysql 1900 10月 29 11:45 my.cnf-rwx------. 1 root root 1307 10月 20 17:06 mysql-rw-rw----. 1 mysql mysql 6 10月 29 11:00 mysqld.pid-rw-r-----. 1 mysql mysql 15090 10月 29 18:49 mysql_nick3307.errsrwxrwxrwx. 1 mysql mysql 0 10月 29 11:00 mysql.sock-rw-rw----. 1 mysql mysql 150 10月 29 18:49 relay-bin.000001-rw-rw----. 1 mysql mysql 340 10月 29 18:52 relay-bin.000002-rw-rw----. 1 mysql mysql 56 10月 29 18:49 relay-bin.index-rw-rw----. 1 mysql mysql 53 10月 29 18:52 relay-log.info#7)查看relay-log.info。[root@localhost 3307]# cat relay-log.info/data/3307/relay-bin.000002340mysql-bin.000013497#8)查看master.info。[root@localhost 3307]# cat data/master.info18mysql-bin.000013497192.168.200.98repnick330660001800.0000 7.测试在主库添加数据,在从库看是否同步过来 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#进入3306(Master)mysql&gt; use test;Database changedmysql&gt;mysql&gt; show tables;Empty set (0.00 sec) mysql&gt; create table student(id int, name varchar(30));Query OK, 0 rows affected (0.03 sec) mysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| student |+----------------+1 row in set (0.00 sec) #插入一条记录mysql&gt; insert into student values(1,&quot;zhangsan2222222&quot;);Query OK, 1 row affected (0.04 sec) #进入3307mysql&gt; use test;Database changedmysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| student |+----------------+1 row in set (0.00 sec) #查询数据mysql&gt; select *from student;+------+-----------------+| id | name |+------+-----------------+| 1 | zhangsan2222222 | #同步成功+------+-----------------+1 row in set (0.00 sec) mysql&gt;","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysqlsla日志分析工具","date":"2017-04-16T04:47:25.703Z","path":"2017/04/16/mysql/mysqlsla日志分析工具/","text":"&emsp;一款帮助语句分析、过滤、分析和排序MySQL慢日志、查询日志、二进制日志和microslow patched日志的分析工具。整体来说, 功能非常强大. 数据报表,非常有利于分析慢查询的原因, 包括执行频率, 数据量, 查询消耗等。 1.下载mysqlsla123[root@localhost tmp]# wget http://hackmysql.com/scripts/mysqlsla-2.03.tar.gz#在目录下已经下载好了，直接上传即可。 2.解压12[root@localhost tmp]# tar -zxvf mysqlsla-2.03.tar.gz[root@localhost tmp]# cd mysqlsla-2.03 3.执行Perl脚本检查包依赖关系123456789101112131415161718[root@localhost mysqlsla-2.03]# perl Makefile.PLChecking if your kit is complete...Looks goodWriting Makefile for mysqlsla @如果依赖检查报错：[root@localhost mysqlsla-2.03]# perl Makefile.PLCan&apos;t locate ExtUtils/MakeMaker.pm in @INC (@INC contains: /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at Makefile.PL line 2.BEGIN failed--compilation aborted at Makefile.PL line 2.@错误2[root@MySQL mysqlsla-2.03]# mysqlsla -lt slow /data/3309/slow.logCan&apos;t locate Time/HiRes.pm in @INC (@INC contains: /usr/local/lib/perl5 /usr/local/share/perl5 /usr/lib/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib/perl5 /usr/share/perl5 .) at /usr/local/bin/mysqlsla line 2095.BEGIN failed--compilation aborted at /usr/local/bin/mysqlsla line 2095. 解决方法如下：yum -y install perl-develyum -y install perl-CPANyum install perl-Time-HiRes -y 4.安装1[root@localhost mysqlsla-2.03]# make &amp;&amp; make install; 5.简单使用1234567891011121314151617181920212223242526272829303132语法：Slow log: mysqlsla -lt slow slow.logGeneral log: mysqlsla -lt general general.logBinary log: mysqlbinlog bin.log | mysqlsla -lt binary - 这里以slow log为例：[root@localhost mysqlsla-2.03]# mysqlsla -lt slow /tmp/127_slow.log | more——————————————————————————————————Report for slow logs: /tmp/127_slow.log24 queries total, 6 uniqueSorted by &apos;t_sum&apos;Grand Totals: Time 16 s, Lock 1 s, Rows sent 18, Rows Examined 2.10M ______________________________________________________________________ 001 ___Count : 18 (75.00%)Time : 15 s total, 833.333 ms avg, 0 to 8 s max (93.75%) 95% of Time : 7 s total, 411.765 ms avg, 0 to 4 s maxLock Time (s) : 0 total, 0 avg, 0 to 0 max (0.00%) 95% of Lock : 0 total, 0 avg, 0 to 0 maxRows sent : 0 avg, 0 to 0 max (0.00%)Rows examined : 116.51k avg, 8 to 1.05M max (99.99%)Database :Users : root@localhost : 100.00% (18) of query, 100.00% (24) of all users Query abstract:INSERT INTO t2 SELECT * FROM t2; Query sample:insert into t2 select * from t2;........ 各个字段说明1234567891011121314总查询次数 (queries total)， 去重后的sql数量 (unique)输出报表的内容排序(sorted by)最重大的慢sql统计信息, 包括 平均执行时间, 等待锁时间, 结果行的总数, 扫描的行总数.Count, sql的执行次数及占总的slow log数量的百分比.Time, 执行时间, 包括总时间, 平均时间, 最小, 最大时间, 时间占到总慢sql时间的百分比.95% of Time, 去除最快和最慢的sql, 覆盖率占95%的sql的执行时间.Lock Time, 等待锁的时间.95% of Lock , 95%的慢sql等待锁时间.Rows sent, 结果行统计数量, 包括平均, 最小, 最大数量.Rows examined, 扫描的行数量.Database, 属于哪个数据库Users, 哪个用户,IP, 占到所有用户执行的sql百分比Query abstract, 抽象后的sql语句Query sample, sql语句 6.sla常用参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051mysqlsla常用参数说明：1) -log-type (-lt) type logs:通过这个参数来制定log的类型，主要有slow, general, binary, msl, udl,分析slow log时通过制定为slow. 2) -sort:制定使用什么参数来对分析结果进行排序，默认是按照t_sum来进行排序。t_sum:按总时间排序c_sum:按总次数排序c_sum_p: sql语句执行次数占总执行次数的百分比。 3) -top:显示sql的数量，默认是10,表示按规则取排序的前多少条 4) –statement-filter (-sf) [+-][TYPE]:过滤sql语句的类型，比如select、update、drop.[TYPE]有SELECT, CREATE, DROP, UPDATE, INSERT，例如&quot;+SELECT,INSERT&quot;，不出现的默认是-，即不包括。 5) db：要处理哪个库的日志： 例如，只取backup库的select语句、按c_sum_p排序的前2条记录 [root@localhost mysqlsla-2.03]# mysqlsla -lt slow -sort c_sum_p -sf &quot;+select&quot; -db backup -top 2 /tmp/127_slow.logReport for slow logs: /tmp/127_slow.log4 queries total, 3 uniqueSorted by &apos;c_sum_p&apos;Grand Totals: Time 1 s, Lock 1 s, Rows sent 18, Rows Examined 195______________________________________________________________________ 001 ___Count : 2 (50.00%)Time : 0 total, 0 avg, 0 to 0 max (0.00%)Lock Time (s) : 0 total, 0 avg, 0 to 0 max (0.00%)Rows sent : 1 avg, 1 to 1 max (11.11%)Rows examined : 86 avg, 77 to 94 max (87.69%)Database :Users : root@localhost : 100.00% (2) of query, 100.00% (4) of all users Query abstract:SELECT SUM(format(duration,N)) AS duration FROM information_schema.profiling WHERE query_id=N; Query sample:select sum(format(duration,6)) as duration from information_schema.profiling where query_id=7;______________________________________________________________________ 002 ___Count : 1 (25.00%)Time : 1 s total, 1 s avg, 1 s to 1 s max (100.00%)Lock Time (s) : 1 s total, 1 s avg, 1 s to 1 s max (100.00%)Rows sent : 4 avg, 4 to 4 max (22.22%)Rows examined : 12 avg, 12 to 12 max (6.15%)Database :Users : root@localhost : 100.00% (1) of query, 100.00% (4) of all users//。。。。。。。。。。。。。。。","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"mysqldump备份工具详解","date":"2017-04-16T04:47:25.701Z","path":"2017/04/16/mysql/mysqldump备份工具详解/","text":"1.逻辑备份和物理备份 逻辑备份: 小于50G的数据量&emsp;&emsp;原理: 将数据库的数据以逻辑的SQL语句的方式导出 物理备份:&emsp;&emsp;1.scp /application/mysql/data/ 拷贝到独立数据库上就可以&emsp;&emsp;2.xtrabackup开源的物理备份工具 2.三种方式来调用mysqldump123456789#Usage: mysqldump [OPTIONS] database [tables]#OR mysqldump [OPTIONS] --databases [OPTIONS] DB1 [DB2 DB3...]#OR mysqldump [OPTIONS] --all-databases [OPTIONS] #如果没有指定任何表或使用了--database或--all--database选项，则转储整个数据库。#要想获得你的版本的mysqldump支持的选项，执行mysqldump --help 3.mysqldump参数详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071--help，-？ #显示帮助消息并退出 --add-drop--database #在每个CREATE DATABASE语句前添加DROP DATABASE语句 --add-drop-tables #在每个CREATE TABLE语句前添加DROP TABLE语句 --add-locking #用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快 --all--database，-A #转储所有数据库中的所有表。与使用---database选项相同，在命令行中命名所有数据库--allow-keywords #允许创建关键字列名。应在每个列名前面加上表名前缀--comments[=&#123;0|1&#125;] #如果设置为 0，禁止转储文件中的其它信息，例如程序版本、服务器版本和主机。--skip—comments与---comments=0的结果相同。 默认值为1，即包括额外信息--compact #产生少量输出。该选项禁用注释并启用--skip-add-drop-tables、--no-set-names、--skip-disable-keys和--skip-add-locking选项。适合调试输出，生产不适用--compatible=name #产生与其它数据库系统或旧的MySQL服务器更兼容的输出。值可以为ansi、mysql323、mysql40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_tables_options或者no_field_options。要使用几个值，用逗号将它们隔开。这些值与设置服务器SQL模式的相应选项有相同的含义--compress，-C #压缩在客户端和服务器之间发送的所有信息（如果二者均支持压缩）--database，-B #转储几个数据库。通常情况，mysqldump将命令行中的第1个名字参量看作数据库名，后面的名看作表名。使用该选项，它将所有名字参量看作数据库名。CREATE DATABASE IF NOT EXISTS db_name和USE db_name语句包含在每个新数据库前的输出中--default-character-set=charset #使用charsetas默认字符集。如果没有指定，mysqldump使用utf8 mysqldump -uroot -p&apos;chenyansong&apos; --default-character-set=utf8 student &gt;/opt/mysql_bak.sql--flush-logs，-F #开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。请注意如果结合--all--database(或-A)选项使用该选项，根据每个转储的数据库刷新日志。例外情况是当使用--lock-all-tables或--master-data的时候：在这种情况下，日志只刷新一次，在所有 表被锁定后刷新。如果你想要同时转储和刷新日志，应使用--flush-logs连同--lock-all-tables或--master-data--host=host_name，-h host_name #从给定主机的MySQL服务器转储数据。默认主机是localhost--lock-all-tables，-x #所有数据库中的所有表加锁。在整体转储过程中通过全局读锁定来实现。该选项自动关闭--single-transaction和--lock-tables--master-data[=value] #该选项将二进制日志的位置和文件名写入到输出中。该选项要求有RELOAD权限，并且必须启用二进制日志。如果该选项值等于1，位置和文件名被写入CHANGE MASTER语句形式的转储输出，如果你使用该SQL转储主服务器以设置从服务器，从服务器从主服务器二进制日志的正确位置开始。如果选项值等于2，CHANGE MASTER语句被写成SQL注释。如果value被省略，这是默认动作 #--master-data选项启用--lock-all-tables，除非还指定--single-transaction(在这种情况下，只在刚开始转储时短时间获得全局读锁定。又见--single-transaction。在任何一种情况下，日志相关动作发生在转储时。该选项自动关闭--lock-tables--no-create-info，-t #只分表数据-t,不写重新创建每个转储表的CREATE TABLE语句--no-data，-d #备份表结构-d,不写表的任何行信息。如果你只想转储表的结构这很有用--password[=password]，-p[password] #连接服务器时使用的密码。如果你使用短选项形式(-p)，不能在选项和密码之间有一个空格。如果在命令行中，忽略了--password或-p选项后面的 密码值，将提示你输入一个--port=port_num，-P port_num #用于连接的TCP/IP端口号--quick，-q #该选项用于转储大的表。它强制mysqldump从服务器一次一行地检索表中的行而不是检索所有行并在输出前将它缓存到内存中--where=&apos;where-condition&apos;, -w &apos;where-condition&apos; #只转储给定的WHERE条件选择的记录。请注意如果条件包含命令解释符专用空格或字符，一定要将条件引用起来。 #例如：&quot;--where=user=&apos;jimf&apos;&quot; &quot;-w userid&gt;1&quot; &quot;-wuserid&lt;1&quot;","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"explain优化慢查询语句","date":"2017-04-16T04:47:25.700Z","path":"2017/04/16/mysql/explain优化慢查询语句/","text":"1.执行计划 — EXPLAIN命令&emsp;执行计划是语句优化的主要切入点，通过执行计划的判读了解语句的执行过程。在执行计划生成方面，MySQL与Oracle明显不同，它不会缓存执行计划，每次都执行“硬解析”。查看执行计划的方法，就是使用EXPLAIN命令。 1.1.基本语法1234567891011121314EXPLAIN QUERY#当在一个Select语句前使用关键字EXPLAIN时，MySQL会解释了即将如何运行该Select语句，它显示了表如何连接、连接的顺序等信息EXPLAIN EXTENDED QUERY#当使用EXTENDED关键字时，EXPLAIN产生附加信息，可以用SHOW WARNINGS浏览。该信息显示优化器限定SELECT语句中的表和列名，重写并且执行优化规则后SELECT语句是什么样子，并且还可能包括优化过程的其它注解。在MySQL5.0及更新的版本里都可以使用，在MySQL5.1里它有额外增加了一个过滤列(filtered)。EXPLAIN PARTITIONS QUERY#显示的是查询要访问的数据分片——如果有分片的话。它只能在MySQL5.1及更新的版本里使用EXPLAIN FORMAT=JSON (5.6新特性)#另一个格式显示执行计划。可以看到诸如表间关联方式等信息。 1.2.EXPLAIN输出字段 id &emsp;MySQL选定的执行计划中查询的序列号。如果语句里没有子查询等情况，那么整个输出里就只有一个SELECT，这样一来每一行在这个列上都会显示一个1。如果语句中使用了子查询、集合操作、临时表等情况，会给ID列带来很大的复杂性。如上例中，WHERE部分使用了子查询，其id=2的行表示一个关联子查询。1234567mysql&gt; explain select * from (select * from student) as a;+----+-------------+------------+--------+---------------+------+---------+------+------+-------+| id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |+----+-------------+------------+--------+---------------+------+---------+------+------+-------+| 1 | PRIMARY | &lt;derived2&gt; | system | NULL | NULL | NULL | NULL | 1 | || 2 | DERIVED | student | ALL | NULL | NULL | NULL | NULL | 1 | |+----+-------------+------------+--------+---------------+------+---------+------+------+-------+ select_type &emsp;语句所使用的查询类型。是简单SELECT还是复杂SELECT(如果是后者，显示它属于哪一种复杂类型)。常用有以下几种标记类型。 DEPENDENT SUBQUERY子查询内层的第一个SELECT，依赖于外部查询的结果集 DEPENDENT UNION子查询中的UNION，且为UNION中从第二个SELECT开始的后面所有SELECT，同样依赖于外部查询的结果集 PRIMARY子查询中的最外层查询，注意并不是主键查询 SIMPLE除子查询或UNION之外的其他查询 SUBQUERY子查询内层查询的第一个SELECT，结果不依赖于外部查询结果集 UNCACHEABLE SUBQUERY结果集无法缓存的子查询 UNIONUNION语句中的第二个SELECT开始后面的所有SELECT，第一个SELECT为PRIMARY UNION RESULTUNION中的合并结果。从UNION临时表获取结果的SELECT DERIVED衍生表查询(FROM子句中的子查询)。MySQL会递归执行这些子查询，把结果放在临时表里。在内部，服务器就把当做一个”衍生表”那样来引用，因为临时表就是源自子查询 table &emsp;这一步所访问的数据库中表的名称或者SQL语句指定的一个别名表。这个值可能是表名、表的别名或者一个为查询产生的临时表的标识符，如派生表、子查询或集合。 type 表的访问方式。以下列出了各种不同类型的表连接，依次是从最好的到最差的 system 系统表，表只有一行记录。这是const表连接类型的一个特例 const 读常量，最多只有一行匹配的记录。由于只有一行记录，优化程序里该行记录的字段值可以被当作是一个恒定值。const用于在和PRIMARY KEY或UNIQUE索引中有固定值比较的情形 eq_ref 最多只会有一条匹配结果，一般是通过主键或唯一键索引来访问。从该表中会有一行记录被读取出来以和从前一个表中读取出来的记录做联合。与const类型不同的是，这是最好的连接类型。它用在索引所有部分都用于做连接并且这个索引是一个PRIMARY KEY或UNIQUE类型。eq_ref可以用于在进行”=”做比较时检索字段。比较的值可以是固定值或者是表达式，表达示中可以使用表里的字段，它们在读表之前已经准备好了。 ref JOIN语句中驱动表索引引用的查询。该表中所有符合检索值的记录都会被取出来和从上一个表中取出来的记录作联合。ref用于连接程序使用键的最左前缀或者是该键不是PRIMARY KEY或UNIQUE索引(换句话说，就是连接程序无法根据键值只取得一条记录)的情况。当根据键值只查询到少数几条匹配的记录时，这就是一个不错的连接类型。ref还可以用于检索字段使用”=”操作符来比较的时候。 ref_or_null 与ref的唯一区别就是在使用索引引用的查询之外再增加一个空值的查询。这种连接类型类似ref，不同的是MySQL会在检索的时候额外的搜索包含NULL值的记录。这种连接类型的优化是从MySQL 4.1.1开始的，它经常用于子查询。 index_merge 查询中同时使用两个(或更多)索引，然后对索引结果进行合并(merge)，再读取表数据。这种连接类型意味着使用了Index Merge优化方法。 unique_subquery 子查询中的返回结果字段组合是主键或唯一约束。 index_subquery 子查询中的返回结果字段组合是一个索引(或索引组合)，但不是一个主键或唯一索引。这种连接类型类似unique_subquery。它用子查询来代替IN，不过它用于在子查询中没有唯一索引的情况下。 range 索引范围扫描。只有在给定范围的记录才会被取出来，利用索引来取得一条记录。 index 全索引扫描。连接类型跟ALL一样，不同的是它只扫描索引树。它通常会比ALL快点，因为索引文件通常比数据文件小。MySQL在查询的字段知识单独的索引的一部分的情况下使用这种连接类型。 fulltext 全文索引扫描。 all 全表扫描。 possible_keys 该字段是指MySQL在搜索表记录时可能使用哪个索引。如果没有任何索引可以使用，就会显示为null。 key 查询优化器从possible_keys中所选择使用的索引。key字段显示了MySQL实际上要用的索引。当没有任何索引被用到的时候，这个字段的值就是NULL key_len 被选中使用索引的索引键长度。key_len字段显示了MySQL使用索引的长度。当key字段的值为NULL时，索引的长度就是NULL。 ref 列出是通过常量，还是某个表的某个字段来过滤的。ref字段显示了哪些字段或者常量被用来和key配合从表中查询记录出来。 rows 该字段显示了查询优化器通过系统收集的统计信息估算出来的结果集记录条数 Extra 该字段显示了查询中MySQL的附加信息 filtered 这个列式在MySQL5.1里新加进去的，当使用EXPLAIN EXTENDED时才会出现。它显示的是针对表里符合某个条件(WHERE子句或联接条件)的记录数的百分比所作的一个悲观估算 2.sql优化 Where子句中使用独立的列 查询中列如果不是独立的，则不会使用索引 关联查询优化 确保ON或者USING子句的列上有索引。一般只需要在关联顺序中的第二个表的相应列上创建索引。 关联字段类型保持一致。 LIKE匹配优化 如果 LIKE 的参数是非通配字符开始的固定字符串，MySQL在做LIKE比较时也可能用到索引 Extra信息中显示使用了索引。like后面使用通配符开始的字符串则不会使用索引 rows列显示599行，也就是customer表的总行数，因此没利用到索引。 避免SQL中出现不必要的类型转换 Select指定列来代替select * 在某些情况下 select * 要比select 指定列 需要浪费更多的资源 如果某些列中含有text等类型，select 指定列可以减少网络传输缓冲区的使用 如果SQL中含有order by ,并且排序不能利用上已用的索引那么，额外的字段会占用更多的sort_buffer_size . Select指定列可以方便使用覆盖索引。 比如下面这个例子，使用到了覆盖索引。 子查询优化 MySQL5.6前，子查询大多时候会先遍历outer table，对于其返回的每一条记录都执行一次subquery，而且子查询没有任何索引，导致子查询相较于关联查询要慢很多（解决方案：表连接代替子查询）； MySQL5.6 后，对子查询进行了大幅度的优化，将子查询结果存入临时表，使得子查询只执行一次，而且优化器还会给子查询产生的派生表添加索引，使得子查询性能得到了强劲的优化。 曾经的“绝对真理”：子查询比关联查询慢很多。——不再成立。 通过子查询优化可以减少多个查询多次对数据进行访问。 但也有时候，子查询可能比关联查询还要快。 GROUP BY优化 表的标识列分组比其他列分组的效率高。1SELECT actor.first_name, actor.last_name, count(*) FROM film_actor INNER JOIN actor USING (actor_id) GROUP BY actor.first_name, actor.last_name; 优化后：1SELECT actor.first_name, actor.last_name,count(*) FROM film_actor INNER JOIN actor USING (actor_id) GROUP BY actor.actor_id ; 因为actor.actor_id是主键，分组效率会提升。 使用GROUP BY子句时，结果集会自动按照分组的字段进行排序，GROUP BY子句中可以直接使用DESC或者ASC关键字，使得分组的结果集按需要的方向排序。 So：如果没有排序需求，可以加ORDER BY NULL,让MySQL不再进行文件排序，从而提高查询效率。 UNION优化 除非需要消除重复的行，否则一定要使用union all，因为没有ALL关键字，MySQL会给临时表加上DISTINCT选项，使得对整个临时表做代价很高的唯一性检查。 由于union产生的临时表无法使用优化器的优化策略，所以可以直接将WHERE, ORDER BY, LIMIT等子句冗余的写一份到各个子查询中。 如果把ORDER BY, LIMIT等子句冗余写一份到各个子查询中。 则排序的基数会有效的得到降低，从而提高效率。 3.抓慢查询1234567891011#a.show full processlist; 或者 [root@MySQL 3309]# mysql -uroot -poldboy123 -S /data/3309/mysql.sock -e &quot;show full processlist;&quot; 用grep 去抓取特定的语句[root@MySQL 3309]# mysql -uroot -poldboy123 -S /data/3309/mysql.sock -e &quot;show full processlist;&quot;|grep select -i#b.分析慢查询日志（下面三行在my.cnf中添加） long_query_time =1 //查询超过1s log-slow-queries = /data/3306/slow.log //慢查询日志 log_queries_not_using_indexes //没有索引的 4.explain语句检查索引执行情况5.对需要建立索引的列建立索引6.使用慢查询工具（每天早晨发邮件，自动化的）查看语句的详细执行时间123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051Examples:mysql&gt; SELECT @@profiling; //查看是否开启+-------------+| @@profiling |+-------------+| 0 |+-------------+1 row in set (0.00 sec) mysql&gt; SET profiling = 1; //设置开启Query OK, 0 rows affected (0.00 sec) mysql&gt; DROP TABLE IF EXISTS t1;Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; CREATE TABLE T1 (id INT);Query OK, 0 rows affected (0.01 sec) mysql&gt; SHOW PROFILES; //显示所有查询记录的时间+----------+----------+--------------------------+| Query_ID | Duration | Query |+----------+----------+--------------------------+| 0 | 0.000088 | SET PROFILING = 1 || 1 | 0.000136 | DROP TABLE IF EXISTS t1 || 2 | 0.011947 | CREATE TABLE t1 (id INT) |+----------+----------+--------------------------+3 rows in set (0.00 sec)//查询某一条记录的各项消耗时间mysql&gt; SHOW PROFILE FOR QUERY 1;+--------------------+----------+| Status | Duration |+--------------------+----------+| query end | 0.000107 || freeing items | 0.000008 || logging slow query | 0.000015 || cleaning up | 0.000006 |+--------------------+----------+4 rows in set (0.00 sec) mysql&gt; SHOW PROFILE CPU FOR QUERY 2;+----------------------+----------+----------+------------+| Status | Duration | CPU_user | CPU_system |+----------------------+----------+----------+------------+| checking permissions | 0.000040 | 0.000038 | 0.000002 || creating table | 0.000056 | 0.000028 | 0.000028 || After create | 0.011363 | 0.000217 | 0.001571 || query end | 0.000375 | 0.000013 | 0.000028 || freeing items | 0.000089 | 0.000010 | 0.000014 || logging slow query | 0.000019 | 0.000009 | 0.000010 || cleaning up | 0.000005 | 0.000003 | 0.000002 |+----------------------+----------+----------+------------+ 参考:解开发者之痛：中国移动MySQL数据库优化最佳实践如何用一款小工具大大加速MySQL SQL语句优化","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"Amoeba实现mysql读写分离(转)","date":"2017-04-16T04:47:25.698Z","path":"2017/04/16/mysql/Amoeba实现mysql读写分离(转)/","text":"转载自 机器配置说明 机器 说明 172.16.1.51 Amoeba 172.16.1.52 mysql多实例 172.16.1.10 测试机 创建用户首先在172.16.1.52服务器完成mysql主从复制，启用3306，3307多实例12345# 1.在主库创建一个用户 grant select,insert,update,delete on *.* to amoeba@&apos;172.16.1.%&apos; identifieentified by &apos;lx&apos;;# 2.在从库回收权限 REVOKE insert,update,delete on *.* FROM &apos;amoeba&apos;@&apos;172.16.1.%&apos;; flush privileges; 环境支持Amoeba框架是基于Java SE1.5开发的，建议使用java SE1.5以上的版本 123456789#下载安装jdkwet http://download.oracle.com/otn-pub/java/jdk/7u80-b15/jdk-7u80-linux-x64.rpmcd /server/toolsrpm -ivh jdk-7u80-linux-x64.rpmln -s /usr/java/jdk1.7.0_80/ /usr/java/r/java/jdk1.7#加入环境变量vim /etc/profileexport JAVA_HOME=/usr/java/jdk1.7export PATH=$PATH_HOME/bin:$PATH_HOME/jre/bin:$PATH. /etc/profile amoeba安装12345678910111213141516171819#下载amoeba-mysql-binary-2.2.0.tar.gz wget http://sourceforge.net/projects/amoeba/files/Amoeba%20for%20mysql/2.2.x/amoeba-mysql-binary-2.2.0.tar.gz/downloadtar xf amoeba-mysql-binary-2.2.0.tar.gz#解压安装mkdir /application/amoebatar -zxvf amoeba-mysql-binary-2.2.0.tar.gz -C /application/amoebacd /application/amoeba[root@lamp01 amoeba]# ll总用量 60drwxr-xr-x 2 root root 4096 2月 18 09:59 benchmarkdrwxr-xr-x 2 root root 4096 2月 29 2012 bin-rw-r--r-- 1 root root 3976 8月 29 2012 changelogs.txtdrwxr-xr-x 2 root root 4096 2月 18 09:59 confdrwxr-xr-x 3 root root 4096 2月 18 09:59 lib-rw-r--r-- 1 root root 34520 8月 29 2012 LICENSE.txt-rw-r--r-- 1 root root 2031 8月 29 2012 README.html 修改配置文件amoeba的配置是基于XML的配置文件1234567891011121314151617181920212223242526272829303132#cd /application/amoeba/conf/#vim amoeba.xml &lt;service name=&quot;Amoeba for Mysql&quot; class=&quot;com.meidusa.amoeba.net.ServerableConnectionManager&quot;&gt; &lt;!-- port --&gt; &lt;property name=&quot;port&quot;&gt;3306&lt;/property&gt; #修改amoeba启动端口 &lt;!-- bind ipAddress --&gt; &lt;property name=&quot;ipAddress&quot;&gt;172.16.1.51&lt;/property&gt; #修改为amoeba服务器ip地址 &lt;property name=&quot;manager&quot;&gt;$&#123;clientConnectioneManager&#125;&lt;/property&gt; &lt;property name=&quot;connectionFactory&quot;&gt; &lt;bean class=&quot;com.meidusa.amoeba.mysql.net.MysqlClientConnectionFactory&quot;&gt; &lt;property name=&quot;sendBufferSize&quot;&gt;128&lt;/property&gt; &lt;property name=&quot;receiveBufferSize&quot;&gt;64&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name=&quot;authenticator&quot;&gt; &lt;bean class=&quot;com.meidusa.amoeba.mysql.server.MysqlClientAuthenticator&quot;&gt; &lt;property name=&quot;user&quot;&gt;amoeba&lt;/property&gt; #定义用户 &lt;property name=&quot;password&quot;&gt;lx&lt;/property&gt; #定义密码 &lt;property name=&quot;filter&quot;&gt; &lt;bean class=&quot;com.meidusa.amoeba.server.IPAccessController&quot;&gt; &lt;property name=&quot;ipFile&quot;&gt;$&#123;amoeba.home&#125;/conf/access_list.conf&lt;/property&gt; &lt;/bean&gt; &lt;/property&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/service&gt; &lt;property name=&quot;defaultPool&quot;&gt;master&lt;/property&gt; #修改amoeba指向后端节点主数据库&lt;property name=&quot;writePool&quot;&gt;master&lt;/property&gt;&lt;property name=&quot;readPool&quot;&gt;slave&lt;/property&gt; 编辑dbServers.xml文件,添加3306,3307多实例vim dbServers.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;dbServer name=&quot;abstractServer1&quot; abstractive=&quot;true&quot;&gt; 将 abstractServer 修改为 abstractServer1 &lt;factoryConfig class=&quot;com.meidusa.amoeba.mysql.net.MysqlServerConnectionFactory&quot;&gt; &lt;property name=&quot;manager&quot;&gt;$&#123;defaultManager&#125;&lt;/property&gt; &lt;property name=&quot;sendBufferSize&quot;&gt;64&lt;/property&gt; &lt;property name=&quot;receiveBufferSize&quot;&gt;128&lt;/property&gt; &lt;!-- mysql port --&gt; &lt;property name=&quot;port&quot;&gt;3306&lt;/property&gt; #3306端口 &lt;!-- mysql schema --&gt; &lt;property name=&quot;schema&quot;&gt;test&lt;/property&gt; #注意查看主从数据库是否存在test数据库 &lt;!-- mysql user --&gt; &lt;property name=&quot;user&quot;&gt;amoeba&lt;/property&gt; #用户名 &lt;!-- mysql password --&gt; &lt;property name=&quot;password&quot;&gt;lx&lt;/property&gt; #密码 &lt;/factoryConfig&gt; &lt;poolConfig class=&quot;com.meidusa.amoeba.net.poolable.PoolableObjectPool&quot;&gt; &lt;property name=&quot;maxActive&quot;&gt;500&lt;/property&gt; &lt;property name=&quot;maxIdle&quot;&gt;500&lt;/property&gt; &lt;property name=&quot;minIdle&quot;&gt;10&lt;/property&gt; &lt;property name=&quot;minEvictableIdleTimeMillis&quot;&gt;600000&lt;/property&gt; &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot;&gt;600000&lt;/property&gt; &lt;property name=&quot;testOnBorrow&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;testOnReturn&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;testWhileIdle&quot;&gt;true&lt;/property&gt; &lt;/poolConfig&gt;&lt;/dbServer&gt;&lt;dbServer name=&quot;abstractServer2&quot; abstractive=&quot;true&quot;&gt; 将 abstractServer 修改为 abstractServer2 &lt;factoryConfig class=&quot;com.meidusa.amoeba.mysql.net.MysqlServerConnectionFactory&quot;&gt; &lt;property name=&quot;manager&quot;&gt;$&#123;defaultManager&#125;&lt;/property&gt; &lt;property name=&quot;sendBufferSize&quot;&gt;64&lt;/property&gt; &lt;property name=&quot;receiveBufferSize&quot;&gt;128&lt;/property&gt; &lt;!-- mysql port --&gt; &lt;property name=&quot;port&quot;&gt;3307&lt;/property&gt; #数据库3307端口号 &lt;!-- mysql schema --&gt; &lt;property name=&quot;schema&quot;&gt;test&lt;/property&gt; #注意查看主从数据库是否存在test数据库 &lt;!-- mysql user --&gt; &lt;property name=&quot;user&quot;&gt;amoeba&lt;/property&gt; #用户帐号 &lt;!-- mysql password --&gt; &lt;property name=&quot;password&quot;&gt;lx&lt;/property&gt; #用户密码 &lt;/factoryConfig&gt; &lt;poolConfig class=&quot;com.meidusa.amoeba.net.poolable.PoolableObjectPool&quot;&gt; &lt;property name=&quot;maxActive&quot;&gt;500&lt;/property&gt; &lt;property name=&quot;maxIdle&quot;&gt;500&lt;/property&gt; &lt;property name=&quot;minIdle&quot;&gt;10&lt;/property&gt; &lt;property name=&quot;minEvictableIdleTimeMillis&quot;&gt;600000&lt;/property&gt; &lt;property name=&quot;timeBetweenEvictionRunsMillis&quot;&gt;600000&lt;/property&gt; &lt;property name=&quot;testOnBorrow&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;testOnReturn&quot;&gt;true&lt;/property&gt; &lt;property name=&quot;testWhileIdle&quot;&gt;true&lt;/property&gt; &lt;/poolConfig&gt;&lt;/dbServer&gt; 修改bin目录的权限1chmod -R 700 /Application/amoeba/bin/ 1234#vim /application/amoeba/bin/amoeba#添加DEFAULT_OPTS=&quot;-server -Xms256m -Xmx256m -Xss256k&quot; 启动1234567/application/amoeba/bin/amoeba start#查看端口[root@MySQL-master-01 conf]# lsof -i:3306COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEjava 19474 root 41u IPv6 238156 0t0 TCP MySQL-master-01:39277-&gt;MySQL-master-02:mysql (ESTABLISHED)java 19474 root 53u IPv6 238162 0t0 TCP MySQL-master-01:mysql (LISTEN) 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123#在web服务端172.16.1.10连接amoeba服务器测试[root@web01 ~]# mysql -uamoeba -plx -h 172.16.1.51Welcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 126729963Server version: 5.1.45-mysql-amoeba-proxy-2.2.0 Source distributionCopyright (c) 2000, 2013, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type &apos;help;&apos; or &apos;\\h&apos; for help. Type &apos;\\c&apos; to clear the current input statement.#测试在172.16.1.52主库上面创建一个表mysql&gt; use database ruby;mysql&gt; use ruby;Database changedmysql&gt; create table lx (id int(10),name varchar(10),address varchar(20));Query OK, 0 rows affected (0.08 sec)#停止从库mysql&gt; stop slave;Query OK, 0 rows affected (0.03 sec)#分别在主库和从库插入一条数据#主库插入：mysql&gt; insert into lx values(1,&apos;lx&apos;,&apos;master&apos;);Query OK, 1 row affected (0.01 sec)mysql&gt; select * from lx;+------+------+---------+| id | name | address |+------+------+---------+| 1 | lx | master |+------+------+---------+1 row in set (0.00 sec)#从库插入：mysql&gt; insert into ruby.lx values(1,&apos;lx&apos;,&apos;slave&apos;); Query OK, 1 row affected (0.00 secmysql&gt; select * from ruby.lx;+------+------+---------+| id | name | address |+------+------+---------+| 1 | lx | slave |+------+------+---------+1 row in set (0.00 sec)#在测试服务器172.16.1.10mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || bbs || dedecms || lixiang || mysql || performance_schema || ruby || test || wordpress |+--------------------+9 rows in set (0.00 sec)mysql&gt; use ruby;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+----------------+| Tables_in_ruby |+----------------+| lx |+----------------+1 row in set (0.01 sec)这个时候我们查询到的数据是从库创建的数据mysql&gt; select * from lx;+------+------+---------+| id | name | address |+------+------+---------+| 1 | lx | slave |+------+------+---------+1 row in set (0.00 sec)此时我们在插入一条数据mysql&gt; insert into ruby.lx values(33,&apos;test33&apos;,&apos;test33&apos;);Query OK, 1 row affected (0.01 sec)#返回172.16.1.52服务器，在主库中查看是否有该条数据mysql&gt; select * from lx; +------+--------+---------+| id | name | address |+------+--------+---------+| 1 | lx | master || 33 | test33 | test33 |+------+--------+---------+2 rows in set (0.00 sec)回到172.16.1.52服务器，在从库中查看是否有该条数据mysql&gt; select * from ruby.lx; +------+------+---------+| id | name | address |+------+------+---------+| 1 | lx | slave |+------+------+---------+1 row in set (0.00 sec)mysql&gt; start slave; #从库开启主从同步Query OK, 0 rows affected (0.00 sec)#再次回到测试服务器172.16.1.10，查看数据情况mysql&gt; select * from ruby.lx;+------+--------+---------+| id | name | address |+------+--------+---------+| 1 | lx | slave || 1 | lx | master || 33 | test33 | test33 |+------+--------+---------+3 rows in set (0.01 sec) 至此，如果得到上面的结果则说明mysql数据读写分离完成，此时在回到从数据库开启主从同步 start slave","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"}]},{"title":"磁盘分区,格式化,挂载","date":"2017-04-16T04:47:25.694Z","path":"2017/04/16/Linux/磁盘/磁盘分区,格式化,挂载/","text":"1.磁盘分区工具1.1.fdisk1.1.1.查看所有分区12345678910111213141516171819202122232425262728293031[root@lamp01 ~]# fdisk -l#第一块磁盘Disk /dev/sda: 8589 MB, 8589934592 bytes255 heads, 63 sectors/track, 1044 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x000e7a75 Device Boot Start End Blocks Id System/dev/sda1 * 1 26 204800 83 LinuxPartition 1 does not end on cylinder boundary./dev/sda2 26 91 524288 82 Linux swap / SolarisPartition 2 does not end on cylinder boundary./dev/sda3 91 1045 7658496 83 Linux #第二块磁盘Disk /dev/sdb: 1073 MB, 1073741824 bytes255 heads, 63 sectors/track, 130 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x000315a8 Device Boot Start End Blocks Id System/dev/sdb1 1 13 102400 83 LinuxPartition 1 does not end on cylinder boundary./dev/sdb2 13 77 512000 5 ExtendedPartition 2 does not end on cylinder boundary./dev/sdb5 14 39 204800 83 Linux/dev/sdb6 39 77 305152 83 Linux 1.1.2.给/dev/sdb分区1.1.2.1.参数简绍123456789101112131415161718[root@lamp01 ~]# fdisk /dev/sdb WARNING: DOS-compatible mode is deprecated. It&apos;s strongly recommended to switch off the mode (command &apos;c&apos;) and change display units to sectors (command &apos;u&apos;).#警告的含义:DOS命令行已经过时,采用-c切换命令行,原来默认的磁盘最小单元为柱面,使用-u切换成扇区 Command (m for help): -m # 帮助命令-p #打印分区表-d #删除分区-n #添加分区-t #改版分区类型-l #查看分区类型-q #不保存退出-w #保存退出 1.1.2.2.新建一个扩展分区 1.1.2.3.新建一个逻辑分区 1.1.2.4.改版分区类型123456789101112Command (m for help): L #查看所有的分区类型号 0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 39 Plan 9 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 3c PartitionMagic 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 40 Venix 80286 84 OS/2 hidden C: c6 DRDOS/sec (FAT- 4 FAT16 &lt;32M 41 PPC PReP Boot 85 Linux extended c7 Syrinx 5 Extended 42 SFS 86 NTFS volume set da Non-FS data 6 FAT16 4d QNX4.x 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS 4e QNX4.x 2nd part 88 Linux plaintext de Dell Utility 8 AIX 4f QNX4.x 3rd part 8e Linux LVM df BootIt 9 AIX bootable 50 OnTrack DM 93 Amoeba e1 DOS access 1.1.2.5.保存上面的分区操作 1.1.2.6.使分区/dev/sdb生效(重读分区表)1partprobe /dev/sdb 1.1.3.非交互式,一次执行上面的操作1echo -e “n\\np\\n1\\n\\n+10G\\nn\\np\\n2\\n\\n+20G\\nw” |fdisk /dev/sdb 1.2.parted&emsp;简述：parted是一个磁盘分区的管理工具，他比fdisk更加的灵活，功能也更加的丰富，同时还支持GUID分区表,这在IA64平台上管理磁盘时非常有用，他同时支持交互模式和非交互模式，他除了能够进行分区的添加，删除等常见操作外，还可以移动分区，制作文件系统，调整文件系统的大小。&emsp;GPT分区全名：Globally Unique Identitier Partition Table Format ,指全局唯一标示磁盘分区表格式。由于MBR分区标的最大可寻址的存储空间只有2Tb(232*512字节)，因此在大硬盘出现的现在，MBR分区方式逐渐被GUID分区表取代。 parted 与fdisk区别: 支持gpt分区表,可以对大于2T的磁盘分区 分区直接生效,不需要使用命令写入磁盘,最好partprobe /dev/sdb 格式化挂载等和fdisk无区别 1.2.1交互式1234567891011121314151617[root@lamp01 ~]# parted /dev/sdb #对/dev/sdb进行分区GNU Parted 2.1使用 /dev/sdbWelcome to GNU Parted! Type &apos;help&apos; to view a list of commands.(parted) p #打印 Model: VMware, VMware Virtual S (scsi)Disk /dev/sdb: 1074MBSector size (logical/physical): 512B/512BPartition Table: msdos Number Start End Size Type File system 标志 1 1049kB 106MB 105MB primary ext4 2 106MB 630MB 524MB extended 5 107MB 317MB 210MB logical 6 318MB 630MB 312MB logical (parted) 1.2.2添加分区:非交互式 其中mkfs是格式化,创建文件系统, mount是挂载 1.2.3非交互式:忽略提示 1.2.4.删除一个分区 1.2.5.退出1quit 1.2.6.注意:gpt到msdos的转化1parted /dev/sdb mklabel msdos 2.格式化&emsp;实质：就是创建文件系统1234567891011121314151617帮助：[root@data-1-2 ~]# mkfs.ext4 -helpmkfs.ext4: invalid option -- &apos;h&apos;Usage: mkfs.ext4 [-c|-l filename] [-b block-size] [-f fragment-size] [-i bytes-per-inode] [-I inode-size] [-J journal-options] [-G meta group size] [-N number-of-inodes] [-m reserved-blocks-percentage] [-o creator-os] [-g blocks-per-group] [-L volume-label] [-M last-mounted-directory] [-O feature[,...]] [-r fs-revision] [-E extended-option[,...]] [-T fs-type] [-U UUID] [-jnqvFKSV] device [blocks-count] -L 是指定卷标，相当于给分区取的名字，那么挂载的时候直接用卷标名来挂载[root@data-1-2 ~]# mkfs.ext4 -L MYDATA /dev/sdb3mke2fs 1.41.12 (17-May-2010)Filesystem label=MYDATA ...... 方式一 1[root@linux-study ~]# mkfs.ext4 /dev/sdb1 方式二 1234mkfs -t ext4 /dev/sdb1#可以指定iNode和block的大小,但是一般我们不指定mkfs -t ext4 -I 1024 -b 8192 /dev/sdb1 格式化之后出现的提示： 2次挂载或者180天之后会自动检查，我们可以取消他，如上图 1234567891011121314151617181920212223[root@data-1-2 ~]# mkfs.ext4 /dev/sdb1mke2fs 1.41.12 (17-May-2010)Filesystem label=OS type: LinuxBlock size=1024 (log=0)Fragment size=1024 (log=0)Stride=0 blocks, Stripe width=0 blocks130560 inodes, 522080 blocks26104 blocks (5.00%) reserved for the super user #默认留5%的空间大小给超级管理员，-m指定First data block=1Maximum filesystem blocks=6763315264 block groups #创建了64个块组8192 blocks per group, 8192 fragments per group #每个块组8192个block2040 inodes per groupSuperblock backups stored on blocks: #超级块备份在下面的组中 8193, 24577, 40961, 57345, 73729, 204801, 221185, 401409 Writing inode tables: done Creating journal (8192 blocks): done #写日志完成Writing superblocks and filesystem accounting information: done This filesystem will be automatically checked every 27 mounts or180 days, whichever comes first. Use tune2fs -c or -i to override. 格式化交换分区 12#在格式化之前的分区，要指定分区类型t=82[root@data-1-2 tdir]# mkswap /dev/sdb4 3.挂载分区实质：为文件系统指定访问入口 1234567891011121314151617181920212223242526272829[root@linux-study /]# mount /dev/sdb1 /mnt mount -a #表示挂载/etc/fstab文件中所有的文件系统mount -n #默认情况下，mount命令每挂载一个设备，都会将挂载的设备信息保存至 /etc/mtab而使用-n表示不将信息写入到/etc/mtab文件中mount -t 指定正在挂载的设备的文件系统类型，不指定默认是调用blkid /dev/sdb1来获取文件系统的类型mount -r 只读挂载mount -w 读写挂载mount -o 指定额外的挂载选项 async ：异步写入 atime：跟新inode的访问时间 auto：使用的是-a选项 default：包含：rw/suid/dev/exec/auto/nouser/async#检查挂载情况df -h 查看block大小df -i 查看inode#挂载交换分区[root@data-1-2 tdir]# swapon /dev/sdb4[root@data-1-2 tdir]# swapoff /dev/sdb4#挂载ISO镜像文件mount -o loop /root/tes.iso /mediall /media/#卸载：umount 设备/挂载点 4.设置开机自动挂载开机时将自动挂载/etc/fstab下的挂载项如果没有这一项，再次重启的时候，将不会自动挂载分区，那么将不能写入文件到该分区vim /etc/fatab 添加： 1/dev/sdb1 /mnt ext4 defaults 0 0 5.加载/etc/fstab进行挂载测试（很重要）1234mount -a Mount all filesystems (of the given types) mentioned in fstab.#如果这里不进行测试，那么重启的时候，加载fatab可能机器起不来了，所以很重要 案例&emsp;如果我要将我的一块大硬盘暂时分成四个分区,同时,还希望有其他的空间可以让我在未来需要的时候再进行分区,那么该如何分区?3p+1e(1L) 剩下的空间保留2p+1e(2L) 剩下的空间保留1p+1e(3L) 剩下的空间保留 整理自老男孩","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"磁盘介绍","date":"2017-04-16T04:47:25.692Z","path":"2017/04/16/Linux/磁盘/磁盘介绍/","text":"1.磁盘相关的名词 英文 汉语 Disk 磁盘 Head 磁头 sector 扇区 Track 磁道 Cylinder 柱面 Units 单元快(一个柱面的大小 Block 数据块 INode 索引节点 2.磁盘实体图 &emsp;扇区:盘面由圆心向四周画直线,不同的磁道被直线分成许多的扇形(弧形)的区域,每个弧形的区域叫做扇区,每个扇区大小一般为512字节,扇区看起来就是圆弧或扇形柱面:磁盘中不同的盘片(或盘面)相同半径的磁道轨迹从上到下所组成的圆柱型区域就称为柱面,柱面看起来是一个圆柱形&emsp;扇区的第一个主要部分是标识符,标识符就是扇区头标,包括组成扇区三维地址的三个数字,扇区所在的磁头(或盘面)/磁道(或柱面号)以及扇区在磁道上的位置即扇区号,头标中还包括一个字段,其中有显示扇区是否能可靠存储数据,或者是是否已发现某个故障因而不宜使用的标记,有些磁盘控制器在扇区头标中还记录有指示字,可在原扇区出错时指引磁盘转到替换扇区或磁道,最后,扇区头标以循环冗余校验(CRC)值作为结果以供控制器检验扇区头标的读出情况,确保准确无误&emsp;扇区的第二个主要部分是存储数据的数据段,可分为数据和保护数据的交错码(ECC),在初始化准备期间,计算机用512个虚拟信息字节(实际数据的存放地)和这些虚拟信息字节相应的ECC数字填入这个部分 &emsp;磁道,柱面,扇区总结&emsp;磁盘最基本的组成部分是由坚硬的金属材料制成的涂以磁性介质的盘片(有很多层)不同容量磁盘的盘片数不等 一块磁盘有2-14个盘片,每个盘片有两个面,每个面对应一个读写磁头,用磁头号来区分盘面,即盘面就是磁头数,盘片数*2=磁头数(盘面数) 不同盘面的磁道被划分为多个扇形区域,每个扇形区域就是一个扇区 同一个盘面,以盘片中心为圆心,每个不同半径的圆形轨迹就是一个磁道(Track) 不同盘面相同半径的磁道组成一个圆柱面就是柱面(Cylinder) 一个柱面包含多个磁道(这些磁道半径相同),一个磁道包含多个扇区 数据信息记录可表示为:某磁头,某磁道(柱面),某扇区 3.磁盘容量计算磁盘容量=柱面数(磁道数)柱面大小(磁道大小磁头数)=512B扇区数磁道数*磁头数 12345678910111213141516[root@lamp01 ~]# fdisk -l /dev/sdb Disk /dev/sdb: 1073 MB, 1073741824 bytes255 heads, 63 sectors/track, 130 cylindersUnits = cylinders of 16065 * 512 = 8225280 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk identifier: 0x000315a8 Device Boot Start End Blocks Id System/dev/sdb1 1 13 102400 83 LinuxPartition 1 does not end on cylinder boundary./dev/sdb2 13 77 512000 5 ExtendedPartition 2 does not end on cylinder boundary./dev/sdb5 14 39 204800 83 Linux/dev/sdb6 39 77 305152 83 Linux 4.机械磁盘读写数据的原理 磁盘是按照柱面为单位读写数据的，即先读取同一个盘面的某一个磁道，读完之后，如果数据没有读完，磁头也不会切换到其他的磁道，而是选择切换磁头，读取下一个盘面的相同半径的磁道，知道所有盘面的相同半径的磁道读取完成之后，如果数据还没有读写完成，才会切换其他不同半径的磁道，这个切换磁道的过程为寻道。 不同磁头间的切换是电子切换，而不同磁道间的切换需要磁头做径向运动，这个径向运动需要步进电机调节，这个动作是机械的切 换。寻道：是机械的，所以很浪费时间 &emsp;为什么按照柱面的方式读写数据？因为同一个柱面上我只要让磁头做电子的切换（即一个磁头读完一个柱面上的一个磁道就电子让下一次磁头读取同一个柱面上的下一个磁道），就能够读取数据（显然电子的切换相比较机械的切换要快很多） 5.磁盘分区的介绍5.1.磁盘存储逻辑结构图 MBR:主引导记录，0磁头，0磁道，1扇区，446字节 5.2.分区表示意图 5.3.16字节分区表含义 5.4.备份主引导记录 5.5.磁盘分区小结 磁盘分区的实质就是针对上述0磁头0磁道1扇区的前446字节后面接下来的64byte的分区表进行设置，即主要划分起始以及结束磁头号，及扇区号和柱面号 一块磁盘的分区表仅有64byte大小，每个分区表要占用16字节，因此一块磁盘仅支持4个分区表信息，即：主分区+扩展分区的总量不超过4个 扩展分区是不能直接使用的，还需要在扩展分区的基础上划分逻辑分区才行。 扩展分区有自己的分区表，因此扩展分区下面的逻辑分区可以有多个。 5.6.主分区primary一般来说，主分区是磁盘上必须存在的分区，一般我们在主分区上安装操作系统 5.7.扩展分区&esmp;严格来说,扩展分区不能算一个正常的分区,而是一个链接,起到一个指向的作用,我们可以在扩展分区内建立逻辑分区,从上面逻辑结构图中我们可以看到,扩展分区就像一个虚拟出来的一个小硬盘一样,但是不同的是,没有MBR,而只有扩展分区表,而且这个扩展分区表是没有64bytes的限制的,所以可以在扩展分区划分多个逻辑分区&esmp;一块硬盘只能存在一个扩展分区,并且扩展分区不能直接存放数据,扩展分区受限于操作系统 5.8.逻辑分区logical&esmp;不能在磁盘中单独直接划分逻辑分区(fdisk),逻辑分区必须存在于扩展分区内,在扩展分区内可以划分多个逻辑分区,逻辑分区的编号从数字5开始,在这个扩展分区内可以划分多个逻辑分区(IDE磁盘大概编号可以是5-63), SATA(编号是5-15)&esmp;实际应用:主分区和逻辑分区,都可以用,一般系统安装在主分区中,存放数据都可以 5.9.分区的注意事项&emsp;一块硬盘的分区方式只能为如下组合之一：P + P + P + PP + P + P + EP + P + EP + EP其中的P为primary，E为Extended 扩展分区不是一个真正可用的分区，建立扩展分区之后，还需要在扩展分区上面建立逻辑分区才可以使用。 对于主分区和逻辑分区在一般的数据存储使用上是没有区别的（对于大多数的数据存储），在安装操作系统时第一个分区要选主分区 分区号1-4留给主分区或扩展分区使用，逻辑分区只能从5开始，即使1-4分区号有剩余，也不会分配给逻辑分区。 对硬盘分区，实际上就是在修改硬盘的分区表，也就是说我们通过fdisk分区实际上就是在改64字节的分区表，分区和对应的数据没有关系，因此理论上调整分区大小，不会删除分区内的数据。特殊说明:磁盘也可以不分区,直接格式化使用,但是不推荐这样做, 主分区最少有一个，而扩展分区最多一个 5.10.磁盘分区方案 方案1:集群架构中的某个节点,数据有多份或者不重要 /boot 100MB linux引导程序swap 物理内存的1.5倍,当内存大于等于8GB时,给8GB即可/ 剩余硬盘大小(相当于Windows只分C盘) 方案2:数据库即存储,有大量重要的数据 /boot 100MB/ 50-200GBswap 物理内存的1.5倍,当内存大于等于8GB时,给8GB即可/data 剩余硬盘大小,放数据库及存储数据的 方案3:门户级别或大网站 /boot 100MBswap 物理内存的1.5倍,当内存大于等于8GB时,给8GB即可/ 50-200GB剩余空间保留,不再进行分区,将来分给哪个部分,拿到服务器的部分,就自己根据需求再分,此种方式更灵活 参见：http://oldboy.blog.51cto.com/2561410/634725 5.11.磁盘分区设备名&emsp;以硬盘为目标来说明设备命名规则。linux将硬盘分为两类：第一类是传统的IDE硬盘，使用hd标示；第 二种是SATA、SCSI、USB硬盘、U盘（严格的说U盘不算硬盘）等，均用sd标示；对于SATA，SCSI，USB硬盘，U盘等，只要把hd改成sd就可以了。命名规则是相同的。&emsp;对应于：第一块硬盘：sda //我们使用a，b，c表示第几块盘&emsp;如果第一块盘有1个主分区和1个扩展分区，2个逻辑分区对应于：sda1 //主分区 sda2 //扩展分区 sda5 //逻辑分区 sda6 //逻辑分区注意：我们使用1,2,3.。。来表示不同的分区号&emsp;每个硬盘只能有四个主分区和扩展分区（三个主分区+一个扩展分区，或者四个主分区），这个是由硬盘本身决定的（64字节，每个分区表16字节），跟操作系统没关系。每个硬盘最多只能有一个扩展分区，这个是由操作系统限制的。我的理解，扩展分区只是标志该分区处被再次分成多个逻辑分区了，因此扩展分区相当于逻辑分区的容器而已，没有必要多个，因此OS限制了扩展分区最多只能有一个。 整理自老男孩","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"磁盘inode原理","date":"2017-04-16T04:47:25.690Z","path":"2017/04/16/Linux/磁盘/磁盘inode图/","text":"1.低级格式化&emsp;&emsp;低级格式化（磁盘厂商来做的），用来划分磁道、扇区，但是并没有任何的分区，需要我们自己来分区 2.磁盘分区(partition)&emsp;&emsp;将磁盘划分成多个逻辑的组成结构，可以在分区的基础上创建独立的文件系统 3.MBR引导程序 Main Boot Record 主引导记录 位置：0盘面0磁道1扇区，512bytes 组成：446bytes：BootLoader 引导加载器（一段引导程序）64bytes：每16byte：标识一个分区，所以之后4个主分区2bytes：Magic Number 标记MBR是否有效 4.inode 磁盘分区之后，要进行文件系统的选择，为了方便我们寻找到数据，文件系统将一个分区划分为两块：一个是存储元数据存储区，用来标示数据存储在哪里另一个就是数据存储区 根的inode是自引用的，已知的目录也是以块来存储的，块中有目录下的文件名和文件名对应的inode号 4.1.inode逻辑结构示意图 4.2.创建文件的过程创建一个文件的过程：touch /backup/test.txt 大小10k (而每个block是2k) 首先在inode 的bitmap上找一个空闲的inode，然后将该inode号(112)占用 找到：/backup目录对应的块，在目录对应的块上写文件名(test.txt)和对应的inode号(112) 找到块位图，找空闲的块，将其分配给inode，更新inode中块的位置 存储连续块的好处：这样磁头可以不用跳跃，直接访问，所以当我们删除文件的时候，会有磁盘碎片的产生，这样访问的时候，会有磁头的跳跃，然后会很慢 4.3.文件查找的过程 4.4.文件删除的过程删除一个文件的过程：mv /backup/test.txt 将：/back中的 文件名和inode号（112）删除 将inode的位图中该inode（112）标记为0 block的位图中对应的块标记为0（未使用） 以上删除并没有删除block中的数据，只是标记了数据被删除，那么只有在下一次进行写满对应的block的时候，原来的数据才会被彻底覆盖 4.5.文件复制的过程复制文件的过程：就是在创建了一个文件的时候，然后将需要复制的文件填充到新创建的文件中 4.6.文件剪切剪切的过程（如果是在同一个文件系统下）那么只是改变的目录中文件的路径， 对于经常访问的文件，我们可以将其放入到缓存中，如：/var/log/messages —&gt;对应的inode号，这样下次将可以直接来访问了 5.超级块和块组最上面的是超级块，然后就是块组，每一个块组中包含（inode，inode 的位图，block位图）","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"用户和用户组管理","date":"2017-04-16T04:47:25.687Z","path":"2017/04/16/Linux/用户和用户组管理/用户和用户组管理/","text":"1.用户配置文件1.1.用户信息文件/etc/passwd123[root@lamp01 ~]# cat /etc/passwdroot:x:0:0:root:/root:/bin/bashchenyansong:x:500:500::/home/chenyansong:/bin/bash root :x :0 :0 :root :/root :/bin/bash 账号名称 :账号密码 :账号UID :账号组GID :用户说明 :用户家目录 :shell解释 1.1.1.各个字段说明 UID(相当于身份证),GID（相当于家庭或者学校）：超级用户：0 ，超级用户的用户名不一定就是root，但是只要UId的是0的用户就一定是超级用户。系统用户（伪用户）：0-499 ，作用：满足文件或程序运行的需要，而创建的，不能登录，不能使用普通用户：500-65535注意：安装系统后可以删除用不到的伪用户，但是最好不删而是注释掉，我们在自己部属服务的时候，也会创建伪用户，满足服务的需求，例如：apache,nginx,mysql,nfs,rsync,nagios.zabbix,redis. 用户家目录：当我们创建一个用户的时候，会默认在home（root除外）下创建一个以用户名命名的家目录 shell解释器：伪用户之所以不能登录，就是因为结尾以nologin结束，如下:1ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin 1.1.2初始组和附加组 初始组：指用户一登录立刻拥有这个用户组 的相关权限，每个用户的初始组只能有一个，一般就是和这个用户的用户名相同的组名作为这个用户的初始组。 附加组：指用户可以加入多个其他的用户组，并拥有这些组的权限，附加组可以有多个。 1.2 影子文件/etc/shadow&emsp;加密密码：如果密码是“!!”或“*”代表没有秘密，不能登录 123root:$6$z88R0.1iC2cYCAxW$PBtFGnl.via1K548GD.7maVCCiPAhCps.1TqI1furgoEM9da9FLfhMPCt2/iq.rv2qv/6M0qMaPjVRlfjdKfu/:16985:0:99999:7:::tcpdump:!!:16985::::::chenyansong:$6$hcQOHdN.$TUZ.b5ttQvpX1dhkWClz6FxStVLPX6gnHpDGzuyDHvUOuxcEWL0kQi5cP7JQX.UHv4KxgORBBTfk9.QjIUyna1:17001:0:99999:7::: &emsp;shadow文件中一行的各个字段详细说明 字段名称 注释说明 账号名称 用户的账号名称 账号密码 用户密码,这是加密过的口令 最近更改密码的时间 从1970年1月1日起,到用户最近一次更改口令的天数 禁止修改密码的天数 从1970年1月1日起,到用户可以更改可以更改密码的天数 用户必须更改口令 的天数 从1970年1月1日起,到用户必须更改密码的天数 不活动时间 在用户密码过期之后到禁用账户的天数 失效天数 从1970年1月1日起,到用户被禁用的天数 标志 保留 1.3组信息文件/etc/group 和密码组文件/etc/gshadow12student_team:x:502:chenyansong,zhangsan组名:组密码标志:GID:组中附加用户 &emsp;group文件中一行的各个字段详细说明 字段名称 注释说明 用户组名 该组的名称 用户组密码 通常不需要设置该密码,由于安全原因,该密码被记录在/etc/gshadow中,因此,显示为”x”,这类似/etc/shadow GID 就是用户组的ID 用户组成员 加入这个组的所有用户账号 &emsp;gshadow文件中一行的各个字段详细说明 字段名称 注释说明 用户组名 该组的名称 用户组密码 用户组密码,这个字段可以是空的或!,如果是空的或!,表示没有密码 用户组管理员账号 用户组管理者,这个字段也可以为空,如果有多个用户组管理者,用逗号分隔 用户组成员 加入这个组的所有用户账号,列表中多个用户通过逗号分隔 2.用户管理命令2.1.用户添加命令useradd2.1.1 useradd -c 添加说明–comment123[root@lamp01 ~]# useradd -c shoumingWenzi test_comment[root@lamp01 ~]# cat /etc/passwd|grep test_commtest_comment:x:510:511:shoumingWenzi:/home/test_comment:/bin/bash 2.1.2 useradd -d 指定家目录–home_dir 如果不存在,就帮你创建家目录,默认是在/home下创建与用户名同名的家目录12345678[root@lamp01 ~]# useradd test_home_dir -d /tmp/chenyansong[root@lamp01 ~]# grep test_home_dir /etc/passwdtest_home_dir:x:511:512::/tmp/chenyansong:/bin/bash[root@lamp01 ~]# su - test_home_dir[test_home_dir@lamp01 ~]$ pwd /tmp/chenyansong #用户登录时,默认是进入到该用户的家目录下 2.1.3 useradd -e 账号过期日期 -e expire_date 账号终止日期,日期的格式为MM/DD/YY 1useradd -e &apos;2016/07/21&apos; chenyansong 2.1.4.useradd -g / useradd -G -g 添加初始组，只有一个 -G 添加附加组，可以有多个 2.1.5.useradd -M 不创建家目录&emsp;不创建家目录，优先于/etc/login.defs 文件的设定，一般创建虚拟用户时不建立家目录，部属服务时需要创建虚拟用户 2.1.6.useradd -s shell&emsp;用户登录后使用的shell名称，默认值为不填写，这样系统会帮你指定预设的登入shell，（根据/etc/default/useradd 预设值） 2.1.7 useradd -u uid&emsp;指定用户的uid 值，这个值必须要唯一，除非用-o选项，数字不可为负值 2.2.添加用户组 groupadd2.2.1 和groupadd 命令有关的文件 /etc/group -用户组相关文件 /etc/gshadow -用户组密码相关文件 1234[root@lamp01 ~]# grep test_g /etc/grouptest_g:x:514:[root@lamp01 ~]# grep test_g /etc/gshadowtest_g:!:: 2.2.2 groupadd 语法12groupadd groupname -g GIDgroupadd incahome -g 506 -g gid 指定用户组GID值,除非接-o参数,否则ID值必须是唯一的数字(不能为负数),如果不指定会从500开始 2.3.修改用户密码passwd&emsp;通用户和超级用户都可以运行passwd命令，但是普通用户只能更改自身的用户密码，root用户则可以修改或设置所有用户的密码。当直接执行passwd命令侯曼不接任何参数或用户名时，则表示修改当前登录用户。普通用户修改密码必须遵守密码的复杂性原则. 2.3.1.passwd一些常用参数1234567891011121314151617181920212223passwd -x ：--maximun=days(maximun passwd lifetime(root only))两次密码修改的最大天数，后面接数字；仅root权限操作passwd -n ：--minimun=days 两次修改密码的最小天数，后面接数字，仅限root权限passwd -w ：-warning=days 在距离多少天提醒用户修改密码，仅限root权限passwd -i ：--inactive=days 在密码过期后多少天，用户被禁掉，仅限root权限passwd -l ：--lock 锁住用户，使其无法登陆，仅限root权限（其实就是在shadow中的密码字符串前加入！！，那么用户就不能登录）passwd -u ：--unclock解除锁定passwd -k ：--keep-tokens 保留即将过期的用户在期满或仍能使用passwd -d ：--delete 删除用户密码passwd -stdin ：--stdin （read new tokens from stdin (root only)）从stdin 读入密码passwd -S 查看密码状态[root@linux-study ~]# passwd -S chenyansong4chenyansong4 PS 2016-07-20 0 99999 7 -1 (密码已设置，使用 SHA512 加密。)##密码设定时间（2016-07-20） 密码修改间隔时间（0） 密码有效期（99999）警告时间（7）密码不失效（-1）- - stdin 是接受前面echo的输入[root@lamp01 ~]# echo 123|passwd --stdin test_home_dir更改用户 test_home_dir 的密码 。passwd： 所有的身份验证令牌已经成功更新#或者密码来自一个文件passwd --stdin oldboy &lt;p.log 2.3.2.举例12345678910111213141516171819202122232425262728# 要求:oldboy用户7天内不能更改密码,60天以后必须修改密码,过期前10天提醒用户,过期30天后禁止用户登录[root@lamp01 ~]# passwd -n 7 -x 60 -w 10 -i 30 oldboy调整用户密码老化数据oldboy。passwd: 操作成功#查看用户的过期时间[root@lamp01 ~]# chage -l oldboyLast password change : Feb 11, 2017Password expires : Apr 12, 2017Password inactive : May 12, 2017Account expires : neverMinimum number of days between password change : 7Maximum number of days between password change : 60Number of days of warning before password expires : 10#注意：普通用户可以修改自己的密码，而管理员可以更改任何用户的密码，但是普通用户的密码命名规则必须是8位以上含有数字字母和其他字符的，但是管理员可以不必。 #在普通用户下，修改自己的密码：[lingzhiling@localhost ~]$ passwd更改用户 lingzhiling 的密码 。为 lingzhiling 更改 STRESS 密码。（当前）UNIX 密码：新的 密码：无效的密码： 过短新的 密码：重新输入新的 密码：passwd： 所有的身份验证令牌已经成功更新。 2.4.修改用户信息usermod2.4.1.参数列表123456789101112131415usermod 的大多数参数和useradd相同-c comment;-d home_dir;-e expire_date;-f inactive_days ; 账号过期几日后永久过期停权-G 附加组-g 初始组-s 指定新登入的shell-u 指定uid（唯一，非负数）-L 冻结账户的密码，使之无法登录-U 解冻-l log_name;变更用户登录时的名称为log_name#例子usermod -c OOldBoy -u 1806 -G root -s /bin/tcsh -d /tmp/oldboy7 oldboy 2.5.查看、更改用户密码信息（chage）2.5.1.chage -l zhangsan 列表123456789[root@lamp01 ~]# chage -l oldboyLast password change : Feb 11, 2017 #上次密码修改时间Password expires : Apr 12, 2017 #密码过期时间Password inactive : May 12, 2017 #密码过期后到禁用账户的天数Account expires : never #账户过期时间Minimum number of days between password change : 7 #禁止修改密码的天数Maximum number of days between password change : 60 #必须修改密码的天数Number of days of warning before password expires : 10 #在密码过期多少天之前提醒 2.5.2.参数含义 选项 说明 -l 列出用户的详细密码状态 -d 日期 修改密码最后一次更改日期 -m 天数 两次密码修改时间 -M 天数 密码有效期 -W 天数 密码过期前警告天数 -I 天数 密码过后宽限天数 -E 日期 账号失效时间 12chage -d 0 lamp#这个命令其实是把密码修改日期归0了,这样用户一登录就要修改密码 案例 123# 要求oldboy用户7天内不能更改密码,60天以后必须修改密码,过期前10天提醒用户,过期30天后禁止用户登录chage -m7 -M60 -W10 -I30 oldboychage -m 7 -M 60 -W 10 -I 30 oldboy 2.6.删除用户userdel2.6.1.语法1useradd [-r] 用户名 # -r删除用户的同时删除用户家目录 &emsp;另外一个手工删除用户12345678vim /etc/passwdvim /etc/shadowvim /etc/groupvim /etc/gshadowrm -rf /var/spool/mail/lamprm -rf /home/lamp #家目录#总结：在生产场景中，请不要轻易用-r参数，这会在删除用户的同时删除用户家目录下的所有文件和目录，如果非要删除家目录，当家目录下有重要的文件时，那么删除之前请先备份文件。另外一种方式：将passwd中相关用户的行注释，或者锁定密码，或者让账号过期等方式 2.7.groupdel 删除用户1groupdel chenyansong 2.8.groupmod 修改用户组1234567groupmod [选项] 组名选项: -g GID 修改组ID -n 新组名 修改组名groupmod -n testgrp group1#把组名group1修改为testgrp 2.9.向组中添加用户，删除组中用户&emsp;如果删除的组中有初始用户，那么该组就不能删除，而如果组中只是附加用户，这个组是可以删除的。1234gpasswd 选项 组名选项: -a 用户名 : 把用户加入组 -d 用户名 : 把用户从组中删除 3.用户组示意图","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"正则表达式","date":"2017-04-16T04:47:25.683Z","path":"2017/04/16/Linux/正则表达式/正则表达式/","text":"1.概述&emsp;正则表达式应用非常广泛，存在于各种语言中，例如：java，PHP，Python等，但是我们这里说的正则表达式是Linux系统运维工作中的正则表达式，即：Linux正则表达式，最常用的正则表达式的命令就是grep（egrep），sed，awk，换句话说Linux三剑客要想能工作的更高效，那一定离不开正则表达式的配合。&emsp;注意：正则表达式和我们常用的通配符特殊字符有本质的区别，通配符例子：ls .log 这里的就是通配符（表示所有），不是正则表达式。&emsp;注意事项： linux正则一般以行为单位匹配处理的 alia grep=’grep –color=auto’ , 这样匹配到的字符可以高亮显示 c.注意字符集，export LC_ALL=C 2.语法2.1^开头，$结尾（匹配空行）1[root@linux-study cys_test]# cat test.txt|grep &quot;^$&quot; 2.2 点(.)表示只能代表任意一个字符1不会匹配到空行（但是会匹配到空字符串） 2.3 \\转义字符12例如：\\. 表示的是“.”这个字符 \\$ 表示的是“$”这个字符 2.4 *重复前面的0个或者多个字符2.5 .* 表示匹配所有字符12^.*：以任意多个字符开头.*$：以任意多个字符结尾 2.6.[0-9A-Za-z]12匹配字符集内任意一个字符[^0-9A-Za-z] 不包含其中的任意字符，注意^（表示以。。开头）和[^]的区别 2.7 a{n,m}123重复前面一个字符(这里是字符：a)n到m次，如果用egrep或者sed -r 可以不用转义a\\&#123;n,\\&#125; : 表示至少重复a字符n次a\\&#123;n&#125; : 表示重复a字符n次 2.8+ 匹配一次或者多次1sa-6+匹配sa-6、sa-666，不能匹配sa- 2.9 （）组12匹配表达式，创建一个用于匹配的子串,这个组后面可以使用到(\\1表示第一个分组)ma(tri)?匹配max或maxtrix 2.10 | 两边任意一项12交替匹配|两边的任意一项ab(c|d)匹配abc或abd 2.11 ？1重复0个或者一个前面的字符 2.12扩展正则1使用egrep 或者是 grep -E 3.元字符元字符（meta character）是一种Perl风格的正则表达式，只有一部分文本处理工具支持它，并不是所有的文本处理工具都支持。 正则表达式 描述 示例 \\b 单词边界 \\bcool\\b 匹配cool,不匹配 coolant \\B 非单词边界 cool\\B 匹配coolant,不匹配cool \\d 单个数字字符 b\\db 匹配b2b,不匹配bcb \\D 单个非数字字符 b\\Db 匹配bcb,不匹配b2b \\w 单个单词字符(字母,数字,_) \\w 匹配1或者a, 不匹配&amp; \\W 单个非单词字符 \\W匹配&amp;,不匹配1或a \\n 换行符 \\n匹配一个新行 \\s 单个空白字符 x\\sx 匹配x x, 不匹配xx \\S 单个非空白字符 x\\Sx匹配xkx, 不匹配x x \\r 回车 \\r匹配回车 \\t 横向制表符 \\v 垂直制表符 \\f 换页符 4.通配符(区别于正则的存在)1234567891011121314151617181920*：代表0或多个字符？：代表任意1个字符；：连续不同命令的分隔符#：配置文件注释|：管道~：用户的家目录-：上一次的目录$：变量前需要加的符号/：路径分隔符&gt;或1&gt;：重定向&gt;&gt;：追加重定向；&lt; (输入重定向)；&lt;&lt;（追加输入重定向）‘：带引号，不具有变量置换功能，输出时所见即所得“：双引号，具有变量置换功能，解析变量后输出`：tab建上面的键，反引号，两个``中间为命令，会先执行，等价$()&#123;&#125;：中间为命令块组合或内容序列!：逻辑运算中的“非”（not）&amp;&amp;：当前一个指令执行成功时，执行后一个指令||：当前一个指令执行失败时，执行后一个指令..：上一级目录.：当前目录 举例1234567891011121314151617181920212223242526272829303132333435363738394041#1&#123;&#125;[root@lamp01 chenyansong]# touch &#123;a,b,c&#125;.log[root@lamp01 chenyansong]# ll总用量 4-rw-r--r-- 1 root root 0 2月 14 10:06 a.log-rw-r--r-- 1 root root 0 2月 14 10:06 b.log-rw-r--r-- 1 root root 0 2月 14 10:06 c.logdrwxr-xr-x 2 root root 4096 2月 13 19:37 etc#2.*[root@lamp01 chenyansong]# ls *.loga.log b.log c.log#3.？[root@lamp01 chenyansong]# ls -l ?.log-rw-r--r-- 1 root root 0 2月 14 10:06 a.log-rw-r--r-- 1 root root 0 2月 14 10:06 b.log-rw-r--r-- 1 root root 0 2月 14 10:06 c.log#4.反引号，双引号，单引号[root@lamp01 chenyansong]# echo datedate[root@lamp01 chenyansong]# echo &quot;date&quot;date#反引号解析命令[root@lamp01 chenyansong]# echo `date`2017年 02月 14日 星期二 10:09:20 CST[root@lamp01 chenyansong]# a=&quot;test&quot;#双引号解析变量[root@lamp01 chenyansong]# echo &quot;$a&quot;test#单引号原样输出[root@lamp01 chenyansong]# echo &apos;$a&apos;$a","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"时间同步","date":"2017-04-16T04:47:25.679Z","path":"2017/04/16/Linux/时间同步/时间同步/","text":"1.查看时间123[root@linux-study cys_test]# date2016年 07月 13日 星期三 16:18:24 CST[root@linux-study cys_test]# 2.临时修改时间12345[root@linux-study cys_test]# date -s &quot;2016/07/12 16:19&quot;2016年 07月 12日 星期二 16:19:00 CST[root@linux-study cys_test]# date2016年 07月 12日 星期二 16:19:03 CST[root@linux-study cys_test]# 3.NTP时间同步123456#时间服务器 time.nist.gov[root@linux-study cys_test]# ntpdate time.nist.gov 13 Jul 15:59:07 ntpdate[4856]: step time server 216.229.0.179 offset 54158.972820 sec[root@linux-study cys_test]# date2016年 07月 13日 星期三 15:59:16 CST[root@linux-study cys_test]# 4.定时任务,时间同步12###### time nsys ######*/5 * * * * /usr/sbin/ntpdate time.nist.gov&gt;/dev/null 2&gt;&amp;1 5.时间同步的架构搭建 6.硬件时间和操作系统时间&emsp;hwclock命令是一个硬件时钟访问工具，它可以显示当前时间、设置硬件时钟的时间和设置硬件时钟为系统时间，也可设置系统时间为硬件时钟的时间。&emsp;在Linux中有硬件时钟与系统时钟等两种时钟。硬件时钟是指主机板上的时钟设备，也就是通常可在BIOS画面设定的时钟。系统时钟则是指kernel中 的时钟。当Linux启动时，系统时钟会去读取硬件时钟的设定，之后系统时钟即独立运作。所有Linux相关指令与函数都是读取系统时钟的设定。 123[root@lamp01 chenyansong]hwclock2017年02月13日 星期一 21时00分29秒 -0.973945 seconds[root@lamp01 chenyansong]","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"定时任务(crontab)","date":"2017-04-16T04:47:25.675Z","path":"2017/04/16/Linux/定时任务/定时任务(crontab)/","text":"1.定时任务crond介绍&emsp;crond是linux系统中用来定期执行命令或指定程序任务的一种服务或软件&emsp;特殊需求：（秒级别）crond服务就无法搞定了，一般工作中写脚本守护进程执行。12crond #守护进程 一直运行着crontab #设置命令 -l list -e edit 1.1.crond是什么&emsp;linux系统的定时任务crond,相当于我们平时生活中的闹钟的功能。可以满足周期性执行任务的需求。 1.2 为什么要使用crond定时任务1.3 不同系统的定时任务和种类1.3.1 windows 7 系统的定时任务&emsp;开始→所有程序→附件→系统工具→选择任务计划程序 1.3.2 linux系统的定时任务&emsp;linux系统中定时任务调度的工作可以分为以下两个情况：&emsp;情况一:linux系统自身定期执行的任务工作：系统周期性执行的任务工作，如轮询系统日志，备份系统数据，清理系统缓存等。 centos5.X例： 12345[root@CentOS5 log]# ll messages*-rw------- 1 root root 372258 Mar 14 20:48 messages-rw------- 1 root root 349535 Nov 11 18:13 messages.1#提示：centos 6.4日志轮询结尾是按时间了。 centos6.X例： 123456[root@CentOS6 log]# ll messages*-rw------- 1 root root 1591 3月 25 21:57 messages-rw------- 1 root root 78304 3月 3 20:40 messages-20140303-rw------- 1 root root 78050 3月 8 19:42 messages-20140311-rw------- 1 root root 745 3月 18 00:46 messages-20140318-rw------- 1 root root 77232 3月 22 21:20 messages-20140325 &emsp;情况二:用户执行的任务工作：某个用户或系统管理员定期要做的任务工作，例如每隔5分钟和互联网上时间服务器进行时间同步，每天晚上0点备份站点数据及数据库数据，一般这些工作需要由每个用户自行设置才行。123[root@CentOS6 log]# crontab -l#time sync by lee at 2014-1-14*/5 * * * * /usr/sbin/ntpdate time.windows.com &gt;/dev/null 2&gt;&amp;1 2.定时任务crond使用说明12345678910111213141516171819[root@CentOS6 log]# crontab --helpcrontab：无效选项 -- -crontab: usage error: unrecognized optionusage: crontab [-u user] file crontab [-u user] [ -e | -l | -r ] (default operation is replace, per 1003.2) -e (edit user&apos;s crontab) #重点 -l (list user&apos;s crontab) #重点 -r (delete user&apos;s crontab) -i (prompt before deleting user&apos;s crontab) -s (selinux context)/*crontab -e === vi /var/spool/cron/rootcrontab -l === cat /var/spool/cron/root如果是root用户，编辑或者是查看的是：/var/spool/cron/root但是如果是用户：zhangsan，则编辑或查看的是：/var/spool/cron/zhangsan*/ 2.1 指令说明通过crontab我们可以在固定的间隔时间执行指定的系统指令或script脚本。时间间隔的单位是分钟，小时，日，月，周及以上的任意组合（注意：日和周不要组合） 2.2 使用者权限及定时任务文件 文件 说明 /etc/cron.deny 该文件中所列用户不允许使用crontab命令。 /etc/cron.allow 该文件中所列用户允许使用crontab命令，优先于/etc/cron.deny /var/spool/cron 所有用户crontab配置文件默认都存放在此目录，文件名以用户名命名。 2.3 指定用户创建定时任务（-u）&emsp;当一个用户(chenyansong)创建一个定时任务的时候，即：crontab -e ,此时会有:/var/spool/cron/chenyansong文件创建&emsp;那么在当前用户下，怎样以其他用户的名义创建定时任务呢？ 方式一： 1234[root@lamp01 chenyansong]# crontab -u chenyansong -e[root@lamp01 chenyansong]# crontab -u chenyansong -l###### 方式2: 123su - chenyansong #切换用户,然后再创建crontab -ecrontab -l 另外一个问题： 那么chenyansong用户又是怎么进入/var/spool/cron/ ，然后去查看 /var/spool/cron/chenyansong 的呢？ 原因：用户（chenyansong）使用：crontab 2.4 指令选项说明表 参数 含义 指定示例 -l(字母) 查看crontab文件内容 crontab -l -e 编辑crontab文件内容 crontab -e -i 删除crontab文件内容,删除前会提示确认 crontab -ri -r 删除crontab文件内容 crontab -r -u user 指定使用的用户执行任务 crontab -u lee -l 提示:crontab{-l,-e}实际上就是操作/var/spool/cron当前用户这样的文件 注意:|crontab -e|/var/spool/cron/root|前者会检查语法，而后者不会。||-|-|-||visudo|/etc/sudoers|前者会检查语法，而后者不会。| 2.5.指令的使用格式&emsp;默认情况下，当用户建立定时任务规则后，该规则记录对应的配置文件会存在于/var/spool/cron中，其crontab配置文件对应的文件名与登录的用户名一致。如：root用户的定时任务配置文件为/var/spool/cron/root。&emsp;crontab用户的定时任务一般分为6段（空格分隔，系统的定时任务则/etc/crontab分为7段），其中前五段位时间设定段，第六段为所要执行的命令或脚本任务段。 2.5.1 crontab基本格式123456* * * * * cmd/*提示：1.cmd为要执行的命令或脚本，例如/bin/sh /server/scripts/lee.sh.2.每个段之间必须要有空格。*/ 2.5.2 crontab语法格式中时间段的含义表 段 含义 取值范围 第一段 代表分钟 00-59 第二段 代表小时 00-23 第三段 代表日期 01-31 第四段 代表月份 01-12 第五段 代表星期 0-7(0和7都代表星期日) 2.5.3 crontab语法格式中特殊符号的含义表 特殊符号 含义 * 号表示任意时间都，就是“每”的意思，举例：如00 01 cmd表示每月每周每日的凌晨1点执行cmd任务 - 减号，表示分隔符，表示一个时间范围段，如17-19点，每小时的00分执行任务。00 17-19 * cmd。就是17,18,19点整点分别执行的意思 ， 逗号，表示分隔时间段的意思。30 17,18,19 cmd 表示每天17,18,19点的半点执行cmd。也可以和“-”结合使用，如： 30 3-5,17-19 cmd /n n代表数字，即”每隔n单位时间”,例如：每10分钟执行一次任务可以写 /10 cmd,其中 /10，*的范围是0-59，也可以写成0-59/10 3.书写crond定时任务7个基本要领3.1 为定时任务规则加必要的注释&emsp;加了注释，就知道定时任务运行的是什么作业，以防以后作业混乱。这是个好习惯和规范。123[root@angelT ~]# crontab -l#time sync by lee at 2014-1-14 #这就是添加的注释*/5 * * * * /usr/sbin/ntpdate time.windows.com &gt;/dev/null 2&gt;&amp;1 3.2 定时任务命令或程序最好写到脚本里执行123[root@angelT ~]# crontab -l#backup www to /backup00 00 * * * /bin/sh /server/scripts/www_bak.sh &gt;/dev/null 2&gt;&amp;1 #写到www_bak.sh脚本中 3.3定时任务执行的脚本要规范路径，如：/server/scripts123[root@angelT ~]# crontab -l#backup www to /backup00 00 * * * /bin/sh /server/scripts/www_bak.sh &gt;/dev/null 2&gt;&amp;1 3.4执行shell脚本任务时前加/bin/sh&emsp;执行定时任务时，如果是执行脚本，尽量在脚本前面带上/bin/sh命名（这样就不用考虑权限的问题），否则有可能因为忘了为脚本设定执行权限，从而无法完成任务。123[root@angelT ~]# crontab -l#backup www to /backup00 00 * * * /bin/sh /server/scripts/www_bak.sh &gt;/dev/null 2&gt;&amp;1 3.5 定时任务结尾加 &gt;/dev/null 2&gt;&amp;1123[root@angelT ~]# crontab -l#backup www to /backup00 00 * * * /bin/sh /server/scripts/www_bak.sh &gt;/dev/null 2&gt;&amp;1 3.5.1 有关/dev/null的说明123# /dev/null为特殊的字符设备文件，表示黑洞设备或空设备。[root@angelT ~]# ll /dev/null crw-rw-rw- 1 root root 1, 3 3月 26 01:10 /dev/null 3.5.2 有关重定向的说明123456789101112&gt;或1&gt; 输出重定向：把前面输出的东西输入到后边的文件中，会删除文件原有内容。&gt;&gt;或1&gt;&gt;追加重定向：把前面输出的东西追加到后边的文件中，不会删除文件原有内容。&lt;或&lt;0 输入重定向：输入重定向用于改变命令的输入，指定输入内容，后跟文件名。&lt;&lt;或&lt;&lt;0输入重定向：后跟字符串，用来表示“输入结束”，也可用ctrl+d来结束输入。2&gt; 错误重定向：把错误信息输入到后边的文件中，会删除文件原有内容。2&gt;&gt; 错误追加重定向：把错误信息追加到后边的文件中，不会删除文件原有内容。标准输入（stdin）：代码为0，使用&lt;或&lt;&lt;。标准输出（stdout）:代码为1，使用&gt;或&gt;&gt;。正常的输出。标准错误输出（sederr）：代码为2，使用2&gt;或2&gt;&gt;。特殊：2&gt;&amp;1就是把标准错误重定向到标准输出（&gt;&amp;）。&gt;/dev/null 2&gt;&amp;1 等价于 1&gt;/dev/null 2&gt;/dev/null 3.5.3 &gt;/dev/null 2&gt;&amp;1的作用如果定时任务规范结尾不加 &gt;/dev/null 2&gt;&amp;1,很容易导致硬盘inode空间被占满，从而系统服务不正常（var/spool/clientmqueue邮件临时队列目录，垃圾文件存放于此，如果是centos 6.4系统，默认不装sendmail服务，所以不会有这个目录。） 3.6 在指定用户下执行相关定时任务&emsp;这里要特别注意不同用户的环境变量问题，如果是调用了系统环境变量/etc/profile，最好在程序脚本中将用到的环境变量重新export下。 3.7生产任务程序不要随意打印输出信息&emsp;在调试好脚本程序后，应尽量把DEBUG及命令输出的内容信息屏蔽掉，如果确实需要输出日志，可定向到日志文件里，避免产生系统垃圾。 4. 配置定时任务规范操作过程 5.生产场景如何调试crond定时任务&emsp;规范的公司开发和运维人员操作流程：个人的开发配置环境–&gt;办公室的测试环境–&gt;idc机房的测试环境–&gt;idc机房的正式环境 5.1 增加执行频率调试任务&emsp;在调试时，把任务执行频率调快一点，看能不能正常执行，如果正常，那就没问题了，再改成需要的任务的执行时间。&emsp;注意：有些任务时不允许频繁执行的，例如：定时往数据库里插入数据，这样的任务要在测试机上测试好，然后正式线上出问题的机会就少了。 5.2调整系统时间调试任务&emsp;用正确的执行任务的时间，设置完成后，可以修改下系统当前时间，改成任务执行时间的前几分钟来测试（或者重启定时任务服务） 5.3通过日志输出调试定时任务&emsp;在脚本中加入日志输出，然后把输出打到指定的日志中，然后观察日志内容的结果，看是否正确执行 5.4注意一些任务命令带来的问题&emsp;注意： * echo “==”&gt;&gt;/tmp/lee.log &gt;/dev/null 2&gt;&amp;1 这里隐藏的无法正确执行的任务配置，原因是前面多了&gt;&gt;,或者去掉结尾的 &gt;/dev/null 2&gt;&amp;1。 5.5 注意环境变量导致的定时任务故障crontab执行shell时只能识别为数不多的系统环境变量，一般用户定义的普通变量无法是别的，如果在编写的脚本中需要使用这些变量，最好使用export重新声明该变量，脚本才能正常执行，例如：在调试java程序任务的时候，注意环境变量，把环境变量的定义加到脚本里。参见：http://oldboy.blog.51cto.com/2561410/1541515 5.6通过定时任务日志调试定时任务12345[root@angelT ~]# tail -f /var/log/cron Mar 26 15:55:01 angelT CROND[3415]: (ida) CMD (/usr/sbin/ntpdate time.windows.com &gt;/dev/null 2&gt;&amp;1)Mar 26 15:55:01 angelT CROND[3416]: (root) CMD (/usr/sbin/ntpdate time.windows.com &gt;/dev/null 2&gt;&amp;1)Mar 26 16:00:01 angelT CROND[3422]: (root) CMD (/usr/sbin/ntpdate time.windows.com &gt;/dev/null 2&gt;&amp;1)Mar 26 16:00:01 angelT CROND[3423]: (root) CMD (/usr/lib64/sa/sa1 1 1) 整理自:老男孩","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"字符集","date":"2017-04-16T04:47:25.672Z","path":"2017/04/16/Linux/优化/字符集/","text":"linux系统支持中文字符集的步骤如下: 将服务器端的字符集(/etc/sysconfig/i18n)改为LANG=”zh_CN.UTF-8” 将客户端字符集(CRT)调整为UTF-8 字符集文件所在路径1234[root@linux-study cys_test]# cat /etc/sysconfig/i18nLANG=&quot;zh_CN.UTF-8&quot;SYSFONT=&quot;latarcyrheb-sun16&quot;[root@linux-study cys_test]# 修改字符集123456789#方法一：使用vim去修改#方法二：sed去修改[root@linux-study cys_test]# sed -i &apos;s#LANG=&quot;en_US.UTF-8&quot;#LANG=&quot;zh_CN.UTF-8&quot;#g&apos; /etc/sysconfig/i18n#方法三：临时生效的方式：LANG=en //en对应于上面的LANG=&quot;en_US.UTF-8&quot;#重新加载到内存中[root@linux-study cys_test]# source /etc/sysconfig/i18n 修改客户端的字符集","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"CentOS（5.8/6.4）linux生产环境若干优化实战(转)","date":"2017-04-16T04:47:25.670Z","path":"2017/04/16/Linux/优化/CentOS（5.8和6.4）linux生产环境若干优化实战(转)/","text":"注意：本次优化都是基于CentOS（5.8/6.4）。关于5.8和6.4两者优化时的小区别，我会在文中提及的。 优化条目： 修改ip地址、网关、主机名、DNS等 关闭selinux，清空iptables 添加普通用户并进行sudo授权管理 更新yum源及必要软件安装 定时自动更新服务器时间 精简开机自启动服务 定时自动清理/var/spool/clientmqueue/目录垃圾文件，放置inode节点被占满 变更默认的ssh服务端口，禁止root用户远程连接 锁定关键文件系统 调整文件描述符大小 调整字符集，使其支持中文 去除系统及内核版本登录前的屏幕显示 内核参数优化 1.修改ip地址、网关、主机名、DNS等12345678910111213141516171819202122232425262728293031323334[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0 #网卡名字BOOTPROTO=static #静态IP地址获取状态 如：DHCP表示自动获取IP地址IPADDR=192.168.1.113 #IP地址NETMASK=255.255.255.0 #子网掩码ONBOOT=yes#引导时是否激活GATEWAY=192.168.1.1[root@localhost ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0BOOTPROTO=staticIPADDR=192.168.1.113NETMASK=255.255.255.0ONBOOT=yesGATEWAY=192.168.1.1[root@localhost ~]# vi /etc/sysconfig/networkHOSTNAME=c64 #修改主机名，重启生效GATEWAY=192.168.1.1 #修改默认网关,如果上面eth0里面不配置网关的话，默认就使用这里的网关了。[root@localhost ~]# cat /etc/sysconfig/networkHOSTNAME=c64GATEWAY=192.168.1.1我们也可以用 hostnamec64 来临时修改主机名，重新登录生效修改DNS[root@localhost ~]# vi /etc/resolv.conf #修改DNS信息nameserver 114.114.114.114nameserver 8.8.8.8[root@localhost ~]# cat /etc/resolv.conf #查看修改后的DNS信息nameserver 114.114.114.114nameserver 8.8.8.8[root@localhost ~]# service network restart #重启网卡，生效#重启网卡，也可以用下面的命令[root@localhost ~]# /etc/init.d/network restart 2.关闭selinux，清空iptables 关闭selinux 123456[root@c64 ~]# sed –i ‘s/SELINUX=enforcing/SELINUX=disabled/g’ /etc/selinux/config #修改配置文件则永久生效，但是必须要重启系统。[root@c64 ~]# grep SELINUX=disabled /etc/selinux/configSELINUX=disabled #查看更改后的结果[root@c64 ~]# setenforce 0#临时生效命令[root@c64 ~]# getenforce #查看selinux当前状态Permissive 清空iptables 123456789[root@c64 ~]# iptables –F #清理防火墙规则[root@c64 ~]# iptables –L #查看防火墙规则Chain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destination[root@c64 ~]#/etc/init.d/iptables save #保存防火墙配置信息 3.添加普通用户并进行sudo授权管理12345[root@c64 ~]# useradd sunsky[root@c64 ~]# echo &quot;123456&quot;|passwd --stdin sunsky&amp;&amp;history –c[root@c64 ~]# visudo在root ALL=(ALL) ALL此行下，添加如下内容sunsky ALL=(ALL) ALL 4.更新yum源及必要软件安装yum安装软件，默认获取rpm包的途径从国外官方源，改成国内的源。国内较快的:阿里云1234#接下来就要安装几个必要的软件了[root@c64 yum.repos.d]# yum install lrzsz ntpdate sysstat -y #lrzsz是一个上传下载的软件#sysstat是用来检测系统性能及效率的工具 5.定时自动更新服务器时间123456789[root@c64 ~]# echo &apos;*/5 * * * * /usr/sbin/ntpdate time.windows.com &gt;/dev/null 2 &gt;&amp;1&apos; &gt;&gt;/var/spool/cron/root[root@c64 ~]# echo &apos;*/10 * * * * /usr/sbin/ntpdate time.nist.gov &gt;/dev/null 2&gt;&amp;1&apos; &gt;&gt;/var/spool/cron/root/*提示：CentOS 6.4的时间同步命令路径不一样6是/usr/sbin/ntpdate5是/sbin/ntpdate扩展：在机器数量少时，以上定时任务同步时间就可以了。如果机器数量大时，可以在网内另外部署一台时间同步服务器NTP Server。此处仅提及，不做部署。*/ 6.精简开机自启动服务刚装完操作系统可以只保留crond，network，syslog，sshd这四个服务。（Centos6.4为rsyslog）1234567[root@c64 ~]# for sun in `chkconfig --list|grep 3:on|awk &apos;&#123;print $1&#125;&apos;`;do chkconfig --level 3 $sun off;done[root@c64 ~]# for sun in crond rsyslog sshd network;do chkconfig --level 3 $sun on;done[root@c64 ~]# chkconfig --list|grep 3:oncrond 0:off 1:off 2:on 3:on 4:on 5:on 6:offnetwork 0:off 1:off 2:on 3:on 4:on 5:on 6:offrsyslog 0:off 1:off 2:on 3:on 4:on 5:on 6:offsshd 0:off 1:off 2:on 3:on 4:on 5:on 6:off 7.定时自动清理/var/spool/clientmqueue/目录垃圾文件，放置inode节点被占满本优化点，在6.4上可以忽略不需要操作即可！1234567[root@c64 ~]# mkdir /server/scripts -p[root@c64 ~]# vi /server/scripts/spool_clean.sh#!/bin/shfind/var/spool/clientmqueue/-typef -mtime +30|xargs rm-f#然后将其加入到crontab定时任务中[root@c64 ~]# echo &apos;*/30 * * * * /bin/sh /server/scripts/spool_clean.sh &gt;/dev/null 2&gt;&amp;1&apos;&gt;&gt;/var/spool/cron/root 8.变更默认的ssh服务端口，禁止root用户远程连接123456789[root@c64 ~]# cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak[root@c64 ~]# vim /etc/ssh/sshd_configPort 52113#ssh连接默认的端口PermitRootLogin no #root用户黑客都知道，禁止它远程登录PermitEmptyPasswords no #禁止空密码登录UseDNS no #不使用DNS[root@c64 ~]# /etc/init.d/sshd reload #从新加载配置[root@c64 ~]# netstat -lnt #查看端口信息[root@c64 ~]# lsof -i tcp:52113 9.锁定关键文件系统12345678[root@c64 ~]# chattr +i /etc/passwd[root@c64 ~]# chattr +i /etc/inittab[root@c64 ~]# chattr +i /etc/group[root@c64 ~]# chattr +i /etc/shadow[root@c64 ~]# chattr +i /etc/gshadow#使用chattr命令后，为了安全我们需要将其改名[root@c64 ~]# /bin/mv /usr/bin/chattr /usr/bin/任意名称 10.调整文件描述符大小1234567891011[root@localhost ~]# ulimit –n #查看文件描述符大小1024[root@localhost ~]# echo &apos;* - nofile 65535&apos; &gt;&gt; /etc/security/limits.conf#提示：也可以把ulimit -SHn 65535命令加入到/etc/rc.local，然后每次重启生效[root@c64 ~]# cat &gt;&gt;/etc/rc.local&lt;&lt;EOF#open filesulimit -HSn 65535#stack sizeulimit -s 65535EOF 配置完成后，重新登录即可查看。 11.调整字符集，使其支持中文12sed-i &apos;s#LANG=&quot;en_US.UTF-8&quot;#LANG=&quot;zh_CN.GB18030&quot;#&apos;/etc/sysconfig/i18nsource/etc/sysconfig/i18n 12.去除系统及内核版本登录前的屏幕显示12[root@c64 ~]# &gt;/etc/redhat-release[root@c64 ~]# &gt;/etc/issue 13.内核参数优化说明：本优化适合apache，nginx，squid多种等web应用，特殊的业务也可能需要略作调整。123456789101112131415161718192021222324[root@c64 ~]# vi /etc/sysctl.conf#by sun in 20131001net.ipv4.tcp_fin_timeout = 2net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 1net.ipv4.tcp_syncookies = 1net.ipv4.tcp_keepalive_time =600net.ipv4.ip_local_port_range = 4000 65000net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_max_tw_buckets = 36000net.ipv4.route.gc_timeout = 100net.ipv4.tcp_syn_retries = 1net.ipv4.tcp_synack_retries = 1net.core.somaxconn = 16384net.core.netdev_max_backlog = 16384net.ipv4.tcp_max_orphans = 16384#一下参数是对iptables防火墙的优化，防火墙不开会有提示，可以忽略不理。net.ipv4.ip_conntrack_max = 25000000net.ipv4.netfilter.ip_conntrack_max = 25000000net.ipv4.netfilter.ip_conntrack_tcp_timeout_established = 180net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait = 120net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait = 60net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait = 120[root@localhost ~]# sysctl –p #使配置文件生效 提示：由于CentOS6.X系统中的模块名不是ip_conntrack，而是nf_conntrack，所以在/etc/sysctl.conf优化时，需要把net.ipv4.netfilter.ip_conntrack_max 这种老的参数，改成net.netfilter.nf_conntrack_max这样才可以。即对防火墙的优化，在5.8上是123456net.ipv4.ip_conntrack_max = 25000000net.ipv4.netfilter.ip_conntrack_max = 25000000net.ipv4.netfilter.ip_conntrack_tcp_timeout_established = 180net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait = 120net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait = 60net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait = 120 在6.4上是123456net.nf_conntrack_max = 25000000net.netfilter.nf_conntrack_max = 25000000net.netfilter.nf_conntrack_tcp_timeout_established = 180net.netfilter.nf_conntrack_tcp_timeout_time_wait = 120net.netfilter.nf_conntrack_tcp_timeout_close_wait = 60net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 120 优化过程中的报错解决5.8版本上123456error: &quot;net.ipv4.ip_conntrack_max&quot;is an unknown keyerror: &quot;net.ipv4.netfilter.ip_conntrack_max&quot;is an unknown keyerror: &quot;net.ipv4.netfilter.ip_conntrack_tcp_timeout_established&quot;is an unknown keyerror: &quot;net.ipv4.netfilter.ip_conntrack_tcp_timeout_time_wait&quot;is an unknown keyerror: &quot;net.ipv4.netfilter.ip_conntrack_tcp_timeout_close_wait&quot;is an unknown keyerror: &quot;net.ipv4.netfilter.ip_conntrack_tcp_timeout_fin_wait&quot;is an unknown key 这个错误可能是你的防火墙没有开启或者自动处理可载入的模块ip_conntrack没有自动载入，解决办法有二，一是开启防火墙，二是自动处理开载入的模块ip_conntrack 6.4版本上123456error: &quot;net.nf_conntrack_max&quot;isan unknown keyerror: &quot;net.netfilter.nf_conntrack_max&quot;isan unknown keyerror: &quot;net.netfilter.nf_conntrack_tcp_timeout_established&quot;isan unknown keyerror: &quot;net.netfilter.nf_conntrack_tcp_timeout_time_wait&quot;isan unknown keyerror: &quot;net.netfilter.nf_conntrack_tcp_timeout_close_wait&quot;isan unknown keyerror: &quot;net.netfilter.nf_conntrack_tcp_timeout_fin_wait&quot;isan unknown key 这个错误可能是你的防火墙没有开启或者自动处理可载入的模块ip_conntrack没有自动载入，解决办法有二，一是开启防火墙，二是自动处理开载入的模块ip_conntrack12modprobe nf_conntrackecho &quot;modprobe nf_conntrack&quot;&gt;&gt; /etc/rc.local 6.4版本上123error: &quot;net.bridge.bridge-nf-call-ip6tables&quot;isan unknown keyerror: &quot;net.bridge.bridge-nf-call-iptables&quot;isan unknown keyerror: &quot;net.bridge.bridge-nf-call-arptables&quot;isan unknown key 这个错误是由于自动处理可载入的模块bridge没有自动载入，解决办法是自动处理开载入的模块ip_conntrack12modprobe bridgeecho &quot;modprobe bridge&quot;&gt;&gt; /etc/rc.local 转载自","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"Linux一些重要的配置文件之软件故障和系统安全问题","date":"2017-04-16T04:47:25.666Z","path":"2017/04/16/Linux/一些重要的配置文件/软件故障和系统安全问题/","text":"/var/log/message, /var/log/secure 1234567891011121314151617#系统的软件出故障了,看message[root@lamp01 ~]# ll /var/log/messages*-rw------- 1 root root 785 2月 12 12:57 /var/log/messages-rw------- 1 root root 6910 9月 6 09:46 /var/log/messages-20160906-rw------- 1 root root 277130 9月 11 09:18 /var/log/messages-20160911-rw------- 1 root root 269711 2月 11 15:56 /var/log/messages-20170211-rw------- 1 root root 90232 2月 12 09:54 /var/log/messages-20170212# 系统出现安全问题了,看secure[root@lamp01 ~]# ll /var/log/secure*-rw------- 1 root root 284 2月 12 13:18 /var/log/secure-rw------- 1 root root 1516 9月 6 08:29 /var/log/secure-20160906-rw------- 1 root root 2521 9月 11 09:42 /var/log/secure-20160911-rw------- 1 root root 1689 2月 11 16:09 /var/log/secure-20170211-rw------- 1 root root 3394 2月 12 09:14 /var/log/secure-20170212","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之网卡文件ifcfg-eth0","date":"2017-04-16T04:47:25.665Z","path":"2017/04/16/Linux/一些重要的配置文件/网卡文件ifcfg-eth0/","text":"1.启动网卡1ifup 2. 停止网卡1ifdown 3.修改网卡之后重启12345/etc/init.d/network restart#orifdown eth0 &amp;&amp; ifup eth0 #关闭和启动 4.ifcfg-eth0各字段的含义123456789101112131415161718192021222324252627[root@lamp01 ~]# cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0 #物理设备名,eth0表示第1块网卡BOOTPROTO=none #其中proto取值如下:/*none: 引导时不使用协议static: 静态分配地址bootp: 使用BOOTP协议dhcp: 使用DHCP协议#办公室网络一般选择dhcp,外部网络选择静态地址*/#HWADDR=00:0c:29:ca:6a:82 #网卡的MAC地址,48位NM_CONTROLLED=yes #ONBOOT=yesTYPE=Ethernet #以太网#UUID=&quot;f1827545-55d8-4241-8aae-4775a48310d3&quot; #UUID用来标识一个物理网卡DNS2=202.106.0.20 #DNSDNS1=8.8.8.8USERCTL=noIPV6INIT=noHWADDR=00:0c:29:38:6a:b1IPADDR=192.168.0.3NETMASK=255.255.255.0 #子网掩码GATEWAY=192.168.0.1 #网关地址,路由器的地址#如果是克隆的虚拟机,需要删除掉MAC地址和UUID,因为克隆的时候,这些事不会变的,但是这些又要是唯一的","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之系统运行级别文件etc/inittab","date":"2017-04-16T04:47:25.662Z","path":"2017/04/16/Linux/一些重要的配置文件/系统运行级别文件inittab/","text":"设定系统启动时init进程将把系统设置成什么样的runlevel运行级别及加载相关的级别对应启动文件设置12[root@lamp01 chenyansong]# cat /etc/inittab |grep -vE &quot;^#&quot;id:3:initdefault:","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之登录之后的字符串提示信息/etc/motd","date":"2017-04-16T04:47:25.661Z","path":"2017/04/16/Linux/一些重要的配置文件/登录之后的字符串提示信息motd/","text":"1234567#/etc/motd[root@lamp01 ~]# vim /etc/motd 欢迎来到测试环境#登录之后如下显示:Last login: Sun Feb 12 09:14:18 2017 from 192.168.0.221欢迎来到测试环境","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之环境变量profile","date":"2017-04-16T04:47:25.659Z","path":"2017/04/16/Linux/一些重要的配置文件/环境变量profile/","text":"12345#添加环境变量vim /etc/profile#使环境变量生效source /etc/profile","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之添加用户时默认的useradd配置","date":"2017-04-16T04:47:25.657Z","path":"2017/04/16/Linux/一些重要的配置文件/添加用户时默认的useradd配置/","text":"涉及到的文件有如下: /etc/default/useradd /etc/login.defs /etc/skel /etc/default/useradd 12345678910[root@lamp01 application]# cat /etc/default/useradd# useradd defaults fileGROUP=100 #依赖于/etc/login.defs 的USERGROUPS_ENAB参数,如果为no,则此处控制HOME=/home #把用户的家目录建在/home中INACTIVE=-1 #是否启用账号过期停权,-1表示不启用EXPIRE= #账号终止日期,不设置表示不启用SHELL=/bin/bash #新用户默认所用的shell类型SKEL=/etc/skel #配置新用户家目录的默认文件存放路径,/etc/skell就是配置在这里,当我们用useradd添加用户时,用户目录下的文件,都是从这里配置的目录中复制过去的CREATE_MAIL_SPOOL=yes #创建mail文件 /etc/login.defs 123456789101112131415161718192021222324252627282930313233343536373839[root@lamp01 application]# cat /etc/login.defs |grep -vE &quot;^#&quot; MAIL_DIR /var/spool/mail#对密码的定义PASS_MAX_DAYS 99999 #一个秘密最长可以使用的天数PASS_MIN_DAYS 0 #更换密码的最小天数PASS_MIN_LEN 5 #密码的最小长度PASS_WARN_AGE 7 #密码失效前多少天开始警告#对UID,GID的定义 UID_MIN 500 #最小UID为500, 也就是说添加用户时,UID是从500开始的UID_MAX 60000 #最大的UID为60000 GID_MIN 500 GID_MAX 60000 #创建用户的时候，是否创建家目录CREATE_HOME yes #对umask的定义UMASK 077 #所以产生用户的家目录的权限就是:777-077 = 700[root@lamp01 home]# pwd/home[root@lamp01 home]# ll总用量 48drwx------. 3 chenyansong chenyansong 4096 2月 11 18:18 chenyansongdrwx------ 2 chenyansong2 chenyansong2 4096 7月 20 2016 chenyansong2drwx------ 2 lisi lisi 4096 7月 18 2016 lisi#注意是:用户的家目录的权限,家目录里面的文件或者是目录是644#删除用户，是否同时删除对应的组USERGROUPS_ENAB yes #加密方式ENCRYPT_METHOD SHA512 /etc/skel &emsp;/etc/skel目录是用来存放新用户配置文件的目录，当我们添加新用户的时候，这个目录下的所有文件会自动被复制到新添加的用户的家目录下；默认情况下，/etc/skel目录下的所有文件都是隐藏文件（以.点开头的文件）通过修改、添加、删除/etc/skel目录下的文件，我们可以为新创建的用户提供统一的、标准的、初始化用户环境。 &emsp;例如：如果我们在/etc/skel/下创建了一个readme.txt文件，那么我们再创建用户的时候，在用户的家目录下就会存在一个叫做readme.txt的文件。 1234567891011121314[root@lamp01 skel]# pwd/etc/skel[root@lamp01 skel]# ll -a总用量 20drwxr-xr-x. 2 root root 4096 7月 3 2016 .drwxr-xr-x. 87 root root 4096 2月 13 00:06 ..-rw-r--r--. 1 root root 18 5月 11 2012 .bash_logout-rw-r--r--. 1 root root 176 5月 11 2012 .bash_profile-rw-r--r--. 1 root root 124 5月 11 2012 .bashrc/*为什么useradd的时候就自动在用户家目录下创建skel目录中的文件？参见：/etc/default/useradd 下的配置项SKEL*/ 案例 12345#请问如下的登录环境故障的原理及解决办法?-bash-4.1$-bash-4.1$#显示不正常，解决的办法：将/etc/skel/.bash*的文件拷贝到用户的家目录下","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之服务启动目录init.d","date":"2017-04-16T04:47:25.655Z","path":"2017/04/16/Linux/一些重要的配置文件/服务启动目录init.d/","text":"使用yum或者是rpm安装的软件,服务都在:/etc/init.d下启动12345service 服务名 start #或者/etc/init.d/服务名 start","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之开机自启动脚本/etc/rc.local","date":"2017-04-16T04:47:25.654Z","path":"2017/04/16/Linux/一些重要的配置文件/开机自启动脚本rc.local/","text":"在所有的初始化脚本执行完成之后,会执行该脚本文件,一般放一些启动的命令,挂载命令等 123456789101112[root@lamp01 ~]# cat /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff. touch /var/lock/subsys/local #### NFS ######/etc/init.d/rpcbind start #启动rpcbind服务mount -t nfs 192.168.1.102:/data /mnt #挂载分区","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之开机自动挂载fstab","date":"2017-04-16T04:47:25.652Z","path":"2017/04/16/Linux/一些重要的配置文件/开机自动挂载fstab/","text":"&emsp;设置文件系统挂载信息的文件，开机能够自动挂载磁盘分区 fstab各个字段的解释123456789101112131415161718192021222324[root@lamp01 chenyansong]# cat /etc/fstab ## /etc/fstab# Created by anaconda on Sun Jul 3 20:57:31 2016## Accessible filesystems, by reference, are maintained under &apos;/dev/disk&apos;# See man pages fstab(5), findfs(8), mount(8) and/or blkid(8) for more info#UUID=ac0d7d40-5964-4813-b54c-7b9d2b967bc6 / ext4 defaults 1 1UUID=f569bd10-8dda-42e9-83b0-78d172417788 /boot ext4 defaults 1 2UUID=3eea297e-1107-415f-a34f-5aed0b17281c swap swap defaults 0 0tmpfs /dev/shm tmpfs defaults 0 0devpts /dev/pts devpts gid=5,mode=620 0 0sysfs /sys sysfs defaults 0 0proc /proc proc defaults 0 0#设备名称 挂载点(在哪儿) 文件系统类型 挂载选项 字段4 字段5[root@lamp01 chenyansong]# /*字段4：每多少天做一次备份（转储频率，0表示不备份，1每天都备份）字段5：文件系统检测次序（0表示不检查，只有根可以为1，其他从2开始）*/ 挂载的两个方式1234567#方式1.mount命令mount -t ext4 -o noexec /dev/sda1 /mnt # -t 是文件系统类型, -o是指选项#方式2.vim /etc/fstab添加/dev/sdb1 /mnt ext4 defaults 0 0 #需要执行mount -a 加载/etc/fstab 文件使挂载生效","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之开机会自动执行的脚本目录/etc/profile.d/","date":"2017-04-16T04:47:25.650Z","path":"2017/04/16/Linux/一些重要的配置文件/开机会自动执行的脚本/","text":"/etc/profile.d/下放的是脚本，在每次开机或者是重启的时候，都会执行他下面的脚本","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之linux目录结构详细介绍(转)","date":"2017-04-16T04:47:25.648Z","path":"2017/04/16/Linux/一些重要的配置文件/linux目录结构详细介绍(转)/","text":"转载自 1.树状目录结构图 2./目录 目录 描述 / 第一层次结构的根、整个文件系统层次结构的根目录 /bin/ 需要在单用户模式可用的必要命令（可执行文件）；面向所有用户，例如：cat、ls、cp，和/usr/bin类似 /boot/ 引导程序文件，例如：kernel、initrd；时常是一个单独的分区[6] /dev/ 必要设备, 例如：, /dev/null. /etc/ 特定主机，系统范围内的配置文件。关于这个名称目前有争议。在贝尔实验室关于UNIX实现文档的早期版本中，/etc 被称为/etcetra 目录，[7]这是由于过去此目录中存放所有不属于别处的所有东西（然而，FHS限制/etc存放静态配置文件，不能包含二进制文件）。[8]自从早期文档出版以来，目录名称已被以各种方式重新称呼。最近的解释包括反向缩略语如：”可编辑的文本配置”（英文 “Editable Text Configuration”）或”扩展工具箱”（英文 “Extended Tool Chest”)。[9] /etc/opt/ /opt/的配置文件 /etc/X11/ X_Window系统(版本11)的配置文件 /etc/sgml/ SGML的配置文件 /etc/xml/ XML的配置文件 /home/ 用户的家目录，包含保存的文件、个人设置等，一般为单独的分区 /lib/ /bin/ and /sbin/中二进制文件必要的库文件 /media/ 可移除媒体(如CD-ROM)的挂载点 (在FHS-2.3中出现) /lost+found 在ext3文件系统中，当系统意外崩溃或机器意外关机，会产生一些文件碎片在这里。\\n当系统在开机启动的过程中fsck工具会检查这里，并修复已经损坏的文件系统。当系统发生问题。可能会有文件被移动到这个目录中，可能需要用手工的方式来修复，或移到文件到原来的位置上。 /mnt/ 临时挂载的文件系统。比如cdrom,u盘等，直接插入光驱无法使用，要先挂载后使用 /opt/ 可选应用软件包 /proc/ 虚拟文件系统，将内核与进程状态归档为文本文件（系统信息都存放这目录下）。例如：uptime、 network。在Linux中，对应Procfs格式挂载。该目录下文件只能看不能改（包括root） /root/ 超级用户的家目录 /sbin/ 必要的系统二进制文件，例如： init、 ip、 mount。sbin目录下的命令，普通用户都执行不了 /srv/ 站点的具体数据，由系统提供。 /tmp/ 临时文件(参见 /var/tmp)，在系统重启时目录中文件不会被保留。 /usr/ 默认软件都会存于该目录下。用于存储只读用户数据的第二层次；包含绝大多数的(多)用户工具和应用程序 /var/ 变量文件——在正常运行的系统中其内容不断变化的文件，如日志，脱机文件和临时电子邮件文件。有时是一个单独的分区。如果不单独分区，有可能会把整个分区充满。如果单独分区，给大给小都不合适 3./etc/目录 目录 描述 /etc/rc /etc/rc.d /etc/rc*.d 启动、或改变运行级时运行的scripts或scripts的目录 /etc/hosts 本地域名解析文件 /etc/sysconfig/network IP、掩码、网关、主机名配置 /etc/resolv.conf DNS服务器配置 /etc/fstab 开机自动挂载系统，所有分区开机都会自动挂载 /etc/inittab 设定系统启动时Init进程将把系统设置成什么样的runlevel及加载相关的启动文件配置 /etc/exports 设置NFS系统用的配置文件路径 /etc/init.d 这个目录来存放系统服务启动脚本 /etc/profile, /etc/csh.login, /etc/csh.cshrc 全局系统环境配置变量 /etc/issue 认证前的输出信息，默认输出版本内核信息 /etc/motd 设置认证后的输出信息， /etc/mtab 当前安装的文件系统列表.由scripts初始化，并由mount 命令自动更新.需要一个当前安装的文件系统的列表时使用，例如df 命令 /etc/group 类似/etc/passwd ，但说明的不是用户而是组. /etc/passwd 用户数据库，其中的域给出了用户名、真实姓名、家目录、加密的口令和用户的其他信息. /etc/shadow 在安装了影子口令软件的系统上的影子口令文件.影子口令文件将/etc/passwd 文件中的加密口令移动到/etc/shadow 中，而后者只对root可读.这使破译口令更困难. /etc/sudoers 可以sudo命令的配置文件 /etc/syslog.conf 系统日志参数配置 /etc/login.defs 设置用户帐号限制的文件 /etc/securetty 确认安全终端，即哪个终端允许root登录.一般只列出虚拟控制台，这样就不可能(至少很困难)通过modem或网络闯入系统并得到超级用户特权. /etc/printcap 类似/etc/termcap ，但针对打印机.语法不同. /etc/shells 列出可信任的shell.chsh 命令允许用户在本文件指定范围内改变登录shell.提供一台机器FTP服务的服务进程ftpd 检查用户shell是否列在 /etc/shells 文件中，如果不是将不允许该用户登录. /etc/xinetd.d 如果服务器是通过xinetd模式运行的，它的脚本要放在这个目录下。有些系统没有这个目录，比如Slackware，有些老的版本也没有。在Redhat Fedora中比较新的版本中存在。 /etc/opt/ /opt/的配置文件 /etc/X11/ X_Window系统(版本11)的配置文件 /etc/sgml/ SGML的配置文件 /etc/xml/ XML的配置文件 /etc/skel/ 默认创建用户时，把该目录拷贝到家目录下 4./usr/目录默认软件都会存于该目录下。用于存储只读用户数据的第二层次；包含绝大多数的用户工具和应用程序。 目录 描述 /usr/X11R6 存放X-Windows的目录 /usr/games 存放着XteamLinux自带的小游戏； /usr/doc Linux技术文档 /usr/include 用来存放Linux下开发和编译应用程序所需要的头文件； /usr/lib 存放一些常用的动态链接共享库和静态档案库； /usr/man 帮助文档所在的目录 /usr/src Linux开放的源代码，就存在这个目录，爱好者们别放过哦 /usr/bin/ 非必要可执行文件 (在单用户模式中不需要)；面向所有用户 /usr/lib/ /usr/bin/和/usr/sbin/中二进制文件的库 /usr/sbin/ 非必要的系统二进制文件，例如：大量网络服务的守护进程 /usr/share/ 体系结构无关（共享）数据 /usr/src/ 源代码,例如:内核源代码及其头文件 /usr/X11R6/ X Window系统版本 11, Release 6. /usr/local/ 本地数据的第三层次，具体到本台主机。通常而言有进一步的子目录，例如：bin/、lib/、share/.这是提供给一般用户的/usr目录，在这里安装一般的应用软件 5./var/目录/var 包括系统一般运行时要改变的数据.每个系统是特定的，即不通过网络与其他计算机共享. 目录 描述 /var/log/message 日志信息，按周自动轮询 /var/spool/cron/root 定时器配置文件目录，默认按用户命名 /var/log/secure 记录登陆系统存取信息的文件，不管认证成功还是认证失败都会记录 /var/log/wtmp 记录登陆者信息的文件，last,who,w命令信息来源于此 /var/spool/clientmqueue/ 当邮件服务未开启时，所有应发给系统管理员的邮件都将堆放在此 /var/spool/mail/ 邮件目录 /var/tmp 比/tmp 允许的大或需要存在较长时间的临时文件. (虽然系统管理员可能不允许/var/tmp 有很旧的文件.) /var/lib 系统正常运行时要改变的文件. /var/local /usr/local 中安装的程序的可变数据(即系统管理员安装的程序).注意，如果必要，即使本地安装的程序也会使用其他/var 目录，例如/var/lock . /var/lock 锁定文件.许多程序遵循在/var/lock 中产生一个锁定文件的约定，以支持他们正在使用某个特定的设备或文件.其他程序注意到这个锁定文件，将不试图使用这个设备或文件. /var/log/ 各种程序的Log文件，特别是login (/var/log/wtmp log所有到系统的登录和注销) 和syslog (/var/log/messages 里存储所有核心和系统程序信息. /var/log 里的文件经常不确定地增长，应该定期清除. /var/run 保存到下次引导前有效的关于系统的信息文件.例如， /var/run/utmp 包含当前登录的用户的信息. /var/cache/ 应用程序缓存数据。这些数据是在本地生成的一个耗时的I/O或计算结果。应用程序必须能够再生或恢复数据。缓存的文件可以被删除而不导致数据丢失。 6./proc/目录虚拟文件系统，将内核与进程状态归档为文本文件（系统信息都存放这目录下）。例如：uptime、 network。在Linux中，对应Procfs格式挂载。该目录下文件只能看不能改（包括root） 目录 描述 /proc/meminfo 查看内存信息 /proc/loadavg 还记得 top 以及 uptime 吧？没错！上头的三个平均数值就是记录在此！ /proc/uptime 就是用 uptime 的时候，会出现的资讯啦！ /proc/cpuinfo 关于处理器的信息，如类型、厂家、型号和性能等。 /proc/cmdline 加载 kernel 时所下达的相关参数！查阅此文件，可了解系统是如何启动的！ /proc/filesystems 目前系统已经加载的文件系统罗！ /proc/interrupts 目前系统上面的 IRQ 分配状态。 /proc/ioports 目前系统上面各个装置所配置的 I/O 位址。 /proc/kcore 这个就是内存的大小啦！好大对吧！但是不要读他啦！ /proc/modules 目前我们的 Linux 已经加载的模块列表，也可以想成是驱动程序啦！ /proc/mounts 系统已经挂载的数据，就是用 mount 这个命令呼叫出来的数据啦！ /proc/swaps 到底系统挂加载的内存在哪里？呵呵！使用掉的 partition 就记录在此啦！ /proc/partitions 使用 fdisk -l 会出现目前所有的 partition 吧？在这个文件当中也有纪录喔！ /proc/pci 在 PCI 汇流排上面，每个装置的详细情况！可用 lspci 来查阅！ /proc/version 核心的版本，就是用 uname -a 显示的内容啦！ /proc/bus/* 一些汇流排的装置，还有 U盘的装置也记录在此喔！ 7./dev/目录设备文件分为两种：块设备文件(b)和字符设备文件(c)设备文件一般存放在/dev目录下，对常见设备文件作如下说明： 目录 描述 /dev/hd[a-t] IDE设备 /dev/sd[a-z] SCSI设备 /dev/fd[0-7] 标准软驱 /dev/md[0-31] 软raid设备 /dev/loop[0-7] 本地回环设备 /dev/ram[0-15] 内存 /dev/null 无限数据接收设备,相当于黑洞 /dev/zero 无限零资源 /dev/tty[0-63] 虚拟终端 /dev/ttyS[0-3] 串口 /dev/lp[0-3] 并口 /dev/console 控制台 /dev/fb[0-31] framebuffer /dev/cdrom =&gt; /dev/hdc /dev/modem =&gt; /dev/ttyS[0-9] /dev/pilot =&gt; /dev/ttyS[0-9] /dev/random 随机数设备 /dev/urandom 随机数设备","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux一些重要的配置文件之DNS文件","date":"2017-04-16T04:47:25.646Z","path":"2017/04/16/Linux/一些重要的配置文件/DNS文件/","text":"12345678910111213141516171819202122232425262728293031323334/*1.客户端DNS可以在网卡配置文件里设置2.客户端DNS也可以在/etc/resolve.conf里配置3.网卡里的设置DNS优先于/etc/resolve.conf*/#/etc/resolve.conf[root@linux-study cys_test]# cat /etc/resolv.conf; generated by /sbin/dhclient-scriptsearch localdomainnameserver 8.8.8.8nameserver 202.106.0.20#/ifcfg-eth0中设置[root@lamp01 chenyansong]# cat /etc/sysconfig/network-scripts/ifcfg-eth0DEVICE=eth0BOOTPROTO=none#HWADDR=00:0c:29:ca:6a:82NM_CONTROLLED=yesONBOOT=yesTYPE=Ethernet#UUID=&quot;f1827545-55d8-4241-8aae-4775a48310d3&quot;DNS2=202.106.0.20 #设置DNSDNS1=8.8.8.8USERCTL=noIPV6INIT=noHWADDR=00:0c:29:38:6a:b1IPADDR=192.168.0.3NETMASK=255.255.255.0GATEWAY=192.168.0.1#如果我们修改了/etc/resolv.conf 中的文件，然后重启网卡：/etc/init.d/network restart, 我们发现，我们修改的东西并没有写入文件，我们使用setup去配置DNS，然后重启网卡，结果也是一样的，也是没有写入resolv.conf文件。这点我们要注意","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"用户切换命令su,sudo,visudo","date":"2017-04-16T04:47:25.643Z","path":"2017/04/16/Linux/sudo/用户切换命令su,sudo,visudo/","text":"1.用户切换命令 su1.1.参数 参数 详解 -;-l; –login 使一个shell成为登录的shell，如执行su - oldboy ,表示该用户想要改变身份为oldboy,并使用oldboy用户的环境变量，如：/home/oldboy/.bash_profile等 -c pass a single command to the shell with -c :切换到一个shell下，执行一个命令，然后退出所切换的用户目录 1234567[root@lamp01 chenyansong]su - chenyansong -c &quot;touch fileByRoot.txt&quot;[root@lamp01 chenyansong]ll总用量 20-rw-rw-r-- 1 chenyansong chenyansong 0 2月 13 21:22 fileByRoot.txt# 可以看到创建的文件是以chenyansong为所属用户,所属组,因为是以chenyansong为用户创建的 1.2.优点如果知道了root的密码，可以从普通用户转到root 1.3.缺点普通用户切换到root之后，拥有root的权限，带来了很大的安全管理问题，例如：如果普通用户将root的密码改了 2.sudo2.1.语法1sudo cmd 2.2.sudo的执行原理 2.3执行原理实操过程2.3.1.visudo添加rm权限viduso进入文件12345## Allow root to run any commands anywhereroot ALL=(ALL) ALLchenyansong ALL=(ALL) /bin/rm#ALL一定是大写的,All还不行 2.3.2.使用sudo删除123456789101112131415[root@lamp01 ~]# su - zhangsan#直接删除,权限不够[zhangsan@lamp01 ~]$ rm -f /etc/oldboy.md5rm: 无法删除&quot;/etc/oldboy.md5&quot;: 权限不够#使用sudo去删除[zhangsan@lamp01 ~]$ sudo rm -f /etc/oldboy.md5[sudo] password for zhangsan:[zhangsan@lamp01 ~]$#使用sudo去cp,但是在sudo中没有配cp执行权限[zhangsan@lamp01 ~]$ sudo cp /etc/hosts /home/zhangsan/Sorry, user zhangsan is not allowed to execute &apos;/bin/cp /etc/hosts /home/zhangsan/&apos; as root on lamp01.[zhangsan@lamp01 ~]$ 2.3.3检查时间戳文件123[root@lamp01 ~]# ll /var/db/sudo/zhangsan/总用量 4-rw------- 1 root zhangsan 28 2月 14 08:07 0 2.3.4.sudo 相关的参数1234567891011121314151617181920212223#sudo -l 列出用户在主机上可用和被禁止的命令[root@lamp01 ~]# sudo -lUser root may run the following commands on this host: (ALL) ALL [zhangsan@lamp01 ~]$ sudo -lUser zhangsan may run the following commands on this host: (ALL) /bin/rm # sudo -k#通-K,删除时间戳,下一个sudo就要求提供密码,前有NOPASSWD:参数,时间戳默认5分钟也会 失效[zhangsan@lamp01 ~]$ sudo -K[zhangsan@lamp01 ~]$ sudo -l[sudo] password for zhangsan: #提供密码User zhangsan may run the following commands on this host: (ALL) /bin/rm #删除时间戳,然后看时间戳文件是否存在[zhangsan@lamp01 ~]$ sudo -K[zhangsan@lamp01 ~]$ logout[root@lamp01 ~]# ll /var/db/sudo/zhangsan/总用量 0 3.visudo等同于：vim /etc/sudoers 3.1.通过echo的方式修改vim /etc/sudoers123[root@lamp01 ~]# echo &quot;chenyansong ALL=(ALL) ALL&quot;&gt;&gt;/etc/sudoers[root@lamp01 ~]# tail -1 /etc/sudoerschenyansong ALL=(ALL) ALL 3.2.visudo -c 语法检查12[root@lamp01 ~]# visudo -c/etc/sudoers: parsed OK 3.3.修改vim /etc/sudoers文件的权限位7771234567891011121314[root@lamp01 ~]# ll /etc/sudoers -r--r----- 1 root root 4131 2月 14 08:24 /etc/sudoers[root@lamp01 ~]# chmod 777 /etc/sudoers[root@lamp01 ~]# su - zhangsan#sudo的授权用户再去执行就会有问题[zhangsan@lamp01 ~]$ sudo rm -f /etc/test.txtsudo: /etc/sudoers is mode 0777, should be 0440sudo: no valid sudoers sources found, quitting#还是要切换回440[root@lamp01 ~]# chmod 440 /etc/sudoers 3.4.viduso文件添加使用权限 12345678910111213141516171819202122#首先需要配置一些Alias，这样在下面配置权限时，会方便一些，不用写大段大段的配置。Alias主要分成4种Host_AliasCmnd_AliasUser_AliasRunas_Alias#配置Host_Alias：就是主机的列表Host_Alias HOST_FLAG = hostname1, hostname2, hostname3#配置Cmnd_Alias：就是允许执行的命令的列表，命令前加上!表示不能执行此命令.命令一定要使用绝对路径，避免其他目录的同名命令被执行，造成安全隐患 ,因此使用的时候也是使用绝对路径!Cmnd_Alias COMMAND_FLAG = command1, command2, command3 ，!command4#配置User_Alias：就是具有sudo权限的用户的列表User_Alias USER_FLAG = user1, user2, user3#配置Runas_Alias：就是用户以什么身份执行（例如root，或者oracle）的列表Runas_Alias RUNAS_FLAG = operator1, operator2, operator3#总体的配置权限的格式如下：USER_FLAG HOST_FLAG=(RUNAS_FLAG) COMMAND_FLAG#如果不需要密码验证的话，则按照这样的格式来配置USER_FLAG HOST_FLAG=(RUNAS_FLAG) NOPASSWD: COMMAND_FLAG 3.5.主机别名 3.6.用户别名(%组) 3.7.命令别名以后可以针对不同的角色用户使用不同的命令别名 3.8.角色别名 3.9.总结&emsp;通过上面的别名的配置，我们在创建新用户的时候，让用户属于上面配置的用户别名组, 这样创建的用户就能够有上面对特定组配置的权限 注意： 授权规则中所有的ALL字符串必须为大写字母 一行内容超长可以用“\\”斜线换行 “！”表示非，就是命令取反的意思，即禁止执行的命令 放在后面的命令会覆盖前面的命令（如果他们有重叠的话） 12下面的/sbin/* 和!/sbin/fdisk就是：拥有/sbin/*下除了/sbin/fdisk的权限/usr/sbin/*,/sbin/*,!/usr/sbin/visudo,!/sbin/fdisk 命令的路径要全路径 如果不需要密码,应该加上NOPASSWD:参数 用户组前面必须加%号 4.sudo su -root 命令 参数 sudo su - 该命令是通过sudo权限进行角色转换(默认是切换到root),执行命令对应账号的密码,非root密码 sudo su - oldboy 该命令是通过sudo以root权限,进行su - oldboy,因此输入的是执行命令当时账号的密码","tags":[{"name":"linux","slug":"linux","permalink":"http://yoursite.com/tags/linux/"}]},{"title":"监控站点目录文件是否被篡改","date":"2017-04-16T04:47:25.636Z","path":"2017/04/16/Linux/shell/监控站点目录文件是否被篡改/","text":"需求&emsp;监控web站点目录(/var/html/www)下所有文件是否被恶意篡改(文件内容被改了),如果有就打印改动的文件名(发送邮件),定时任务每3分钟执行一次 1.用inotify监控，如果发生变化就把变化的文件同步到其他目录123456789101112131415#!/bin/bash#par. /etc/init.d/functionssrc=/data/des=/tmp/data.loginotify_home=/application/inotify$&#123;inotify_home&#125;/bin/inotifywait -mrq --timefmt &apos;%d/%m/%y %H:%M&apos; --format &apos;%T %w%f&apos; -e close_write,delete,create,attrib $src &gt;&gt; $des if [ -s &quot;$des&quot; ] ##判断文件中是否有内容，有，则说明存在变化的文件或目录 then action &quot;web site change&quot; /bin/truecat /dev/null &gt; $des else action &quot;web site no change&quot; /bin/truefi 2.根据md5sum 对比(文件是否修改、删除)，使用diff 看目录树是否变化2.1.文件说明12345678[root@MySQL monitor_web]# tree ../monitor_web/../monitor_web/ #所有的文件放在该目录下|-- diff_file.log #将md5值不同和添加、删除的文件的名字放在这个log中（配置文件生成）|-- md5_file.txt #对文件生成的MD5值放在这里（配置文件生成）|-- monit_web.cfg #配置文件（详细见下）|-- monitor_web_file_check_md5.sh #监控的脚本文件`-- tree_all_file.txt #将初始化时的所有文件名写在里面（配置文件生成）（用于比较数量） 2.2.配置文件的内容123456[root@MySQL monitor_web]# cat monit_web.cfgmonitor_dir=/tmp/chenyansong #要监控的站点目录file_list=/server/scripts/monitor_web/diff_file.log #将md5值不同和添加、删除的文件的名字放在这个log中md5_file=/server/scripts/monitor_web/md5_file.txt #对文件生成的MD5值放在这里is_remake_md5_file=false #是否重新生成一次md5_file文件和tree_all_file文件（如果是true会重新生成，然后在程序中改为false）tree_all_file=/server/scripts/monitor_web/tree_all_file.txt #将初始化时的所有文件名写在里面（用于比较数量） 2.3.shell脚本1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[root@MySQL monitor_web]# cat monitor_web_file_check_md5.sh#!/bin/sh#取到配置文件中的要生成的文件名monit_cfg=&quot;/server/scripts/monitor_web/monit_web.cfg&quot;;monitor_dir=&quot;`egrep &quot;monitor_dir&quot; &quot;$&#123;monit_cfg&#125;&quot;|awk -F &quot;=&quot; &apos;&#123;print $2&#125;&apos;`&quot;;file_list=&quot;`egrep &quot;file_list&quot; $monit_cfg|awk -F &quot;=&quot; &apos;&#123;print $2&#125;&apos;`&quot;;md5_file=&quot;`egrep &quot;\\bmd5_file=&quot; $monit_cfg|awk -F &quot;=&quot; &apos;&#123;print $2&#125;&apos;`&quot;;is_remake_md5_file=`egrep &quot;is_remake_md5_file&quot; $monit_cfg|awk -F &quot;=&quot; &apos;&#123;print $2&#125;&apos;`;tree_all_file=&quot;`egrep &quot;tree_all_file&quot; $monit_cfg|awk -F &quot;=&quot; &apos;&#123;print $2&#125;&apos;`&quot;; # gender md5 message of files to file 生成md5文件和tree文件（监控数量）gender_md5_message()&#123; #touch file [ ! -e $file_list ]&amp;&amp;touch $file_list [ ! -e $md5_file ]&amp;&amp;touch $md5_file if [ &quot;$is_remake_md5_file&quot; == &quot;true&quot; ];then #make md5 message find $monitor_dir -type f -name &quot;*&quot; | xargs md5sum &gt; $md5_file sed -i &apos;s/true/false/g&apos; $monit_cfg; #file_num tree -i $monitor_dir &gt; $tree_all_file fi &#125; #md5sum -c 校验，并发送邮件check_md5()&#123; md5num=`md5sum -c $md5_file|grep -i FAILED|wc -l` &amp;&gt;/dev/null; if [ $md5num -ne 0 ];then #send mail echo &quot;####$&#123;file_list&#125;#####&quot;; echo &quot;`md5sum -c $md5_file|grep -i FAILED`&quot; &gt; $file_list echo `md5sum -c $md5_file|grep -i FAILED`|mail -s &quot;md5_monitor...&quot; 1327401579@qq.com fi &#125; #站点文件数量diff比较，并将不同的部分打印到文件中（diff_file）check_dir_num()&#123; tree -i $monitor_dir &gt; /tmp/new.txt echo &quot;=============file number is change ===================&quot; &gt;&gt; $file_list; echo &quot;`diff $tree_all_file /tmp/new.txt`&quot; &gt;&gt; $file_list&#125;main()&#123; gender_md5_message; check_md5; check_dir_num;&#125; main;","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"监控服务是否正常的方法","date":"2017-04-16T04:47:25.634Z","path":"2017/04/16/Linux/shell/监控服务是否正常的方法/","text":"1.本地方式1.1检测端口12345678#通过：netstat / ss / lsof losf -i:3306|wc -l#通过：Telnet[root@lnmp02 ~]# echo -e &quot;\\n&quot;|telnet www.etiantian.org 80|grep Connected Connection closed by foreign host.Connected to www.etiantian.org.[root@lnmp02 ~]# 1.2.检测进程12 ps -ef|grep -v grep|grep mysqld 2.远程方式2.1.wget1234567891011121314#通过wget 看80服务是否正常，然后使用echo $? 看是否执行成功[root@lnmp02 sbin]# lsof -i:80[root@lnmp02 sbin]# wget -t[重试次数] 2 -T[超时时间] 10 -q[静默输出] --spider[只是抓取，不下载文件] www.etiantian.org[root@lnmp02 sbin]# echo $? 4 //不是0，表示不正常[root@lnmp02 sbin]# ./nginx //开启nginx服务[root@lnmp02 sbin]# lsof -i:80COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnginx 3538 root 7u IPv4 14388 0t0 TCP *:http (LISTEN)nginx 3539 nginx 7u IPv4 14388 0t0 TCP *:http (LISTEN)[root@lnmp02 sbin]# wget -t 2 -T 10 -q --spider www.etiantian.org[root@lnmp02 sbin]# echo $? 0 //正常[root@lnmp02 sbin]# 2.2.通过curl,取第一行的状态码12345678910#方式一：[root@lnmp02 application]# curl -I -s www.etiantian.org|head -1HTTP/1.1 200 OK[root@lnmp02 application]# curl -I -s www.etiantian.org|head -1|grep &quot;\\b200\\b&quot;|wc -l1#方式二：使用curl ，取head的状态码[root@lnmp02 sbin]# curl -I[只输出请求Header信息] -s[不输出错误等] -w[输出特定格式的字段，后面就是指定的格式] &quot;%&#123;http_code&#125;\\n&quot; www.etiantian.org -o[表示输出到文件] /dev/null200 3.通过连接服务123456789101112131415161718[root@lnmp02 application]# mysql -uroot -p123456 -e &quot;show databases;&quot;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema |+--------------------+[root@lnmp02 application]# echo $?0[root@lnmp02 application]# /etc/init.d/mysqld stopShutting down MySQL. SUCCESS![root@lnmp02 application]# /etc/init.d/mysqld statusERROR! MySQL is not running[root@lnmp02 application]# mysql -uroot -p123456 -e &quot;show databases;&quot;ERROR 2002 (HY000): Can&apos;t connect to local MySQL server through socket &apos;/tmp/mysql.sock&apos; (2)[root@lnmp02 application]# echo $?1","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"环境变量","date":"2017-04-16T04:47:25.633Z","path":"2017/04/16/Linux/shell/环境变量/","text":"1.设置环境变量的方法123456789101112131415#1export varName = value#2varName = valueexport varName#3declare -x varName = value#eg:export NAME = chenyansongdeclare -x NAME = chenyansongNAME = chenyansong ; export NAME 2.用户环境变量和全局环境变量123456789101112131415161718192021222324252627282930#用户环境变量：在用户的家目录下：.bash_profile .bashrc[root@MySQL ~]# ll /root/.bashrc /root/.bash_profile-rw-r--r--. 1 root root 176 5月 20 2009 /root/.bash_profile-rw-r--r--. 1 root root 176 9月 23 2004 /root/.bashrc#全局环境变量：[root@MySQL ~]# ll /etc/profile /etc/bashrc-rw-r--r--. 1 root root 2681 6月 22 2012 /etc/bashrc-rw-r--r-- 1 root root 1967 8月 12 23:02 /etc/profile[root@MySQL ~]##需要登录后显示加载的内容可以把脚本文件放在/etc/profile.d下 [root@MySQL ~]# ll /etc/profile.d/总用量 56-rw-r--r--. 1 root root 1127 4月 17 2012 colorls.csh-rw-r--r--. 1 root root 1143 4月 17 2012 colorls.sh//.。。。。-rw-r--r--. 1 root root 169 5月 20 2009 which2.sh#如下面的文件[root@MySQL profile.d]# ll /etc/profile.d/welcome.sh-rw-r--r-- 1 root root 28 8月 16 21:33 /etc/profile.d/welcome.sh[root@MySQL profile.d]# cat welcome.sh echo &quot;Welcome to my world!&quot;#重新登录显示：Last login: Wed Feb 15 02:04:23 2017 from 192.168.0.221欢迎来到测试环境 #这个是由/etc/motd 文件内容控制Welcom to my world! 3.java环境变量参见JDK安装 4.显示或取消环境变量12345678910111213141516171819#1.通过echo或printf命令答应环境变量echo $HOMEecho $UID#2.通过env(printenv)或set显示默认的环境变量[root@lamp01 ~]# envHOSTNAME=lamp01TERM=linuxSHELL=/bin/bashHISTSIZE=1000SSH_CLIENT=192.168.0.221 53043 22SSH_TTY=/dev/pts/0USER=root#set也是一样#3.取消环境变量unset HOSTNAME 5.局部变量5.1.本地变量的生命周期&emsp;本地变量在用户当前的shell生存期的脚本中使用，例如：本地变量OLDBOY取值为zhangsan，这个值只在用户当前的shell生存期中有意义，如果在shell中启动另一个进程或退出，本地变量OLDBOY值将无效 5.2.定义变量的例子123varName = valuevarName = ‘value’varName = “value” 5.3.变量的书写规范 在脚本中定义普通字符串变量,尽量把变量的内容用双引号引起来 在单纯数字内容可以不加引号 希望原样输出加上单引号 希望引用命令就用反引号 12345678910#特殊例子:awk调用shell变量引号例子[root@lamp01 ~]# age=24[root@lamp01 ~]# awk &apos;BEGIN &#123;print &quot;$age&quot;&#125;&apos;$age[root@lamp01 ~]# awk &apos;BEGIN &#123;print &apos;$age&apos;&#125;&apos;24[root@lamp01 ~]# awk &apos;BEGIN &#123;print $age&#125;&apos; [root@lamp01 ~]##以上的结果正好与前面的结论相反,这是awk调用shell的特殊用法 5.4.变量的使用(打印变量)123456789$valName #or $&#123;varName&#125; #or “$&#123;varName&#125;” 5.5.变量内容是命令 要用反引号或者而是$() 把变量括起来使用 5.6.位置变量12345678echo “$0” 获取当前执行的shell脚本的文件名$n获取当前执行的shell脚本第n个参数值，超过9要用：$&#123;10&#125;,$&#123;11&#125;....$* 获取当前shell的所有参数“$1 $2 $3 ...”$@ 获取程序的所有参数”$1” “$2” ...$# 当前shell命令行中参数的总个数$! 执行上一个指令的PID$$ 当前shell的进程号$? 获取执行上一个指令的返回值 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#$*[root@lamp01 ~]# cat test.shfor i in &quot;$*&quot; ;do echo $idone#执行脚本[root@lamp01 ~]# sh test.sh I am handsome boyI am handsome boy #$@[root@lamp01 ~]# cat test.shfor i in &quot;$@&quot; ;do echo $idone#执行结果[root@lamp01 ~]# sh test.sh I am handsome boyIamhandsomeboy #$$ 当前shell的进程号[root@lamp01 ~]# cat pid.shecho $$&gt;/tmp/a.pidsleep 300[root@lamp01 ~]# sh pid.sh &amp;[1] 8447[root@lamp01 ~]# ps -ef|grep pid.sh|grep -v greproot 8447 8382 0 02:31 pts/0 00:00:00 sh pid.sh[root@lamp01 ~]# cat /tmp/a.pid8447#显示当前脚本的进程号，这样我们就可以管理该进程了，如：杀掉进程#下面的脚本是：保证每次只是存在一个进程[root@lamp01 ~]# cat pid.sh#!/bin/shpidpath=/tmp/a.pid if [ -f &quot;$pidpath&quot; ];then kill -USR2 `cat $pidpath` &gt;/dev/null 2&gt;&amp;1 #如果pid存在,就杀死他 rm -f $pidpath #删除文件fiecho $$&gt;$pidpath #重新生成pid文件sleep 300#$0带路径执行和不带路径执行的区别root@MySQL shell]# ll总用量 4-rwxr-xr-x 1 root root 8 8月 16 16:18 test.sh[root@MySQL shell]# cat test.shecho $0[root@MySQL shell]# sh test.shtest.sh[root@MySQL shell]# sh /home/chenyansong/shell/test.sh/home/chenyansong/shell/test.sh 5.7.echo $?1234560 #表示运行成功2 # 权限拒绝1-125 #表示运行失败,脚本命令,系统命令错误或参数传递错误126 #找到该命令了但无法执行127 #未找到要运行的命令&gt;128 #命令被系统强制结束 企业场景返回值的用法 判断命令或者脚本是否执行成功 通过脚本调用执行exit数字,则脚本返回这个数字给$?如果是函数里return 0 返回返回值给$?","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"条件测试,控制流及for循环","date":"2017-04-16T04:47:25.630Z","path":"2017/04/16/Linux/shell/条件测试,控制流及for循环/","text":"1.条件测试1查看帮助：man test 1.1.语法123test condition#或 [ condition ] #注意中括号（［］）、参数之间必须有一个空格 1.2.逻辑运算符 符号 描述 -a 逻辑与 -o 逻辑或 ! 逻辑否 &amp;&amp;, \\ \\ 1234567891011121314151617181920212223242526272829#1. 测试两文件是否均可读$[ -w result.txt –a –w scores.txt ]$echo $?#2. 测试两文件中其中一个是否可执行$[ -x dream –o –x dream2 ]#3. 判断是否可写可执行$[ -w dream –a –x dream ]$echo $?#4. 判断文件是非可执行文件$[ ! –x dream ]#5. [条件判断表达式1]&amp;&amp; [条件判断表达式2]|| [条件判断表达式3]#6. 在[[]] 中，只能用&amp;&amp;、||，而在[]中只能用-a、-o[root@MySQL ~]# [ -f /etc/rc.local &amp;&amp; -f /etc/hosts ]&amp;&amp;echo 1||echo 0-bash: [: missing `]&apos;0[root@MySQL ~]# [ -f /etc/rc.local -a -f /etc/hosts ]&amp;&amp;echo 1||echo 0 1 [root@MySQL ~]# [[ -f /etc/rc.local -a -f /etc/hosts ]]&amp;&amp;echo 1||echo 0 -bash: syntax error in conditional expression-bash: syntax error near `-a&apos;[root@MySQL ~]# [[ -f /etc/rc.local &amp;&amp; -f /etc/hosts ]]&amp;&amp;echo 1||echo 0 1[root@MySQL ~]# 1.3.文件条件测试 符号 描述 -d 目录 -f 普通文件（Regular file） -L 符号链接 -r Readable（文件、目录可读） -b 块专用文件 -e 文件存在 -g 如果文件的set-group-id位被设置则结果为真 -s 文件长度大于0，非空 -z 文件长度=0 -w Writable（文件、目录可写） -u 文件有suid位设置 -x Executable（文件可执行、目录可浏览） -c 字符专用文件 -L 符号链接 123456测试文件是否可写$test –w dream$echo $?或者$[ -w dream ] 1.4.字符串测试12345678910111213#5种语法格式:test “str”test str_operator “str”test “str1” str_operator “str2”[ string_operator str1 ] [ string string_operator string2 ] #操作符两边有空格：[ “aaa” = “cccc” ] 不要： [ “aaa”=“cccc” ]#string_operator有如下的形式= #两字符串相等!= #两字符串不等-z #空串 [zero]-n #非空串 [nozero] 实例123456789101112131415161718#1.测试环境变量是否为空$[ -z $EDITOR ]$echo $?#2. 测试是否为某字符串$[ $EDITOR = “vi” ]$echo $?#3.如果操作符两边没有空格的结果[root@MySQL shell]# [ &quot;a&quot; = &quot;a&quot; ]&amp;&amp;echo 1||echo 01[root@MySQL shell]# [ &quot;a&quot; = &quot;aa&quot; ]&amp;&amp;echo 1||echo 00[root@MySQL shell]# [ &quot;a&quot;=&quot;a&quot; ]&amp;&amp;echo 1||echo 0 1[root@MySQL shell]# [ &quot;a&quot;=&quot;aa&quot; ]&amp;&amp;echo 1||echo 01[root@MySQL shell]# 1.5.数值测试123456789101112131415161718#2种格式“number” numberic_operator “number”# 或 [ “number” numberic_operator “number”]#Numberic_operator 算术比较-eq #数值相等（equal）-ne #不等（not equal）-gt #A&gt;B（greater than）-lt #A&lt;B（less than）-le #A&lt;=B（less、equal）-ge #A&gt;=B（greater、equal）#例子$SOURCE=13$DEST=15$[ “$SOURCE” –gt “$DEST”] 1.6.比较两个文件123456FILE1 -ef FILE2 //比较两个文件是否是同一个 #FILE1 and FILE2 have the same device and inode numbersFILE1 -nt FILE2 //比较两个文件的修改日期 #FILE1 is newer (modification date) than FILE2FILE1 -ot FILE2 #FILE1 is older than FILE2 2.控制流2.1.if2.1.1语法 单分支结构 双分支结构 多分支结构 2.1.2.书写格式 2.2.case2.2.1.语法 2.2.2.书写格式 2.3.while2.3.1.语法 2.3.2.书写格式 2.3.3.通过while读文件的方式12345678910111213141516171819202122#方式1(效率最低)cat data.dat | while read linedo echo &quot;File:$&#123;line&#125;&quot;done#方式2while read linedo echo &quot;File:$&#123;line&#125;&quot;done &lt; data.dat#方式3(效率最高)for line in $(cat data.dat)do echo &quot;File:$&#123;line&#125;&quot;done for line in `cat data.dat`do echo &quot;File:$&#123;line&#125;&quot;done 3.for循环3.1.语法 3.2.书写格式 退出break、continue、exit、return","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"数组","date":"2017-04-16T04:47:25.628Z","path":"2017/04/16/Linux/shell/数组/","text":"1.数组的定义123456789101112#方式一：array=(1 2 3)#方式二:array=(red green blue yellow magenta)array=(red green blue yellow magenta) 2.去数组的长度123456[root@lamp01 chenyansong]# echo $&#123;#array[@]&#125;3[root@lamp01 chenyansong]# echo $&#123;#array[*]&#125;3#使用$&#123;#数组名[@或*]&#125; 可以得到数组长度 3.取数组元素123456789101112[root@lnmp02 ~]# arr=(1 2 3)[root@lnmp02 ~]# echo $arr #arr数组名默认代表第一个元素1[root@lnmp02 ~]# echo $arr[1] #因为要将arr[1]当做一个变量名，所以此时取变量的值，要用$&#123;&#125;1[1][root@lnmp02 ~]# echo $&#123;arr[1]&#125;2[root@lnmp02 ~]# echo $&#123;arr[*]&#125; #打印所有元素1 2 3[root@lnmp02 ~]# echo $&#123;arr[@]&#125;1 2 3 4.数组赋值123456[root@lamp01 chenyansong]# echo $&#123;array[@]&#125;1 2 3[root@lamp01 chenyansong]# array[3]=4[root@lamp01 chenyansong]# echo $&#123;array[@]&#125;1 2 3 4 5.删除数组12345[root@lamp01 chenyansong]# echo $&#123;array[@]&#125;1 2 3 4[root@lamp01 chenyansong]# unset array[2][root@lamp01 chenyansong]# echo $&#123;array[@]&#125;1 2 4 6.for循环打印数组12345678910for((i=0;i&lt;$&#123;#array[*]&#125;;i++))do echo $&#123;array[i]&#125;done#orfor i in $&#123;array[*]&#125;do echo $idone 7.将命令执行的结果放在数组中12345#!/bin/sharr=($(ls)); # $(ls) 是一个命令的执行结果,然后将解雇放入(),形成一个数组for ((i=0;i&lt;$&#123;#arr[*]&#125;;i++));do echo $&#123;arr[i]&#125;;done","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"启动脚本服务添加到chkconfig中","date":"2017-04-16T04:47:25.627Z","path":"2017/04/16/Linux/shell/启动脚本服务添加到chkconfig中/","text":"1.脚本添加复制到/etc/init.d/目录下123-rwxr-xr-x 1 root root 1413 8月 19 21:56 nginxd#注意：sh脚本要有 +x 的权限：chmod +x nginxd 2.在脚本下加入1234#!/bin/sh# chkconfig: 2345 54 65# description: stop/start nginx scripts//上面的数字不能重复:2345 是在2345级别下启动，54 ，65 是启动顺序和关闭顺序 3.添加到chkconfig12345[root@lnmp02 init.d]# chkconfig --add nginxd[root@lnmp02 init.d]# chkconfig nginxd on[root@lnmp02 init.d]# chkconfig --list nginxdnginxd 0:关闭 1:关闭 2:启用 3:启用 4:启用 5:启用 6:关闭[root@lnmp02 init.d]#","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"变量的赋值和引用","date":"2017-04-16T04:47:25.624Z","path":"2017/04/16/Linux/shell/变量的赋值和引用/","text":"1.命名规则 使用变量无需事先声明 首个字符必须为字母（a-z，A-Z） 只能包含字母、数字和下划线，并且不能以数字开头， 不能使用bash里的关键字（可用help命令查看保留关键字） 需要给变量赋值时，可以这么写：变量名=值 取一个变量的值，只需在变量名前面加一个$ ( 注意: 给变量赋值的时候，不能在”=”两边留空格 ） 2.错误举例123456789101112131415#案例1：num=2echo &quot;this is the $numnd&quot;上述脚本并不会输出&quot;this is the 2nd&quot;而是&quot;this is the&quot;；这是由于shell会去搜索变量numnd的值，而实际上这个变量此时并没有值。#案例2：错误方式：var=1var=$var+1echo $var打印出来的不是2而是1＋1。正确方式：let &quot;var+=1&quot; 3.给变量赋值3.1.使用read命令12345[root@MySQL shell]# read -p &quot;please input name:&quot; nameplease input name:chenyansong[root@MySQL shell]# echo $namechenyansong[root@MySQL shell]# 3.2.直接赋值12345[root@MySQL shell]# x=chenyansong[root@MySQL shell]# echo $xchenyansong[root@MySQL shell]# x = chenyansong //注意=号两边是不能有空格的-bash: x: command not found 3.3.使用命令行参数12sh /server/script/test.sh chenyansong#此时会将”chenyansong” 赋值给位置变量$1，依次类推 3.4.使用命令行的输出结果1234567891011121314#方式一：[root@MySQL shell]# path=`pwd` //赋值语句中使用反引号[root@MySQL shell]# echo $path/home/chenyansong/shell[root@MySQL shell]# #方式二：[root@MySQL shell]# path=$(path)-bash: path: command not found //说明$()中需要一个命令[root@MySQL shell]# path=$(pwd)[root@MySQL shell]# echo $path /home/chenyansong/shell[root@MySQL shell]# 4.取变量的值1234567891011$变量名称$&#123;变量名称&#125; $((算术式)) 算术扩展$(x) 这个可以用来当做命令调用 如果变量在语句当中被引用，必须要使用$&#123;x&#125;才可以，取得数组的变量值时候也需要使用$&#123;&#125;来调用 [root@lovelace 51cto]# listnum=100[root@lovelace 51cto]# echo $listnum100[root@lovelace 51cto]# echo $&#123;listnum&#125;100","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"sh和bash的区别","date":"2017-04-16T04:47:25.622Z","path":"2017/04/16/Linux/shell/sh和bash的区别/","text":"1.脚本开头(第一行)12345678910脚本开头（第一行）,由哪个程序（解释器）来执行脚本#!/bin/bash #或者#!/bin/sh#其中开头的#!字符又称为幻数，在执行bash脚本的时候，内核会根据“#!”来选择解释器，解释器确定用哪个程序解释这个脚本中的内容。注意：这一行必须在每个脚本的第一行，如果不是第一行则为脚本注释行：vi test1.sh#!/bin/bashecho “oldboy test”#!/bin/bash //这里就是注释了 2.sh和bash的区别12345[root@lamp01 ~]# ll /bin/shlrwxrwxrwx. 1 root root 4 7月 3 2016 /bin/sh -&gt; bash[root@lamp01 ~]# ll /bin/bash-rwxr-xr-x 1 root root 874248 5月 11 2012 /bin/bash","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell调试","date":"2017-04-16T04:47:25.621Z","path":"2017/04/16/Linux/shell/shell调试/","text":"1.vim直接跳到某一行1vim for_.sh +4 2.sh -x 调试脚本该方法是调整整个脚本 3.set -x&emsp;在set -x 和set +x 调试部分脚本（在脚本中设置）&emsp;如果在脚本文件中加入了命令set –x ，那么在set命令之后执行的每一条命令以及加载命令行中的任何参数都会显示出来，每一行都会加上加号（+），提示它是跟踪输出的标识，在子shell中执行的shell跟踪命令会加2个叫号（++）。 4.在脚本中打印变量输出echo 变量 5.使用exit在特定的位置退出,只执行部分脚本","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本执行","date":"2017-04-16T04:47:25.619Z","path":"2017/04/16/Linux/shell/shell脚本执行/","text":"1.shell脚本的执行shell脚本的执行通常可以采用如下的几种方式: bash sript-name 或 sh script-name(推荐使用) path/script-name 或 ./script-name (当前路径下执行脚本) source script-name 或 . script-name # 注意 “.” 点号 sh&lt;script-name 或 cat script-name|bash (同样适用于sh) 2.sh(bash)和source(.)去执行脚本的区别2.1. sh和bash 和 ./filename12345678910sh FileName bash FileName#该filename文件可以无&quot;执行权限&quot;./FileName #该filename文件需要&quot;执行权限&quot;#作用:打开一个子shell来读取并执行FileName中命令。#注：运行一个shell脚本时会启动另一个命令解释器. 2.2.source filename 和 . filename123source FileName #source的程序主体是bash，脚本中的$0变量的值是bash，而且由于作用于当前bash环境，脚本中set的变量将直接起效. FileName #作用:在当前bash环境下读取并执行FileName中的命令。该filename文件可以无&quot;执行权限&quot; 2.3.父shell和子shell示意图&emsp;其中a图是在当前bash下执行脚本；b图是在一个子shell中执行脚本，然后返回到父shell中；c图是在一个子shell中执行，然后在后台运行（&amp;）。假定你有一个简单的shell脚本alice，它包含了命令hatter和gryphon。 2.4.举例12345678910#已知如下命令及返回结果,请问echo $user的返回结果为()[root@lamp01 ~]# cat test.shuser=`whoami`[root@lamp01 ~]# sh test.sh[root@lamp01 ~]# echo $user/*注释：因为使用的是sh去执行脚本，所以是启用一个子shell执行，然后返回结果到父shell，但是echo $user 是在父shell中执行的，所以结果打印为空*/","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之解决DOS攻击脚本","date":"2017-04-16T04:47:25.617Z","path":"2017/04/16/Linux/shell/shell脚本实例之解决DOS攻击脚本/","text":"写一个脚本解决DOS攻击产生案例 &emsp;提示:根据web日志或者网络连接数,监控当某个IP并发连接数或者短时间内PV达到100,即调用防火墙命令封掉对应的IP,监控频率每隔3分钟,防火墙命令为: iptables -A INPUT -s 10.0.1.10 -j DROP 1234567891011121314151617181920212223242526[root@lamp01 ~]# cat dos.sh#!/bin/sh netstat -an|grep ESTABLISHED|awk -F &quot;[ :]+&quot; &apos;&#123;print $6&#125;&apos;|sort|uniq -c&gt;b.log exec&lt;b.logwhile read line;do pv=`echo $line|awk &apos;&#123;print $1&#125;&apos;` ip=`echo $line|awk &apos;&#123;print $2&#125;&apos;` if [ $pv -gt 4]&amp;&amp;[ `iptables -L -n|grep &quot;$ip&quot;|wc -l` -eq 0 ];then iptables -A INPUT -s $ip -j DROP fidone sleep 180/*说明:1.netstat -an|grep ESTABLISHED 拿到所有的已经建立连接的IP2.awk -F &quot;[ :]+&quot; &apos;&#123;print $6&#125;&apos; 取IP3.sort|uniq -c&gt;b.log 进行IP汇总统计,并输出到一个文件中4.iptables -L -n：list 列出指定链上的所有的规则,n表示数字5.grep &quot;$ip&quot; 取跟指定ip相关的规则,如果没有(-eq 0)就执行if6. iptables -A INPUT -s $ip -j DROP添加一条规则, -s, --source address 检查报文中的源ip地址是否符号位此处理指定的地址或范围7.-j DROP # -j后面接处理动作,DROP:丢弃*/","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之获取随机数的方法","date":"2017-04-16T04:47:25.614Z","path":"2017/04/16/Linux/shell/shell脚本实例之获取随机数的方法/","text":"1.获取随机数的方法1.1.RANDOM123456789101112131415161718192021#RANDOM是bash中的内置变量[root@lnmp02 shell]# man bashRANDOM Each time this parameter is referenced, a random integer between 0 and 32767 is generated. [root@lnmp02 shell]# echo $RANDOM6421#使用md5去加密[root@lnmp02 shell]# echo $RANDOM|md5sum 0ee03cc589c2d6c87a55dfb12837992d -#取md5加密之后的5-10[root@lnmp02 shell]# echo $RANDOM|md5sum|cut -c 5-10 f61bc1 #因为$RANDOM的范围是：between 0 and 32767,数量还是有限的,所以通过遍历32767次,然后md5sum加密,还是能够算出密码的, 所以一般($RANDOM str)一起使用(因为str是不确定的)[root@lnmp02 shell]# echo $RANDOM oldboy18449 oldboy[root@lnmp02 shell]# echo $RANDOM oldboy|md5sum|cut -c 5-1096fe97 1.2.openssl rand产生随机数&emsp;openssl rand 用于产生指定长度个bytes的随机字符。-base64或-hex对随机字符串进行base64编码或用hex格式显示。12345[root@lnmp02 shell]# openssl rand -base64 8 #八位字母和数字的组合NHKIT0Q4Els=[root@lnmp02 shell]# openssl rand -base64 8 | md5sum | cut -c1-8 #openssl rand 之后再使用md5加密 ，cut 截取 1.3.时间随机数123456789101112date +%s%N #生成19位数字，1287764807051101270 ，%s 是秒数，%N是纳秒数 date +%s%N | cut -c 6-13 #取八位数字，21793709date +%s%N | md5sum | head -c 8 #八位字母和数字的组合，87022fda ，-c是字节 #生成一个m-n之间的随机数function rand()&#123; min=$1 max=$(($2-$min+1)) num=$(date +%s%N) echo $(($num%$max+$min))&#125; 1.4.UUID1234#UUID码全称是通用唯一识别码 (Universally Unique Identifier, UUID)，UUID格式是：包含32个16进制数字，以“-”连接号分为五段，形式为8-4-4-4-12的32个字符。linux的uuid码也是有内核提供的，在/proc/sys/kernel/random/uuid这个文件内。cat /proc/sys/kernel/random/uuid每次获取到的数据都会不同。 [root@lnmp02 shell]# cat /proc/sys/kernel/random/uuid1f309506-c862-4b94-9d1a-bcb0f3486b4b 1.5.expect中的mkpasswd123456789101112131415161718yum install expect -y#mkpasswd语法usage: mkpasswd [args] [user] where arguments are: -l # (length of password, default = 9) -d # (min # of digits, default = 2) -c # (min # of lowercase chars, default = 2) -C # (min # of uppercase chars, default = 2) -s # (min # of special chars, default = 1) -v (verbose, show passwd interaction) -p prog (program to set password, default = passwd)#使用[root@lamp01 ~]# mkpasswd -l 8B8ir8^qG[root@lamp01 ~]# mkpasswd -l 8uVD,49rm 2.测试随机数的唯一性123456#通过对产生的随机数统计排序,查看是否唯一性for n in `seq 20`;do date +%s%N|md5sum|cut -c 1-9;done|sort|uniq -c|sort -rn -k1for n in `seq 20`;do echo $RANDOM|md5sum|cut -c 1-9;done|sort|uniq -c|sort -rn -k1# sort -rn -k1 # 其中k1是以第一个字段进行排序 3.组合使用12345[root@lamp01 ~]# echo $RANDOM `mkpasswd -l 8`14081 F~1zeBw6[root@lamp01 ~]# echo $RANDOM `mkpasswd -l 8`|md5sum9570c8be65dbfafb2eb644593b2d3b38 -","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之监控MySQL主从同步是否异常","date":"2017-04-16T04:47:25.612Z","path":"2017/04/16/Linux/shell/shell脚本实例之监控MySQL主从同步是否异常/","text":"1.问题描述&emsp;（生产实战案例）：监控MySQL主从同步是否异常，如果异常，则发送短信或者邮件给管理员。提示：如果没主从同步环境,可以用下面文本放到文件里读取来模拟： 阶段1：开发一个守护进程脚本每30秒实现检测一次。 阶段2：如果同步出现如下错误号（1158,1159,1008,1007,1062），则跳过错误。 阶段3：请使用数组技术实现上述脚本（获取主从判断及错误号部分） 2.分析 监控是否同步 忽略特定的错误号 3.脚本实现12345678910111213141516171819202122#!/bin/sherror_code=(1158 1159 1008 1007 1062);COM=&quot;mysql -uroot -poldboy456 -S /data/3307/mysql.sock&quot;main ()&#123; while true ;do status=($($COM -e &quot;show slave status\\G;&quot;|egrep &quot;_Running|Seconds_Behind_Master|Last_SQL_Errno&quot;|awk &apos;&#123;print $NF&#125;&apos;)); #judge slave is ok if [ &quot;$&#123;status[0]&#125;&quot; == &quot;Yes&quot; -a &quot;$&#123;status[1]&#125;&quot; == &quot;Yes&quot; -a &quot;$&#123;status[2]&#125;&quot; == &quot;0&quot; ];then echo &quot;mysql slave is ok&quot;; else #inogre errors for ((i=0;i&lt;$&#123;#error_code[*]&#125;;i++));do if [ $&#123;error_code[i]&#125; -eq $&#123;status[3]&#125; ];then $COM -e &quot;stop slave;set global sql_slave_skip_counter=1;start slave;&quot;; echo &quot;mysql slave is not ok.&quot;; fi done echo &quot;mysql slave is not ok&quot;|mail -s &quot;mysql_slave_status:&quot; 1327401579@qq.com; fi sleep 30; done&#125;","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之查看某一网段中ping的通的主机","date":"2017-04-16T04:47:25.610Z","path":"2017/04/16/Linux/shell/shell脚本实例之查看某一网段中ping的通的主机/","text":"12345678910111213#!/bin/bashHOST=&quot;192.168.0.&quot;; #指定默认的网段main()&#123; for host_ip in &#123;0..254&#125; ;do &#123; ping -c 2 -w 2 $&#123;HOST&#125;&quot;$&#123;host_ip&#125;&quot; &amp;&gt;/dev/null; #-c ping的次数，-w 超时时间（秒）》将输出定位到null if [ $? -eq 0 ];then echo &quot;$&#123;HOST&#125;$&#123;host_ip&#125;&quot;&gt;&gt;host.txt; #将可以ping通的ip放到一个文件中 fi &#125;&amp; #每一个for循环都在一个新的shell进程中执行，这样执行的速度将非常的快，并行执行的 done&#125;main;","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之批量添加用户设置密码","date":"2017-04-16T04:47:25.604Z","path":"2017/04/16/Linux/shell/shell脚本实例之批量添加用户设置密码/","text":"1.使用shell批量添加用户设置密码1234567891011121314151617#!/bin/sh for username in `echo oldboy&#123;13..14&#125;`;do useradd $username; pass=`echo $RANDOM|md5sum|cut -c 10-15`; echo &quot;$pass&quot;|passwd --stdin $username echo -e &quot;$&#123;username&#125; \\t $&#123;pass&#125;&quot; &gt;&gt;adduser.logdone/*说明:1.$RANDOM是一个内置变量,用来生成一个随机数2.md5sum是一个加密函数,生成一个加密字符串3.cut -c 10-15 是取加密字符串的10-15位,作为密码4.echo -e 表示转义*/ 2.使用chpasswd 批量创建密码123456789101112131415[root@lnmp02 shell]# useradd chenle[root@lnmp02 shell]# useradd chenhao[root@lnmp02 shell]# cat password_.txt #格式一定要是：（用户名：密码），并且用户已经存在chenle:chenlechenhao:chenhao[root@lnmp02 shell]# chpasswd &lt;password_.txt#检查[root@lnmp02 shell]# tail -3 /etc/passwdoldboy14:x:523:524::/home/oldboy14:/bin/bashchenle:x:524:525::/home/chenle:/bin/bashchenhao:x:525:526::/home/chenhao:/bin/bash[root@lnmp02 shell]# 密码的规则要求 使用4种类别字符 足够长,大于7位 使用随机字符串 定期更换 循环周期要足够大 (不要使用最近使用的密码)","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之开机自启动优化使用shell脚本实现","date":"2017-04-16T04:47:25.602Z","path":"2017/04/16/Linux/shell/shell脚本实例之开机自启动优化使用shell脚本实现/","text":"123for serviceName in `chkconfig --list|grep &quot;3:on&quot;|awk &apos;&#123;print $1&#125;&apos;|grep -vE &quot;crond|network|sshd|rsyslog&quot;`;do chkconfig $serviceName offdone","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本实例之分库分表备份脚本","date":"2017-04-16T04:47:25.601Z","path":"2017/04/16/Linux/shell/shell脚本实例之分库分表备份脚本/","text":"1.场景&emsp;当我们只需要恢复一个库的时候（这个库很小），而整个所有的数据库很大，此时我们就需要分库备份 2.shell脚本2.1.多实例 12345678910111213#sed的作用[root@lamp01 ~]# mysql -e &quot;show databases&quot;+--------------------+| Database |+--------------------+| information_schema || mysql || test |+--------------------+[root@lamp01 ~]# mysql -e &quot;show databases&quot;|sed &quot;1d&quot; #sed &quot;1d&quot;就是删除第1行information_schemamysqltest 2.2.单实例123456789101112131415#!/bin/sh USER=&quot;root&quot;;PASSWORD=&quot;123456&quot;;MY_CMD=&quot;mysql -u$USER -p$PASSWORD&quot;;MY_DUMP=&quot;mysqldump -u$USER -p$PASSWORD&quot;;BAK_PATH=&quot;/home/chenyansong/shell/$(date +%F)&quot;;[ ! -d $BAK_PATH ] &amp;&amp; mkdir -p $BAK_PATH for dbName in `$MY_CMD -e &quot;show databases;&quot;|sed &quot;1d&quot;|grep -v &quot;_schema&quot;`;do $MY_DUMP --events -x -B $dbName|gzip&gt;$&#123;BAK_PATH&#125;/$&#123;dbName&#125;.sql.gz if [ $? -eq 0 ];then echo &quot;$&#123;dbName&#125;:$(date +&apos;%F %H:%M:%S&apos;)&quot;&gt;&gt;$&#123;BAK_PATH&#125;/$&#123;dbName&#125;.log fidone 生成的结果文件如下: 3.分表备份3.1.多实例 3.2.单实例123456789101112131415161718192021222324252627#!/bin/sh USER=&quot;root&quot;;PASSWORD=&quot;123456&quot;;MY_CMD=&quot;mysql -u$USER -p$PASSWORD&quot;;MY_DUMP=&quot;mysqldump -u$USER -p$PASSWORD&quot;;BAK_PATH=&quot;/home/chenyansong/shell/$(date +%F)&quot;;[ ! -d $BAK_PATH ] &amp;&amp; mkdir -p $BAK_PATH for dbName in `$MY_CMD -e &quot;show databases;&quot;|sed &quot;1d&quot;|grep -v &quot;_schema&quot;`;do for tname in `$MY_CMD -e &quot;show tables from $dbName&quot;|sed &quot;1d&quot;`;do $MY_DUMP --events -x $dbName $tname|gzip&gt;$&#123;BAK_PATH&#125;/$&#123;dbName&#125;_$&#123;tname&#125;.sql.gz if [ $? -eq 0 ];then echo &quot;$&#123;dbName&#125;_$&#123;tname&#125;:$(date +&apos;%F %H:%M:%S&apos;)&quot;&gt;&gt;$&#123;BAK_PATH&#125;/$&#123;dbName&#125;.log fi done done/*注意：分表备份中，不能使用-B （如果使用，就会当做是多个库），然后就是指定（库名 表名）$MY_DUMP --events -x $dbName $tname恢复某一个表[root@lnmp02 2016-08-21]# mysql -uroot -p123456 test &lt;test_t1.sql */","tags":[{"name":"mysql","slug":"mysql","permalink":"http://yoursite.com/tags/mysql/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell脚本书写规范","date":"2017-04-16T04:47:25.599Z","path":"2017/04/16/Linux/shell/shell脚本书写规范/","text":"1.脚本第一行指定脚本解释器12345#!/bin/sh #or #!/bin/bash 2.脚本开头加版本版权等信息1234567#Date: 16:20 2011-11-11#Author: Created by chenyansong#Mail: xxx.qq.com#Function: This script function is ....#Version: 1.1#提示：可以配置vim编辑文件时自动加上以上信息，方法是修改~/.vimrc配置文件 3.脚本中不要出现中文注释防止本机或者系统切换环境后出现中文乱码的困扰 4.脚本以.sh为扩展名5.代码书写优秀习惯技巧123456#成对的符号内容尽量一次写出来，防止遗漏，如：&#123;&#125;、[]、“”等if语句格式一次写完if 条件内容 then 内容fi 6.代码位置规范在/server/script目录下，有：bin（脚本文件）、conf（配置文件）、func（函数文件）","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell之算术运算","date":"2017-04-16T04:47:25.597Z","path":"2017/04/16/Linux/shell/shell之算术运算/","text":"1.(())1234567891011121314151617181920212223242526272829303132#语法:（表达式1,表达式2…））((a+1,b++,c++))/*特点：1、在双括号结构中，所有表达式可以像c语言一样，如：a++,b--等。2、在双括号结构中，所有变量可以不加入：“$”符号前缀。3、双括号可以进行逻辑运算，四则运算4、双括号结构 扩展了for，while,if条件测试运算5、支持多个表达式运算，各个表达式之间用“，”分开，在for中使用分号：for((i=0;i&lt;5;i++))*/[root@lnmp02 shell]# ((a=1+1))[root@lnmp02 shell]# echo $a2[root@lnmp02 shell]# ((a=1 + 1)) #+号两边有空格[root@lnmp02 shell]# echo $a 2[root@lnmp02 shell]# ((a = 21 + 1)) #+和=之间都有空格[root@lnmp02 shell]# echo $a 22[root@lnmp02 shell]# (( a = 21 + 12 )) #空号内有空格[root@lnmp02 shell]# echo $a 33[root@lamp01 chenyansong]# echo &quot;`seq -s &apos;+&apos; 10`&quot;=&quot;$((`seq -s &apos;+&apos; 10`))&quot;1+2+3+4+5+6+7+8+9+10=55 2.let123456789101112131415161718192021222324252627282930313233#语法let expression#使用let 引用变量无需在添加$前缀a=4;b=5;let result=a+becho $resultlet a++echo $alet a--echo $alet a+=100echo $alet &quot;x=x/(y+1)&quot;#注意双引号被用来忽略括号的特殊含义。同样如果你希望使用空格来分隔操作符和操作符的时候，就必须使用双引号，当使用逻辑和关系操作符，(!,&lt;=,&gt;=,&lt;,&gt;,++,~=),的时候，shell会返回一个代码变量，?会反映结果是真还是假，再一次说明，必须使用双引号来防止shell将大于和小于运算符当作I/O重定向。[root@lnmp02 shell]# let a=1+1 [root@lnmp02 shell]# echo $a2[root@lnmp02 shell]# let a=1 + 1 #不能有空格-bash: let: +: syntax error: operand expected (error token is &quot;+&quot;)[root@lnmp02 shell]# let a= 1+1-bash: let: a=: syntax error: operand expected (error token is &quot;=&quot;)[root@lnmp02 shell]# let a = 1+1-bash: let: =: syntax error: operand expected (error token is &quot;=&quot;)[root@lnmp02 shell]# [root@lnmp02 shell]# let &quot;a = 1 + 13&quot; #如果有空格，需要加上双引号[root@lnmp02 shell]# echo $a 14[root@lnmp02 shell]# 3.expr如表达式中和运算符号之间的空格及一些运算符号需要转义，还有一点需要记住，expr只适用于整数之间的运算 3.1.整数间的四则运算（+、-、*、/、%）123456789101112$expr 9 + 8 - 7 \\* 6 / 5 + \\( 4 - 3 \\) \\* 211#注意其中的反引号：思想就是：拼接字符串，然后将拼接之后的字符串交给命令执行[root@MySQL shell]# echo `seq -s &quot;+&quot; 10`=`seq -s &quot; + &quot; 10|xargs expr`1+2+3+4+5+6+7+8+9+10=55[root@MySQL shell]##判断输入是否为整数expr $1 + 1 &amp;&gt;/dev/null$? -ne 0 3.2.字符串匹配12345678910111213STRING : REGEXP anchored pattern match of REGEXP in STRINGmatch STRING REGEXP same as STRING : REGEXP [root@MySQL shell]# expr match &quot;aaabbbcc&quot; &quot;aa*&quot;3[root@MySQL shell]# expr match &quot;aaabbbcc&quot; &quot;aad&quot;0[root@MySQL shell]#[root@MySQL shell]# expr &quot;aaabbbcc&quot; : &quot;aa*&quot;3[root@MySQL shell]# expr &quot;aaabbbcc&quot; : &quot;aad&quot;0[root@MySQL shell]# 3.3.取字符串的子串1234substr STRING POS LENGTH substring of STRING, POS counted from 1 [root@MySQL shell]# expr substr &quot;aaabbbccc&quot; 2 4 aabb 3.4.取字符的下标1234index STRING CHARS index in STRING where any CHARS is found, or 0 [root@MySQL shell]# expr index &quot;aaabbbccc&quot; a1 3.5.字符串的长度1234length STRING length of STRING [root@MySQL shell]# expr length &quot;aaabbbccc&quot;9 4.计算小数bc在shell命令行直接输入bc及能进入bc语言的交互模式。bc也可以进行非交互式的运算，方法是与echo一起使用。 12[root@MySQL shell]# echo &quot;4 * 0.56&quot;|bc2.24","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell之字符串操作","date":"2017-04-16T04:47:25.595Z","path":"2017/04/16/Linux/shell/shell之字符串操作/","text":"1.判断读取字符串值1234567891011$&#123;var&#125; #变量var的值, 与$var相同 $&#123;var-DEFAULT&#125; #如果var没有被声明, 那么就以$DEFAULT作为其值 * $&#123;var:-DEFAULT&#125; #如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * $&#123;var=DEFAULT&#125; #如果var没有被声明, 那么就以$DEFAULT作为其值 * $&#123;var:=DEFAULT&#125; #如果var没有被声明, 或者其值为空, 那么就以$DEFAULT作为其值 * $&#123;var+OTHER&#125; #如果var声明了, 那么其值就是$OTHER, 否则就为null字符串 $&#123;var:+OTHER&#125; #如果var被设置了, 那么其值就是$OTHER, 否则就为null字符串 $&#123;var?ERR_MSG&#125; #如果var没被声明, 那么就打印$ERR_MSG * $&#123;var:?ERR_MSG&#125; #如果var没被设置, 那么就打印$ERR_MSG * $&#123;!varprefix*&#125; #匹配之前所有以varprefix开头进行声明的变量 $&#123;!varprefix@&#125; #匹配之前所有以varprefix开头进行声明的变量 2.字符串操作（长度，读取，替换）12345678910$&#123; #string&#125; #$string的长度 $&#123;string:position&#125; # 在$ string中, 从位置$ position开始提取子串 $&#123;string:position:length&#125; # 从变量$string的开头, 删除最短匹配$substring的子串 $&#123;string # #substring&#125; #从变量$string的开头, 删除最长匹配$substring的子串 $&#123;string%substring&#125; #从变量$string的结尾, 删除最短匹配$substring的子串 $&#123;string%%substring&#125; #从变量$string的结尾, 删除最长匹配$substring的子串 $&#123;string/substring/replacement&#125; #使用$replacement, 来代替第一个匹配的$substring $&#123;string//substring/replacement&#125; #使用$replacement, 代替所有匹配的$substring $&#123;string/ #substring/replacement&#125; #如果$string的前缀匹配$substring, 那么就用$replacement来代替匹配到的$substring $&#123;string/%substring/replacement&#125; #如果$string的后缀匹配$substring, 那么就用$replacement来代替匹配到的$substring","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"shell中各种括号的作用()、(())、[]、[[]]、{}","date":"2017-04-16T04:47:25.594Z","path":"2017/04/16/Linux/shell/shell中各种括号的作用()、(())、[]、[[]]、{} /","text":"1.()1.1.命令组&emsp;括号中的命令将会新开一个子shell顺序执行，所以括号中的变量不能够被脚本余下的部分使用。括号中多个命令之间用分号隔开，最后一个命令可以没有分号，各命令和括号之间不必有空格。 1.2.用于初始化数组1array=(a b c d) 2.(())2.1.四则运算1a=5; ((a++)) 可将 $a 重定义为6 2.2.逻辑运算1直接使用for((i=0;i&lt;5;i++)) 3.[]3.1.test测试详见：条件测试及控制流.docx 3.2.数组下标在一个array 结构的上下文中，中括号用来引用数组中每个元素的编号 4[[]]使用[[ … ]]条件判断结构 直接使用if [[ \\$a != 1 &amp;&amp; \\$a != 2 ]], 如果不适用双括号, 则为if [ \\$a -ne 1] &amp;&amp; [ \\$a != 2 ]或者if [ \\$a -ne 1 -a \\$a != 2 ]。 5.{}5.1对大括号中的以逗号分割的文件列表进行拓展123touch &#123;a,b&#125;.txt结果为：a.txt b.txt 5.2对大括号中以点点（..）分割的顺序文件列表起拓展作用12如：touch &#123;a..d&#125;.txt结果为a.txt b.txt c.txt d.txt 5.3.代码块&emsp;又被称为内部组，这个结构事实上创建了一个匿名函数 。与小括号中的命令不同，大括号内的命令不会新开一个子shell运行，即脚本余下部分仍可使用括号内变量。括号内的命令间用分号隔开，最后一个也必须有分号。{}的第一个命令和左括号之间必须要有一个空格 5.4其他12类似于下面的结构，对字符串进行操作，详细见：shell.docx 中“字符串处理函数”$&#123;var:-DEFAULT&#125;","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"function函数","date":"2017-04-16T04:47:25.592Z","path":"2017/04/16/Linux/shell/function函数/","text":"1.语法123456789101112#简单语法格式:函数名()&#123; //指令... return n&#125;#规范语法格式:function 函数名()&#123; //指令... return n&#125; 2.函数的调用 直接执行函数名即可(不带括号),函数定义及函数体必须在要执行的函数名的前面定义(即:先定义后使用) 带参数的函数执行方法1234567891011121314函数名 参数1 参数2 ../*shell的位置参数( $1,$2,...$&#123;10&#125;...) , $10 不能获取第十个参数，获取第十个参数需要$&#123;10&#125;。当n&gt;=10时，需要使用$&#123;n&#125;来获取参数$0 比较特殊,他仍然是父脚本的名称$# 传递到脚本的参数个数$* 以一个单字符串显示所有向脚本传递的参数$! 后台运行的最后一个进程的ID号$$ 脚本运行的当前进程ID号$@ 与$*相同，但是使用时加引号，并在引号中返回每个参数$? 显示最后命令的退出状态。0表示没有错误，其他任何值表明有错误在shell函数里面,return命令功能与shell里面的exit类似,但是shell函数体里使用exit会退出整个shell脚本,而不是shell脚本,return语句会返回一个退出值(返回值)给函数的调用者*/ 3.获取函数返回值12345678910111213#!/bin/bash - function mytest() &#123; echo &quot;arg1 = $1&quot; if [ $1 = &quot;1&quot; ] ;then return 1 else return 0 fi &#125; mytest 1 echo $? # print return result 4.书写规范&emsp;一般在一个shell脚本中将代码写在一个模块中，即写在函数中，这样可以根据函数的名称来确定该代码块在进行何种操作，然后就是在所有函数定义的下方调用函数，我们一般将这个总体的调用函数写在main函数中，这是在c中的写法，这里可以参考","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"rsync","date":"2017-04-16T04:47:25.588Z","path":"2017/04/16/Linux/rsync/rsync/","text":"1.什么是rsync&emsp;Rsync是一款开源的、快速的额、多功能的、可实现全量及增量的本地货远程数据同步备份的优秀工具&emsp;Rsync全称为（Remote synchronization）,Rsync具有可使本地和远程两台主机之间的数据块快速复制同步镜像、远程备份的功能，这个功能类似于ssh的scp命令，但又优于scp命令的功能，scp每次都是全量拷贝，而rsync可以增量拷贝。利用Rsync还可以实现删除文件和目录的功能，这又相当于rm命令&emsp;在同步备份数据时，默认情况下，Rsync通过其独特的“quick check”算法，他仅同步大小或者最后修改时间发生变化的文件或目录，当然也可以根据权限、属组等属性的变化同步，但需要指定相应的参数，甚至可以实现只同步一个文件里有变化的内容部分，所以，可以实现快速的同步备份数据 1.1官方文档https://www.samba.org/ftp/rsync/rsync.html 2. rsync工作场景说明 定时同步：cron + rsync 实时数据同步：rsync + inotify 或sersync 3. rsync工作方式Rsync大致使用三种主要的传输数据的方式，man rsync看： 3.1.单个主机本地传输数据（此时类似于cp命令的功能）12345678rsync -avz /etc/hosts /tmp/ #等价于cpmkdir /nullrsync -avz --delete /null/ /tmp/#等价于:rm -rf /tmp/ 3.2.借助rcp，ssh等通道来传输数据（此时类似于scp命令的功能） 3.3以守护进程（socket）的方式传输数据（这个是Rsync自身的重要的功能）见7.2节详细的例子/tmp/ 表示tmp下内容，如果是/tmp则表示是tmp目录以及tmp下的所有内容 4.创建配置文件 man rsyncd.conf 可以看配置的详细说明123456789101112131415161718192021#Rsync serveruid = rsyncgid = rsyncuse chroot = nomax connections = 2000timeout = 600pid file = /var/run/rsyncd.pidlock file = /var/run/rsyncd.locklog file = /var/log/rsyncd.logignore errorsread only = falselist = falsehosts allow = 192.0.0.0/24hosts deny = 0.0.0.0/32auth users = rsync_backupsecrets file = /etc/rsync.passwd #####################[backup]comment = backup serverpath = /backup 5.服务端配置步骤5.1.vim /etc/rsyncd.conf 加入一堆配置文件5.2.创建rsync用户，及共享的目录/backup123456a)useradd rsync -s /sbin/nologin -Mb)id rsyncc)mkdir /backupd)chown -R rsync /backup#注意：这个用户（rsync）要和/etc/rsyncd.conf中的uid和gid对应，rsync服务账户要有对被同步目录(/backup)的写入更新权限。 5.3.创建密码文件12a)echo “rsync_backup:oldboy”&gt;/etc/rsync.passwordb)chmod 600 /etc/rsync.password 5.4rsync –daemon12a)netstat -lntup|grep rsyncb)ps -ef|grep rsync|grep -v grep 5.5加入开机自启动12a)echo “rsync --daemon” &gt;&gt;etc/rc.localb)cat /etc/rc.local 6.客户端配置6.1创建密码文件12echo “oldboy”&gt;/etc/rsync.passwordchmod 600 /etc/rsync.password 6.2 rsync6.2.1 push(推) 1rsync -avz /tmp/ rsync_backup@192.168.0.103::backup --password-file=/etc/rsync.password 6.2.2 pull（拉）1rsync -avz rsync_backup@192.168.0.103::backup /tmp/ --password-file=/etc/rsync.password 6.2.3注意推拉都是客户端的操作7. rsync常用参数1234567891011121314-v --verbose 详细模式输出，传输时的进度等信息-z --compress 传输时进行压缩以提高传输效率，--compress-level=NUM可按级别压缩-a --archive 归档模式，表示以递归的方式传输文件，并保持所有文件属性。-r --recursive 对子目录以递归模式，及目录下的所有目录都同样传输，注意是小写r-t --time 保持文件时间属性-o --owner 保持文件属主信息-p --perms 保持文件权限-g --group 保持文件属组信息-P --progress 显示同步过程及传输时的进度等信息-D --device 保持设备文件信息-l --links 保持软链接-e --rsh=COMMAND 使用的信道协议，指定替代rsh的shell程序，例如：ssh-exclude=PATTERN 指定排除不需要传输的文件模式-exclude-from=file （文件名所在的目录文件） 7.1无差异同步（–delete）1234567rsync -avz --delete /tm/ rsync:rsync_backup@192.168.0.104/back --password-file=/etc/rsync.password#本地有，远端就有，本地没有，就删除远端的/*使用--delete参数，一定要备份覆盖端的数据（备份服务端或者是客户端），这样避免将数据删除使用--delete很危险，慎用！*/ 7.2 –exclude 跨地域数据同步：1.使用一条专线连接两地或者2.使用VPN形成局域网，然后同步 8.定时备份（cron + rsync）8.1准备要同步的目录（ip+date 命名）12[root@nfs-server /]# mkdir /backup[root@nfs-server /]# mkdir /backup/`ifconfig eth0|awk -F &apos;[ :]+&apos; &apos;NR==2 &#123;print $4&#125;&apos;`_$(date +%F) -p 然后将ip+date的目录推到服务端即可 8.2准备要同步的文件（目录下的文件）1[root@nfs-server backup]# cp /var/spool/cron/root /backup/192.168.0.104_2016-07-29/cron_root_$(date +%F) 8.3通过脚本方式实现&emsp;将所有的脚本文件放在一个特定的目录下（这里是/server/scripts），这是一个好的习惯 8.4测试脚本12/bin/sh /server/scripts/bak.sh 8.5写进定时任务12crontab -e00 01 * * * /bin/sh /server/scripts/bak.sh &gt;/dev/null 2&gt;&amp;1 9.排错9.1rsync服务端排错思路 查看rsync服务配置文件路径是否正确，正确的配置路径为：/etc/rsyncd.conf 查看配置文件里host alow ,host deny 允许的ip网段是否是允许客户端访问的ip网段 查看配置文件中path参数里的路径是否存在，权限是否正确（正常应为配置文件中的UID参数对应的属主和组） 查看rsync服务是否启动：ps -ef|grep rsync 端口是否存在：netstat -lntup|grep 873 查看iptables 防火墙和selinux是否开启允许rsync服务通过，也可考虑关闭 查看服务端rsync配置的密码文件是否为600的权限，密码文件格式是否正确，正确格式为：用户名：密码 ，文件路径和配置 文件里的secrect files 参数对应 如果是推送数据，要查看配置rsyncd.conf文件中用户是否对模块下的目录有可读的权限（chown -R rsync /backup） 9.2rsync客户端排错思路 查看客户端rsync配置的密码文件是否为600的权限，密码文件的格式是否正确，注意：需要密码，并且和服务端的密码一致 使用Telnet连接rsync服务器的ip 地址 873端口：telnet 192.168.0.104 873 客户端执行命令是：rsync -avzP rsync_backup@192.168.0.104::backup/test/ /test/ –password-file=/etc/rsync.password 9.3常见的错误FAQ9.3.1 auth failed on module xxxx 9.3.2服务端没有指定共享目录（如：backup目录） 9.3.3 password file must not be other-accessible 9.3.4chroot failed chown -R rsync /backup/ 9.3.5 从客户端推送报错 9.3.6 客户端@ERROR：chdir failed 9.3.7查看log1cat /var/log/rsyncd.log 10.优缺点 优点: 增量备份同步,支持socket(daeme),集中备份 缺点: 大量小文件同步的时候,比对时间较长,有的时候,rsync进程停止,解决:a.打包同步,b.drbd(文件系统复制block块) 同步大文件,10G这样的大文件有时也会有问题,中断,未完成同步前视隐藏文件,同步完成改为正常文件","tags":[{"name":"rsync","slug":"rsync","permalink":"http://yoursite.com/tags/rsync/"}]},{"title":"inotify+rsync","date":"2017-04-16T04:47:25.587Z","path":"2017/04/16/Linux/rsync/inotify+rsync/","text":"1.应用场景：实现nfs数据的热备份 2.原理inotify（sersync）工具会实时监控/data的增删改查，然后使用rsync进行同步，如下： 3.在nfs上做一个rsync的客户端3.1创建/etc/rsync.password12345echo chenyansong&gt;/etc/rsync.password cat /etc/rsync.password chmod 600 /etc/rsync.password ll /etc/rsync.password 3.2推送/data数据1rsync -avz /data rsync_backup@192.168.0.103::backup --password-file=/etc/rsync.password 4.安装inotify工具&emsp;Inotify是一种强大的、细粒度的、异步的文件系统事件监控机制，Linux内核从2.6.13起，加入了Inotify的支持，通过Inotify可以监控文件系统中的添加、删除、修改等各种事件，利用这个内核接口，第三方软件就可以架空文件系统下文件的各种变化情况，而Inotify-tools真实实施这样监控的软件，类似的还有国人在金山公司开发的sersync 4.1查看当前系统是否支持Inotify1234567891011a. uname -rb. ls -l /proc/sys/fs/inotify/[root@lamp01 chenyansong]# ls -l /proc/sys/fs/inotify/总用量 0-rw-r--r-- 1 root root 0 2月 14 17:14 max_queued_events-rw-r--r-- 1 root root 0 2月 14 17:14 max_user_instances-rw-r--r-- 1 root root 0 2月 14 17:14 max_user_watches#显示上面三个文件则表示支持 4.2下载源码包12wget https://github.com/downloads/rvoicilas/inotify-tools/inotify-tools-3.14.tar.gz#如果不行就直接将后面的地址放到浏览器中下载，然后通过win scp 上传即可 4.3解压安装 5.Inotify各个目录文件的含义 6.软件工具&emsp;一共安装了2个工具（命令），即inotifywait和inotifywatch&emsp;inotifywait：在被监控的文件或目录上等待特定的文件系统事件（open,close,delete等）发生，执行后处于阻塞状态，适合在shell脚本中使用。&emsp;inotifywatch：收集被监视的文件系统使用度统计数据，指文件系统事件发生的次数统计。 6.1 inotifywait6.1.1查看帮助文档在/usr/local/inotify-tools 目录下 bin/inotifywait –help 6.1.2inotifywait命令常用参数详解123456789101112131415-r --recursive #监视一个目录下的所有子目录-q --quiet #打印很少的信息，仅仅打印监控事件的信息-m --monitor #始终保持事件的监听状态(接收到一个事情而不退出，无限期地执行。默认的行为是接收到一个事情后立即退出)-excludei #排除文件或目录时不区分大小写-exclude #排除文件或者目录,大小写敏感--timefmt #指定时间的输出格式,用于–format选项中的%T格式 --format #%w 表示发生事件的目录%f 表示发生事件的文件%e 表示发生的事件%Xe 事件以“X”分隔%T 使用由–timefmt定义的时间格式 -e #指定监控事件，如果省略，所有的事件将被监控 6.1.3可以监听的事件：inotify-tools/bin/inotifywait –help 中的 -e 表示事件 access 文件读取 modify 文件更改 attrib 文件属性更改，如权限，时间戳等 close_write 以可写模式打开的文件被关闭，不代表此文件一定已经写入数据 close_nowrite 以只读模式打开的文件被关闭 close 文件被关闭，不管它是如何打开的 open 文件打开 moved_to 一个文件或目录移动到监听的目录，即使是在同一目录内移动，此事件也触发 moved_from 一个文件或目录移出监听的目录，即使是在同一目录内移动，此事件也触发 move 包括moved_to和 moved_from move_self 文件或目录被移除，之后不再监听此文件或目录。 create 文件或目录创建 delete 文件或目录删除 delete_self 文件或目录移除，之后不再监听此文件或目录 unmount 文件系统取消挂载，之后不再监听此文件系统。 6.1.4 写一个测试案例 然后重新开启一个窗口： 打印结果如下： 6.1.5 监控多个事件（-e） 6.1.6写成shell 脚本 放入后台(&amp;表示放入后台，这样就不会出现阻塞) 写入rc.local中去： 6.2inotifywatch自行Google 7.优化 8.优缺点 优点: 配合rsync实现实时数据同步 缺点: 如果并发大于200个文件(10-100K),同步就会有延迟 我们前面写的脚本,每次都是全部全部推送一次,但确实是增量的 监控到事件后,调用rsync同步是单进程的(加&amp;并发),sersync是多进程同步,既然有了inotify-tools,为什么还要开发sersync?sersync功能多:1.配置文件,2.真正的守护进程socket,3.可以对失败文件定时重传,4,第三方的HTTP接口,5.默认多线程同步 9.数据实时同步方案drbd 是基于数据块的同步 高并发数据实时同步方案小结: inotify(sersync)+rsync 文件级别 drbd 文件系统级别 第三方软件的同步功能:mysql ,oracle,mongodb 程序双写 业务逻辑解决","tags":[{"name":"inotify","slug":"inotify","permalink":"http://yoursite.com/tags/inotify/"}]},{"title":"配置虚拟主机","date":"2017-04-16T04:47:25.583Z","path":"2017/04/16/Linux/nginx/配置虚拟主机/","text":"1.配置文件的格式 2.搭建基于域名的多虚拟主机2.1.修改配置文件1234567891011121314151617181920212223242526272829303132333435363738[root@lamp01 nginx]# egrep -v &quot;#|^$&quot; /application/nginx/conf/nginx.conf.default &gt;/application/nginx/conf/nginx.conf[root@lamp01 nginx]# cat ./conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 80; server_name www.etiantian.org; #域名 location / &#123; root html/www; #www是服务的站点 index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; server &#123; listen 80; server_name bbs.etiantian.org; #域名 location / &#123; root html/bbs; #服务站点 index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 2.2.创建站点目录1[root@lamp01 nginx]# mkdir ./html/&#123;www,bbs&#125; -p 2.3.向站点目录中加入首页文件123456[root@lamp01 nginx]# echo &quot;www.etiantian.org&quot;&gt; ./html/www/index.html[root@lamp01 nginx]# echo &quot;bbs.etiantian.org&quot;&gt; ./html/bbs/index.html [root@lamp01 nginx]# cat ./html/&#123;www,bbs&#125;/index.htmlwww.etiantian.orgbbs.etiantian.org 2.4.检查语法1234[root@lamp01 nginx]# ./sbin/nginx -tnginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is oknginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful[root@lamp01 nginx]# 2.5.使配置文件生效1234567891011#如果nginx没有启动,则:[root@lamp01 nginx]# /application/nginx/sbin/nginx#如果nginx已经启动,则[root@lamp01 nginx]# /application/nginx/sbin/nginx -s reload#检查[root@lamp01 nginx]# lsof -i:80COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEnginx 2016 root 6u IPv4 14340 0t0 TCP *:http (LISTEN)nginx 2017 nginx 6u IPv4 14340 0t0 TCP *:http (LISTEN) 2.6.加入/etc/hosts域名解析,使Linux本地也能访问123456789101112[root@lamp01 nginx]# cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 www.etiantian.org::1 localhost localhost.localdomain localhost6 localhost6.localdomain6127.0.0.1 www.etiantian.org bbs.etiantian.org #添加#工作中只需要将域名解析成对应的公网IP即可使用#linux访问[root@lamp01 nginx]# curl www.etiantian.orgwww.etiantian.org[root@lamp01 nginx]# curl bbs.etiantian.orgbbs.etiantian.org 2.7.在Windows的hosts文件修改,然后浏览器访问1hosts中添加上图中的：192.168.0.3 www.etiantian.org bbs.etiantian.org 如果我们直接输入ip地址，那么将直接显示第一个server12[root@lamp01 nginx]# curl 192.168.0.3www.etiantian.org 3.基于端口的多虚拟主机3.1.修改配置文件12345678910111213141516171819202122232425262728293031323334353637[root@lamp01 conf]# cat nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 8001; #改成指定的端口 server_name www.etiantian.org; location / &#123; root html/www; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; server &#123; listen 8002; #改成指定的端口 server_name bbs.etiantian.org; location / &#123; root html/bbs; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125;#就是将上面监听的端口修改成为我们指定的端口：如 listen 8001 3.2.检查语法123[root@lamp01 conf]# ../sbin/nginx -tnginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is oknginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful 3.3.重启生效1[root@lamp01 conf]# ../sbin/nginx -s reload 3.4.使用netstat查看端口是否生效123[root@lamp01 conf]# netstat -lntup|grep nginxtcp 0 0 0.0.0.0:8001 0.0.0.0:* LISTEN 2111/nginx tcp 0 0 0.0.0.0:8002 0.0.0.0:* LISTEN 2111/nginx 3.5.测试1234[root@lamp01 conf]# curl www.etiantian.org:8001www.etiantian.org[root@lamp01 conf]# curl bbs.etiantian.org:8002bbs.etiantian.org 4.基于IP的虚拟主机ip addr 的使用12man ipip addr &#123; add | del &#125; IFADDR dev STRING 4.1.添加一个ip地址1234567891011121314[root@lamp01 conf]# ip addr add 192.168.0.110/24 dev eth0[root@lamp01 conf]# ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 16436 qdisc noqueue state UNKNOWN link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:0c:29:38:6a:b1 brd ff:ff:ff:ff:ff:ff inet 192.168.0.3/24 brd 192.168.0.255 scope global eth0 inet 192.168.0.110/24 scope global secondary eth0 inet6 fe80::20c:29ff:fe38:6ab1/64 scope link valid_lft forever preferred_lft forever 4.2.修改配置文件1234567891011121314151617181920212223242526272829303132333435[root@lamp01 conf]# cat nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; server &#123; listen 192.168.0.3:80; #换成指定的ip server_name www.etiantian.org; location / &#123; root html/www; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; server &#123; listen 192.168.0.110:80; #换成指定的ip server_name bbs.etiantian.org; location / &#123; root html/bbs; index index.html index.htm; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 4.3.检查语法123[root@lamp01 conf]# ../sbin/nginx -tnginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is oknginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful 4.4.使配置文件生效1[root@lnmp02 conf]# ../sbin/nginx -s reload 4.5.修改/etc/hosts123#add192.168.0.3 www.etiantian.org192.168.0.110 bbs.etiantian.org 4.6.测试12345678910[root@lamp01 conf]# netstat -lntup|grep nginxtcp 0 0 192.168.0.110:80 0.0.0.0:* LISTEN 2111/nginx tcp 0 0 192.168.0.3:80 0.0.0.0:* LISTEN 2111/nginx [root@lamp01 conf]# curl bbs.etiantian.org bbs.etiantian.org[root@lamp01 conf]# curl www.etiantian.org www.etiantian.org 5.为虚拟主机配置单独的配置文件5.1.在主文件nginx.conf中添加include123456789101112131415161718192021[root@lamp01 conf]# cat nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; #add include include extra/www.conf include extra/bbs.conf include extra/blog.conf &#125;/* include extra/www.conf 是包含单个文件 include extra/*.conf 是包含extra下的所有以conf结尾的文件*/ 5.2.添加目录extra12#在conf目录下：mkdir extra 5.3.在每个配置文件中（如：extra/bbs.conf）添加如下12345678910[root@lamp01 extra]# cat bbs.conf server &#123; listen 80; server_name bbs.etiantian.org; location / &#123; root html/bbs; index index.html index.php index.htm; &#125; &#125; 5.4.检查语法1[root@lnmp02 conf]# ../sbin/nginx -t 5.5.使配置生效1[root@lnmp02 conf]# ../sbin/nginx -s reload server { listen 192.168.0.3:80; server_name www.etiantian.org; location / { root html/www; index index.html index.htm; } error_page 500 502 503 504 /50x.html; location = /50x.html { root html; } } 6.为一个虚拟主机配置别名123456789101112#别名就是：为一个虚拟主机配置多个访问的入口，如：www.baidu.com , baidu.com 都会跳转到百度的首页 server &#123; listen 80; server_name www.etiantian.org etiantian.org ; location / &#123; root html/www; index index.html index.htm; &#125; &#125;#应用场景：在别名可用用来区分多个不同的主机","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx负载均衡","date":"2017-04-16T04:47:25.581Z","path":"2017/04/16/Linux/nginx/nginx负载均衡/","text":"1.官网中的模块http://nginx.org/en/docs/http/ngx_http_upstream_module.html 2.环境准备2台lamp（都安装好nginx）1台负载均衡（安装好nginx） 3.配置一个简单的负载均衡配置文件创建一个最简的配置文件(去掉注释的部分)1egrep -v &quot;#|^$&quot; /application/nginx/conf/nginx.conf.default &gt; /application/nginx/conf/nginx.conf 在负载均衡服务器上的：12345678910111213141516171819[root@lb01 nginx]# cat /application/nginx/conf/nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; upstream www_server_pools &#123; server 192.168.0.4:80; server 192.168.0.3:80; &#125; server &#123; listen 80; server_name www.etiantian.org; location / &#123; proxy_pass http://www_server_pools; &#125; &#125;&#125; 4.upstream中server后的参数12345678910111213141516171819 upstream www_server_pools &#123; server 192.168.0.4:80 weight=1 max_fails=3; server 192.168.0.3:80 backup; &#125;/*可用的参数说明:server 10.0.10.8:80 可以是ip或域名，端口不写，默认是80 ，高并发场景IP可换成域名，通过DNS做负载均衡weight=1 代表服务器的权重，默认是1，权重数字越大代表接受的请求越多max_fails=1 Nginx尝试连接后端主机失败的此时，这个值配合：proxy_next_upstream,fastcgi_netx_upstream and memcached_next_upstream 这三个参数使用，当Nginx接收到后端服务器返回这三个参数定义的状态码时，会将这个请求转发给正常工作的后端服务器，例如：404,502,503。默认值1，企业场景：建议2-3次，根据业务去配置backup 就是在所有的server都不可用时，才使用backup服务器，就相当于备胎, 当算法为ip_hash的时候，后端算法不能是weight和backup，因为他直接根据ip就决定访问哪台服务器了，就不用weight和backup来决定访问fail_timeout=10s 在max_fails定义的失败次数后,距离下次检查的间隔时间,默认是10s,如果max_fails是5,他就检查5次,如果5次都是502,那么,他就会根据fail_timeout的值,等待10s再去检查,还是只检查一次,如果持续502,在不重新加载nginx配置的情况下,每隔10s都只检测一次,常规业务2-3秒比较合理,比如京东3秒,蓝汛3秒,可根据业务需求去配置down 这标志着服务器永远不可用,这个参数可配合ip_hash使用max_conns=number 单个RS最大并发连接数限制,防止请求过载,保护节点服务器route 设置server路由的名字slow_start=time 宕机的服务器从恢复开始,多长时间内被认为是健康的*/ 5.upstream调度算法&emsp;调度算法一般分为两类:第一类为静态调度算法,即负载均衡器更具自身设定的规则进行分配,不需要考虑后端节点服务器的情况,例如:rr, wrr, ip_hash等都属于静态调度算法&emsp;第二类为动态调度算法,即:负载均衡器会根据后端节点的当前状态来决定是否分发请求,例如:连接数少的优先获得请求,相应时间短的获得请求,例如:least_conn, fair等都属于动态调度算法 5.1.rr轮询(默认调度算法,静态调度算法)&emsp;rr ；你一个，我一个，你一个，我一个&emsp;按客户端请求顺序把客户端的请求逐一分配到不同的后端节点服务器,这相当于LVS中的rr算法,如果侯丹节点服务器宕机(默认情况下nginx只检测80端口),宕机的服务器会被自动从节点服务器池中剔除,以使客户端用户访问不受影响,新的请求会分配给正常的服务器 5.2.wrr(权重轮询,静态调度算法)&emsp;在rr轮询的基础上加上权重,即为权重轮询算法,当使用该算法时,权重和用户访问成正比,权重值越大,被访问的请求也就越多,可以根据服务器的配置和性能指定权重值大小,可以有效解决新旧服务器性能不均带来的请求分配问题&emsp;举个例子:123后端服务器:192.168.1.2的配置为: E5520*2CPU, 8GB内存后端服务器:192.168.1.3的配置为: Xeon(TM)2.80GHz*2, 4GB内存假设希望在有30个请求到达前端时,其中20个请求交给192.168.1.3处理,剩余10个交给192.168.1.2 5.3.ip_hash(静态调度算法)&emsp;每个请求按客户端IP的hash结果分配,当新的请求到达时,先将其客户端ip通过哈希算法哈希出一个值,在随后的客户端请求中,客户IP的哈希值只要相同,机会被分配至同一台服务器,该调度算法可以解决动态网页的session共享问题,但有时会导致请求分配不均,即无法保证1:1的负载均衡,因为在国内大多数公司都是NAT上网模式,多个客户端会对应一个外部IP,所以这些客户端都会被分配到同一节点服务器,从而导致请求分配不均 5.4.fair(动态调度算法)fair：谁响应快就给谁发&emsp;此算法会根据后端节点服务器的相应时间来分配请求,响应时间短的优先分配&emsp;这是更加智能的调度算法,此种算法可以依据页面大小和加载时间长短智能的进行负载均衡,也就是根据后端服务器的响应时间来分配请求,响应时间短的优先分配,Nginx本身不支持fair调度算法,如果需要使用这种调度算法,必须下载Nginx的相关模块upstream_fair 5.5.least_connleast_conn ：谁最少连接数，就给谁发&emsp;least_conn算法会根据后端节点的连接数来决定分配情况,哪个机器连接数少就分发,此外还有一些第三方的调度算法,例如:url_hash, 一致性HASH算法等 5.6.url_hash算法&emsp;和ip_hash类似,这里是根据访问url的hash结果来分配请求的,让每个url定向到同一个后端服务器,后端服务器为缓存服务器时效果显著,在upstream中加入hash语句,server语句中不能写入weight等其他的参数,hash_methodshiyo使用的是hash算法&emsp;url_hash按访问url的hash结果来分配请求,使每个url定向到同一个后端服务器,可以进一步提高后端缓存服务器的效率命中率,Nginx本身不支持url_hash的,如果需要使用这种调度算法,必须安装Nginx的hash软件包 url_hash的问题&emsp;当新增服务器或者是一个缓存服务器挂了，那么url_hash将重新排列，此时原先的缓存服务器中的缓存的数据就无效了，因为可能重新排过的url，将不会找原来的服务器了，因为根据服务器的数据来取模，然后找对应的服务器，如果数量变了，那么取模之后找的服务器就会变，所以原来的缓存将会失效 5.7.一致性HASH算法&emsp;一致性HASH算法一般用于代理后端业务为缓存服务的场景,通过将用户请求的URI或者指定字符串进行计算,然后调度到后端的服务器上,此后任何用户查找同一个URI都会被调度到这一台服务器上,因此后端的每个节点缓存的内容都是不同的,一致性HASH算法可以让后端某个或几个节点宕机后,缓存的数据动荡的最小","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx缓存","date":"2017-04-16T04:47:25.580Z","path":"2017/04/16/Linux/nginx/nginx缓存/","text":"参见: https://linux.cn/article-7726-1.html http://seanlook.com/2015/06/02/nginx-cache-check/","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"Nginx状态码","date":"2017-04-16T04:47:25.575Z","path":"2017/04/16/Linux/nginx/Nginx状态码/","text":"nginx 403 forbidden 引起nginx 403 forbidden有二种原因，一是缺少索引文件，二权限问题。 缺少index.html或者index.php文件 1234567server &#123; listen 80; server_name localhost; index index.php index.html; root /home/zhangy/www; #如果在/home/zhang/www下面没有index.php,index.html的时候，直接访问域名，找不到文件，会报403 forbidden。例如：你访问www.test.com而这个域名，对应的root指定的索引文件不存在。 权限问题 因为权限问题引起的403，个人觉得比较难查找，因为一时想不起1234567server &#123; listen 80; server_name localhost; index index.php index.html; root /home/zhangy/www; #我把web目录放在用户的所属目录下面，nginx的启动用户默认是nginx的，所以对目录根本没有读的权限，这样就会报403错误了。这个时候，把web目录的权限改大，或者是把nginx的启动用户改成目录的所属用户，重起一下就能解决。","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"Nginx日志","date":"2017-04-16T04:47:25.574Z","path":"2017/04/16/Linux/nginx/Nginx日志/","text":"1.错误日志1.1.语法12error_log file level;#关键字 日志文件 错误日志级别 1.2.常见的错误日遏制级别&emsp;有【debug|info|notice|warn|error|crit|alert|emerg】从左到右，级别越高，记录的信息越少，一般情况下是warn|error| crit这三个级别之一，注意在生产环境中不要配置info等低级别，会带来大的磁盘I/O消耗 1.3.默认值1#default：error_log logs/error.log error; 1.4.可以放置的标签段为1#context：main , http, server, location 2.访问日志2.1.日志格式log_format12#他属于http_log_module模块http://nginx.org/en/docs/http/ngx_http_log_module.html 2.1.1.格式及各个字段的含义 123456789101112131415/*$remote_addr 与$http_x_forwarded_for 用以记录客户端的ip地址；$remote_user ：用来记录客户端用户名称；$time_local ： 用来记录访问时间与时区； $request ： 用来记录请求的url与http协议；$status ： 用来记录请求状态；成功是200，$body_bytes_s ent ：记录发送给客户端文件主体内容大小；$http_referer ：用来记录从那个页面链接访问过来的；$http_user_agent ：记录客户毒啊浏览器的相关信息； -：空白，用一个“-”占位符替代，历史原因导致还存在。 $http_x_forwarded_for 当前端有代理服务器时，设置Web节点记录客户端地址的配置，此参数生效的前提是代理服务器上也要有进行相关的x_forwarded_for设置。*/ 2.1.2.一条日志的例子1192.168.1.102 - scq2099yt [18/Mar/2013:23:30:42 +0800] &quot;GET /stats/awstats.pl?config=scq2099yt HTTP/1.1&quot; 200 899 &quot;http://192.168.1.1/pv/&quot; &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows XXX; Maxthon)&quot; 2.1.3.注意事项需要注意的是：log_format配置必须放在http内，否则会出现如下警告信息：1nginx: [warn] the &quot;log_format&quot; directive may be used only on &quot;http&quot; level in /etc/nginx/nginx.conf:97 2.2.access_log12345678Syntax: access_log path [format [buffer=size] [gzip[=level]] [flush=time] [if=condition]];access_log off; #表示不记录访问日志#默认配置: access_log logs/access.log combined;#可以放置的位置: http, server, location, if in location, limit_except#参数说明：buffer=size 为存放访问日志的缓冲区大小，flush=time为将缓冲区的日志刷到磁盘的时间，gzip[=level]表示压缩的级别，#一般将访问日志放在对应的虚拟主机下，这样不同的站点有不同的访问日志，我们可以更好的统计 3.日志轮询shell脚本&emsp;思想：就是写一个shell脚本，然后将对应虚拟主机的访问日志，改名，改成带时间戳的文件名，然后重启Nginx，这样就又会生成一个对应虚拟主机的访问日志文件 3.1.写shell脚本 3.2.测试 3.3.定时任务","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx实现动静分离","date":"2017-04-16T04:47:25.572Z","path":"2017/04/16/Linux/nginx/nginx实现动静分离/","text":"1.应用场景&emsp;在企业中，有事希望只用一个域名对外提供服务，不希望使用多个域名对应同一个产品业务，此时就需要在代理服务器上通过配置规则，使得匹配不同规则的请求会交给不同的服务器池处理，这类业务有： 业务的域名没有拆分或者不希望拆分，但是希望实现动静分离，多业务分离。 不同的客户端设备（例如：手机和PC端）使用同一个域名访问同一个业务网站，就需要根据规则将不同设备的用户请求交给后端不同的服务器处理，以便得到最佳的用户体验。 2.拆分&emsp;将文件上传程序、动态web程序和静态数据请求程序分别放在不同的服务器中，这样实现业务拆分上传服务器：将上传文件的地址放到数据库中，将文件存储到NFS中 动态web服务器：直接和数据库打交道 静态数据请求服务器：直接请求NFS数据 3.程序没有拆分&emsp;如果：文件上传程序、动态web程序和静态数据请求程序没有拆分，那么将所有的程序在下面三种服务器中都存放一份，然后，对于负载均衡器中的请求，要根据条件过滤是找静态数据请求服务器，还是找动态程序服务器（此时上传程序和动态web程序一起了） 4.实战14.1.先进行企业案例需求梳理 当用户请求www.etiantian.org/upload/xx地址时实现由upload上传服务器池处理请求 当用户请求www.etiantian.org/static/xx地址时实现由静态服务器池处理请求 除此之外，对于其他访问请求，全都由默认的动态服务器池处理请求 4.2配置说明1234567891011121314#static_pools 为静态服务器池，有一个服务器，地址为10.0.0.9，端口为80upload static_pools &#123; server 10.0.0.9:80 weight=1&#125;#upload_pools为上传服务器池，有一个服务器，地址为10.0.0.10，端口为80upload upload_pools &#123; server 10.0.0.10:80 weight=1&#125;#default_pools为动态服务器池，有一个服务器，地址为10.0.0.9，端口为8000upload default_pools &#123; server 10.0.0.9:8000 weight=1&#125; 4.3.location正则匹配 4.4.if判断 5.实战25.1根据PC端浏览器的不同设置对应的匹配规则12345678910location / &#123; if ($http_user_agent ~* “MSIE”)&#123; #如果请求的浏览器为IE（MSIE），请求有static_pools池处理 proxy_pass http://static_pools; &#125; if ($http_user_agent ~* “Chrome”)&#123; #如果请求的浏览器为谷歌，请求有upload_pools池处理 proxy_pass http://upload_pools; &#125;&#125; #$http_user_agent 是客户端代理，即客户端是使用的什么来访问的 5.2根据移动端设备拆分","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx官方文档指南","date":"2017-04-16T04:47:25.570Z","path":"2017/04/16/Linux/nginx/nginx官方文档指南/","text":"&emsp;我们要查询一些nginx的资料的话，可以上官网看：http://nginx.org/en/docs/因为nginx是分模块的，所以我们可以看对应模块下的东西：如下图：","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx安装","date":"2017-04-16T04:47:25.569Z","path":"2017/04/16/Linux/nginx/nginx安装/","text":"官网：www.nginx.org 1.检查版本&emsp;在安装之前，要先检查机器的版本，否则安装的时候可能出现系统不兼容的情况 1234567[root@lamp01 ~]# cat /etc/redhat-releaseCentOS release 6.3 (Final)[root@lamp01 ~]# uname -r2.6.32-279.el6.i686[root@lamp01 ~]# uname -mi686 2.安装pcre pcre-devel12345#Pcre (Perl Compatible Regular Expressions) perl兼容的正则表达式，官网：www.pcre.org#HTTP rewrite modul requires the PCRE library (rewrite伪静态需要pcre的支持)rpm -qa pcre pcre-develyum install pcre-devel -y 3.安装openssl12345/*ssh的加密软件SSL module require the OpenSSL library */yum install openssl-devel -y 4.安装nginxhttp://nginx.org/download/nginx-1.6.3.tar.gz 4.1.下载12345678910mkdir /root/oldboy/tools -pcd /root/oldboy/toolswget -q http://nginx.org/download/nginx-1.6.3.tar.gz/*其中：-q是取消输出下载地址， 可以去nginx的官网download页面，然后右键复制地址*/#解压 tar -zxvf nginx-1.6.3.tar.gz 4.2.编译123456789./configure --prefix=/application/nginx-1.6.3 --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module# prefix : 表示软件安装在哪#user, group 是以什么用户和组来安装软件#http_ssl_module 是安全模块; http_stub_status_module 是监控模块#之所以用编译的方式，不用yum的方式，是我们可以定制，哪些模块需要我们就安装，哪些模块不需要我们就不编译安装#我们将：--without-pcre 我们就可以取消对pcre模块的加入,对于陌生的软件，我们需要看他的readme：#查看编译的参数：如果软件不是我们安装的，我们要知道其中安装了哪些模块，使用:-V 来查看，如下： 4.3.安装1make &amp;&amp; make install 4.4.创建需要的用户1useradd nginx -s /sbin/nologin -M 4.5.创建软链接1ln -s /application/nginx-1.6.3/ /application/nginx 5.启动测试1234567891011#1.启动/application/nginx/sbin/nginx#2.检查端口lsof -i :80 ##查看服务和端口（默认端口是80）ps -ef |grep nginx#3.使用浏览器访问（安装nginx的主机ip为：http://192.168.0.106/）默认端口是：80http://192.168.0.106/ #这里有点相当于tomcat &emsp;如果使用浏览器访问不了：1.先ping ip ，2然后Telnet ip port 3.查看防火墙是否关闭，或者是selinux打开了，使用setgotten 0将其临时关闭即可 6.启动参数说明123456789101112131415[root@lamp01 ~]# cd /application/nginx/[root@lamp01 nginx]# ./sbin/nginx -hnginx version: nginx/1.6.3Usage: nginx [-?hvVtq] [-s signal] [-c filename] [-p prefix] [-g directives] Options: -?,-h : this help -v : show version and exit -V : show version and configure options then exit #显示编译的时候加入的模块 -t : test configuration and exit #相当于语法检查 -q : suppress non-error messages during configuration testing -s signal : send signal to a master process: stop, quit, reopen, reload #启动参数 -p prefix : set prefix path (default: /application/nginx-1.6.3/) -c filename : set configuration file (default: conf/nginx.conf) -g directives : set global directives out of configuration file Nginx目录结构说明1234567891011121314151617181920[root@lamp01 nginx]# ll /application/nginx/总用量 36drwx------ 2 nginx root 4096 8月 1 2016 client_body_tempdrwxr-xr-x 3 root root 4096 2月 15 18:21 confdrwx------ 2 nginx root 4096 8月 1 2016 fastcgi_tempdrwxr-xr-x 4 root root 4096 8月 1 2016 htmldrwxr-xr-x 2 root root 4096 8月 3 2016 logsdrwx------ 2 nginx root 4096 8月 1 2016 proxy_tempdrwxr-xr-x 2 root root 4096 8月 1 2016 sbindrwx------ 2 nginx root 4096 8月 1 2016 scgi_tempdrwx------ 2 nginx root 4096 8月 1 2016 uwsgi_temp#其中temp都是一些临时文件[root@lamp01 nginx]# ll|grep -v temp 总用量 36drwxr-xr-x 3 root root 4096 2月 15 18:21 conf #配置文件目录drwxr-xr-x 4 root root 4096 8月 1 2016 html #站点目录drwxr-xr-x 2 root root 4096 8月 3 2016 logs #日志目录drwxr-xr-x 2 root root 4096 8月 1 2016 sbin #启动目录 7.错误排查没有安装pcre-devel 安装操作系统的时候没有安装gcc 需要安装下面的软件包1yum groupinstall &quot;Development tools&quot; 查看log日志","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx健康检查","date":"2017-04-16T04:47:25.567Z","path":"2017/04/16/Linux/nginx/nginx健康检查/","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273[root@lamp01 conf]# cat check_web.sh ###################################################################### File Name: check_web.sh# Author: chenyansong# Description: 其实就是通过curl返回的状态,来判断Nginx是否良好# Created Time: 2016-09-01 19:34:22######################################################################!/bin/bash#要检查的Nginx节点rs_arr=( 192.168.0.3 192.168.0.4 )file_location=/var/html/test.html#通过curl查看网站返回状态是否正常function web_result &#123; rs=`curl -I -s $1|awk &apos;NR==1&#123;print $2&#125;&apos;` return $rs #返回状态码&#125;#拼接tablefunction new_row &#123;cat &gt;&gt; $file_location &lt;&lt;eof &lt;tr&gt; &lt;td bgcolor=&quot;$4&quot;&gt;$1&lt;/td&gt; &lt;td bgcolor=&quot;$4&quot;&gt;$2&lt;/td&gt; &lt;td bgcolor=&quot;$4&quot;&gt;$3&lt;/td&gt; &lt;/tr&gt;eof&#125;function auto_html &#123; web_result $2 rs=$? if [ $rs -eq 200 ] then new_row $1 $2 up green else new_row $1 $2 down red fi&#125;main()&#123;while truedocat &gt;&gt; $file_location &lt;&lt;eof &lt;h4&gt;he Status Of RS :&lt;/h4&gt; &lt;meta http-equiv=&quot;refresh&quot; content=&quot;1&quot;&gt; &lt;table border=&quot;1&quot;&gt; &lt;tr&gt; &lt;th&gt;NO:&lt;/th&gt; &lt;th&gt;IP:&lt;/th&gt; &lt;th&gt;Status:&lt;/th&gt; &lt;/tr&gt;eoffor ((i=0;i&lt;$&#123;#rs_arr[*]&#125;;i++)); do auto_html $i $&#123;rs_arr[$i]&#125;donecat &gt;&gt; $file_location &lt;&lt;eof&lt;/table&gt;eofsleep 2&gt; $file_locationdone&#125;main","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"},{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"}]},{"title":"Nginx status查看状态信息","date":"2017-04-16T04:47:25.565Z","path":"2017/04/16/Linux/nginx/Nginx status查看状态信息/","text":"&emsp;Nginx软件的功能模块中有一个ngx-http_stub_status_module 模块，这个模块能记录Nginx的基本访问状态信息，让使用者能够记录Nginx的基本访问状态信息。 1.检查是否有http_stub_status_module模块12345[root@lamp01 ~]# /application/nginx/sbin/nginx -V nginx version: nginx/1.6.3built by gcc 4.4.6 20120305 (Red Hat 4.4.6-4) (GCC)TLS SNI support enabledconfigure arguments: --prefix=/application/nginx-1.6.3 --user=nginx --group=nginx --with-http_ssl_module --with-http_stub_status_module 2.新建一个虚拟主机，用于查看状态用123456789[root@lamp01 extra]# cat /application/nginx/conf/extra/status.conf server &#123; listen 80; server_name status.etiantian.org; location / &#123; stub_status on; #将状态打开 access_log off; &#125; &#125; 3.添加到主配置文件中123456789101112131415[root@lamp01 conf]# cat ./nginx.confworker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; sendfile on; keepalive_timeout 65; include extra/www.conf; include extra/bbs.conf; include extra/status.conf; #添加到主配置文件中 &#125; 4.测试1234567891011121314151617#修改/etc/hosts,添加192.168.0.3 status.etiantian.org#192.168.0.3是我的机器地址#语法检查[root@lamp01 conf]# ../sbin/nginx -t nginx: the configuration file /application/nginx-1.6.3/conf/nginx.conf syntax is oknginx: configuration file /application/nginx-1.6.3/conf/nginx.conf test is successful#重新加载[root@lamp01 conf]# ../sbin/nginx -s reload#访问[root@lamp01 conf]# curl status.etiantian.orgActive connections: 1server accepts handled requests1 1 1Reading: 0 Writing: 1 Waiting: 0 状态参数说明1234567891011121314Active connections: 1#表示Nginx正在处理的活动连接数1个server accepts handled requests1 1 1#第一个server表示Nginx启动到现在共处理了1个连接#第二个accepts表示Nginx启动到现在成功创建1次握手,请求丢失数=(握手数-连接数), 可以看出,本次状态显示美誉丢失请求#第三个handled requests ,表示总共处理了1次请求Reading: 0 Writing: 1 Waiting: 0Reading: Nginx读取客户端的Header信息数Writing: Nginx返回给客户端的Header信息数Waiting: Nginx已经处理完正在等候下一次请求指令的驻留连接,开启keep-alive的情况下,这个值等于active-(reading+writing)","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"nginx rewrite 重写","date":"2017-04-16T04:47:25.563Z","path":"2017/04/16/Linux/nginx/nginx rewrite 重写/","text":"官方：http://nginx.org/en/docs/http/ngx_http_rewrite_module.html参考：http://seanlook.com/2015/05/17/nginx-location-rewrite/#more 1.介绍&emsp;rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。rewrite只能放在server{},location{},if{}中，并且只能对域名后边的除去传递的参数外的字符串起作用，例如http://seanlook.com/a/we/index.php?id=1&amp;u=str 只对/a/we/index.php重写。需要PCRE软件的支持，即通过Perl兼容正则表达式语法进行规制匹配，实现URL的重写 2.语法1234#语法：rewrite regex replacement [flag] #将匹配（regex）到的部分替换（replacement ）#默认：none#应用位置：server, location, if 3.flag标志位 last :本条规则匹配完成后,继续向下匹配新的locationURI规则 break : 本条规则匹配完成即终止,不再匹配后面的任何规则 redirect : 返回302临时重定向，地址栏会显示跳转后的地址 permanent : 返回301永久重定向，地址栏会显示跳转后的地址 &emsp;因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解： last一般写在server和if中，而break一般使用在location中 last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配 break和last都能组织继续执行后面的rewrite指令 4.if中的rewrite4.1.语法12#语法if(condition)&#123;...&#125;，对给定的条件condition进行判断 4.2.说明1234567#当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false#直接比较变量和内容时，使用=或!=~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配-f和!-f用来判断是否存在文件-d和!-d用来判断是否存在目录-e和!-e用来判断是否存在文件或目录-x和!-x用来判断文件是否可执行 4.3.实例12345678910111213141516171819202122232425262728293031if ($http_user_agent ~ MSIE) &#123; rewrite ^(.*)$ /msie/$1 break;&#125; //如果UA包含&quot;MSIE&quot;，rewrite请求到/msid/目录下 if ($http_cookie ~* &quot;id=([^;]+)(?:;|$)&quot;) &#123; set $id $1;&#125; //如果cookie匹配正则，设置变量$id等于正则引用部分 if ($request_method = POST) &#123; return 405;&#125; //如果提交方法为POST，则返回状态405（Method not allowed）。return不能返回301,302 if ($slow) &#123; limit_rate 10k;&#125; //限速，$slow可以通过 set 指令设置 if (!-f $request_filename)&#123; break; proxy_pass http://127.0.0.1;&#125; //如果请求的文件名不存在，则反向代理到localhost 。这里的break也是停止rewrite检查 if ($args ~ post=140)&#123; rewrite ^ http://example.com/ permanent;&#125; //如果query string中包含&quot;post=140&quot;，永久重定向到example.com location ~* \\.(gif|jpg|png|swf|flv)$ &#123; valid_referers none blocked www.jefflei.com www.leizhenfang.com; if ($invalid_referer) &#123; return 404; &#125; //防盗链&#125; 4.4.下面是可以用作if判断的全局变量1234567891011121314151617181920212223242526272829$args ： #这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 例：http://localhost:88/test1/test2/test.php$host：localhost$server_port：88$request_uri：http://localhost:88/test1/test2/test.php$document_uri：/test1/test2/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test2/test.php 5.常用正则123456789101112. ： 匹配除换行符以外的任意字符? ： 重复0次或1次+ ： 重复1次或更多次* ： 重复0次或更多次\\d ：匹配数字^ ： 匹配字符串的开始$ ： 匹配字符串的介绍&#123;n&#125; ： 重复n次&#123;n,&#125; ： 重复n次或更多次[c] ： 匹配单个字符c[a-z] ： 匹配a-z小写字母的任意一个小括号()之间匹配的内容，可以在后面通过$1来引用，$2表示的是前面第二个()里的内容。正则里面容易让人困惑的是\\转义特殊字符。 6.rewrite实例12rewrite ^/images/(.*)_(\\d+)x(\\d+)\\.(png|jpg|gif)$ /resizer/$1.$4?width=$2&amp;height=$3? last;#对形如/images/bla_500x400.jpg的文件请求，重写到/resizer/bla.jpg?width=500&amp;height=400地址，并会继续尝试匹配location。 7.应用场景 可以调整用户浏览的URL，看起来更规范，合乎开发及产品人员的需求 为了让搜索引擎收录网站内容及用户体验更好，企业会将动态URL地址伪装成静态地址提供服务 网站换新域名后，让旧的域名的访问跳转到新的域名上 根据特殊变量、目录、客户端的信息进行URL跳转等。","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"location正则写法","date":"2017-04-16T04:47:25.562Z","path":"2017/04/16/Linux/nginx/location正则写法/","text":"参考（非常详细）；http://fantefei.blog.51cto.com/2229719/919431 1.匹配说明 已=开头表示精确匹配 ^~ 开头表示uri以某个常规字符串开头，不是正则匹配 ~ 开头表示区分大小写的正则匹配; ~* 开头表示不区分大小写的正则匹配 / 通用匹配, 如果没有其它匹配,任何请求都会匹配到 &emsp;所有的匹配都是主机名之后的字符的匹配，如：www.etiantian.org/static/iphone.jpg，中只有/static/iphone.jpg会在location中进行匹配，其中/匹配到主机。 2.实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051location = / &#123; # 精确匹配 / ，主机名后面不能带任何字符串 [ configuration A ]&#125;location / &#123; # 因为所有的地址都以 / 开头，所以这条规则将匹配到所有请求 # 但是正则和最长字符串会优先匹配 [ configuration B ]&#125;location /documents/ &#123; # 匹配任何以 /documents/ 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ configuration C ]&#125;location ~ /documents/Abc &#123; # 匹配任何以 /documents/Abc 开头的地址，匹配符合以后，还要继续往下搜索 # 只有后面的正则表达式没有匹配到时，这一条才会采用这一条 [ configuration CC ]&#125;location ^~ /images/ &#123; # 匹配任何以 /images/ 开头的地址，匹配符合以后，停止往下搜索正则，采用这一条。 [ configuration D ]&#125;location ~* \\.(gif|jpg|jpeg)$ &#123; # 匹配所有以 gif,jpg或jpeg 结尾的请求 # 然而，所有请求 /images/ 下的图片会被 config D 处理，因为 ^~ 到达不了这一条正则 [ configuration E ]&#125;location /images/ &#123; # 字符匹配到 /images/，继续往下，会发现 ^~ 存在 [ configuration F ]&#125;location /images/abc &#123; # 最长字符匹配到 /images/abc，继续往下，会发现 ^~ 存在 # F与G的放置顺序是没有关系的 [ configuration G ]&#125;location ~ /images/abc/ &#123; # 只有去掉 config D 才有效：先最长匹配 config G 开头的地址，继续往下搜索，匹配到这一条正则，采用 [ configuration H ]&#125;location ~* /js/.*/\\.js 3.匹配优先级12#顺序 no优先级：(location =) &gt; (location 完整路径) &gt; (location ^~ 路径) &gt; (location ~,~* 正则顺序) &gt; (location 部分起始路径) &gt; (/) 4.实际使用建议1234567891011121314151617181920212223所以实际使用中，个人觉得至少有三个匹配规则定义，如下：#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。#这里是直接转发给后端应用服务器了，也可以是一个静态首页# 第一个必选规则location = / &#123; proxy_pass http://tomcat:8080/index&#125;# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用location ^~ /static/ &#123; root /webroot/static/;&#125;location ~* \\.(gif|jpg|jpeg|png|css|js|ico)$ &#123; root /webroot/res/;&#125;#第三个规则就是通用规则，用来转发动态请求到后端应用服务器#非静态文件请求就默认是动态请求，自己根据实际把握#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了location / &#123; proxy_pass http://tomcat:8080/&#125;","tags":[{"name":"nginx","slug":"nginx","permalink":"http://yoursite.com/tags/nginx/"}]},{"title":"NFS(网络文件系统)","date":"2017-04-16T04:47:25.548Z","path":"2017/04/16/Linux/NFS/NFS(网络文件系统)/","text":"1.什么是NFS?&emsp;NFS(Network File System) 网络文件系统，他的主要功能是通过网络（一般是局域网）让不同的主机系统之间可以共享文件或目录。NFS适用于中小型网站的数据共享，而大型的网站可能会用到更复杂的分布式文件系统，例如：Moosefs(mfs),glusterfs,FastDFS 2.应用场景&emsp;NFS网络文件系统一般被用来存储共享视屏、图片、附件等静态资源文件，一般是把网站用户上传的文件都放到NFS共享里，例如：BBS产品的图片、附件、头像，注意网站BBS程序不要放到NFS共享里，然后前端所有的节点访问这些静态资源时都会读取NFS存储上的资源。 3.为什么需要共享存储角色？&emsp;下面通过图解展示集群架构需要共享存储服务的理由：例如：A用户传图片到Web1服务器，然后让B用户访问这张图片，结果B用户访问的请求分发到了Web2上，因为Web2上没有这张图片，结果无法看到A用户上传的图片，如果此时有一个共享存储，A用户上传图片无论分发到Web1还是Web2，最终都存储到共享存储上，此时，B用户访问图片时，无论分发到Web1还是Web2上，最终也都会去共享存储上访问，这样可以访问到资源了。下面是原理图： 4.NFS系统原理4.1原理图 4.2查看挂载信息 4.3客户端和服务端通信原理图 注意：NFS的rpc服务在CentOS5.x下名称为portmap，而在CentOS6.x下名称为rpcbind 4.4比喻：NFS工作流程图 5.安装5.1检查是否已经安装rpcbind 和nfs-utils12[root@nfs-server ~]# rpm -qa nfs-utils rpcbind#默认是没有安装nfs软件包（CentOS5默认是会安装） 5.2 安装软件包123456yum install nfs-utils rpcbind -y[chenyansong@lamp01 ~]$ rpm -aq nfs-utils rpcbindrpcbind-0.2.0-12.el6.i686nfs-utils-1.2.3-70.el6_8.1.i686#出现上述两个包表示安装成功 6.服务端启动6.1启动rpcbind6.1.1查看rpcbind状态12[root@nfs-server ~]# /etc/init.d/rpcbind statusrpcbind is stopped 6.1.2启动1234[root@nfs-server ~]# /etc/init.d/rpcbind startStarting rpcbind: [ OK ][root@nfs-server ~]# /etc/init.d/rpcbind statusrpcbind (pid 2159) is running... 6.1.3查看对应的端口（默认端口是111）123456789101112[root@lamp01 chenyansong]# netstat -lntup|grep rpcbindtcp 0 0 0.0.0.0:111 0.0.0.0:* LISTEN 1181/rpcbind tcp 0 0 :::111 :::* LISTEN 1181/rpcbind udp 0 0 0.0.0.0:111 0.0.0.0:* 1181/rpcbind udp 0 0 0.0.0.0:932 0.0.0.0:* 1181/rpcbind udp 0 0 :::111 :::* 1181/rpcbind udp 0 0 :::932 :::* 1181/rpcbind [root@lamp01 chenyansong]##还可以通过lsof 查看[root@nfs-server ~]# lsof -i :111 6.1.4开机自启动12345678910111213#编辑：/etc/rc.local ,添加启动的命令 /etc/init.d/rpcbind start[root@lamp01 chenyansong]# cat /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff. touch /var/lock/subsys/local #### NFS ######/etc/init.d/rpcbind start 6.1.5查看房源关系映射rpcinfo1234567891011121314rpcinfo -p localhost#查看rpc映射信息，-p表示协议[root@lamp01 chenyansong]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 55295 status 100024 1 tcp 35345 status[root@lamp01 chenyansong]# 6.2启动nfs6.2.1查看nfs状态12345[root@nfs-server ~]# /etc/init.d/nfs statusrpc.svcgssd 已停rpc.mountd is stoppednfsd is stoppedrpc.rquotad is stopped 6.2.2 启动12345678910111213[root@nfs-server ~]# /etc/init.d/nfs startStarting NFS services: [ OK ]Starting NFS quotas: [ OK ]Starting NFS mountd: [ OK ]Starting NFS daemon: [ OK ]正在启动 RPC idmapd： [确定]#再次查看状态[root@lamp01 chenyansong]# /etc/init.d/nfs statusrpc.svcgssd 已停rpc.mountd (pid 7221) 正在运行...nfsd (pid 7236 7235 7234 7233 7232 7231 7230 7229) 正在运行...rpc.rquotad (pid 7217) 正在运行... 6.2.3查看映射关系1234567891011121314151617181920212223242526272829303132333435363738#rpcinfo -p localhost 产生了很多端口映射[root@lamp01 chenyansong]# rpcinfo -p localhost program vers proto port service 100000 4 tcp 111 portmapper 100000 3 tcp 111 portmapper 100000 2 tcp 111 portmapper 100000 4 udp 111 portmapper 100000 3 udp 111 portmapper 100000 2 udp 111 portmapper 100024 1 udp 55295 status 100024 1 tcp 35345 status 100011 1 udp 875 rquotad 100011 2 udp 875 rquotad 100011 1 tcp 875 rquotad 100011 2 tcp 875 rquotad 100005 1 udp 37316 mountd 100005 1 tcp 34211 mountd 100005 2 udp 44362 mountd 100005 2 tcp 49558 mountd 100005 3 udp 40356 mountd 100005 3 tcp 41507 mountd 100003 2 tcp 2049 nfs 100003 3 tcp 2049 nfs 100003 4 tcp 2049 nfs 100227 2 tcp 2049 nfs_acl 100227 3 tcp 2049 nfs_acl 100003 2 udp 2049 nfs 100003 3 udp 2049 nfs 100003 4 udp 2049 nfs 100227 2 udp 2049 nfs_acl 100227 3 udp 2049 nfs_acl 100021 1 udp 53347 nlockmgr 100021 3 udp 53347 nlockmgr 100021 4 udp 53347 nlockmgr 100021 1 tcp 56689 nlockmgr 100021 3 tcp 56689 nlockmgr 100021 4 tcp 56689 nlockmgr[root@lamp01 chenyansong]# 6.2.4配置开机自启动&emsp;因为rpcbind 要在nfs前面启动，所以将他们配置成为开机自启动，两种方式 在chkconfig 中配置, 要让rpcbind在nfs前面启动 在rc.local中配置（推荐）我们一般在rc.local中配置，我们一眼就能够看到我们配置了哪些文件，相比较chkconfig更加的清晰。 123456789101112[root@lamp01 chenyansong]# cat /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff. touch /var/lock/subsys/local #### NFS ######/etc/init.d/rpcbind start/etc/init.d/nfs start 7.nfs服务常见进程说明1[root@lamp01 chenyansong]# ps -ef|egrep &quot;rpc|nfs&quot; 8.服务端配置详解及实战配置8.1详解/etc/exports12345#nfs的配置文件，默认是空的[root@lamp01 chenyansong]# ll /etc/exports-rw-r--r--. 1 root root 0 1月 12 2010 /etc/exports[root@lamp01 chenyansong]# cat /etc/exports[root@lamp01 chenyansong]# 8.1.1/etc/exports书写格式1234#格式：NFS 共享目录 NFS客户端地址1（参1，参2....）客户端地址2（参1，参2....）NFS 共享目录 NFS客户端地址1（参1，参2....）eg: /data 192.168.0.0/24(rw,sync,all_squash) 8.1.2格式说明 8.1.3参数权限图解 8.1.4man export 8.1.5语法检查reload&emsp;配置了/etc/exports之后，我们要进行语法检查，来判断配置是否正确：/etc/init.d/nfs reload reload平滑过渡，用来检查语法 配NFS生产重要技巧: 确保所有客户端服务器对NFS共享目录具备相同的用户访问权限1.1. all_squash把所有客户端都压缩成固定的匿名用户(UID相同)1.2. 就是anonuid,annongid指定的UID和GID的用户 所有的客户端和服务端都需要有一个相同的UID和GID的用户,及nfsnobody(UID必须相同) 8.2服务器端配置步骤8.2.1安装软件1yum install nfs-utils rpcbind -y 8.2.2启动服务（注意先后顺序）123456/etc/init.d/rpcbind status/etc/inti.d/rpcbind startrpcinfo -p localhost //查看是否有映射信息/etc/inti.d/nfs status/etc/inti.d/nfs startrpcinfo -p localhost 8.2.3设置开机自启动12345678910111213141516#两种方法：#1.在chkconfig中配置chkconfig nfs onchkconfig rpcbind on#2.在rc.local中配置（推荐）[root@lamp01 chenyansong]# cat /etc/rc.local#!/bin/sh## This script will be executed *after* all the other init scripts.# You can put your own initialization stuff in here if you don&apos;t# want to do the full Sys V style init stuff. #### NFS ######/etc/init.d/rpcbind start/etc/init.d/nfs start #配置 8.2.4配置nfs服务(/etc/exports)1234echo “/data 192.168.0.0/24(rw,sync,all_squash)”&gt;&gt;/etc/exportsmkdir -p /datachown -R nfsnobody.nfsnobody /data#(查看nfs默认使用的用户以及共享的参数:cat /var/lib/nfs/etab ) 8.2.5重新加载服务(优雅重启)1/etc/init.d/nfs reload ====== exports -r 8.2.6检查或测试挂载12showmount -e localhostmount -t nfs 192.168.0.104:/data /mnt 9.客户端配置详解及实战配置9.1安装软件1yum install nfs-utils rpcbind -y 9.2启动服务rpcbind12/etc/init.d/rpcbind status/etc/inti.d/rpcbind start 9.3测试服务端共享情况1showmount -e server_ip 9.4挂载12mkdir -p /mntmount -t nfs server_ip:/data /mnt 9.5测试读写12touch /mnt/test.txt#然后在服务器端观察是否创建成功 9.6开机自启动(rc.local)12345vim /etc/rc.local /etc/init.d/rpcbind startmount -t nfs 192.168.1.102:/data /mnt#这样开机将自动挂载 10.FAQ10.1 RPC:Program not registered可能的原因是：nfs没有启动或者是nfs较rpcbind早启动，导致nfs没有向rpcbind注册 10.2PRC:Port mapper failure 解决方法:1.ping 192.168.0.104;2 telnet rpc服务的端口（111）： 192.168.0.104 111 10.3写数据写不了（Permission denied） 可能的原因：服务器端共享的/data目录没有设置权限，参见：8.2.4节 11.几个重要的文件11.1 /etc/exports123#/etc/exports :NFS服务器配置文件，配置NFS具体共享服务的地点，默认内容为空，以行为单位[root@nfs-server data]# cat /etc/exports/data 192.168.0.0/24(rw,sync,all_squash) 11.2. /usr/sbin/exportfs12NFS服务的管理命令，例如：可以加载NFS配置生效（等价优雅重启：/etc/init.d/nfs reload），还可以直接配置NFS目录exportfs -rv =======/etc/init.d/nfs reload 11.3 /usr/sbin/showmount1234showmount - show mount information for an NFS server#分别在客户端和服务器端，查看NFS配置及挂载情况NFS服务器端：showmount -e localhostNFS客户端：showmount -e server_ip 11.4 /var/lib/nsf/etab1234#NFS服务端可以通过/cat /var/lib/nfs/etab查看NFS服务端配置的参数细节（有很多没有配置但是默认就有的NFS参数）[root@nfs-server data]# cat /var/lib/nfs/etab/data 192.168.0.0/24(rw,sync,wdelay,hide,nocrossmnt,secure,root_squash,all_squash,no_subtree_check,secure_locks,acl,anonuid=65534,anongid=65534,sec=sys,rw,root_squash,all_squash) 11.5 /proc/mounts在NFS客户端可以通过cat /proc/mounts查看mount的挂载参数细节，如下： 12.mount 挂载12.1NFS客户端挂载的格式12mount -t nfs 192.168.0.104:/data /mnt #此命令在客户端执行,/mnt(必须存在) 12.2执行挂载的过程","tags":[{"name":"NFS","slug":"NFS","permalink":"http://yoursite.com/tags/NFS/"}]},{"title":"Linux基础命令之设置连接服务器的超时时间TMOUT","date":"2017-04-16T04:47:25.544Z","path":"2017/04/16/Linux/linux基础命令/设置连接服务器的超时时间TMOUT/","text":"1.临时生效123[root@linux-study cys_test]# export TMOUT=120[root@linux-study cys_test]# echo $TMOUT 120 2.永久生效1234567[root@linux-study ~]# echo &quot;export TMOUT=300&quot; &gt;&gt;/etc/profile[root@linux-study ~]# tail -1 /etc/profileexport TMOUT=300[root@linux-study ~]# source /etc/profile[root@linux-study ~]# echo $TMOUT300[root@linux-study ~]# 3.查看变量1echo $TMOUT","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之给文件重命名rename","date":"2017-04-16T04:47:25.542Z","path":"2017/04/16/Linux/linux基础命令/给文件重命名rename/","text":"语法123456#语法格式:rename from to filefrom #需要替换或者是需要处理的字符(一般是文件名的一部分也包括扩展名)to #把前面from的部分替换成to部分的内容file #待处理的文件,可以用*处理所有文件 实例123456789101112131415161718[root@lamp01 tardir]# touch stu_&#123;0..5&#125;.jpg[root@lamp01 tardir]# ll总用量 0-rw-r--r-- 1 root root 0 2月 13 15:01 stu_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:01 stu_1.jpg-rw-r--r-- 1 root root 0 2月 13 15:01 stu_2.jpg-rw-r--r-- 1 root root 0 2月 13 15:01 stu_3.jpg-rw-r--r-- 1 root root 0 2月 13 15:01 stu_4.jpg[root@lamp01 tardir]# rename &quot;jpg&quot; &quot;html&quot; ./*.jpg[root@lamp01 tardir]# ll总用量 0-rw-r--r-- 1 root root 0 2月 13 15:01 stu_0.html-rw-r--r-- 1 root root 0 2月 13 15:01 stu_1.html-rw-r--r-- 1 root root 0 2月 13 15:01 stu_2.html-rw-r--r-- 1 root root 0 2月 13 15:01 stu_3.html-rw-r--r-- 1 root root 0 2月 13 15:01 stu_4.html 批量修改文件名12345678910111213141516171819202122232425262728293031323334353637383940#方式1: 先拼接mv ，然后交给bash去执行[root@lamp01 tardir]# ls aa_finished_*.jpgaa_finished_0.jpg aa_finished_2.jpg aa_finished_4.jpgaa_finished_1.jpg aa_finished_3.jpg aa_finished_5.jpg[root@lamp01 tardir]# ls aa_finished_*.jpg|awk -F &quot;_finished&quot; &apos;&#123;print &quot;mv &quot;$0&#125;&apos;mv aa_finished_0.jpgmv aa_finished_1.jpgmv aa_finished_2.jpgmv aa_finished_3.jpgmv aa_finished_4.jpgmv aa_finished_5.jpg[root@lamp01 tardir]# ls aa_finished_*.jpg|awk -F &quot;_finished&quot; &apos;&#123;print &quot;mv &quot;$0&quot; &quot; $1$2&#125;&apos;mv aa_finished_0.jpg aa_0.jpgmv aa_finished_1.jpg aa_1.jpgmv aa_finished_2.jpg aa_2.jpgmv aa_finished_3.jpg aa_3.jpgmv aa_finished_4.jpg aa_4.jpgmv aa_finished_5.jpg aa_5.jpg[root@lamp01 tardir]# ls aa_finished_*.jpg|awk -F &quot;_finished&quot; &apos;&#123;print &quot;mv &quot;$0&quot; &quot; $1$2&#125;&apos;|bash[root@lamp01 tardir]# ll总用量 0-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_2.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_3.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_4.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_5.jpg#方式2:使用for 循环for file in `ls aa_finished_*.jpg`;do mv $file `echo $file|sed &apos;s#_finished##g&apos; `done#方式3:renamerename &quot;_finished&quot; &quot;&quot; aa_finished_*.jpg","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之ps","date":"2017-04-16T04:47:25.527Z","path":"2017/04/16/Linux/linux基础命令/查看进程的命令ps/","text":"1234[root@lamp01 ~]# ps -ef|grep sshroot 1332 1 0 09:12 ? 00:00:00 /usr/sbin/sshdroot 1471 1332 0 09:13 ? 00:00:00 sshd: root@pts/1 root 1577 1473 0 09:53 pts/1 00:00:00 grep ssh","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之查看系统内核版本号,主机名称,操作系统类型","date":"2017-04-16T04:47:25.519Z","path":"2017/04/16/Linux/linux基础命令/查看系统内核版本号,主机名称,操作系统类型/","text":"12345678910111213141516171819#版本[root@lamp01 ~]# cat /etc/redhat-releaseCentOS release 6.3 (Final)#内核[root@lamp01 ~]# uname -r2.6.32-279.el6.i686#32位还是64位[root@lamp01 ~]# uname -mi686#all[root@lamp01 ~]# uname -aLinux lamp01 2.6.32-279.el6.i686 #1 SMP Fri Jun 22 10:59:55 UTC 2012 i686 i686 i386 GNU/Linux#主机名[root@lamp01 ~]# hostnamelamp01","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之查看指定端口对应的服务名称(lsof,netstat)","date":"2017-04-16T04:47:25.509Z","path":"2017/04/16/Linux/linux基础命令/查看指定端口对应的服务名称(lsof,netstat)/","text":"1234567891011121314#已知一个端口为333, 如何查看端口对应的是什么服务?#使用lsof[root@linux-study network-scripts]# lsof -i :52113[root@linux-study network-scripts]# lsof -i tcp:52113#使用netstat[root@linux-study network-scripts]# netstat -lntup|grep 52113-l list-n 数字-t tcp-u udpp programing name 进程名","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux一些重要的配置文件之查看所有登录用户who和查看当前登录用户whoami","date":"2017-04-16T04:47:25.508Z","path":"2017/04/16/Linux/linux基础命令/查看所有登录用户who和查看当前登录用户whoami/","text":"1234567891011121314151617181920212223242526272829303132333435#查看登录的所有用户[root@lamp01 ~]# whoroot pts/1 2017-02-12 14:04 (192.168.0.221)root pts/0 2017-02-12 13:18 (192.168.0.221)#登录用户名 登录终端 tty本地终端 pts远程终端 登录时间 IP地址#本地登录是没有ip地址的[root@lamp01 ~]# w 14:06:22 up 4:54, 2 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/1 192.168.0.221 14:04 0.00s 0.01s 0.00s wroot pts/0 192.168.0.221 13:18 1:44 0.02s 0.02s -bash#查看当前用户[root@lamp01 ~]# whoamiroot#超管和普通用户的区别[root@lamp01 ~]# su - chenyansong[chenyansong@lamp01 ~]$[chenyansong@lamp01 ~]$ logout[root@lamp01 ~]# #角色切换功能描述：从一个用户切换到另外一个用户语法：su - 用户名 [chenyansong@linux-study ~]$ su - root #注意：“-”表示的是带环境变量的切换，从普通用户切换到root需要密码，但是从root切换到普通用户，不需要密码","tags":[{"name":"Linux重要配置文件","slug":"Linux重要配置文件","permalink":"http://yoursite.com/tags/Linux重要配置文件/"}]},{"title":"Linux基础命令之查看命令所在的路径(locate,which,whereis)","date":"2017-04-16T04:47:25.506Z","path":"2017/04/16/Linux/linux基础命令/查看命令所在的路径(locate,which,whereis)/","text":"1.locate(查询文件所在路径)1234567891011121314151617181920212223242526272829303132333435#功能描述：在文件资料库中查找文件语法：locate 文件名 #范例1：[root@linux-study cys_test]# lltotal 4-rw-r--r--. 1 root root 51 Jul 12 20:02 people.txt[root@linux-study cys_test]# locate people.txt/cys_test/people.txt #范例2：//用locate去查找本身locate locate//会发现这样的文件，用locate就是在该文件中进行搜索，该文件会定期更新/var/lib/mlocate/mlocate.db #范例3：//创建一个文件，然后用locate去查询，但是查询不到，/var/lib/mlocate/mlocate.db The database searched by default.这时就要升级文件资料库（mlocate.db），来帮助我们找到对应的文件。[root@localhost /]# touch /home/lingzhiling/chenyansong.list[root@localhost /]# locate chenyansong.list[root@localhost /]# updatedb #更新[root@localhost /]# locate chenyansong.list/home/lingzhiling/chenyansong.list #范例4：不区分大小写，查找//locate不区分大小写[root@linux-study cys_test]# locate peoplE.txt[root@linux-study cys_test]# locate -i peoplE.txt/cys_test/people.txt[root@linux-study cys_test]##注意：但是如果你要找的文件在/tmp 临时文件中，那么它不会去搜索的到的。 2.which(搜索命令所在的目录和别名)12345678#功能描述：搜索命令所在的目录及别名信息语法：which 命令 #范例：[root@linux-study cys_test]# which cpalias cp=&apos;cp -i&apos; /bin/cp[root@linux-study cys_test]# 3.whereis(搜索命令所在目录及帮助文档路径)1234567#功能描述：搜索命令所在目录及帮助文档路径语法：whereis 命令名称 #范例：[root@linux-study cys_test]# whereis cpcp: /bin/cp /usr/share/man/man1p/cp.1p.gz /usr/share/man/man1/cp.1.gz[root@linux-study cys_test]#","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之文件和目录处理命令","date":"2017-04-16T04:47:25.504Z","path":"2017/04/16/Linux/linux基础命令/文件和目录处理命令/","text":"1.目录处理命令1.1.帮助命令12man [命令] ：如 man cpcp --help 1.2.mkdir 创建目录（可递归）123456789101112命令名称：mkdir命令英文原意：make directories命令所在路径：/bin/mkdir执行权限：所有用户语法：mkdir -p [目录名]功能描述：创建新的目录 ， -p 表示递归创建如果没有-p只能是一个一个的创建范例：mkdir -p /tmp/Japan/canjin同时创建多个目录：mkdir -p /tmp/Japan/longze /tmp/Japan/boduo 1.3.pwd （显示当前目录）1234567描述：显示当前目录目录原义：print working directory目录所在的路径：/bin/pwd 范例：[root@linux-study cy1]# pwd/cys_test/cy1 1.4.cp123456789101112131415161718192021222324252627282930引文原义：copy功能描述：复制文件或者目录语法：cp [原文件或者目录] [目标目录]cp -r 复制目录（递归）cp -p 保留文件属性cp -a 相当于-pdr (复制目录） #范例：#复制一个文件到指定的目录下cp /tmp/Japan/aa.confg /tmp/Japan #复制一个文件到指定的目录下，然后保留文件的属性（如创建日期等）cp -p /tmp/Japan/aa.config /tmp/Japan #复制多个文件到指定的目录cp /tmp/Japan/aa.config /tmp/Japan/bb.config /tmp/Japan/ #复制一个目录到指定的目录下cp -r /tmp/Japan /tmp/Japan #复制一个目录到指定的目录下，并修改复制之后的目录的名字cp -r /tmp/Japan /tmp/Japan/changeName#复制一个文件到指定的目录下，并修改复制之后的文件的名字 cp /tmp/Japan/aa.config /tmp/Japan/changeFileName #注意：cp source target ，source可有有多个，而target只能有一个。当目标中指定的文件或者是目录没有的话，那么就是要修改复制文件的名字或者是目录名。当要复制多个文件时，最后一个参数一定要是目录 1.5.mv （移动）12345678910111213功能描述：剪切、重命名文件或者目录原义：move语法： mv [原文件或目录] [目标目录] 范例：#将chenyansong 移动到cys_test目录下[root@linux-study changename]# mv chenyansong.txt /cys_test/ #将error.txt 移动到cys_test目录下, 改名为err.a[root@linux-study cys_test]# mv /cys_test/cy2/changename/error.txt /cys_test/err.a #同时也是可以为移动的目录或者是文件改名的，如果在同一个目录下移动文件，就可以为一个文件改名字。 1.6.rmdir （删除空目录）1234567目录名称：rmdir英文：remove empty directories语法：rmdir [目录名]功能描述：删除空目录 范例：[root@linux-study cys_test]# rmdir cy1 1.7.rm （删除文件）1234567891011121314英文：remove语法：rm -rf [文件或目录]rm -r 删除目录rm -f 强制执行rm /tmp/yum.log #删除文件yum.logrm -rf /tmp/Japan #删除目录/*在Linux中是没有回收站的概念，我们要注意。如果目录下有子目录或者是文件，他会一个一个的问你是否要删除，所以可以使用-f （强制执行），就可以一次删除文件下的所有的东西。*/ 2.文件处理命令2.1.touch创建新文件1234567891011121314功能描述：创建空文件命令所在路径：/bin/touch #范例：[root@linux-study cy1]# touch chenyansong.txt[root@linux-study cy1]# lltotal 0-rw-r--r--. 1 root root 0 Jul 9 11:18 chenyansong.txt 注意：创建带空格的文件：touch japan girl //这是创建了两个文件touch “japan girl” //这才是创建了一个文件为：Japan girl ,但是我们并不建议创建带空格的文件名，因为这样管理起来比较的麻烦。 2.2.cat1234567891011121314151617181920212223242526描述：显示内容，-n 显示行号 范例：[root@linux-study cy1]# cat chenyansong.txtrking directory #显示行号[root@linux-study cy1]# cat -n chenyansong.txt 1 2 3 4 rking directory#使用cat添加多行内容到文件中[root@lamp01 chenyansong]# cat test.txtI am oldboy,myqq is 122344[root@lamp01 chenyansong]# cat &gt;&gt;test.txt &lt;&lt;EOF&gt; add new line&gt; EOF[root@lamp01 chenyansong]# cat test.txt I am oldboy,myqq is 122344add new line[root@lamp01 chenyansong]##注意：cat并不适合浏览文件内容比较长的文件 2.3.echo123456789101112131415161718192021222324252627282930描述：添加单行或者多行文本到文件中，配置 &gt; 或者 &gt;&gt; 可以覆盖或者是追加内容范例：#覆盖[root@linux-study cy1]# echo chenyansong &gt; chenyansong.txt [root@linux-study cy1]# cat chenyansong.txtchenyansong#追加[root@linux-study cy1]# echo chenyansong2222222 &gt;&gt; chenyansong.txt [root@linux-study cy1]# cat chenyansong.txtchenyansongchenyansong2222222 #添加多行内容，需要用双引号[root@linux-study cy1]# echo &quot;chen1 &gt; chen1&gt; chen2&gt; chen3&quot; &gt;&gt; chenyansong.txt[root@linux-study cy1]# cat chenyansong.txtchen1chen1chen2chen3#输出不换行：-n [root@lamp01 chenyansong]# echo -n &quot;zhangsan&quot; ;echo &quot;---lizi&quot;zhangsan---lizi[root@lamp01 chenyansong]# echo -n &quot;zhangsan&quot; ;echo -n &quot;---lizi&quot;zhangsan---lizi[root@lamp01 chenyansong]# # 因为没有换行,所以\\n不显示 2.4.head123456789功能描述：显示文件的前面几行， -n 指定的行数 范例：[root@linux-study fils]# head -11 /etc/services[root@linux-study fils]# head -n 5 /etc/services 注意：head -n 20 /etc/services 如果没有指定20 默认就是10 ，即：head -n /etc/serviceshead -20 是上面的简写形式 2.5.tail123456789101112131415161718192021222324252627功能描述：显示文件后面几行-n 指定行数-f 动态显示文件末尾内容（跟踪一个文件尾部的实时变化）范例：tail -f /var/log/messages#可以查看日志信息，如果日志信息有变动，那么会在其中动态的显示变动的信息。#练习：取一个文件（假设有100行）的17到20行#方式1 很菜，真的很菜，只是为了练习使用1.首先创建一个带行数的100行的文件[root@linux-study cys_test]# seq 100 &gt; /cys_test/seq.txt 2. head和tail收尾的结合使用[root@linux-study cys_test]# head -20 seq.txt | tail -417181920#方式2 ：（简单，高效，推荐） 使用sed ，-n表示取消默认的输出[root@linux-study cys_test]# sed -n &apos;17,20p&apos; seq.txt 17181920[root@linux-study cys_test]# 2.6.more1234567功能描述：分页显示文件内容语法：more 文件名（空格）或f 翻页 回车 换行 Q或者q 退出#如果想要一行一行的浏览，那么使用“enter”，如果想要一页一页的往后，使用空格或者是f, 如果想要退出，q或者Q 2.7.less12345功能描述：分页显示文件内容（可向上翻页）语法：less 文件名 less 命令不仅有more的功能（空格或者f翻页，回车一行一行的浏览，同时可以使用上下箭头浏览行，可以使用pageDn或者是pageUP来上下翻页）搜索功能：less之后，按“/要搜索的内容”，然后按n（next)就可以直接向下搜索要找的内容，他会标记出来。 3.标准输入和输出123456789101112131415161718192021222324252627282930313233#标准输入和输出1.标注输入（stdin）:代码为0，使用&lt; 或者 &lt;&lt; 数据流向从右向左2.正常输出（stdout）:代码为1，使用&gt; 或者&gt;&gt; ，数据流向从左向右3.错误输出（stderr）：代码为2，使用2&gt;或者2&gt;&gt; 范例#标准输出[root@linux-study cy1]# echo 111 1&gt;chenyansong.txt[root@linux-study cy1]# cat chenyansong.txt111 #清空一个文件[root@linux-study cys_test]# cat sed_n.txtchenchen[root@linux-study cys_test]# &gt;sed_n.txt[root@linux-study cys_test]# cat sed_n.txt[root@linux-study cys_test]# #错误输出：下面ec这条命令不存在，所以就会将错误的信息：-bash: ec: command not found 输出到chenyansong.txt中[root@linux-study cy1]# ec 111 2&gt;chenyansong.txt[root@linux-study cy1]# cat chenyansong.txt-bash: ec: command not found[root@linux-study cy1]# #一次性判断正确和错误[root@linux-study cy1]# echo 44 1&gt;ok.txt 2&gt;error.txt[root@linux-study cy1]# cat ok.txt44[root@linux-study cy1]# ech 551&gt;ok.txt 2&gt;error.txt[root@linux-study cy1]# cat error.txt-bash: ech: command not found","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之挂载(mount)与卸载(umount)","date":"2017-04-16T04:47:25.503Z","path":"2017/04/16/Linux/linux基础命令/挂载(mount)与卸载(umount)/","text":"1.mount 挂载1234#语法mount [选项] &lt;-t 类型&gt; [-o 挂载选项] &lt;设备&gt; &lt;挂载点&gt;mount -t ext3 /dev/hdb1 /mnt 2.umount卸载123456#语法umount &lt;挂载点|设备&gt;$ umount /mnt#or$ umount /dev/hdb1 &emsp;有些时候，可能某些设备(通常是 CD-ROM)正忙或无法响应。此时，大多数用户的解决办法是重新启动计算机。我们大可不必这么做。例如，如果 umount /dev/hdc 失败的话，您可以试试“lazy” umount。语法十分简单：umount -l &lt;挂载点|设备&gt; &emsp;卸载的时候,需要umount设备文件名,但是注意不要在挂载的设备中执行卸载,这样是卸载不掉的,因为坐在椅子上如何将椅子拿掉,所以需要在挂载设备的外面进行卸载","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之大括号{}生成序列","date":"2017-04-16T04:47:25.501Z","path":"2017/04/16/Linux/linux基础命令/大括号{}生成序列/","text":"123456789[root@lamp01 chenyansong]echo student&#123;01..10&#125;student01 student02 student03 student04 student05 student06 student07 student08 student09 student10[root@lamp01 chenyansong]echo student&#123;1..10&#125;student1 student2 student3 student4 student5 student6 student7 student8 student9 student10#和seq不同的就是他可以是字母生成的[root@lamp01 chenyansong]echo 11_&#123;a..z&#125;11_a 11_b 11_c 11_d 11_e 11_f 11_g 11_h 11_i 11_j 11_k 11_l 11_m 11_n 11_o 11_p 11_q 11_r 11_s 11_t 11_u 11_v 11_w 11_x 11_y 11_z","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之命令行快捷键","date":"2017-04-16T04:47:25.499Z","path":"2017/04/16/Linux/linux基础命令/命令行快捷键/","text":"快捷键 描述 ctrl+a 切换到命令行开始 Ctrl+e 切换到命令行末尾 Ctrl+c 终止当前命令或脚本 Ctrl+d 退出当前shell,相当于exit Ctrl+l 清除屏幕内容,相当于clear Ctrl+u 清除(剪切)光标之前的内容 Ctrl+k 清除(剪切)光标之后的内容 Ctrl+r 查找 tab 路径补全功能 Ctrl+y 粘贴剪切的内容 Ctrl+f forward 向前跳转一个字符 Ctrl+b backward 向后跳转一个字符","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之取路径(dirname )和取文件名(basename)","date":"2017-04-16T04:47:25.497Z","path":"2017/04/16/Linux/linux基础命令/取路径和取文件名/","text":"1234[root@MySQL ~]# dirname /data/3309/my.cnf/data/3309[root@MySQL ~]# basename /data/3309/my.cnfmy.cnf","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之别名alias","date":"2017-04-16T04:47:25.495Z","path":"2017/04/16/Linux/linux基础命令/别名alias/","text":"alias ddd=’df -Th’ 取一个别名 unalias ddd 取消别名 alias 看系统中的所有的别名 vim ~/.bashrc 永久的修改别名 别名生效的位置: 针对root用户,/root/.bashrc 所有用户生效: /etc/bashrc 或 /etc/profile定义 生效: source /etc/profile 练习 1234567891011/*这样在Linux下输入cp命令实际上运行的是cp -i，加上一个 \\ 符号或者写cp全路径/bin/cp就是让此次的cp命令不使用别名(cp -i)运行。 练习：将一个同名的文件拷贝到另外一个文件中，如何不提示是否覆盖的方法？因为如果直接使用cp，那么会调用上图中的方式“cp -i” -i表示提示，而下面两种方式是绕过别名*/方式一：\\cp chenyansong.txt cys/ 方式二：/bin/cp chenyansong.txt cys/","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之关闭防火墙","date":"2017-04-16T04:47:25.493Z","path":"2017/04/16/Linux/linux基础命令/关闭防火墙/","text":"查看防火墙状态 1[root@linux-study ~]# /etc/init.d/iptables status 关闭防火墙 1[root@linux-study ~]# /etc/init.d/iptables stop 设置开启自启动：关闭防火墙 1[root@linux-study ~]# chkconfig iptables off","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之关闭SELinux","date":"2017-04-16T04:47:25.492Z","path":"2017/04/16/Linux/linux基础命令/关闭SELinux/","text":"&emsp;SELinux(Security-Enhanced Linux)是美国国家安全局对于强制访问控制的实现,这个功能让系统管理员又爱又恨,这里考虑还是将其关闭了,至于安全问题,后面通过其他手段来解决 123456789101112131415[root@lamp01 chenyansong]# cat /etc/selinux/config # This file controls the state of SELinux on the system.# SELINUX= can take one of these three values:# enforcing - SELinux security policy is enforced.# permissive - SELinux prints warnings instead of enforcing.# disabled - No SELinux policy is loaded.SELINUX=disabled# SELINUXTYPE= can take one of these two values:# targeted - Targeted processes are protected,# mls - Multi Level Security protection.SELINUXTYPE=targeted#SELinux有几种模式: enforcing , permissive , disabled #我们需要将其改为disabled 使之生效1234567[root@lamp01 chenyansong]# getenforceDisabled[root@lamp01 chenyansong]# setenforce 0setenforce: SELinux is disabled[root@lamp01 chenyansong]# getenforce Disabled[root@lamp01 chenyansong]#","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之关机/重启/注销","date":"2017-04-16T04:47:25.490Z","path":"2017/04/16/Linux/linux基础命令/关机,重启,注销命令/","text":"1.关机123456789shutdown -h now --&gt;立刻关机（生产常用）shutdown -h +1 --&gt;1分钟以后关机-c #取消前一个关机命令-h #关机-r #重启init 0 --&gt;切换运行级别到0halt --&gt;立即停止系统，需要人工关闭电源（生产常用）poweroff --&gt;立即停止系统，并且关闭电源 2.重启1234reboot #生产常用shutdown -r now #生产常用shutdown -r +1 #1分钟以后重启init 6 # 运行级别切换到6 3.注销123logout exitCtrl + d #快捷键(生产常用)","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之修改命令行前缀提示符ps1","date":"2017-04-16T04:47:25.488Z","path":"2017/04/16/Linux/linux基础命令/修改命令行前缀提示符ps1/","text":"ps1 修改 [root@linux-study /]# 显示 12345678[root@lamp01 tardir]# set|grep -i ps1PS1=&apos;[\\u@\\h \\W]\\$ &apos;_=ps1[root@lamp01 tardir]# [root@lamp01 tardir]# PS1=&apos;[\\u@\\h \\W \\t]&apos;[root@lamp01 tardir 15:24:33]","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之主机名和IP的映射","date":"2017-04-16T04:47:25.486Z","path":"2017/04/16/Linux/linux基础命令/主机名和IP的映射/","text":"规范步骤 方式一: 123456789#1hostname oldboylinux 临时生效，重启失效#2vi /etc/sysconfig/network 主机名#3vi /etc/hosts 主机名和ip的映射#注意：必须要有步骤一，因为要使2,3的修改生效，我们必须要重启服务器，而我们并不希望重启服务器，所以要使命令行生效，必须要有步骤1","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之wget","date":"2017-04-16T04:47:25.477Z","path":"2017/04/16/Linux/linux基础命令/wget/","text":"语法1234567891011121314151617181920212223242526272829wget [option]... [URL]... -q --quiet Turn off Wget’s output. -O file --output-document=file --spider When invoked with this option, Wget will behave as a Web spider, which means that it will not download the pages, just check that they are there. For example, you can use Wget to check your bookmarks: wget --spider --force-html -i bookmarks.html This feature needs much more work for Wget to get close to the functionality of real web spiders. -T seconds --timeout=seconds --limit-rate=amount Limit the download speed to amount bytes per second. Amount may be expressed in bytes, kilobytes with the k suffix, or megabytes with the m suffix. For example, --limit-rate=20k will limit the retrieval rate to 20KB/s. This is useful when, for whatever reason, you don’t want Wget to consume the entire available bandwidth. --tries=number Set number of retries to number. 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445# -O 下载到指定路径并改名[root@lamp01 chenyansong]wget -O ./test_wget www.baidu.com--2017-02-13 17:25:24-- http://www.baidu.com/正在解析主机 www.baidu.com... 119.75.217.109, 119.75.218.70正在连接 www.baidu.com|119.75.217.109|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：2381 (2.3K) [text/html]正在保存至: “./test_wget” 100%[==========================================================================================&gt;] 2,381 --.-K/s in 0.004s 2017-02-13 17:25:34 (640 KB/s) - 已保存 “./test_wget” [2381/2381])# 不显示下载的详情wget -q -O ./test_wget2 www.baidu.com#只是检查文件是否存在不进行下载[root@lamp01 chenyansong]wget --spider http://www.cnblogs.com/peida/archive/2013/03/18/2965d369.html开启 Spider 模式。检查是否存在远程文件。--2017-02-13 17:47:15-- http://www.cnblogs.com/peida/archive/2013/03/18/2965d369.html正在解析主机 www.cnblogs.com... 42.121.252.58正在连接 www.cnblogs.com|42.121.252.58|:80... 已连接。已发出 HTTP 请求，正在等待回应... 404 Not Found远程文件不存在 -- 链接断开！！！ [root@lamp01 chenyansong]echo $? #使用echo $?去检查上一条命令的执行结果,也就是说文件是否存在8[root@lamp01 chenyansong]wget --spider http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html开启 Spider 模式。检查是否存在远程文件。--2017-02-13 17:47:39-- http://www.cnblogs.com/peida/archive/2013/03/18/2965369.html正在解析主机 www.cnblogs.com... 42.121.252.58正在连接 www.cnblogs.com|42.121.252.58|:80... 已连接。已发出 HTTP 请求，正在等待回应... 200 OK长度：78156 (76K) [text/html]存在远程文件且该文件可能含有更深层的链接，但不能进行递归操作 -- 无法取回。 [root@lamp01 chenyansong]#使用wget –tries增加重试次数wget --tries=40 URL 详细参见","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之wc(统计行号)","date":"2017-04-16T04:47:25.475Z","path":"2017/04/16/Linux/linux基础命令/wc(统计行号)/","text":"语法1234567891011121314-c, --bytes #字节数 print the byte counts -m, --chars #字符数 print the character counts -l, --lines #所有的行数 print the newline counts -L, --max-line-length #最长行的长度 print the length of the longest line -w, --words #单词数 print the word counts 实例123456789101112131415161718192021[root@lamp01 chenyansong]cat wc_test.txtaaabbbcccccee#字节数[root@lamp01 chenyansong]wc -c wc_test.txt17 wc_test.txt#字符数[root@lamp01 chenyansong]wc -m wc_test.txt 17 wc_test.txt#所有的行数[root@lamp01 chenyansong]wc -l wc_test.txt 4 wc_test.txt#最长行的长度[root@lamp01 chenyansong]wc -L wc_test.txt 5 wc_test.txt","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之w(显示登录者,以及他们在干什么)","date":"2017-04-16T04:47:25.473Z","path":"2017/04/16/Linux/linux基础命令/w(显示登录者,以及他们在干什么)/","text":"&emsp;Show who is logged on and what they are doing.12345[root@lamp01 chenyansong]w 17:15:06 up 23:17, 2 users, load average: 0.00, 0.00, 0.00USER TTY FROM LOGIN@ IDLE JCPU PCPU WHATroot pts/1 192.168.0.221 17:15 4.00s 0.02s 0.02s -bash #在执行bash命令root pts/0 192.168.0.221 Sun22 0.00s 5.00s 0.00s w","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之uptime(系统运行了多久,多少用户登录,3平均负载)","date":"2017-04-16T04:47:25.472Z","path":"2017/04/16/Linux/linux基础命令/uptime(系统运行了多久,多少用户登录,3平均负载)/","text":"1234567[root@lamp01 chenyansong]uptime 16:54:10 up 22:56, 2 users, load average: 0.00, 0.00, 0.00 16:54:10 #是当前时间22:56 #是系统运行的时间(22h 56min) 2 users #当前登录用户数load average: 0.00, 0.00, 0.00 #在过去的1 , 5, 15 min 内的平均负载","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之uniq","date":"2017-04-16T04:47:25.469Z","path":"2017/04/16/Linux/linux基础命令/uniq/","text":"语法12345678910111213-c, --count #计数 prefix lines by the number of occurrences -d, --repeated #只打印重复的行,2次或2次以上的行,默认的去重包含1次 only print duplicate lines #1.仅显示相邻重复出现的行 #2.在1的基础上去重包含1次 -i, --ignore-case #在判断重复行时忽略大小写 ignore differences in case when comparing -u, --unique #仅显示出现一次的行 only print unique lines 实例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#只对相邻的行去重[root@lamp01 chenyansong]cat test_cc.txt10.0.0.1.txt10.0.0.4.txt10.0.0.2.txt10.0.0.1.txt10.0.0.1.txt10.0.0.1.txt10.0.0.3.txt10.0.0.4.txt10.0.0.5.txt[root@lamp01 chenyansong]uniq test_cc.txt10.0.0.1.txt10.0.0.4.txt10.0.0.2.txt10.0.0.1.txt10.0.0.3.txt10.0.0.4.txt10.0.0.5.txt#所以要使用sort之后,再uniq[root@lamp01 chenyansong]sort test_cc.txt|uniq -c 4 10.0.0.1.txt 1 10.0.0.2.txt 1 10.0.0.3.txt 2 10.0.0.4.txt 1 10.0.0.5.txt#仅显示重复的行[root@lamp01 chenyansong]cat test_cc.txt10.0.0.1.txt10.0.0.4.txt10.0.0.2.txt10.0.0.1.txt10.0.0.1.txt10.0.0.1.txt10.0.0.3.txt10.0.0.4.txt10.0.0.5.txt[root@lamp01 chenyansong]uniq -d test_cc.txt10.0.0.1.txt[root@lamp01 chenyansong]#仅显示出现一次的行[root@lamp01 chenyansong]cat test_cc.txt 10.0.0.1.txt10.0.0.4.txt10.0.0.2.txt10.0.0.1.txt10.0.0.1.txt10.0.0.1.txt10.0.0.3.txt10.0.0.4.txt10.0.0.5.txt[root@lamp01 chenyansong]uniq -u test_cc.txt 10.0.0.1.txt10.0.0.4.txt10.0.0.2.txt10.0.0.3.txt10.0.0.4.txt10.0.0.5.txt 通常uniq和sort配合使用","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之tr转换或删除字符","date":"2017-04-16T04:47:25.467Z","path":"2017/04/16/Linux/linux基础命令/tr转换或删除字符/","text":"&emsp;从标准输入中替换、缩减和/或删除字符，并将结果写到标准输出。 123456[root@lamp01 chenyansong]ll总用量 4drwxr-xr-x. 3 root root 4096 2月 13 16:00 tardir[root@lamp01 chenyansong]ls -l|cut -c 2-10|tr rwx- 4210用量 4421401401","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之tree","date":"2017-04-16T04:47:25.466Z","path":"2017/04/16/Linux/linux基础命令/tree/","text":"范例123456显示到指定的层级[root@linux-study yum.repos.d]# tree -L 1 / 范例：只显示目录[root@linux-study yum.repos.d]# tree -Ld 1 / 安装12yum install tree# CentOS上面默认是没有tree命令的，使用yum install tree安装tree命令，下面是tree的使用方法： 语法选项123456789#常见的用法:tree -a #显示所有tree -d #仅显示目录tree -L n #n代表数字..表示要显示几层...tree -f #显示完整路径.. #当然tree支持重定向至文件...tree -L 4 &gt;dirce.doc即可生成UTF8格式的文档..我们也可以在windows 下查看..#注意:生成的TXT或其他文件在win下面打开时也为乱码...这时我们要选择字符编码为UTF-8..当然..UTF-8是你Linux下的默认字符集才可以......","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之time(统计命令执行时间)","date":"2017-04-16T04:47:25.464Z","path":"2017/04/16/Linux/linux基础命令/time(统计命令执行时间)/","text":"1.两种类型的time123[root@MySQL ~]# type -a timetime is a shell keywordtime is /usr/bin/time 2.time显示的字段含义123456[root@MySQL shell]# time lscat1.txt cat3.txt cat.txt menu.sh sg.sh test.shreal 0m0.007suser 0m0.000ssys 0m0.006s[root@MySQL shell]# 输出的信息分别显示了该命令所花费的real时间、user时间和sys时间 real时间是指挂钟时间，也就是命令开始执行到结束的时间。这个短时间包括其他进程所占用的时间片，和进程被阻塞时所花费的时间。 user时间是指进程花费在用户模式中的CPU时间，这是唯一真正用于执行进程所花费的时间，其他进程和花费阻塞状态中的时间没有计算在内。 sys时间是指花费在内核模式中的CPU时间，代表在内核中执系统调用所花费的时间，这也是真正由进程使用的CPU时间。 3./usr/bin/time&emsp;shell内建也有一个time命令，当运行time时候是调用的系统内建命令，应为系统内建的功能有限，所以需要时间其他功能需要使用time命令可执行二进制文件/usr/bin/time。 3.1.写入指定文件-o12#使用-o选项将执行时间写入到文件中：/usr/bin/time -o outfile.txt ls 3.2.追加到指定文件12#使用-a选项追加信息：/usr/bin/time -a -o outfile.txt ls 3.2.格式化输出12345[root@lamp01 chenyansong]/usr/bin/time -f &quot;time: %U&quot; lsoutfile.txt test2.txt test3.txt test.gar.gz test_soft.txt test_sort.txt time_te.txttardir test333 test_exec.tar.gz test_hard.txt test_sort_2.log test.txttime: 0.00[root@lamp01 chenyansong] 格式化可选的参数表 参数 描述 %E real时间，显示格式为[小时:]分钟:秒 %U user时间 %S sys时间 %C 进行计时的命令名称和命令行参数,如:ls %D 进程非共享数据区域，以KB为单位 %x 命令退出状态 %k 进程接收到的信号数量 %w 进程被交换出主存的次数 %Z 系统的页面大小，这是一个系统常量，不用系统中常量值也不同 %P 进程所获取的CPU时间百分百，这个值等于user+system时间除以总共的运行时间 %K 进程的平均总内存使用量（data+stack+text），单位是KB %w 进程主动进行上下文切换的次数，例如等待I/O操作完成 %c 进程被迫进行上下文切换的次数（由于时间片到期）","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之telnet","date":"2017-04-16T04:47:25.462Z","path":"2017/04/16/Linux/linux基础命令/telnet/","text":"测试远程的某个端口是否打开1234telnet ip porttelnet 192.168.9.1 3306 #之所以进行端口扫描就是看远程的开启了哪些服务，然后在看该服务是否有漏洞，然后进行攻击","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之tar压缩,解压缩","date":"2017-04-16T04:47:25.460Z","path":"2017/04/16/Linux/linux基础命令/tar压缩,解压缩/","text":"1.压缩1.1.tar -zcvf (压缩) 1.2.tar -tf (查看压缩列表) 1.3.–include=文件(排除不需要打包的文件) 1.4.tar -zcvfX (排除指定的文件列表文件) 1.5.打包压缩的过程1234#打包成tar文件：tar cf test.tar /var/www/html#打包成tar.gz文件（经过压缩z）tar zcf test.tar.gz /var/www/html 1.6.到要打包的文件或者是目录上级进行打包123456789101112131415161718[root@lamp01 chenyansong]ll总用量 4drwxr-xr-x 3 root root 4096 2月 13 16:00 tardir[root@lamp01 chenyansong]tar zcvf ta11.tar.gz tardir/tardir/tardir/aa_1.jpgtardir/test/tardir/test/1.txttardir/aa_0.jpg[root@lamp01 chenyansong]tar zcvf ta22.tar.gz ./tardir/./tardir/./tardir/aa_1.jpg./tardir/test/./tardir/test/1.txt./tardir/aa_0.jpg[root@lamp01 chenyansong]#??不懂为什么??? 1.7.对软链接文件打包1234[root@lb01 keepalived]# ll -h /etc/rc.locallrwxrwxrwx. 1 root root 13 7月 3 21:00 /etc/rc.local -&gt; rc.d/rc.localtar zcvfh rc.tar.gz /etc/rc.local-h follow symlinks; archive and dump the files they point to 2.解压缩2.1.tar zxvf12[root@linux-study cys_test]# tar zxvf test_tar2.tar.gz# x是解压缩 2.2.tar -zxvf -C 解压到指定目录123[root@linux-study cys_test]# tar zxvf test_tar3.tar.gz -C ./aa/ -C, --directory=DIR change to directory DIR 2.3.打包成xx.tar.bz格式12345使用tar jcvf test.tar.bz /cys_test/ #此时不用z进行压缩,用j解包：tar jxvf test.tar.bz 通用的解包方式：tar vf test.tar.bz or tar vf test.tar.gz让程序去判断 3.其他压缩解压缩命令3.1.gzip/gunzip 压缩和解压缩gzip只能压缩文件，不能压缩目录，并且压缩之后源文件不见了，只剩下压缩文件。 3.2.zip 能够改变源文件 能够压缩目录12zip 选项[-r] [压缩后文件名] [文件或目录] #-r表示压缩目录 3.3.bzip212bzip2 选项[-k] [文件] # -k 产生压缩文件后保留原文件","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之stat显示文件系统状态","date":"2017-04-16T04:47:25.459Z","path":"2017/04/16/Linux/linux基础命令/stat显示文件系统状态/","text":"1234567891011121314[root@linux-study cys_test]# stat /cys_test/test.txt File: &quot;/cys_test/test.txt&quot; Size: 7 Blocks: 8 IO Block: 4096 普通文件Device: 803h/2051d Inode: 130085 Links: 2Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2016-07-16 07:32:51.752587018 +0800Modify: 2016-07-16 07:32:47.362510531 +0800Change: 2016-07-16 07:33:08.963445624 +0800[root@linux-study cys_test]#Access: 访问时间 find -atimeModify: 修改时间,内容发生变化,find -mtimeChange: 变化时间,包括Modify,权限,属主,用户组, find -ctime 取文件的权限:如644123456789101112131415161718192021222324252627282930[root@lamp01 chenyansong]# stat /etc/hosts File: &quot;/etc/hosts&quot; Size: 223 Blocks: 8 IO Block: 4096 普通文件Device: 803h/2051d Inode: 130078 Links: 2Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-02-12 16:00:01.123897002 +0800Modify: 2016-08-31 21:03:38.328210183 +0800Change: 2016-08-31 21:03:38.354798349 +0800[root@lamp01 chenyansong]# stat -c %a /etc/hosts644[root@lamp01 chenyansong]# stat -c %A /etc/hosts-rw-r--r--更多的格式化,参见:man stat %a Access rights in octal %A Access rights in human readable form %b Number of blocks allocated (see %B) %B The size in bytes of each block reported by %b %C SELinux security context string %d Device number in decimal......#反正前面打印的信息,使用-c %xxx 都可以获取到","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之ssh远程登录优化","date":"2017-04-16T04:47:25.457Z","path":"2017/04/16/Linux/linux基础命令/ssh远程登录优化/","text":"&emsp;/etc/ssh/sshd_config文件 中是配置ssh登陆的&emsp;下面是优化的步骤: 1.优化的配置1.1.修改默认的端口(22)1#Port 22 #改成你自己指定的端口 1.2修改使用DNS 为no&emsp;不使用DNS的原因是，我们登陆的时候就是使用的是ip，不需要去解析了，所以DNS解析就用不到了，不使用DNS我们的访问将更快12#UseDNS yesUseDNS no 1.3.修改登录时监听的IP&emsp;我们将登陆的IP换成我们指定的内网IP，那样外网的ip地址就不能直接访问了，避免了不必要的攻击，那么外面的人如何进行登录呢，我们可以使用VPN，登录了VPN之后，就相当于用户变成了内网用户，我们可以使用指定的IP去登录了12#ListenAddress 0.0.0.0ListenAddress 192.168.36.129 1.4.不允许root登录&emsp;我们不允许root进行登录，如果要使用root的功能，可以使用其他用户使用su去切换登录，但是不允许直接连接登录12#PermitRootLogin yesPermitRootLogin no 1.5.GSSAPIAuthentication 解决SSH远程连接服务慢的问题1#GSSAPIAuthentication no #如果默认是no,那么默认就行了 2.修改上面提到的配置文件&emsp;手动修改文件（一个一个的该，最后将他们放在一起，并且注释，标明时间）12345Port 52113PermitRootLogin noPermitEmptyPasswords noUseDNS noGSSAPIAuthentication no &emsp;通过sed的方式修改文件 &emsp;方式一：123456789101112 #增加：[root@oldboy ssh]# sed -i &apos;13 iPort 52113\\nPermitRootLogin no\\nPermitEmptyPasswords no\\nUseDNS no\\nGSSAPIAuthentication no&apos; sshd_config#查看： [root@oldboy ssh]# sed -n &apos;13,17p&apos; sshd_configPort 52113PermitRootLogin noPermitEmptyPasswords noUseDNS noGSSAPIAuthentication no &emsp;方式二（推荐）：他有个好的习惯就是在修改文件之前，将文件备份（并加上时间）123456789#下面是一个执行脚本echo &quot;#--------sshConfig修改ssh默认登录端口,禁止root登录----------------------------#&quot;\\cp /etc/ssh/sshd_config /etc/ssh/sshd_config.$(date +&quot;%F&quot;-$RANDOM)sed -i &apos;s%#Port 22%Port 52113%&apos; /etc/ssh/sshd_configsed -i &apos;s%#PermitRootLogin yes%PermitRootLogin no%&apos; /etc/ssh/sshd_configsed -i &apos;s%#PermitEmptyPasswords no%PermitEmptyPasswords no%&apos; /etc/ssh/sshd_configsed -i &apos;s%#UseDNS yes%UseDNS no%&apos; /etc/ssh/sshd_configsed -i &apos;s%GSSAPIAuthentication yes%GSSAPIAuthentication no%&apos; /etc/ssh/sshd_configegrep &quot;UseDNS|52113|RootLogin|EmptyPass|GSSAPIAuthentication&quot; /etc/ssh/sshd_config 3.重新加载配置到内存中&emsp;因为刚刚修改的数据只是在硬盘中，需要重新加载到内存中123456789[root@lamp01 chenyansong]# /etc/init.d/sshd restart停止 sshd： [确定]正在启动 sshd： [确定]#或者[root@lamp01 chenyansong]# /etc/init.d/sshd reload重新载入 sshd： [确定][root@lamp01 chenyansong]#","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之sort排序.md","date":"2017-04-16T04:47:25.455Z","path":"2017/04/16/Linux/linux基础命令/sort排序/","text":"1.语法1234567891011121314151617#以行为单位对文件进行排序sort [参数] [&lt;文件&gt;...]#常用参数:-b #忽略前导的空格-d #只考虑空格,字母和数字-f #忽略字母的大小写-i #只考虑可打印字符-M #排序月份-n #根据字符串的数值进行排序-r #逆向排序-u #对相同的行只输出一行+n #n为数字,对指定的列进行排序, +0表示第1列,以空格或制表符作为列的间隔符-t #指定分隔符-kn #指定分隔符之后的第n列(从1开始)作为排序列 2.实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118# -u 取出唯一的行[root@lamp01 chenyansong]# cat test_sort.txt10.0.0.710.0.0.810.0.0.710.0.0.710.0.0.910.0.0.710.0.0.710.0.0.710.0.0.7[root@lamp01 chenyansong]# sort -u test_sort.txt10.0.0.710.0.0.810.0.0.9#-r 按数字倒序[root@lamp01 chenyansong]# cat test_sort.txt10.0.0.710.0.0.810.0.0.710.0.0.710.0.0.910.0.0.710.0.0.710.0.0.710.0.0.7[root@lamp01 chenyansong]# sort -r test_sort.txt10.0.0.910.0.0.810.0.0.710.0.0.710.0.0.710.0.0.710.0.0.710.0.0.710.0.0.7#按数字排序[root@lamp01 chenyansong]# cat test_sort.txt 10.0.0.710.0.0.810.0.0.710.0.0.710.0.0.910.0.0.710.0.0.710.0.0.710.0.0.7[root@lamp01 chenyansong]# sort -n test_sort.txt 10.0.0.710.0.0.710.0.0.710.0.0.710.0.0.710.0.0.710.0.0.710.0.0.810.0.0.9#-t 指定分隔符排序, -k1 取分隔符之后的第一列排序[root@lamp01 chenyansong]# cat test_sort.txt10.9.0.710.1.0.810.2.0.710.3.0.710.11.0.910.0.0.710.8.0.710.5.0.710.25.0.7[root@lamp01 chenyansong]# sort -t &quot;.&quot; -k2 test_sort.txt10.0.0.710.1.0.810.11.0.910.2.0.710.25.0.710.3.0.710.5.0.710.8.0.710.9.0.7[root@lamp01 chenyansong]# sort -r -t &quot;.&quot; -k2 test_sort.txt10.9.0.710.8.0.710.5.0.710.3.0.710.25.0.710.2.0.710.11.0.910.1.0.810.0.0.7[root@lamp01 chenyansong]# sort -rn -t &quot;.&quot; -k2 test_sort.txt10.25.0.710.11.0.910.9.0.710.8.0.710.5.0.710.3.0.710.2.0.710.1.0.810.0.0.7#表示第2个字段的第一个字符开始排序到第3个字段的第1个字符结束[root@lamp01 chenyansong]# sort -t &quot;.&quot; -k 2.1,3.1 test_sort.txt10.11.0.910.11.2.810.11.8.710.2.0.710.25.0.710.3.0.710.5.0.710.8.0.710.9.0.7# -t &quot;.&quot; -k 1,1 用点来做分隔符,表示第一个字段开始排序到第一个字段结束 面试题: 123456789101112131415161718192021222324252627#将文件test_sort_2.log中的域名取出并更具域名进行计数排序处理[root@lamp01 chenyansong]# awk -F / &apos;&#123;print $3&#125;&apos; test_sort_2.logwww.baidu.comwww.baidu.compost.baidu.commp3.baidu.comwww.baidu.compost.baidu.com[root@lamp01 chenyansong]# awk -F / &apos;&#123;print $3&#125;&apos; test_sort_2.log|sortmp3.baidu.compost.baidu.compost.baidu.comwww.baidu.comwww.baidu.comwww.baidu.com[root@lamp01 chenyansong]# awk -F / &apos;&#123;print $3&#125;&apos; test_sort_2.log|sort|uniq -c 1 mp3.baidu.com 2 post.baidu.com 3 www.baidu.com[root@lamp01 chenyansong]# awk -F / &apos;&#123;print $3&#125;&apos; test_sort_2.log|sort|uniq -c|sort -r 3 www.baidu.com 2 post.baidu.com 1 mp3.baidu.com","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之seq","date":"2017-04-16T04:47:25.453Z","path":"2017/04/16/Linux/linux基础命令/seq/","text":"语法123seq [OPTION] ... LASTseq [OPTION] ... FIRST LASTseq [OPTION] ... FIRST INCREMENT LAST 实例 生成指定的范围的序列 12345678910111213141516[root@linux-study ~]# seq 512345 [root@linux-study ~]# seq 3 5 #指定开始和结束345 [root@linux-study ~]# seq 1 2 5 #2是指定的增量135 指定分隔符 123456 -s, --separator=STRING use STRING to separate numbers (default: \\n) #默认的分隔符是\\n(换行)#测试[root@linux-study ~]# seq -s &apos;@&apos; 51@2@3@4@5 生成的序列添加到指定的文件中 1[root@linux-study cys_test]# seq 55 &gt;seq_n 生成指定宽度的序列 123456789101112131415 -w, --equal-width equalize width by padding with leading zeroes#测试[root@lamp01 ~]# seq -w 1001020304050607080910","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之sed","date":"2017-04-16T04:47:25.451Z","path":"2017/04/16/Linux/linux基础命令/sed/","text":"1.语法1234567功能描述：过滤输出文件的内容（取行）sed -n ‘/过滤的内容/处理的命令’ 文件-n 表示取消sed的默认输出-p print 打印-d delete 删除-i 改变文件的内容(重点)-r, --regexp-extended (可以使用正则，不用转义) use extended regular expressions in the script. 2.-d 删除不显示12345678910111213 [root@linux-study cys_test]# cat err.a-bash: ech: command not founddfsdfsfsd [root@linux-study cys_test]# sed &apos;/dfs*/d&apos; err.a-bash: ech: command not found [root@linux-study cys_test]# sed &apos;/com*/d&apos; err.adfsdfsfsd [root@linux-study cys_test]# cat err.a-bash: ech: command not founddfsdfsfsd 3.取行12345678910111213141516[root@linux-study cys_test]# cat sed_tst.txtchenyansong2chenyansong3chenyansong4chenyansong5chenyansong6 // -n表示取消默认的输出[root@linux-study cys_test]# sed -n &apos;3p&apos; sed_tst.txtchenyansong4 [root@linux-study cys_test]# sed -n &apos;1,3p&apos; sed_tst.txtchenyansong2chenyansong3chenyansong4[root@linux-study cys_test]# 4.全局查找和替换123456789101112131415161718192021222324252627282930[root@linux-study cys_test]# cat sed_tst.txtchenyansong2chenyansong3chenyansong4chenyansong5chenyansong6 // -i 表示改变文件内容，s 表示search，g表示global[root@linux-study cys_test]# sed -i &apos;s#song#he#g&apos; sed_tst.txt[root@linux-study cys_test]# cat sed_tst.txtchenyanhe2chenyanhe3chenyanhe4chenyanhe5chenyanhe6 [root@linux-study cys_test]# sed -ri &apos;s#(chen)yanhe*#\\1#g&apos; sed_tst.txt #查找到的是chenyanhe,然后取组: (chen),替换掉查找到的内容[root@linux-study cys_test]# cat sed_tst.txtchen2chen3chen4chen5chen6 /*注意： s 是查找和替换，用一个字符串去替换匹配到的另一个字符串，#是分隔符，可以使用@ / 等替换-r 表示不用对需要转义的字符进行转义，如这里的括号等\\1 是取到的第一个分组*/ 5.增加 a 追加文本到指定行后 i插入文本到指定行前 -i是修改文件内容12345678910111213141516171819202122232425262728293031323334353637383940[root@lamp01 chenyansong]# sed -i &apos;2a testadd&apos; test3.txt[root@lamp01 chenyansong]# cat test3.txt aaaa testaddaa cc ddAAA[root@lamp01 chenyansong]# sed &apos;2a testadd2222&apos; test3.txt aaaa testadd2222testaddaa cc ddAAA[root@lamp01 chenyansong]# cat test3.txtaaaa testaddaa cc ddAAA[root@lamp01 chenyansong]# sed &apos;2i testadd2222&apos; test3.txtaaaatestadd2222 testaddaa cc ddAAA[root@lamp01 chenyansong]# cat test3.txtaaaa testaddaa cc ddAAA","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之route网关","date":"2017-04-16T04:47:25.450Z","path":"2017/04/16/Linux/linux基础命令/route网关/","text":"#1.命令行配置路由123456789101112131415161718192021222324&apos;添加&apos;add -host #主机路由-net #网关路由route add -net 10.0.0.0/8 gw 192.168.10.1 #到达10.0.0.0/8，走网关：192.168.10.1 route add default gw 192.168.0.1 #设置默认路由&apos;删除路由&apos; del -host -netroute del -net 10.0.0.0/8route del -net 0.0.0.0 #删除默认路由&apos;显示&apos;-n route -n #以数字显示主机和网关&apos;在指定设备上添加&apos;route add -net 224.0.0.0 netmask 240.0.0.0 dev eth0route add -net 224.0.0.0/8 gw 192.168.0.1 dev eth0","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之pkill,kill杀进程","date":"2017-04-16T04:47:25.448Z","path":"2017/04/16/Linux/linux基础命令/pkill,kill杀进程/","text":"pkill 进程名1234567[root@lamp01 tardir]lsof -i:3306COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmysqld 4385 mysql 10u IPv4 25845 0t0 TCP *:mysql (LISTEN)[root@lamp01 tardir]pkill mysqld#再次查看[root@lamp01 tardir]lsof -i:3306 killall 进程名&emsp;因为killall 是比较平滑的杀进程，所以我们要连续的杀，知道提示“没有进程被杀死”1234567[root@lamp01 tardir]lsof -i:3306 COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEmysqld 4555 mysql 10u IPv4 26148 0t0 TCP *:mysql (LISTEN)[root@lamp01 tardir]killall mysqld[root@lamp01 tardir]killall mysqldmysqld: 没有进程被杀死[root@lamp01 tardir] kill 进程号123456789101112[root@lamp01 tardir]ps -ef|grep sshdroot 1332 1 0 Feb12 ? 00:00:00 /usr/sbin/sshd #1332就是sshd的进程号root 2960 1332 0 07:29 ? 00:00:04 sshd: root@pts/0 root 4757 2962 0 15:36 pts/0 00:00:00 grep sshdkill 1332#orkill `cat /var/run/sshd.pid`#平滑处理kill -HUP `cat /var/run/sshd.pid`kill -USR2 `cat /var/run/sshd.pid`","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之ping","date":"2017-04-16T04:47:25.446Z","path":"2017/04/16/Linux/linux基础命令/ping/","text":"实例1234567891011121314151617[root@lamp01 chenyansong]# ping 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.254 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.024 ms.....#指定参数[root@lamp01 chenyansong]# ping -c 2 -w 2 192.168.0.3PING 192.168.0.3 (192.168.0.3) 56(84) bytes of data.64 bytes from 192.168.0.3: icmp_seq=1 ttl=64 time=0.021 ms64 bytes from 192.168.0.3: icmp_seq=2 ttl=64 time=0.024 ms --- 192.168.0.3 ping statistics ---2 packets transmitted, 2 received, 0% packet loss, time 1000msrtt min/avg/max/mdev = 0.0#-c ping的次数，-w 超时时间（秒）》将输出定位到null 禁止ping1234567#方式1echo &quot;net.ipv4.icmp_echo_ignore_all=1&quot; &gt;&gt; /etc/sysctl.confsysctl -p#上述方式不推荐使用,因为加上之后,我们自己也是ping不通的#方式2#通过防火墙的方式进行 查看某一个网段中可以ping通的主机shell脚本12345678910111213#!/bin/bashHOST=&quot;192.168.0.&quot;; #指定默认的网段main()&#123; for host_ip in &#123;0..254&#125; ;do &#123; ping -c 2 -w 2 $&#123;HOST&#125;&quot;$&#123;host_ip&#125;&quot; &amp;&gt;/dev/null; #-c ping的次数，-w 超时时间（秒）》将输出定位到null if [ $? -eq 0 ];then echo &quot;$&#123;HOST&#125;$&#123;host_ip&#125;&quot;&gt;&gt;host.txt; #将可以ping通的ip放到一个文件中 fi &#125;&amp; #每一个for循环都在一个新的shell进程中执行，这样执行的速度将非常的快，并行执行的 done&#125;main; ping不通的可能原因12有时候,ssh不能连接,ping的时候不通开了360的流量防火墙服务，导致了这个恶果，直接把360退了，OK，互ping畅通，开始ssh传输","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之netstat(网络连接，路由，端口信息)","date":"2017-04-16T04:47:25.445Z","path":"2017/04/16/Linux/linux基础命令/netstat(网络连接，路由，端口信息)/","text":"12345678910111213141516171819202122 #查看52113端口的进程名[root@linux-study network-scripts]# netstat -lntup|grep 52113-l list-n 数字(显示IP地址和端口)-t tcp-r 路由-u udpp programing name 进程名#查看本机所有的网络连接netstat -an#查看本机路由表[root@lamp01 chenyansong]# netstat -rnKernel IP routing tableDestination Gateway Genmask Flags MSS Window irtt Iface192.168.0.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0169.254.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth00.0.0.0 192.168.0.1 0.0.0.0 UG 0 0 0 eth0","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之ls","date":"2017-04-16T04:47:25.443Z","path":"2017/04/16/Linux/linux基础命令/ls/","text":"1.语法123456#功能描述：显示目录文件语法：ls 选项[-aldh] [文件或目录] -a all显示所有文件，包括隐藏文件 -l long详细信息显示，长格式 -d 查看目录属性 -h human 2.ll所查看的文件各个字段的属性含义 -表示的是一个文件，d 表示的是一个目录，l表示的是一个软连接 文件将用户分成三类：user 所有者，group 说数组，other 其他人 和selinux 相关：当selinux开着，有点的存在 3.显示文件的权限 4.只显示目录本身的信息12[root@localhost boot]# ls -ld /etcdrwxr-xr-x. 102 root root 12288 10月 31 05:50 /etc 5.ll -F 给目录文件添加后缀123456789101112[root@lamp01 tardir]ll总用量 4-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpgdrwxr-xr-x 2 root root 4096 2月 13 15:59 test[root@lamp01 tardir]ll -F总用量 4-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpgdrwxr-xr-x 2 root root 4096 2月 13 15:59 test/ 6.-p给目录加上/1234567891011[root@lamp01 tardir]ll总用量 4-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpgdrwxr-xr-x 2 root root 4096 2月 13 15:59 test[root@lamp01 tardir]ll -lp总用量 4-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpgdrwxr-xr-x 2 root root 4096 2月 13 15:59 test/[root@lamp01 tardir] 7.按修改时间倒序1234567891011121314[root@lamp01 tardir]ll总用量 8-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg-rw-r--r-- 1 root root 4 2月 13 16:02 aa_1.jpgdrwxr-xr-x 2 root root 4096 2月 13 16:01 test[root@lamp01 tardir]ll -rt ./总用量 8-rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpgdrwxr-xr-x 2 root root 4096 2月 13 16:01 test-rw-r--r-- 1 root root 4 2月 13 16:02 aa_1.jpg#-r reverse 反转排序, -t按修改时间排序 8.显示长格式的修改时间12345[root@lamp01 tardir]ll --time-style=long-iso总用量 8-rw-r--r-- 1 root root 0 2017-02-13 15:11 aa_0.jpg-rw-r--r-- 1 root root 4 2017-02-13 16:02 aa_1.jpgdrwxr-xr-x 2 root root 4096 2017-02-13 16:01 test 9.查看文件的inode编号inode：index node 索引节点编号，他可以是文件或者是目录，在磁盘里的唯一标识，Linux读取文件首先要读取到这个索引节点。书的目录 12345[root@lamp01 tardir]ls -li ./总用量 0 7198 -rw-r--r-- 1 root root 0 2月 13 15:11 aa_0.jpg # inode号719822672 -rw-r--r-- 1 root root 0 2月 13 15:11 aa_1.jpg22677 -rw-r--r-- 1 root root 0 2月 13 15:11 aa_2.jpg","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之Linux中rz和sz命令用法详解","date":"2017-04-16T04:47:25.441Z","path":"2017/04/16/Linux/linux基础命令/Linux中rz和sz命令用法详解/","text":"介绍rz，sz是Linux/Unix同Windows进行ZModem文件传输的命令行工具。 优点就是不用再开一个sftp工具登录上去上传下载文件。sz：将选定的文件发送（send）到本地机器rz：运行该命令会弹出一个文件选择窗口，从本地选择文件上传到Linux服务器 安装命令1yum install lrzsz 从服务端发送文件到客户端：1sz filename 从客户端上传文件到服务端：12rz#在弹出的框中选择文件，上传文件的用户和组是当前登录的用户 SecureCRT设置默认路径：Options -&gt; Session Options -&gt; Terminal -&gt; Xmodem/Zmodem -&gt;DirectoriesXshell设置默认路径：右键会话 -&gt; 属性 -&gt; ZMODEM -&gt; 接收文件夹测试：开发板接收文件： 进入开发板要接收文件的目录 开发板执行命令# rz 在minicom下，按住Ctrl+A键不放，按下Z键 按下S键选择发送文件 选择zmodem，用回车键确认 用空格选择主机要发送的文件，用回车键确认 传输完成后按任意键返回开发板发送文件： 进入开发板要发送文件的目录 进入主机要接收文件的目录 主机执行命令# rz 开发板执行命令# sz filename","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之last(显示登录,退出信息)","date":"2017-04-16T04:47:25.439Z","path":"2017/04/16/Linux/linux基础命令/last(显示登录,退出信息)/","text":"123456789101112131415161718#显示登录，退出信息[root@lamp01 chenyansong]# lastroot pts/0 192.168.0.221 Sun Feb 12 22:44 still logged in #still还在线root pts/1 192.168.0.221 Sun Feb 12 14:04 - 22:28 (08:24) # -表示已经退出root pts/0 192.168.0.221 Sun Feb 12 13:18 - 22:28 (09:10) root pts/1 192.168.0.221 Sun Feb 12 09:14 - 13:18 (04:03) #显示所有用户的登录信息[root@lamp01 chenyansong]# lastlog用户名 端口 来自 最后登陆时间root pts/0 192.168.0.221 日 2月 12 22:44:16 +0800 2017bin **从未登录过**daemon **从未登录过**adm **从未登录过**lp **从未登录过**sync **从未登录过**shutdown **从未登录过**","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之id(打印用户和组信息)","date":"2017-04-16T04:47:25.438Z","path":"2017/04/16/Linux/linux基础命令/id(打印用户和组信息)/","text":"1234567#打印指定用户[root@lamp01 chenyansong]# id chenyansonguid=500(chenyansong) gid=500(chenyansong) 组=500(chenyansong),502(student_team)#打印当前用户[root@lamp01 chenyansong]# iduid=0(root) gid=0(root) 组=0(root)","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之修改hostname(主机名)","date":"2017-04-16T04:47:25.436Z","path":"2017/04/16/Linux/linux基础命令/hostname修改主机名文件/","text":"更改主机名: vim /etc/sysconfig/network hostname 主机名 上述两者同时修改","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之history(历史记录)","date":"2017-04-16T04:47:25.434Z","path":"2017/04/16/Linux/linux基础命令/history(历史记录)/","text":"1.命令行的历史记录数量12345678[root@linux-study ~]# export HISTSIZE=5[root@linux-study ~]# history 568 tail -1 /etc/profile 569 source /etc/profile 570 echo $TMOUT 571 export HISTSIZE=5 572 history[root@linux-study ~]# 2.历史记录文件的命令数量12345678910#在用户的家目录下存在一个.bash_history文件,用于存放历史记录[root@linux-study ~]# export HISTFILESIZE=5[root@linux-study ~]# cat ~/.bash_history datehistoryecho $TMOUTexport TMOUT=120echo $TMOUT[root@linux-study ~]# 3.清除所有的历史记录1[root@linux-study ~]# history -c 4.清除指定的历史记录123456789101112131415[root@linux-study ~]# history 1 echo $TMOUT 2 history 3 ll 4 fsdfs 5 history[root@linux-study ~]# history -d 3[root@linux-study ~]# history 1 echo $TMOUT 2 history 3 fsdfs 4 history 5 history -d 3 6 history[root@linux-study ~]# 5.把配置参数放入配置文件,使得配置永久生效12345678910echo &apos;TMOUT=300&apos; &gt;&gt;/etc/profileecho &apos;HISTSIZE=5&apos; &gt;&gt;/etc/profileecho &apos;HISTFILESIZE=5&apos; &gt;&gt;/etc/profiletail -3 /etc/profilesource /etc/profile#其他HISTCONTROL=ignorespace #以空格开头的命令就不会进入到历史记录了 叹号(历史命令) 命令 描述 !+字母 表示调出最近一次以此字母开头的命令 !! 表示使用最近一次操作的命令 !+数字 表示调出历史的第几条命令","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之groups(查看用户所属组)","date":"2017-04-16T04:47:25.433Z","path":"2017/04/16/Linux/linux基础命令/groups(查看用户所属组)/","text":"12345678910111213141516#查看当前用户[root@lamp01 chenyansong]# su - chenyansong[chenyansong@lamp01 ~]$ groupschenyansong student_team[chenyansong@lamp01 ~]$ [root@lamp01 chenyansong]# iduid=0(root) gid=0(root) 组=0(root)#查看指定用户[root@lamp01 chenyansong]# groups chenyansongchenyansong : chenyansong student_team[root@lamp01 chenyansong]#[root@lamp01 chenyansong]# id chenyansonguid=500(chenyansong) gid=500(chenyansong) 组=500(chenyansong),502(student_team)","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之grep","date":"2017-04-16T04:47:25.431Z","path":"2017/04/16/Linux/linux基础命令/grep/","text":"1.语法12345678功能描述：在文件中搜索字符串匹配的行并输出语法：grep -vi [指定字符串] [文件]-v 排除指定的字符串-i 不区分大小写-c 计算找到“搜索字符串”的次数-o 仅显示匹配regexp的内容（用于统计出现在文中的次数）-n 行首显示行号-E 扩展的grep，简写：egrep 2.指定开头（^）,指定结尾（$）123#去掉以#开头的行，其中^ 表示以。。。开头 ，$ 以。。结尾的[root@lamp01 chenyansong]# grep -v &quot;^#&quot; /etc/inittabid:3:initdefault: 3.grep -A -B -C12345678910111213141516171819# 第一个30是表示匹配到字符串30所在的行，然后向前取10行[root@linux-study cys_test]# grep 30 -B 10 seq_n2021222324252627282930/* 注意：-B 表示除了显示匹配的一行之外，并显示该行之前的num行-A 表示除了显示匹配的一行之外，并显示该行之后的num行-C 表示除了显示匹配的一行之外，并显示该行之前的各num行*/ 4.-0只是打印匹配到的字符串12345#-o 表示只是打印匹配到的字符串，而不是默认的整行[root@linux-study cys_test]# grep -o &quot;chen.*&quot; people.txtchenyansong 22 man[root@linux-study cys_test]# grep -o &quot;chen&quot; people.txt chen 5. -E指定多个字符串进行匹配123456789101112#-E 表示拓展的grep，简写：egrep，可以不用转义[root@linux-study cys_test]# grep -E &quot;chenyansong|zhangsan&quot; people.txtchenyansong 22 manzhangsan 33 man#找出端口为3306或者是1521的服务[root@linux-study cys_test]# grep -E &quot;3306|1521&quot; /etc/servicesmysql 3306/tcp # MySQLmysql 3306/udp # MySQLncube-lm 1521/tcp # nCube License Managerncube-lm 1521/udp # nCube License Manager 6.改匹配到的字符串添加颜色标识1234#给匹配到的字符串添加颜色[root@lamp01 chenyansong]# grep --color=auto 3306 /etc/servicesmysql 3306/tcp # MySQLmysql 3306/udp # MySQL 7. 显示行号12345[root@linux-study cys_test]# grep -n &apos;:&apos; people.txt #对匹配到的行在原有行中的位置2:chenyansong 22 man3:zhangsan 33 man4:lisi 44 women6:aaaa 8. 统计匹配到的行：-c123456789[root@lamp01 chenyansong]# cat test3.txtaaaa aa cc dd[root@lamp01 chenyansong]# grep &quot;a&quot; test3.txtaaaaaa cc dd[root@lamp01 chenyansong]# grep -c &quot;a&quot; test3.txt2 8.匹配取反:-v1234#匹配非空的行（空字符串不是空行）[root@lamp01 chenyansong]# grep -v &quot;^$&quot; test3.txtaaaaaa cc dd 9忽略大小写匹配：-i123456789[root@lamp01 chenyansong]# cat test3.txtaaaa aa cc ddAAA[root@lamp01 chenyansong]# grep -i &quot;aa&quot; test3.txtaaaaaa cc ddAAA 10.取ip123456[root@lamp01 chenyansong]# cat /etc/sysconfig/network-scripts/ifcfg-eth0|grep &quot;IPADDR&quot;IPADDR=192.168.0.3[root@lamp01 chenyansong]# cat /etc/sysconfig/network-scripts/ifcfg-eth0|grep &quot;IPADDR&quot;|cut -d= -f2192.168.0.3","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之free(查看系统内存使用情况)","date":"2017-04-16T04:47:25.429Z","path":"2017/04/16/Linux/linux基础命令/free(查看系统内存使用情况)/","text":"12345678910111213141516[root@lamp01 chenyansong]# free -m total used free shared buffers cachedMem: 1004 155 849 0 53 49-/+ buffers/cache: 52 951Swap: 511 0 511/*实际空闲的内存是951,他会将部分内存当做缓存使用,951是去掉缓存的部分-/+ buffers/cache意思就是：buffers和cache占用了部分内存（849+53+49 就是实际空闲的内存951是去掉了buffers和cached之后的可用内存）buffers为写入缓冲区cache为读取数据的缓冲区硬盘是机械的,无论是写入还是读取都太慢了,所以读取和写入都是使用了缓存技术*/","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之find文件搜索","date":"2017-04-16T04:47:25.428Z","path":"2017/04/16/Linux/linux基础命令/find文件搜索/","text":"1.find语法1find [搜索范围] [匹配条件] 2.根据文件名搜索(-name)12345678910[root@linux-study ~]# find /cys_test/ -name &quot;err.a&quot;/cys_test/err.a/cys_test/cy1/err.a #模糊匹配文件[root@linux-study ~]# find /cys_test/ -name &quot;er*&quot;/cys_test/err.a/cys_test/cy1/err.a/cys_test/cy1/cy1/error.txt 3.根据文件大小搜索123456789在Linux中是以数据块来区分文件大小的，一个数据块是512 字节，即0.5kfind /cys_test/ -size +n #+n 大于；-n小于；n等于[root@linux-study ~]# find /cys_test/ -size +6/cys_test//cys_test/cy1/cys_test/cy1/changename/cys_test/cy1/cy1[root@linux-study ~]# 4 根据文件所有者或者所属组搜索12[root@linux-study ~]# find /cys_test/ -user root[root@linux-study ~]# find /cys_test/ -group root 5 根据时间属性来查找123456789#在/etc下查找5分钟内被修改过属性的文件和目录find /etc -cmin -5 /*其中：-amin 访问时间 access-cmin 文件属性 change （就是定义文件的元数据）-mmin 文件内容 modify*/ 6.查询n天前被修改的文件12345678910111213 -mtime n File’s data was last modified n*24 hours ago. See the comments for -atime to understand how rounding affects the interpretation of file modification times.#+n表示n天之前被修改过的文件，-n：反之，n表示前n天的当天[root@linux-study cys_test]# find ./ -type f -mtime -1./people.txt[root@linux-study cys_test]#-atime #n为数字, 意义为在n天之前的[一天之内]被access过的档案 -ctime #n为数字, 意义为在n天之前的[一天之内]被change过的档案 -mtime #n为数字, 意义为在n天之前的[一天之内]被modify过的档案 -newer file #file为一个存在的档案,意思是说:只要档案比file还要新,就会被列出来 7.连接选项：-a 或者是 -o 表示 ：and or123[root@localhost tmp]# find /etc -name init* -a -type d #表示名字以init开头的并且是目录/etc/init/etc/rc.d/init.d 8.根据文件类型查找12345find -type f#f 文件；d 目录；l 软链接文件 [root@linux-study cys_test]# find /cys_test/ -type f 9.根据路径层级查找（maxdepth）123456#查找层级深度为1的目录文件[root@linux-study cys_test]# find ./ -maxdepth 1 -type d ././ee./aa[root@linux-study cys_test]# 10.一个实例12345678910111213141516171819#删除一个目录下的所有的文件，但是保留指定文件#假设这个目录是/xx/，里面有file1,file2,file3..file10 十个文件[root@oldboy xx]# touch file&#123;1..10&#125;[root@oldboy xx]# lsfile1 file10 file2 file3 file4 file5 file6 file7 file8 file9[root@oldboy xx]# lsfile1 file10 file2 file3 file4 file5 file6 file7 file8 file9[root@oldboy xx]# find /xx -type f ! -name &quot;file10&quot;|xargs rm -f[root@oldboy xx]# lsfile10#或者[root@oldboy xx]# find /xx -type f ! -name &quot;file10&quot; -exec rm -f &#123;&#125; \\; [root@oldboy xx]# lsfile10","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之exec与xargs区别","date":"2017-04-16T04:47:25.427Z","path":"2017/04/16/Linux/linux基础命令/exec与xargs区别/","text":"123456#对找到的结果一次性打包find /home/chenyansong/ -type f|xargs tar zcvf test.gar.gz#对每一个找到的文件都进行打包一次(其实就是对最后一个文件进行了打包,因为后面的打包的文件名对前面的进行了覆盖)find /home/chenyansong/ -type f -exec tar zcvf test_exec.tar.gz &#123;&#125; \\; 举例说明:123456789101112131415161718192021$find test/ -type ftest/myfile.nametest/files/role_filetest/files/install_file #使用xargs去打印找到的文件$find test/ -type f |xargs echotest/myfile.name test/files/role_file test/files/install_file #使用-exec去打印找到的文件$find test/ -type f -exec echo &#123;&#125; \\;test/myfile.nametest/files/role_filetest/files/install_file/*很明显是:xargs把\\n转换成了空格,exec是对每个找到的文件执行一次命令，除非这单个的文件名超过了几k，否则不会出现命令行超长出报错的问题。而xargs是把所有找到的文件名一股脑的转给命令。当文件很多时，这些文件名组合成的命令行参数很容易超长，导致命令出错。*/ &emsp;另外， find | xargs 这种组合在处理有空格字符的文件名时也会出错，因为这时执行的命令已经不知道哪些是分割符、哪些是文件名中的空格！ 而用exec则不会有这个问题。 12345678910$touch test/&apos;test zzh&apos; $find test/ -name *zzhtest/test zzh $find test/ -name *zzh |xargs rmrm: cannot remove `test/test&apos;: No such file or directoryrm: cannot remove `zzh&apos;: No such file or directory $find test/ -name *zzh -exec rm &#123;&#125; \\; 总结 相比之下，也不难看出各自的缺点1、exec 每处理一个文件或者目录，它都需要启动一次命令，效率不好;2、exec 格式麻烦，必须用 {} 做文件的代位符，必须用 \\; 作为命令的结束符，书写不便。3、xargs 不能操作文件名有空格的文件,当文件很多时，这些文件名组合成的命令行参数很容易超长，导致命令出错； &emsp;综上，如果要使用的命令支持一次处理多个文件，并且也知道这些文件里没有带空格的文件，那么使用 xargs比较方便; 否则，就要用 exec了。","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之dumpe2fs(查看block和inode大小)","date":"2017-04-16T04:47:25.425Z","path":"2017/04/16/Linux/linux基础命令/dumpe2fs(查看block和inode大小)/","text":"123456dumpe2fs [root@linux-study cys_test]# dumpe2fs /dev/sda1|grep -iE &quot;block size|inode size&quot;dumpe2fs 1.41.12 (17-May-2010)Block size: 1024Inode size: 128","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之du 查看文件或者是目录的大小","date":"2017-04-16T04:47:25.423Z","path":"2017/04/16/Linux/linux基础命令/du 查看文件或者是目录的大小/","text":"123456789101112131415161718#du 查看文件或者是目录的大小-s sum-h human[root@lamp01 application]# ll总用量 4lrwxrwxrwx 1 root root 25 8月 4 2016 nginx -&gt; /application/nginx-1.6.3/drwxr-xr-x 11 root root 4096 8月 1 2016 nginx-1.6.3#查看目录大小[root@lamp01 application]# du -s nginx-1.6.3/44232 nginx-1.6.3/[root@lamp01 application]# du -sh nginx-1.6.3/44M nginx-1.6.3/#查看文件大小[root@lamp01 application]# du -h /root/install.log24K /root/install.log","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之df(查看磁盘的挂载信息)","date":"2017-04-16T04:47:25.422Z","path":"2017/04/16/Linux/linux基础命令/df(查看磁盘的挂载信息)/","text":"1234567891011121314#查看磁盘挂载inode情况[root@linux-study cys_test]# df -i文件系统 Inode 已用(I) 可用(I) 已用(I)%% 挂载点/dev/sda3 479552 51486 428066 11% /tmpfs 128621 1 128620 1% /dev/shm/dev/sda1 51200 38 51162 1% /boot#查看磁盘使用情况[root@linux-study cys_test]# df -h文件系统 容量 已用 可用 已用%% 挂载点/dev/sda3 7.2G 1.3G 5.6G 18% /tmpfs 503M 0 503M 0% /dev/shm/dev/sda1 194M 26M 158M 15% /boot","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之date","date":"2017-04-16T04:47:25.420Z","path":"2017/04/16/Linux/linux基础命令/date/","text":"1.date +FORMAT 获取格式化的时间字符串&emsp; 注意：date 和+之间的空格 12#语法格式date [OPTION]... [+FORMAT] 格式 说明 %F full date; same as %Y-%m-%d eg:date +%F –&gt; 2016-07-19 %d day of month (e.g, 01) %D date; same as %m/%d/%y %m month (01..12) %M minute (00..59) %s seconds since 1970-01-01 00:00:00 UTC %S second (00..60) %y last two digits of year (00..99) %Y year date +%Y—&gt;2016 %h same as %b %H hour (00..23) %s seconds since 1970-01-01 00:00:00 UTC %S second (00..60) %t a tab %T time; same as %H:%M:%S %w day of week (0..6); 0 is Sunday %W week number of year, with Monday as first day of week (00..53) 2. date -s 临时修改时间123[root@linux-study ~]# date -s &quot;2016-07-20 15:52:49&quot;2016年 07月 20日 星期三 15:52:49 CST[root@linux-study ~]# 3.date -d 获取过去或者未来的时间123456789101112131415161718192021[root@lamp01 ~]# date2017年 02月 12日 星期日 17:24:17 CST#后2天的日期[root@lamp01 ~]# date +%F -d &quot;2day&quot;2017-02-14#前2天的日期[root@lamp01 ~]# date +%F -d &quot;-2day&quot;2017-02-10[root@lamp01 ~]# date +%F-%H2017-02-12-17[root@lamp01 ~]# date +%F-%H -d &quot;+2hour&quot;2017-02-12-19#类似的还有分钟,秒-d &quot;-2min&quot;-d &quot;2min&quot;-d &quot;-2sec&quot; 4.举例12345678910111213141516171819202122#打印“YYYY-mm-dd HH:mm:ss”[root@lamp01 ~]# date +&quot;%Y-%m-%d %H:%M:%S&quot;2017-02-12 17:29:30#or[root@lamp01 ~]# date +%F\\ %T2017-02-12 17:29:43#按照时间打包[root@lamp01 ~]# ll-rw-r--r-- 1 root root 0 7月 26 2016 f.103[root@lamp01 ~]# tar zcvf cys_test_`date +%F`.tar.gz ./f.103 #反引号中是命令./f.103[root@lamp01 ~]# ll-rw-r--r-- 1 root root 0 7月 26 2016 f.103-rw-r--r-- 1 root root 111 2月 12 17:31 cys_test_2017-02-12.tar.gz#or[root@lamp01 ~]# tar zcvf cys_test_$(date +%F).tar.gz ./f.103#解析命令的方式:反引号`` 或 $()","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之cut","date":"2017-04-16T04:47:25.419Z","path":"2017/04/16/Linux/linux基础命令/cut/","text":"1.语法12345678910-c, --characters=LIST select only these characters -d, --delimiter=DELIM use DELIM instead of TAB for field delimiter -f, --fields=LIST select only these fields; also print any line that contains no delimiter character, unless the -s option is specified 2.举例123456789[root@linux-study cys_test]# echo &quot;my name is chenyansong&quot; &gt;&gt; name.txt #d 分隔符，f字段[root@linux-study cys_test]# cut -d &quot; &quot; -f2,4 name.txtname chenyansong #c 字符,13- 表示第13个字符到最后[root@linux-study cys_test]# cut -c 1-11,13- name.txtmy name is","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之curl","date":"2017-04-16T04:47:25.417Z","path":"2017/04/16/Linux/linux基础命令/curl/","text":"123456789101112131415161718192021#需求:测试能否正常访问一个网站(即:求返回的状态码 200表示正常)[root@lnmp02 sbin]# curl -I -s --w &quot;%&#123;http_code&#125;\\n&quot; www.etiantian.org -o /dev/null200[root@lnmp02 sbin]##说明-I/--head (HTTP/FTP/FILE) Fetch the HTTP-header only 只是显示请求头信息 (-I 是i的大写) -s/--silent Silent or quiet mode. Don’t show progress meter or error messages. Makes Curl mute. (不显示下载进度) -o/--output &lt;file&gt; Write output to &lt;file&gt; instead of stdout. (-w/--write-out &lt;format&gt;All variables are specified as %&#123;variable_name&#125; and to output a normal % you just write them as %%. You can output a newline by using \\n, a carriage return with \\r and a tab space with \\t. #输出响应状态的某一个字段（通过指定特定的格式） # -w可以指定的字段：http_code The numerical response codehttp_connect The numerical code。。。。。。。。(可以使用man curl查看)","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之chkconfig","date":"2017-04-16T04:47:25.416Z","path":"2017/04/16/Linux/linux基础命令/chkconfig/","text":"&emsp; chkconfig命令主要用来更新（启动或停止）和查询系统服务的运行级信息。谨记chkconfig不是立即自动禁止或激活一个服务，它只是简单的改变了符号连接。 1.chkconfig 的使用语法123chkconfig [--add][--del][--list][系统服务] #或 chkconfig [--level &lt;levels等级代号&gt;][系统服务][on/off/reset] 2.参数用法：12345678910--add 增加所指定的系统服务，让chkconfig指令得以管理它，并同时在系统启动的叙述文件内增加相关数据。--del 删除所指定的系统服务，不再由chkconfig指令管理，并同时在系统启动的叙述文件内删除相关数据。--level&lt;等级代号&gt; 指定读系统服务要在哪一个执行等级中开启或关毕。 等级0表示：表示关机 等级1表示：单用户模式 等级2表示：无网络连接的多用户命令行模式 等级3表示：有网络连接的多用户命令行模式 等级4表示：系统保留 等级5表示：带图形界面的多用户模式 等级6表示：重新启动 &emsp;需要说明的是，level选项可以指定要查看的运行级而不一定是当前运行级。对于每个运行级，只能有一个启动脚本或者停止脚本。当切换运行级时，init不会重新启动已经启动的服务，也不会再次去停止已经停止的服务。&emsp;chkconfig –list [name]：显示所有运行级系统服务的运行状态信息（on或off）。如果指定了name，那么只显示指定的服务在不同运行级的状态。&emsp;chkconfig –add name：增加一项新的服务。chkconfig确保每个运行级有一项启动(S)或者杀死(K)入口。如有缺少，则会从缺省的init脚本自动建立。&emsp;chkconfig –del name：删除服务，并把相关符号连接从/etc/rc[0-6].d删除。&emsp;chkconfig [–level levels] name：设置某一服务在指定的运行级是被启动，停止还是重置。&emsp;运行级文件：&emsp;每个被chkconfig管理的服务需要在对应的init.d下的脚本加上两行或者更多行的注释。第一行告诉chkconfig缺省启动的运行级以及启动和停止的优先级。如果某服务缺省不在任何运行级启动，那么使用 - 代替运行级。第二行对服务进行描述，可以用\\ 跨行注释。 123#例如，random.init包含2行：# chkconfig: 2345 20 80# description: Saves and restores system entropy pool for 3.使用范例12345678chkconfig --list #列出所有的系统服务chkconfig --add httpd #增加httpd服务chkconfig --del httpd #删除httpd服务chkconfig --level 2345 httpd on #设置httpd在运行级别为2、3、4、5的情况下都是on（开启）的状态chkconfig --list #列出系统所有的服务启动情况chkconfig --list mysqld #列出mysqld服务设置情况chkconfig --level 35 mysqld on #设定mysqld在等级3和5为开机运行服务，--level 35表示操作只在等级3和5执行，on表示启动，off表示关闭chkconfig mysqld on #设定mysqld在各等级为on，“各等级”包括2、3、4、5等级 &emsp; 如何增加一个服务：以mysqld 为例 服务脚本必须存放在/etc/init.d/目录下； chkconfig –add mysqld #添加服务，在chkconfig工具服务列表中增加此服务，此时服务会被在/etc/rc.d/rcN.d中赋予K/S入口； chkconfig –level 35 mysqld on # 修改服务的默认启动等级。 4.服务的启动/关闭1、系统服务：一直在内存中，而且一直在运行，并提供服务的被称为服务；2、而服务也是一个运行的程序，则这个运行的程序则被称为daemons；3、这些服务的启动脚本一般放置在： /etc/init.d4、在CentOS中服务启动脚本放置在：/etc/rc.d/init.d而/etc/init.d这个目录为公认的目录，在centos中/etc/init.d就是一个链接档案5、/etc/sysconfig 服务初始化环境变量配置都在这个档案中。6、/var/lib 各个服务产生的数据库都在这个目录下，最简单的在这里找到 mysql 使用 vim 打开就可以看到，你建立的数据库以及系统默认产生的数据库名称都在这里面！7、启动/停止/重启服务 ： /etc/init.d/ serverName start/stop /restart/status8、启动/停止/重启服务： service serverName start/stop/restart 为什么可以这样写？9、service 是一个script 他可以分析你后面下达的参数，然后根据你的参数在到/etc/init.d 下去取得正确的服务来 stop start restart","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之cd","date":"2017-04-16T04:47:25.414Z","path":"2017/04/16/Linux/linux基础命令/cd/","text":"cd （切换路径）123456命令名称：cd命令英文原意：change directory命令所在路径：shell 内置命令执行权限：所有用户语法：cd [目录名]功能描述：切换目录 &emsp;范例： 123456789cd /tmp/Japan/boduo 切换到指定的目录cd .. 回到上一级的目录[root@linux-study cys_test]# cd /tmp[root@linux-study tmp]# env | grep -i OLDPWDOLDPWD=/cys_test[root@linux-study tmp]# cd - #直接切换到上一次目录下/cys_test[root@linux-study cys_test]#","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之cat","date":"2017-04-16T04:47:25.412Z","path":"2017/04/16/Linux/linux基础命令/cat/","text":"1.作用123456789101112131415161718192021#1.显示文件内容，cat filename#2.将几个文件连接起来显示cat &gt; filename 只能创建新文件,不能编辑已有文件.cat file1 file2 &gt; file#3.从标准输入读取内容并显示//输出到一个文件中 [root@localhost test]# cat &gt;log.txt &lt;&lt;EOF&gt; Hello&gt; World&gt; Linux&gt; EOF//直接打印[root@MySQL shell]# cat &lt;&lt;EOF&gt; A&gt; B&gt; EOFAB 2.语法1cat [选项] [文件]... 3.参数选项 参数 含义 -A, –show-all 等价于 -vET -b, –number-nonblank 对非空输出行编号 -e 等价于 -vE -E, –show-ends 在每行结束处显示 $ -n, –number 对输出的所有行编号,由1开始对所有输出的行数编号 -s, –squeeze-blank 有连续两行以上的空白行，就代换为一行的空白行 -t 与 -vT 等价 -T, –show-tabs 将跳格字符显示为 ^I -u (被忽略) -v, –show-nonprinting 使用 ^ 和 M- 引用，除了 LFD 和 TAB 之外 4.使用实例4.1.&lt;&lt;EOF输出到文件123456789101112131415[root@web setup]# cat &gt;2.txt &lt;&lt;EOF&gt; Hello&gt; Bash&gt; Linux&gt; PWD=$(pwd)&gt; EOF [root@web setup]# ls -l 2.txt-rw-r--r-- 1 root root 33 11-02 21:35 2.txt[root@web setup]# cat 2.txtHelloBashLinuxPWD=/root/setup [root@web setup]# 4.2.在shell中使用cat&lt;&lt;EOF12345678910#!/bin/bashmain()&#123;cat &lt;&lt; AA==============================monday;FDSFSDFSDFSF=============================AA&#125;main; 4.3.输出行号12345678[root@MySQL shell]# cat -n test.sh 1 [ &quot;$1&quot; -eq &quot;$2&quot; ]&amp;&amp;echo &quot;=&quot; 2 [ &quot;$1&quot; -gt &quot;$2&quot; ]&amp;&amp;echo &quot;&gt;&quot; 3 [ &quot;$1&quot; -lt &quot;$2&quot; ]&amp;&amp;echo &quot;&lt;&quot; 4 5 6[root@MySQL shell]# 4.4.空行不显示行号,用-b12345678910111213141516[root@MySQL shell]# cat -b test.sh 1 [ &quot;$1&quot; -eq &quot;$2&quot; ]&amp;&amp;echo &quot;=&quot; 2 [ &quot;$1&quot; -gt &quot;$2&quot; ]&amp;&amp;echo &quot;&gt;&quot; 3 [ &quot;$1&quot; -lt &quot;$2&quot; ]&amp;&amp;echo &quot;&lt;&quot; [root@MySQL shell]# ```shell## 4.5.连续两行以上的空白行,就代换为一行的空白行```shell[root@MySQL shell]# cat -s test.sh[ &quot;$1&quot; -eq &quot;$2&quot; ]&amp;&amp;echo &quot;=&quot;[ &quot;$1&quot; -gt &quot;$2&quot; ]&amp;&amp;echo &quot;&gt;&quot;[ &quot;$1&quot; -lt &quot;$2&quot; ]&amp;&amp;echo &quot;&lt;&quot; [root@MySQL shell]# 4.6.打印两个文件的内容1234567891011121314151617[root@MySQL shell]# cat cat.txt cat1.txtthis is cat.txtthis is cat1.txt[root@MySQL shell]# [root@MySQL shell]# cat -n cat.txt cat1.txt 1 this is cat.txt 2 this is cat1.txt [root@MySQL shell]# cat -n cat.txt cat1.txt &gt;cat3.txt[root@MySQL shell]# cat cat3.txt 1 this is cat.txt 2 this is cat1.txt[root@MySQL shell]#","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"Linux基础命令之awk","date":"2017-04-16T04:47:25.410Z","path":"2017/04/16/Linux/linux基础命令/awk/","text":"1 语法 定义：一门语言，过滤内容（取列） 语法：# awk ‘条件1{动作1} 条件2{动作2} …’ 文件名 条件：一般使用关系表达式作为条件，如：x&gt;10 or x&lt;=10 or x&gt;=10 动作：格式化输出 or 流程控制语句 2. 指定分隔符12345678[root@linux-study cys_test]# echo &quot;name@@age@@gender&quot; &gt; new[root@linux-study cys_test]# cat newname@@age@@gender[root@linux-study cys_test]# awk -F &quot;@@&quot; &apos;&#123;print $1&#125;&apos; new name[root@linux-study cys_test]##注意：-F 表示分隔符，$1 第一列，$2第二列...... $NF最后一列，$(NF-1) 倒数第二列 3. 指定多个分隔符12345678[root@lamp01 chenyansong]# echo &quot;I am oldboy,myqq is 122344&quot; &gt; test.txt[root@lamp01 chenyansong]# cat test.txtI am oldboy,myqq is 122344#指定多个分隔符[root@lamp01 chenyansong]# awk -F &quot;[, ]&quot; &apos;&#123;print $3 &quot;:::&quot; $6&#125;&apos; test.txtoldboy:::122344 4.在开头或者是结尾处添加字符串1234567891011[root@lamp01 chenyansong]# echo &quot;chenyansong@@123.com&quot; &gt; test2.txt[root@lamp01 chenyansong]# cat test2.txtchenyansong@@123.com[root@lamp01 chenyansong]# awk -F &quot;@@&quot; &apos;BEGIN&#123;printf &quot;hello-&quot;&#125;&#123;print $1&#125;&apos; test2.txthello-chenyansong[root@lamp01 chenyansong]# awk -F &quot;@@&quot; &apos;END&#123;print &quot;--world&quot;&#125;&#123;printf $1&#125;&apos; test2.txtchenyansong--world#注意print和printf是有换行的区别的 5.添加条件表达式1234[root@linux-study cys_test]# seq 55 &gt;seq_n[root@linux-study cys_test]# awk &apos;&#123;if(NR&gt;33) print $1&#125;&apos; seq_n #其中：NR 表示行号 6.NR1234567891011121314151617#获取文件的访问权限(此时是644)[root@lamp01 chenyansong]# stat /etc/hosts File: &quot;/etc/hosts&quot; Size: 223 Blocks: 8 IO Block: 4096 普通文件Device: 803h/2051d Inode: 130078 Links: 2Access: (0644/-rw-r--r--) Uid: ( 0/ root) Gid: ( 0/ root)Access: 2017-02-11 15:56:14.414759618 +0800Modify: 2016-08-31 21:03:38.328210183 +0800Change: 2016-08-31 21:03:38.354798349 +0800[root@lamp01 chenyansong]# stat /etc/hosts [root@lamp01 chenyansong]# stat /etc/hosts|awk -F &apos;[0/]&apos; &apos;NR==4 &#123;print $2&#125;&apos;644#NR==4表示取第4行,然后对第4行用0或者/分割,取第2段#注意：“&#123;&#125;”和‘&#123;&#125;’的区别，因为&#123;&#125; 里面是命令块，所以不需要解析，所以用‘’单引号 7.awk相加各列123456789#求一个文件的数字访问权限(如644)[root@lamp01 chenyansong]# ls -l test2.txt|cut -c 2-10|tr rwx- 4210|awk -F &quot;&quot; &apos;&#123;print $1+$2+$3 $4+$5+$6 $7+$8+$9&#125;&apos;644/* 1.首先使用cut将权限字符取到2.使用tr去掉rwx-,换成42103.使用awk取列,然后相加各列*/ 8.awk数组123456$ ps aux | awk &apos;NR!=1&#123;a[$1]+=$6;&#125; END &#123; for(i in a) print i &quot;, &quot; a[i]&quot;KB&quot;;&#125;&apos;dbus, 540KBmysql, 99928KBwww, 3264924KBroot, 63644KBhchen, 6020KB","tags":[{"name":"Linux基础命令","slug":"Linux基础命令","permalink":"http://yoursite.com/tags/Linux基础命令/"}]},{"title":"反射的模板代码","date":"2017-04-16T04:47:25.405Z","path":"2017/04/16/java/反射/反射的模板代码/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157package cn.itcast_04_reflect;import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;import java.util.ArrayList;import java.util.List;import org.junit.Before;import org.junit.Test;public class MyReflect &#123; public String className = null; @SuppressWarnings(&quot;rawtypes&quot;) public Class personClass = null; /** * 反射Person类 * @throws Exception */ @Before public void init() throws Exception &#123; className = &quot;cn.itcast_04_reflect.Person&quot;; personClass = Class.forName(className); &#125; /** *获取某个class文件对象 */ @Test public void getClassName() throws Exception &#123; System.out.println(personClass); &#125; /** *获取某个class文件对象的另一种方式 */ @Test public void getClassName2() throws Exception &#123; System.out.println(Person.class); &#125; /** *创建一个class文件表示的实例对象，底层会调用空参数的构造方法 */ @Test public void getNewInstance() throws Exception &#123; System.out.println(personClass.newInstance()); &#125; /** *获取非私有的构造函数 */ @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;) @Test public void getPublicConstructor() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Person person = (Person)constructor.newInstance(100L,&quot;zhangsan&quot;); System.out.println(person.getId()); System.out.println(person.getName()); &#125; /** *获得私有的构造函数 */ @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;) @Test public void getPrivateConstructor() throws Exception &#123; Constructor con = personClass.getDeclaredConstructor(String.class); con.setAccessible(true);//强制取消Java的权限检测 Person person2 = (Person)con.newInstance(&quot;zhangsan&quot;); System.out.println(&quot;**&quot;+person2.getName()); &#125; /** *访问非私有的成员变量 */ @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;) @Test public void getNotPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class,String.class); Object obj = constructor.newInstance(100L,&quot;zhangsan&quot;); Field field = personClass.getField(&quot;name&quot;); field.set(obj, &quot;lisi&quot;); System.out.println(field.get(obj)); &#125; /** *访问私有的成员变量 */ @SuppressWarnings(&#123; &quot;rawtypes&quot;, &quot;unchecked&quot; &#125;) @Test public void getPrivateField() throws Exception &#123; Constructor constructor = personClass.getConstructor(Long.class); Object obj = constructor.newInstance(100L); Field field2 = personClass.getDeclaredField(&quot;id&quot;); field2.setAccessible(true);//强制取消Java的权限检测 field2.set(obj,10000L); System.out.println(field2.get(obj)); &#125; /** *获取非私有的成员函数 */ @SuppressWarnings(&#123; &quot;unchecked&quot; &#125;) @Test public void getNotPrivateMethod() throws Exception &#123; System.out.println(personClass.getMethod(&quot;toString&quot;)); Object obj = personClass.newInstance();//获取空参的构造函数 Method toStringMethod = personClass.getMethod(&quot;toString&quot;); Object object = toStringMethod.invoke(obj); System.out.println(object); &#125; /** *获取私有的成员函数 */ @SuppressWarnings(&quot;unchecked&quot;) @Test public void getPrivateMethod() throws Exception &#123; Object obj = personClass.newInstance();//获取空参的构造函数 Method method = personClass.getDeclaredMethod(&quot;getSomeThing&quot;); method.setAccessible(true); Object value = method.invoke(obj); System.out.println(value); &#125; /** * */ @Test public void otherMethod() throws Exception &#123; //当前加载这个class文件的那个类加载器对象 System.out.println(personClass.getClassLoader()); //获取某个类实现的所有接口 Class[] interfaces = personClass.getInterfaces(); for (Class class1 : interfaces) &#123; System.out.println(class1); &#125; //反射当前这个类的直接父类 System.out.println(personClass.getGenericSuperclass()); /** * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。 */ //path 不以’/&apos;开头时默认是从此类所在的包下取资源，以’/&apos;开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。 System.out.println(personClass.getResourceAsStream(&quot;/log4j.properties&quot;)); System.out.println(personClass.getResourceAsStream(&quot;log4j.properties&quot;)); //判断当前的Class对象表示是否是数组 System.out.println(personClass.isArray()); System.out.println(new String[3].getClass().isArray()); //判断当前的Class对象表示是否是枚举类 System.out.println(personClass.isEnum()); System.out.println(Class.forName(&quot;cn.itcast_04_reflect.City&quot;).isEnum()); //判断当前的Class对象表示是否是接口 System.out.println(personClass.isInterface()); System.out.println(Class.forName(&quot;cn.itcast_04_reflect.TestInterface&quot;).isInterface()); &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"一个简单的例子说明动态代理","date":"2017-04-16T04:47:25.402Z","path":"2017/04/16/java/动态代理/一个简单的例子说明动态代理/","text":"1.实现的过程图示 2.代码实现代理类（经纪人）123456789101112131415161718192021222324252627282930313233343536373839404142package cn.itcast.proxy;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;public class StarProxy &#123; private Person star = new Star(); public Person getProxy()&#123; /** * loader：指定一个类加载器 * interfaces:被代理类的接口 * 方法处理器，就是new InvocationHandler的匿名内部类 */ // Proxy.newProxyInstance(loader, interfaces, h) return (Person) Proxy.newProxyInstance(StarProxy.class.getClassLoader(), star.getClass().getInterfaces(), new InvocationHandler()&#123; /** * proxy : 把代理对象自己传递进来 * method：把代理对象当前调用的方法传递进来 * args:把方法参数传递进来 */ public Object invoke(Object proxy, Method method, Object[] args)throws Throwable &#123; //编码指定返回的代理对象干的工作 if(method.getName().equals(&quot;sing&quot;))&#123; System.out.println(&quot;搞一万块钱来！！&quot;); return method.invoke(star, args); //找明星唱歌 &#125; if(method.getName().equals(&quot;dance&quot;))&#123; System.out.println(&quot;搞2万块钱来！！&quot;); return method.invoke(star, args); //找明星跳舞 &#125; return null; &#125; &#125;); &#125;&#125; 被代理类（娱乐明星） 12345678910111213package cn.itcast.proxy;public class Star implements Person &#123; public String sing(String name)&#123; System.out.println(&quot;明星唱&quot;+name+&quot;歌！！&quot;); return &quot;飞吻！！&quot;; &#125; public String dance(String name)&#123; System.out.println(&quot;明星跳&quot;+name+&quot;舞！！&quot;); return &quot;多谢多谢老板！！&quot;; &#125; &#125; 被代理类的接口12345package cn.itcast.proxy;public interface Person &#123; String sing(String name); String dance(String name);&#125; 测试 12345678910111213141516171819package cn.itcast.proxy;public class Test &#123; /** * @param args */ public static void main(String[] args) &#123; StarProxy proxy = new StarProxy(); Person p = proxy.getProxy(); /*String value = p.sing(&quot;我爱你&quot;); System.out.println(value);*/ String value = p.dance(&quot;跳舞&quot;); System.out.println(value); &#125;&#125; 打印结果 123搞2万块钱来！！明星跳舞舞！！多谢多谢老板！！","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"}]},{"title":"通道channel","date":"2017-04-16T04:47:25.400Z","path":"2017/04/16/java/socket/通道channel/","text":"1.Channel的API12345package java.nio.channels;public interface Channel&#123; public boolean isOpen( ); public void close( ) throws IOException;&#125; 2.socket通道&emsp;Socket通道有三个，分别是ServerSocketChannel、SocketChannel和DatagramChannel，而它们又分别对 应java.net包中的Socket对象ServerSocket、Socket和DatagramSocket；Socket通道被实例化时，都会创 建一个对等的Socket对象。Socket通道可以运行非阻塞模式并且是可选择的，非阻塞I/O与可选择性是紧密相连的，这也正是管理阻塞的API要在 SelectableChannel中定义的原因。设置非阻塞非常简单，只要调用configureBlocking(false)方法即可。如果需要中 途更改阻塞模式，那么必须首先获得blockingLock()方法返回的对象的锁 2.1.ServerSocketChannel&emsp;ServerSocketChannel是一个基于通道的socket监听器。但它没有bind()方法，因此需要取出对等的Socket对象并使用它来 绑定到某一端口以开始监听连接。在非阻塞模式下，当没有传入连接在等待时，其accept()方法会立即返回null。正是这种检查连接而不阻塞的能力实 现了可伸缩性并降低了复杂性，选择性也因此得以实现。1234567891011121314151617ByteBuffer buffer = ByteBuffer.wrap(&quot;Hello World&quot;.getBytes()); ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(12345)); ssc.configureBlocking(false); for (;;) &#123; System.out.println(&quot;Waiting for connections&quot;); SocketChannel sc = ssc.accept(); if (sc == null) TimeUnit.SECONDS.sleep(2000); else &#123; System.out.println(&quot;Incoming connection from:&quot; + sc.socket().getRemoteSocketAddress()); buffer.rewind(); sc.write(buffer); sc.close(); &#125; &#125; 2.2.SocketChannel&emsp;相对于ServerSocketChannel，它扮演客户端，发起到监听服务器的连接，连接成功后，开始接收数据。要注意的是，调用它的open()方法仅仅是打开但并未连接，要建立连接需要紧接着调用connect()方法；也可以两步合为一步，调用open(SocketAddress remote)方法。你会发现connect()方法并未提供timout参数，作为替代方案，你可以用isConnected()、isConnectPending()或finishConnect()方法来检查连接状态。","tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"},{"name":"NIO","slug":"NIO","permalink":"http://yoursite.com/tags/NIO/"}]},{"title":"选择器Selector","date":"2017-04-16T04:47:25.398Z","path":"2017/04/16/java/socket/选择器Selector/","text":"1.通道、选择键、选择器之间的关系示意图 2.Selector12345678910111213141516public abstract class Selector&#123; public static Selector open( ) throws IOException #实例化 public abstract boolean isOpen( ); public abstract void close( ) throws IOException; public abstract SelectionProvider provider( ); /* 当再次调用select( )方法时（或者一个正在进行的select()调用结束时），已取消的键的集合中的被取消的键将被清理掉，并且相应的注销也将完成。 通道会被注销，而新的SelectionKey将被返回。依赖于特定的select( )方法调用，如果没有通道已经准备好，线程可能会在这时阻塞，通常会有一个超时值 */ public abstract int select( ) throws IOException; public abstract int select (long timeout) throws IOException; public abstract int selectNow( ) throws IOException; public abstract void wakeup( ); public abstract Set keys( ); public abstract Set selectedKeys( );&#125; 3.SelectionKey1234567891011121314public abstract class SelectionKey&#123; public static final int OP_READ public static final int OP_WRITE public static final int OP_CONNECT public static final int OP_ACCEPT public abstract SelectableChannel channel( ); public abstract Selector selector( ); public final boolean isReadable( ) public final boolean isWritable( ) public final boolean isConnectable( ) public final boolean isAcceptable( ) public final Object attach (Object ob) public final Object attachment( )&#125; 4.Example123456Selector selector = Selector.open( );channel1.register (selector, SelectionKey.OP_READ); #将通道注册到选择器上，并指定对应的事件channel2.register (selector, SelectionKey.OP_WRITE);channel3.register (selector, SelectionKey.OP_READ | SelectionKey.OP_WRITE);// Wait up to 10 seconds for a channel to become readyreadyCount = selector.select (10000); #阻塞方法，知道过10s或者至少有一个通道的I/O操作准备好","tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"},{"name":"NIO","slug":"NIO","permalink":"http://yoursite.com/tags/NIO/"}]},{"title":"自定义rpc框架的设计思路","date":"2017-04-16T04:47:25.396Z","path":"2017/04/16/java/socket/自定义rpc框架的设计思路/","text":"关联的知识 spring的注解和自定义注解 netty 反射 动态代理 zookeeper 1.实现原理图(详细) 2.实现简图 2.1.对客户端来说我只是需要调用一个业务的实现，所以我调用对应业务实现的接口中的方法 2.2.对服务端来说我只是实现业务接口中的方法，然后自定义注解，将业务实现类交给spring进行管理 3.工程rpc-sample-server3.1.服务端的启动入口加载spring的配置文件，构造其中的bean 12345public class RpcBootstrap &#123; public static void main(String[] args) &#123; new ClassPathXmlApplicationContext(&quot;spring.xml&quot;); &#125;&#125; 3.2.spring的配置文件12345678910111213141516171819202122&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xmlns:context=\"http://www.springframework.org/schema/context\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd\"&gt; &lt;!-- 扫描指定包下的类 --&gt; &lt;context:component-scan base-package=\"cn.itcast.rpc.sample.server\"/&gt; &lt;!-- 指定加载一个properties文件 --&gt; &lt;context:property-placeholder location=\"classpath:rpc.properties\"/&gt; &lt;bean id=\"serviceRegistry\" class=\"cn.itcast.rpc.registry.ServiceRegistry\"&gt; &lt;constructor-arg name=\"registryAddress\" value=\"$&#123;registry.address&#125;\"/&gt;&lt;!-- 使用ognl表达式，key-value在properties中配置了 --&gt; &lt;/bean&gt; &lt;bean id=\"rpcServer\" class=\"cn.itcast.rpc.server.RpcServer\"&gt; &lt;constructor-arg name=\"serverAddress\" value=\"$&#123;server.address&#125;\"/&gt; &lt;constructor-arg name=\"serviceRegistry\" ref=\"serviceRegistry\"/&gt;&lt;!-- 构造函数的参数指向了另一个Bean --&gt; &lt;/bean&gt;&lt;/beans&gt; 3.3.业务实现类 实现接口中的方法 自定义注解RpcService 1234@RpcService(HelloService.class)public class HelloServiceImpl implements HelloService &#123; //..........&#125; 4.工程rpc-server4.1.spring启动的过程中会调用RpcServer中指定的方法由于本类实现了ApplicationContextAware 和 InitializingBean,spring构造本对象时会调用setApplicationContext()方法，从而可以在方法中通过自定义注解获得用户的业务接口和实现，还会调用afterPropertiesSet()方法，在方法中启动netty服务器 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public class RpcServer implements ApplicationContextAware, InitializingBean &#123; private static final Logger LOGGER = LoggerFactory.getLogger(RpcServer.class); private String serverAddress; private ServiceRegistry serviceRegistry; //用于存储业务接口和实现类的实例对象(由spring所构造) private Map&lt;String, Object&gt; handlerMap = new HashMap&lt;String, Object&gt;(); //服务器绑定的地址和端口由spring在构造本类时从配置文件中传入 public RpcServer(String serverAddress, ServiceRegistry serviceRegistry) &#123; this.serverAddress = serverAddress; //用于向zookeeper注册名称服务的工具类 this.serviceRegistry = serviceRegistry; &#125; /** * 通过注解，获取标注了rpc服务注解的业务类的----接口及impl对象，将它放到handlerMap中 */ public void setApplicationContext(ApplicationContext ctx) throws BeansException &#123; Map&lt;String, Object&gt; serviceBeanMap = ctx.getBeansWithAnnotation(RpcService.class); if (MapUtils.isNotEmpty(serviceBeanMap)) &#123; for (Object serviceBean : serviceBeanMap.values()) &#123; //从业务实现类上的自定义注解中获取到value，从来获取到业务接口的全名 String interfaceName = serviceBean.getClass().getAnnotation(RpcService.class).value().getName(); handlerMap.put(interfaceName, serviceBean); &#125; &#125; &#125; /** * 在此启动netty服务，绑定handle流水线： * 1、接收请求数据进行反序列化得到request对象 * 2、根据request中的参数，让RpcHandler从handlerMap中找到对应的业务imple，调用指定方法，获取返回结果 * 3、将业务调用结果封装到response并序列化后发往客户端 * */ public void afterPropertiesSet() throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap bootstrap = new ServerBootstrap(); bootstrap .group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel channel) throws Exception &#123; channel.pipeline() .addLast(new RpcDecoder(RpcRequest.class))// 注册解码 IN-1 .addLast(new RpcEncoder(RpcResponse.class))// 注册编码 OUT .addLast(new RpcHandler(handlerMap));//注册RpcHandler IN-2 &#125; &#125;).option(ChannelOption.SO_BACKLOG, 128) .childOption(ChannelOption.SO_KEEPALIVE, true); String[] array = serverAddress.split(&quot;:&quot;); String host = array[0]; int port = Integer.parseInt(array[1]); ChannelFuture future = bootstrap.bind(host, port).sync(); LOGGER.debug(&quot;server started on port &#123;&#125;&quot;, port); if (serviceRegistry != null) &#123; serviceRegistry.register(serverAddress); &#125; future.channel().closeFuture().sync(); &#125; finally &#123; workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); &#125; &#125;&#125; 5.工程rpc-sample-app5.1.调用业务接口中方法123456789101112131415public class HelloServiceTest &#123; //注解 @Autowired private RpcProxy rpcProxy; @Test public void helloTest1() &#123; // 调用代理的create方法，代理HelloService接口 HelloService helloService = rpcProxy.create(HelloService.class); // 调用代理的方法，执行invoke String result = helloService.hello(&quot;World&quot;); System.out.println(&quot;服务端返回结果：&quot;); System.out.println(result); &#125; &#125; 6.工程rpc-client6.1.获取代理向zk查找可用的服务主机地址在代理中调用rpc client发送消息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class RpcProxy &#123; private String serverAddress; private ServiceDiscovery serviceDiscovery; public RpcProxy(String serverAddress) &#123; this.serverAddress = serverAddress; &#125; public RpcProxy(ServiceDiscovery serviceDiscovery) &#123; this.serviceDiscovery = serviceDiscovery; &#125; /** * 创建代理 * * @param interfaceClass * @return */ @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T create(Class&lt;?&gt; interfaceClass) &#123; return (T) Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] &#123; interfaceClass &#125;, new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; //创建RpcRequest，封装被代理类的属性 RpcRequest request = new RpcRequest(); request.setRequestId(UUID.randomUUID().toString()); //拿到声明这个方法的业务接口名称 request.setClassName(method.getDeclaringClass().getName()); request.setMethodName(method.getName()); request.setParameterTypes(method.getParameterTypes()); request.setParameters(args); //查找服务 if (serviceDiscovery != null) &#123; serverAddress = serviceDiscovery.discover(); &#125; //随机获取服务的地址 String[] array = serverAddress.split(&quot;:&quot;); String host = array[0]; int port = Integer.parseInt(array[1]); //创建Netty实现的RpcClient，链接服务端 RpcClient client = new RpcClient(host, port); //通过netty向服务端发送请求 RpcResponse response = client.send(request); //返回信息 if (response.isError()) &#123; throw response.getError(); &#125; else &#123; return response.getResult(); &#125; &#125; &#125;); &#125;&#125; 7.工程rpc-registry 提供rpc server向zk注册 rpc client可以查找可用的主机地址 8.工程rpc-common1.对通信过程中的流数据解码封装成类，或者编码成为流 9.代码实现github地址","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"rpc","slug":"rpc","permalink":"http://yoursite.com/tags/rpc/"}]},{"title":"缓冲区-Buffer","date":"2017-04-16T04:47:25.395Z","path":"2017/04/16/java/socket/缓冲区_Buffer/","text":"在NIO中，不得不提到的是关于抽象的数据容器ByteBuffer，ByteBuffer允许相同的数据在不同ByteBuffer之间共享而不需要再拷贝一次来处理(参见：《传统的IO原理一文》) 1.缓冲区的属性所有的缓冲区都具有四个属性来提供关于其所包含的数据元素的信息，他们是： 容量（Capacity） 缓冲区能够容纳的数据元素的最大数量，这一容量在缓冲创建时被设定，并且永远不能被改变 上界（Limit） 缓冲区的第一个不能被读或写的元素，或者说，缓冲区中现存元素的计数 位置（Position） 下一个要被读或写的元素的索引，位置会自动由相应的get()和put()函数更新 标记（Mark） 一个备忘位置，调用mark()来设定mark = position，调用reset()设定position = mark，标记前是未定义的（underfined） 2.新建的缓冲区示意图 位置被设为 0，而且容量和上界被设为 10，刚好经过缓冲区能够容纳的最后一个字节。标记最初未定义。容量是固定的，但另外的三个属性可以在使用缓冲区时改变。 3.缓冲区API12345678910111213141516package java.nio;public abstract class Buffer &#123; public final int capacity( ) public final int position( ) public final Buffer position (int newPositio public final int limit( ) public final Buffer limit (int newLimit) public final Buffer mark( ) public final Buffer reset( ) public final Buffer clear( ) public final Buffer flip( ) public final Buffer rewind( ) public final int remaining( ) public final boolean hasRemaining( ) public abstract boolean isReadOnly( );&#125; 4.缓冲区存取上文所列出的的 Buffer API 并没有包括 get()或 put()方法。每一个 Buffer 类都有这两个方法，但它们所采用的参数类型，以及它们返回的数据类型，对每个子类来说都是唯一的，所以它们不能在顶层 Buffer 类中被抽象地声明。它们的定义必须被特定类型的子类所遵从。如：ByteBuffer1234567public abstract class ByteBuffer extends Buffer implements Comparable&#123; // This is a partial API listing public abstract byte get( ); public abstract byte get (int index); public abstract ByteBuffer put (byte b); public abstract ByteBuffer put (int index, byte b);&#125; 5.填充到缓冲区填充1buffer.put((byte)&apos;H&apos;).put((byte)&apos;e&apos;).put((byte)&apos;l&apos;).put((byte)&apos;l&apos;).put((byte)&apos;o&apos;); 修改1buffer.put(0,(byte)&apos;M&apos;).put((byte)&apos;w&apos;); 6.翻转&emsp;我们已经写满了缓冲区，现在我们必须准备将其清空。我们想把这个缓冲区传递给一个通道，以使内容能被全部写出。但如果通道现在在缓冲区上执行 get()，那么它将从我们刚刚插入的有用数据之外取出未定义数据。如果我们将位置值重新设为 0，通道就会从正确位置开始获取，但是它是怎样知道何时到达我们所插入数据末端的呢？这就是上界属性被引入的目的。上界属性指明了缓冲区有效内容的末端。我们需要将上界属性设置为当前位置，然后将位置重置为 0。我们可以人工用下面的代码实现：1buffer.limit(buffer.position()).position(0); //将limit设置为当前position，然后将position=0 Buffer.flip(); 方法就是实现了上面的人工代码 7.释放缓冲区1buffer.clear( ); 此时position=0；limit=capacity 8.创建缓冲区123456789public abstract class CharBuffer extends Buffer implements CharSequence, Comparable&#123; // This is a partial API listing public static CharBuffer allocate (int capacity) public static CharBuffer wrap (char [] array) public static CharBuffer wrap (char [] array, int offset, int length) public final boolean hasArray( ) public final char [] array( ) public final int arrayOffset( )&#125; 要分配一个容量为 100 个 char 变量的 Charbuffer:12CharBuffer charBuffer = CharBuffer.allocate (100);#这段代码隐含地从堆空间中分配了一个 char 型数组作为备份存储器来储存 100 个 char变量 提供您自己的数组用做缓冲区的备份存储器，请调用 wrap()方法123456789char [] myArray = new char [100];CharBuffer charbuffer = CharBuffer.wrap (myArray);CharBuffer charbuffer = CharBuffer.wrap (myArray, 12, 42);/*创建了一个 position 值为 12， limit 值为 54，容量为 myArray.length 的缓冲区这个方法并不像您可能认为的那样，创建了一个只占用了一个数组子集的缓冲区。这个缓冲区可以存取这个数组的全部范围； offset 和 length 参数只是设置了初始的状态。调用使用上面代码中的方法创建的缓冲区中的 clear()方法，然后对其进行填充，直到超过上界值，这将会重写数组中的所有元素。*/ 9.Example12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.itcast.test;import java.nio.IntBuffer;public class NIOBuffer &#123; public static void main(String[] args) &#123; IntBuffer buffer = IntBuffer.allocate(10); int[] array = new int[]&#123;1,3,5&#125;; //使用数组来创建一个缓冲区视图【1,3，5】 buffer = buffer.wrap(array); //利用数组的某一个区间来创建视图// buffer = buffer.wrap(array,0,2); //对数据缓冲区某一个位置上面的元素进行修改 buffer.put(0,7); //index,value System.out.println(&quot;缓冲区中的数据如下：&quot;); //遍历缓冲区中的数据 for(int i=0;i&lt;buffer.limit()-1;i++)&#123; System.out.println(buffer.get()+&quot;\\t&quot;); &#125; //验证缓冲区视图和数组中的元素是对应的 System.out.println(&quot;数组中的数据如下：&quot;); for(int a: array)&#123; System.out.println(a+&quot;\\t&quot;); &#125; //复制一个新的缓冲区 IntBuffer duplicate = buffer.duplicate(); duplicate.clear(); System.out.println(duplicate); duplicate.put(0,11); for(int i=0;i&lt;duplicate.limit();i++)&#123; System.out.println(&quot;duplicate:&quot;+duplicate.get()+&quot;\\t&quot;); &#125; buffer.clear(); for(int i=0;i&lt;buffer.limit();i++)&#123; System.out.println(&quot;buffer:&quot;+buffer.get()+&quot;\\t&quot;); &#125; for(int a: array)&#123; System.out.println(&quot;array:&quot;+a+&quot;\\t&quot;); &#125; //对缓冲区中的内容进行反转// System.out.println(&quot;flip前：&quot;+buffer);//flip前：java.nio.HeapIntBuffer[pos=3 lim=3 cap=3]// buffer.flip();// limit=pos; pos=0// System.out.println(&quot;flip后：&quot;+buffer);//flip后：java.nio.HeapIntBuffer[pos=0 lim=3 cap=3] //完全清零（当limit和capacity一样的时候，是和flip函数是一样的） System.out.println(&quot;clear前：&quot;+buffer);//flip前：java.nio.HeapIntBuffer[pos=3 lim=3 cap=3] buffer.clear();//limit=capacity; pos=0 System.out.println(&quot;clear后：&quot;+buffer);//flip后：java.nio.HeapIntBuffer[pos=0 lim=3 cap=3] &#125;&#125;","tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"},{"name":"NIO","slug":"NIO","permalink":"http://yoursite.com/tags/NIO/"}]},{"title":"传统的IO原理","date":"2017-04-16T04:47:25.393Z","path":"2017/04/16/java/socket/传统的IO原理/","text":"下图简单描述了数据从外部磁盘向运行中的进程的内存区域移动的过程。 进程使用read( )系统调用，要求其缓冲区被填满。 内核随即向磁盘控制硬件发出命令，要求其从磁盘读取数据。 磁盘控制器把数据直接写入内核内存缓冲区，这一步通过DMA完成，无需主CPU协助。 一旦磁盘控制器把缓冲区装满，内核即把数据从内核空间的临时缓冲区拷贝到进程执行read( )调用时指定的缓冲区 注意图中用户空间和内核空间的概念。用户空间是常规进程所在区域。 JVM就是常规进程，驻守于用户空间。用户空间是非特权区域：比如，在该区域执行的代码就不能直接访问硬件设备。内核空间是操作系统所在区域。内核代码有特别的权力：它能与设备控制器通讯，控制着用户区域进程的运行状态，等等。最重要的是，所有I/O都直接（如这里所述）或间接通过内核空间。 当进程请求I/O操作的时候，它执行一个系统调用（有时称为陷阱）将控制权移交给内核。C/C++程序员所熟知的底层方法open( )、 read( )、 write( )和close( )要做的无非就是建立和执行适当的系统调用。当内核以这种方式被调用，它随即采取任何必要步骤，找到进程所需数据，并把数据传送到用户空间内的指定缓冲区。内核试图对数据进行高速缓存或预读取，因此进程所需数据可能已经在内核空间里了。如果是这样，该数据只需简单地拷贝出来即可。如果数据不在内核空间，则进程被挂起，内核着手把数据读进内存。看了图 1-1， 您可能会觉得，把数据从内核空间拷贝到用户空间似乎有些多余。 为什么不直接让磁盘控制器把数据送到用户空间的缓冲区呢？这样做有几个问题。首先，硬件通常不能直接访问用户空间 ，其次，像磁盘这样基于块存储的硬件设备操作的是固定大小的数据块，而用户进程请求的可能是任意大小的或非对齐的数据块。在数据往来于用户空间与存储设备的过程中，内核负责数据的分解、再组合工作，因此充当着中间人的角色。 2.虚拟内存虚拟内存意为使用虚假（或虚拟）地址取代物理（硬件RAM）内存地址。把内核空间地址与用户空间的虚拟地址映射到同一个物理地址，这样，DMA硬件（只能访问物理内存地址）就可以填充对内核与用户空间进程同时可见的缓冲区（见图），这样做好处颇多，总结起来可分为两大类： 一个以上的虚拟地址可指向同一个物理内存地址。 虚拟内存空间可大于实际可用的硬件内存。 这样真是太好了，省去了内核与用户空间的往来拷贝，但前提条件是，内核与用户缓冲区必须使用相同的页对齐，缓冲区的大小还必须是磁盘控制器块大小（通常为 512 字节磁盘扇区）的倍数。操作系统把内存地址空间划分为页，即固定大小的字节组。内存页的大小总是磁盘块大小的倍数，通常为 2 次幂（这样可简化寻址操作）","tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"}]},{"title":"socket实现远程对象方法调用","date":"2017-04-16T04:47:25.391Z","path":"2017/04/16/java/socket/socket实现远程对象方法调用/","text":"1.调用过程图示 2.代码实现服务端 1234567891011121314151617package cn.itcast.bigdata.socket;import java.net.InetSocketAddress;import java.net.ServerSocket;import java.net.Socket;public class ServiceServer &#123; public static void main(String[] args) throws Exception &#123; // 创建一个serversocket，绑定到本机的8899端口上 ServerSocket server = new ServerSocket(); server.bind(new InetSocketAddress(&quot;localhost&quot;, 8899)); // 接受客户端的连接请求;accept是一个阻塞方法，会一直等待，到有客户端请求连接才返回 while (true) &#123; Socket socket = server.accept(); //因为服务端要处理多个客户端的socket请求，所以这里使用的是：来一个socket请求，然后将new一个线程去处理该socket new Thread(new ServiceServerTask(socket)).start(); &#125; &#125;&#125; 线程类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071package cn.itcast.bigdata.socket;import java.io.BufferedReader;import java.io.IOException;import java.io.InputStream;import java.io.InputStreamReader;import java.io.OutputStream;import java.io.PrintWriter;import java.lang.reflect.Method;import java.net.Socket;public class ServiceServerTask implements Runnable&#123; Socket socket ; InputStream in=null; OutputStream out = null; public ServiceServerTask(Socket socket) &#123; this.socket = socket; &#125; //业务逻辑：跟客户端进行数据交互 @SuppressWarnings(\"unchecked\") @Override public void run() &#123; try &#123; //从socket连接中获取到与client之间的网络通信输入输出流 in = socket.getInputStream(); out = socket.getOutputStream(); BufferedReader br = new BufferedReader(new InputStreamReader(in)); //从网络通信输入流中读取客户端发送过来的数据 //注意：socketinputstream的读数据的方法都是阻塞的 String params = br.readLine(); String className = \"\";//class全路径名称 String methodName = \"\";//反射要调用的方法名 String data = \"\";//传递到方法的参数 //将socket传过来的参数分解（cn.itcast.bigdata.socket.GetDataServiceImpl:getData:chenyansong） if(params!=null&amp;&amp;!params.equals(\"\")&amp;&amp;params.length()&gt;=3)&#123; String[] paramArr = params.split(\":\"); className = paramArr[0]; methodName = paramArr[1]; data = paramArr[2]; &#125; Class cl = Class.forName(className); Method method = cl.getMethod(methodName, String.class); String result = (String) method.invoke(cl.newInstance(), data);//反射调用传递过来的方法 //将调用结果写到sokect的输出流中，以发送给客户端 PrintWriter pw = new PrintWriter(out); pw.println(result); pw.flush(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125;finally&#123; try &#123; in.close(); out.close(); socket.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 客户端 1234567891011121314151617181920212223242526272829303132package cn.itcast.bigdata.socket;import java.io.BufferedReader;import java.io.InputStream;import java.io.InputStreamReader;import java.io.OutputStream;import java.io.PrintWriter;import java.net.Socket;public class ServiceClient &#123; public static void main(String[] args) throws Exception &#123; // 向服务器发出请求建立连接 Socket socket = new Socket(&quot;localhost&quot;, 8899); // 从socket中获取输入输出流 InputStream inputStream = socket.getInputStream(); OutputStream outputStream = socket.getOutputStream(); PrintWriter pw = new PrintWriter(outputStream); pw.println(&quot;cn.itcast.bigdata.socket.GetDataServiceImpl:getData:chenyansong&quot;); //传递到对端的数据，server端会对参数进行split pw.flush(); BufferedReader br = new BufferedReader(new InputStreamReader(inputStream)); String result = br.readLine(); System.out.println(result); inputStream.close(); outputStream.close(); socket.close(); &#125;&#125; 反射实现类123456789//注意：反射的类要有空的构造函数，如果没写构造函数，默认是有一个空的构造函数的package cn.itcast.bigdata.socket;public class GetDataServiceImpl implements GetDataService&#123; public String getData(String param)&#123; return &quot;ok-&quot;+param; &#125;&#125; 接口：GetDataService1234package cn.itcast.bigdata.socket;public interface GetDataService &#123; &#125;","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"rpc","slug":"rpc","permalink":"http://yoursite.com/tags/rpc/"}]},{"title":"NIO代码示例","date":"2017-04-16T04:47:25.389Z","path":"2017/04/16/java/socket/NIO代码示例/","text":"1.阻塞式IO和非阻塞式IO的区别Block IO会对每个连接创建一个线程，因此这极大限制了JVM创建线程的数量（当然线程池可缓解这个问题，但是也仅仅是缓解），如图所示 NIO会通过专门的Selector来管理请求，然后可由一个线程来处理请求，如图所示： 说明: 客户端和服务端通信是通过channel进行的(即:上图中的每一个read/write就相当于一个channel) channel需要注册到selector中进行统一管理 selector中保存的是Map,通过key就可以取出对应的channel 循环取出key,得到channel,然后判断该channel上的是accept,or read or write操作(详见下面的:服务端代码) 2.NIO服务端时序列图 2.1.步骤说明步骤1：打开ServerSocketChannel，用于监听客户端的连接，它是所有客户端连接的父通道，代码示例如下：1ServerSocketChannel acceptorSvr = ServerSocketChannel.open(); 绑定监听端口，设置客户端连接方式为非阻塞模式，示例代码如下12acceptorSvr.socket().bind(new InetSocketAddress(InetAddress.getByName(“IP”), port));acceptorSvr.configureBlocking(false); 步骤3：创建Reactor线程，打开多路复用器并启动服务端监听线程，通常情况下，可以采用线程池的方式创建Reactor线程。示例代码如下：12Selector selector = Selector.open();New Thread(new ReactorTask()).start(); 步骤4：将ServerSocketChannel注册到Reactor线程的多路复用器Selector上，监听ACCEPT状态位，示例代码如下：1SelectionKey key = acceptorSvr.register( selector, SelectionKey.OP_ACCEPT, ioHandler); 步骤5：多路复用器在线程run方法的无限循环体内轮询准备就绪的Key，通常情况下需要设置一个退出状态检测位，用于优雅停机。代码如下：1234567int num = selector.select();Set selectedKeys = selector.selectedKeys();Iterator it = selectedKeys.iterator();while (it.hasNext()) &#123; SelectionKey key = (SelectionKey)it.next(); // ... deal with I/O event ...&#125; 步骤6：多路复用器监听到有新的客户端接入，处理新的接入请求，完成TCP三次握手后，与客户端建立物理链路，示例代码如下：12channel.configureBlocking(false);channel.socket().setReuseAddress(true); 步骤8：将新接入的客户端连接注册到Reactor线程的多路复用器上，监听读操作位，用来读取客户端发送的网络消息，示例代码如下：1SelectionKey key = socketChannel.register( selector, SelectionKey.OP_READ, ioHandler); 步骤9：异步读取客户端请求消息到服务端缓冲区，示例代码如下：1int readNumber = channel.read(receivedBuffer); 步骤10：对ByteBuffer进行解码，如果有半包消息指针Reset，继续读取后续的报文，将解码成功的消息封装成Task，投递到业务线程池中，进行业务逻辑编排，示例代码如下：123456789101112131415161718192021Object message = null;while(buffer.hasRemain())&#123; byteBuffer.mark(); Object message = decode(byteBuffer); if (message == null) &#123; byteBuffer.reset(); break; &#125; messageList.add(message );&#125;if (!byteBuffer.hasRemain())byteBuffer.clear();else byteBuffer.compact();if (messageList != null &amp; !messageList.isEmpty())&#123;for(Object messageE : messageList) handlerTask(messageE);&#125; 步骤11：将POJO对象encode成ByteBuffer，调用SocketChannel的异步write接口，将消息异步发送给客户端，示例代码如下：如果发送区TCP缓冲区满，会导致写半包，此时，需要注册监听写操作位，循环写，直到整包消息写入TCP缓冲区。1socketChannel.write(buffer); 3.服务端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128package cn.itcast.test;import java.io.IOException;import java.net.InetSocketAddress;import java.net.ServerSocket;import java.nio.ByteBuffer;import java.nio.channels.Channel;import java.nio.channels.FileChannel;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;public class NIOServer &#123; private int flag = 1; private int blockSize = 4096; private ByteBuffer sendbuffer = ByteBuffer.allocate(blockSize); private ByteBuffer receivebuffer = ByteBuffer.allocate(blockSize); Selector selector; public NIOServer(int port) throws IOException &#123; //开启一个channel ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //设置为非阻塞的 serverSocketChannel.configureBlocking(false); ServerSocket serverSocket = serverSocketChannel.socket(); //绑定IP和端口 serverSocket.bind(new InetSocketAddress(port)); //打开选择器 selector = Selector.open(); //注册channel到selector中（并指定事件类型） serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); System.out.println(&quot;Server start -&gt;&quot;+port); &#125; //监听 public void listen() throws IOException&#123; while(true)&#123; int n = selector.select();//会将失效的SelectionKey清除掉，如果没有通道已经准备好，线程可能会在这时阻塞，通常会有一个超时值 if(n==0)&#123; continue; &#125; Set&lt;SelectionKey&gt; selectedKeysSet = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iterator = selectedKeysSet.iterator(); while(iterator.hasNext())&#123; SelectionKey selectionKey = iterator.next(); //// Remove key from selected set; it&apos;s been handled iterator.remove(); //业务逻辑 handleKey(selectionKey); &#125; &#125; &#125; //处理业务逻辑 public void handleKey(SelectionKey selectionKey) throws IOException&#123; ServerSocketChannel server = null; SocketChannel client = null; String reciveText; String sendText; int count = 0; if(selectionKey.isAcceptable())&#123;//可接收的channel //从select中去取对应key的信道（channel） server = (ServerSocketChannel) selectionKey.channel(); //接收客户端连接 client = server.accept(); //阻塞为false client.configureBlocking(false); //注册读事件 client.register(selector, SelectionKey.OP_READ); &#125;else if(selectionKey.isReadable())&#123;//读事件 //获取和客户端的channel连接 client = (SocketChannel) selectionKey.channel(); count = client.read(receivebuffer);//读到缓冲区中 if(count&gt;0)&#123; //接收到客户端的数据 reciveText = new String(receivebuffer.array(),0,count); System.out.println(&quot;服务端接收到客户端的数据&quot;); //注册服务端的写事件 client.register(selector, SelectionKey.OP_WRITE); &#125; &#125;else if(selectionKey.isWritable())&#123;//写事件 //清空写的缓冲区 sendbuffer.clear(); //获取和客户端的channel连接 client = (SocketChannel) selectionKey.channel(); sendText = &quot;msg send to client&quot;+flag++; //将数据放入到缓冲区中 sendbuffer.put(sendText.getBytes()); //将buff翻转，这样在读的时候，会从起始的地方读取 sendbuffer.flip(); //发送数据到客户端 client.write(sendbuffer); System.out.println(&quot;服务器端发送数据给客户端：&quot;+sendText); &#125; &#125; public static void main(String[] args) throws IOException &#123; int port = 7080; NIOServer server = new NIOServer(port); //监听 server.listen(); &#125;&#125; 4.客户端代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107package cn.itcast.test;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectableChannel;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;public class NIOClient &#123; private static int flag = 1; private static int blockSize = 4096; private static ByteBuffer sendbuffer = ByteBuffer.allocate(blockSize); private static ByteBuffer receivebuffer = ByteBuffer.allocate(blockSize); private final static InetSocketAddress serverAddress = new InetSocketAddress(&quot;127.0.0.1&quot;, 7080); /** * @throws IOException */ public static void main(String[] args) throws IOException &#123; SocketChannel socketChannel = SocketChannel.open(); //设置为非阻塞 socketChannel.configureBlocking(false); //打开选择器 Selector selector = Selector.open(); //向selector注册连接事件 socketChannel.register(selector, SelectionKey.OP_CONNECT); //连接 socketChannel.connect(serverAddress); Set&lt;SelectionKey&gt; selectionKeySet; Iterator&lt;SelectionKey&gt; iterator; SelectionKey selectKey; SocketChannel client; String receiveText; String sendText; int count = 0; while(true)&#123; selector.select();//这是一个阻塞的方法，当有至少一个channel被选择或者线程被中断的时候，会阻塞结束 /* It returns only after at least one channel is selected, this selector&apos;s wakeup method is invoked, or the current thread is interrupted, whichever comes first. */ selectionKeySet = selector.selectedKeys(); iterator = selectionKeySet.iterator(); while(iterator.hasNext())&#123; selectKey = iterator.next(); if(selectKey.isConnectable())&#123;//连接事件 System.out.println(&quot;client connet...&quot;); client = (SocketChannel) selectKey.channel();; if(client.isConnectionPending())&#123;//如果是正在连接就完成连接 client.finishConnect();//完成连接 System.out.println(&quot;客户端完成连接操作....&quot;); //先清空buff sendbuffer.clear(); //向buff中添加数据 sendbuffer.put(&quot;Hello Server&quot;.getBytes()); sendbuffer.flip();//将缓冲区翻转，为读该缓冲区做准备 client.write(sendbuffer); &#125; //注册读事件 client.register(selector, SelectionKey.OP_READ); &#125;else if(selectKey.isReadable())&#123;//读server端数据 client = (SocketChannel) selectKey.channel();; receivebuffer.clear(); count = client.read(receivebuffer); if(count&gt;0)&#123;//服务端有发送数据过来 receiveText = new String(receivebuffer.array(),0,count); System.out.println(&quot;客户端接收到服务端的数据&quot;+receiveText); //注册写事件 client.register(selector, selectKey.OP_WRITE); &#125; &#125;else if(selectKey.isWritable())&#123;//写事件 sendbuffer.clear(); client = (SocketChannel) selectKey.channel(); sendText = &quot;Msg send to Server-&gt;&quot;+flag++; sendbuffer.put(sendText.getBytes()); sendbuffer.flip(); client.write(sendbuffer); System.out.println(&quot;客户端发送数据给服务端：&quot;+sendText); //注册读事件 client.register(selector, SelectionKey.OP_READ); &#125; &#125; //清除 selectionKeySet.clear(); &#125; &#125;&#125;","tags":[{"name":"socket","slug":"socket","permalink":"http://yoursite.com/tags/socket/"},{"name":"NIO","slug":"NIO","permalink":"http://yoursite.com/tags/NIO/"}]},{"title":"netty的第一个helloworld","date":"2017-04-16T04:47:25.387Z","path":"2017/04/16/java/netty/netty的第一个helloworld/","text":"服务端线程模型一种比较流行的做法是服务端监听线程和IO线程分离，类似于Reactor的多线程模型，它的工作原理图如下： 另外一种描述: bossGroup线程组实际就是Acceptor线程池，负责处理客户端的TCP连接请求，如果系统只有一个服务端端口需要监听，则建议bossGroup线程组线程数设置为1。 workerGroup是真正负责I/O读写操作的线程组，通过ServerBootstrap的group方法进行设置，用于后续的Channel绑定。 服务端启动类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package cn.itcast_03_netty.sendstring.server;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import cn.itcast_03_netty.sendobject.coder.PersonDecoder;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 * * @author wilson * */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //server端引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); //连接池处理数据 eventLoopGroup = new NioEventLoopGroup(); //装配bootstrap serverBootstrap.group(eventLoopGroup) .channel(NioServerSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .localAddress(&quot;localhost&quot;,port)//设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123;//设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new EchoServerHandler());//注册handler &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println(&quot;开始监听，端口为：&quot; + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; 服务端回调方法（处理器）1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.itcast_03_netty.sendstring.server;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;import cn.itcast_03_netty.sendobject.bean.Person;//继承了ChannelInboundHandlerAdapterpublic class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(&quot;server 读取数据……&quot;); //读取数据 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, &quot;UTF-8&quot;); System.out.println(&quot;接收客户端数据:&quot; + body); //向客户端写数据 System.out.println(&quot;server向client发送数据&quot;); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;server 读取数据完毕..&quot;); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 客户端现场模型 另外一种描述: 客户端启动类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.itcast_03_netty.sendstring.client;import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel;import java.net.InetSocketAddress;import cn.itcast_03_netty.sendobject.coder.PersonEncoder;/** * • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接 * * @author wilson * */public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; // 客户端引导类 Bootstrap bootstrap = new Bootstrap(); // EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); bootstrap.group(nioEventLoopGroup)//多线程处理 .channel(NioSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .remoteAddress(new InetSocketAddress(host, port))//地址 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;//业务处理类 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler());//注册handler &#125; &#125;); // 链接服务器 ChannelFuture channelFuture = bootstrap.connect().sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoClient(&quot;localhost&quot;, 20000).start(); &#125;&#125; 客户端回调方法（处理器）12345678910111213141516171819202122232425262728293031323334353637383940package cn.itcast_03_netty.sendstring.client;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import cn.itcast_03_netty.sendobject.bean.Person;//继承了SimpleChannelInboundHandlerpublic class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 客户端连接服务器后被调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;客户端连接服务器，开始发送数据……&quot;); byte[] req = &quot;QUERY TIME ORDER&quot;.getBytes();//消息 ByteBuf firstMessage = Unpooled.buffer(req.length);//发送类 firstMessage.writeBytes(req);//发送 ctx.writeAndFlush(firstMessage);//flush &#125; // • 从服务器接收到数据后调用 @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println(&quot;client 读取server数据..&quot;); // 服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, &quot;UTF-8&quot;); System.out.println(&quot;服务端数据为 :&quot; + body); &#125; // • 发生异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(&quot;client exceptionCaught..&quot;); // 释放资源 ctx.close(); &#125;&#125; 参考:Netty系列之Netty线程模型Netty工作原理架构图","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"netty发送对象","date":"2017-04-16T04:47:25.386Z","path":"2017/04/16/java/netty/netty发送对象/","text":"1.简介&emsp;Netty中，通讯的双方建立连接后，会把数据按照ByteBuf的方式进行传输，例如http协议中，就是通过HttpRequestDecoder对ByteBuf数据流进行处理，转换成http的对象。基于这个思路,我自定义一种通讯协议：Server和客户端直接传输java对象。&emsp;实现的原理是通过Encoder把java对象转换成ByteBuf流进行传输，通过Decoder把ByteBuf转换成java对象进行处理，处理逻辑如下图所示： 2.代码服务器端代码server 1234567891011121314151617181920212223242526272829303132333435363738394041public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //创建ServerBootstrap实例来引导绑定和启动服务器 ServerBootstrap serverBootstrap = new ServerBootstrap(); //创建NioEventLoopGroup对象来处理事件，如接受新连接、接收数据、写数据等等 eventLoopGroup = new NioEventLoopGroup(); //指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel //设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 serverBootstrap.group(eventLoopGroup).channel(NioServerSocketChannel.class).localAddress(&quot;localhost&quot;,port) .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123; //设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; //注册解码的handler ch.pipeline().addLast(new PersonDecoder()); //IN1 反序列化 //添加一个入站的handler到ChannelPipeline ch.pipeline().addLast(new EchoServerHandler()); //IN2 &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println(&quot;开始监听，端口为：&quot; + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; handler 12345678910111213141516171819202122232425public class EchoServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; Person person = (Person) msg; System.out.println(person.getName()); System.out.println(person.getAge()); System.out.println(person.getSex()); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;server 读取数据完毕..&quot;); ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125; 解码的handler 123456789101112131415161718 /** * 反序列化 * 将Byte[]转换为Object * @author wilson * */public class PersonDecoder extends ByteToMessageDecoder &#123; @Override protected void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; //工具类：将ByteBuf转换为byte[] ByteBufToBytes read = new ByteBufToBytes(); byte[] bytes = read.read(in); //工具类：将byte[]转换为object Object obj = ByteObjConverter.byteToObject(bytes); out.add(obj); &#125; &#125; 客户端代码client123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接 * * @author wilson * */public class EchoClient &#123; private final String host; private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; // 创建Bootstrap对象用来引导启动客户端 Bootstrap bootstrap = new Bootstrap(); // 创建EventLoopGroup对象并设置到Bootstrap中，EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); // 创建InetSocketAddress并设置到Bootstrap中，InetSocketAddress是指定连接的服务器地址 bootstrap.group(nioEventLoopGroup)// .channel(NioSocketChannel.class)// .remoteAddress(new InetSocketAddress(host, port))// .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;// // 添加一个ChannelHandler，客户端成功连接服务器后就会被执行 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; // 注册编码的handler ch.pipeline().addLast(new PersonEncoder()); //out //注册处理消息的handler ch.pipeline().addLast(new EchoClientHandler()); //in &#125; &#125;); // • 调用Bootstrap.connect()来连接服务器 ChannelFuture f = bootstrap.connect().sync(); // • 最后关闭EventLoopGroup来释放资源 f.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoClient(&quot;localhost&quot;, 20000).start(); &#125;&#125; client-handler12345678910111213141516171819202122232425262728293031323334public class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123; // 客户端连接服务器后被调用 @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; Person person = new Person(); person.setName(&quot;angelababy&quot;); person.setSex(&quot;girl&quot;); person.setAge(18); ctx.write(person); ctx.flush(); &#125; // • 从服务器接收到数据后调用 @Override protected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println(&quot;client 读取server数据..&quot;); // 服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, &quot;UTF-8&quot;); System.out.println(&quot;服务端数据为 :&quot; + body); &#125; // • 发生异常时被调用 @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(&quot;client exceptionCaught..&quot;); // 释放资源 ctx.close(); &#125;&#125; 编码的handler 1234567891011121314151617 /** * 序列化 * 将object转换成Byte[] * @author wilson * */public class PersonEncoder extends MessageToByteEncoder&lt;Person&gt; &#123; @Override protected void encode(ChannelHandlerContext ctx, Person msg, ByteBuf out) throws Exception &#123; //工具类：将object转换为byte[] byte[] datas = ByteObjConverter.objectToByte(msg); out.writeBytes(datas); ctx.flush(); &#125;&#125; 转换工具12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061public class ByteObjConverter &#123; /** * 使用IO的inputstream流将byte[]转换为object * @param bytes * @return */ public static Object byteToObject(byte[] bytes) &#123; Object obj = null; ByteArrayInputStream bi = new ByteArrayInputStream(bytes); ObjectInputStream oi = null; try &#123; oi = new ObjectInputStream(bi); obj = oi.readObject(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bi.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; oi.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return obj; &#125; /** * 使用IO的outputstream流将object转换为byte[] * @param bytes * @return */ public static byte[] objectToByte(Object obj) &#123; byte[] bytes = null; ByteArrayOutputStream bo = new ByteArrayOutputStream(); ObjectOutputStream oo = null; try &#123; oo = new ObjectOutputStream(bo); oo.writeObject(obj); bytes = bo.toByteArray(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; bo.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; try &#123; oo.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; return bytes; &#125;&#125;","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"netty中handler的执行顺序","date":"2017-04-16T04:47:25.384Z","path":"2017/04/16/java/netty/netty中handler的执行顺序/","text":"1.handler处理器简介&emsp;Handler在netty中，无疑占据着非常重要的地位。Handler与Servlet中的filter很像，通过Handler可以完成通讯报文的解码编码、拦截指定的报文、统一对日志错误进行处理、统一对请求进行计数、控制Handler执行与否。一句话，没有它做不到的只有你想不到的。&emsp;Netty中的所有handler都实现自ChannelHandler接口。按照输出输出来分，分为ChannelInboundHandler、ChannelOutboundHandler两大类。ChannelInboundHandler对从客户端发往服务器的报文进行处理，一般用来执行解码、读取客户端数据、进行业务处理等；ChannelOutboundHandler对从服务器发往客户端的报文进行处理，一般用来进行编码、发送报文到客户端。&emsp;Netty中，可以注册多个handler。ChannelInboundHandler按照注册的先后顺序执行；ChannelOutboundHandler按照注册的先后顺序逆序执行，如下图所示，按照注册的先后顺序对Handler进行排序,request进入Netty后的执行顺序为： 2.Example代码2.1.服务端启动类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package cn.itcast_03_netty.sendorder.server;import io.netty.bootstrap.ServerBootstrap;import io.netty.channel.Channel;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.nio.NioServerSocketChannel;import cn.itcast_03_netty.sendobject.coder.PersonDecoder;/** * • 配置服务器功能，如线程、端口 • 实现服务器处理程序，它包含业务逻辑，决定当有一个请求连接或接收数据时该做什么 * * @author wilson * */public class EchoServer &#123; private final int port; public EchoServer(int port) &#123; this.port = port; &#125; public void start() throws Exception &#123; EventLoopGroup eventLoopGroup = null; try &#123; //server端引导类 ServerBootstrap serverBootstrap = new ServerBootstrap(); //连接池处理数据 eventLoopGroup = new NioEventLoopGroup(); serverBootstrap.group(eventLoopGroup) .channel(NioServerSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .localAddress(&quot;localhost&quot;,port)//设置InetSocketAddress让服务器监听某个端口已等待客户端连接。 .childHandler(new ChannelInitializer&lt;Channel&gt;() &#123;//设置childHandler执行所有的连接请求 @Override protected void initChannel(Channel ch) throws Exception &#123; // 注册两个InboundHandler，执行顺序为注册顺序，所以应该是InboundHandler1 InboundHandler2 // 注册两个OutboundHandler，执行顺序为注册顺序的逆序，所以应该是OutboundHandler2 OutboundHandler1 ch.pipeline().addLast(new EchoInHandler1()); ch.pipeline().addLast(new EchoInHandler2()); ch.pipeline().addLast(new EchoOutHandler1()); ch.pipeline().addLast(new EchoOutHandler2()); &#125; &#125;); // 最后绑定服务器等待直到绑定完成，调用sync()方法会阻塞直到服务器完成绑定,然后服务器等待通道关闭，因为使用sync()，所以关闭操作也会被阻塞。 ChannelFuture channelFuture = serverBootstrap.bind().sync(); System.out.println(&quot;开始监听，端口为：&quot; + channelFuture.channel().localAddress()); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; eventLoopGroup.shutdownGracefully().sync(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new EchoServer(20000).start(); &#125;&#125; 2.2.服务端回调方法（处理器）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123###################################### EchoInHandler1 #################################################################package cn.itcast_03_netty.sendorder.server;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;import cn.itcast_03_netty.sendobject.bean.Person;public class EchoInHandler1 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(&quot;in1&quot;); // 通知执行下一个InboundHandler ctx.fireChannelRead(msg); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125;################################## EchoInHandler2 #####################################################################package cn.itcast_03_netty.sendorder.server;import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelInboundHandlerAdapter;import java.util.Date;import cn.itcast_03_netty.sendobject.bean.Person;public class EchoInHandler2 extends ChannelInboundHandlerAdapter &#123; @Override public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; System.out.println(&quot;in2&quot;); ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, &quot;UTF-8&quot;); System.out.println(&quot;接收客户端数据:&quot; + body); //向客户端写数据 System.out.println(&quot;server向client发送数据&quot;); String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); &#125; @Override public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; ctx.flush();//刷新后才将数据发出到SocketChannel &#125; @Override public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; cause.printStackTrace(); ctx.close(); &#125;&#125;################################# EchoOutHandler1 ######################################################################package cn.itcast_03_netty.sendorder.server; import java.util.Date; import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelOutboundHandlerAdapter;import io.netty.channel.ChannelPromise; public class EchoOutHandler1 extends ChannelOutboundHandlerAdapter &#123;@Override // 向client发送消息 public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println(&quot;out1&quot;); /*System.out.println(msg);*/ String currentTime = new Date(System.currentTimeMillis()).toString(); ByteBuf resp = Unpooled.copiedBuffer(currentTime.getBytes()); ctx.write(resp); ctx.flush(); &#125;&#125;################################ EchoOutHandler2 #######################################################################package cn.itcast_03_netty.sendorder.server;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.ChannelOutboundHandlerAdapter;import io.netty.channel.ChannelPromise;public class EchoOutHandler2 extends ChannelOutboundHandlerAdapter &#123; @Override public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; System.out.println(&quot;out2&quot;); // 执行下一个OutboundHandler /*System.out.println(&quot;at first..msg = &quot;+msg); msg = &quot;hi newed in out2&quot;;*/ super.write(ctx, msg, promise); &#125;&#125; 2.3.客户端启动类12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.itcast_03_netty.sendorder.client; import io.netty.bootstrap.Bootstrap;import io.netty.channel.ChannelFuture;import io.netty.channel.ChannelInitializer;import io.netty.channel.EventLoopGroup;import io.netty.channel.nio.NioEventLoopGroup;import io.netty.channel.socket.SocketChannel;import io.netty.channel.socket.nio.NioSocketChannel; import java.net.InetSocketAddress; import cn.itcast_03_netty.sendobject.coder.PersonEncoder; /*** • 连接服务器 • 写数据到服务器 • 等待接受服务器返回相同的数据 • 关闭连接** @author wilson**/public class EchoClient &#123; private final String host;private final int port; public EchoClient(String host, int port) &#123; this.host = host; this.port = port;&#125; public void start() throws Exception &#123; EventLoopGroup nioEventLoopGroup = null; try &#123; // 客户端引导类 Bootstrap bootstrap = new Bootstrap(); // EventLoopGroup可以理解为是一个线程池，这个线程池用来处理连接、接受数据、发送数据 nioEventLoopGroup = new NioEventLoopGroup(); bootstrap.group(nioEventLoopGroup)//多线程处理 .channel(NioSocketChannel.class)//指定通道类型为NioServerSocketChannel，一种异步模式，OIO阻塞模式为OioServerSocketChannel .remoteAddress(new InetSocketAddress(host, port))//地址 .handler(new ChannelInitializer&lt;SocketChannel&gt;() &#123;//业务处理类 @Override protected void initChannel(SocketChannel ch) throws Exception &#123; ch.pipeline().addLast(new EchoClientHandler());//注册handler &#125; &#125;); // 链接服务器 ChannelFuture channelFuture = bootstrap.connect().sync(); channelFuture.channel().closeFuture().sync(); &#125; finally &#123; nioEventLoopGroup.shutdownGracefully().sync(); &#125;&#125; public static void main(String[] args) throws Exception &#123; new EchoClient(&quot;localhost&quot;, 20000).start();&#125;&#125; 2.4.客户端回调方法（处理器）123456789101112131415161718192021222324252627282930313233343536373839package cn.itcast_03_netty.sendorder.client; import io.netty.buffer.ByteBuf;import io.netty.buffer.Unpooled;import io.netty.channel.ChannelHandlerContext;import io.netty.channel.SimpleChannelInboundHandler;import cn.itcast_03_netty.sendobject.bean.Person; public class EchoClientHandler extends SimpleChannelInboundHandler&lt;ByteBuf&gt; &#123;// 客户端连接服务器后被调用@Overridepublic void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println(&quot;客户端连接服务器，开始发送数据……&quot;); byte[] req = &quot;QUERY TIME ORDER&quot;.getBytes();//消息 ByteBuf firstMessage = Unpooled.buffer(req.length);//发送类 firstMessage.writeBytes(req);//发送 ctx.writeAndFlush(firstMessage);//flush&#125; // • 从服务器接收到数据后调用@Overrideprotected void channelRead0(ChannelHandlerContext ctx, ByteBuf msg) throws Exception &#123; System.out.println(&quot;client 读取server数据..&quot;); // 服务端返回消息后 ByteBuf buf = (ByteBuf) msg; byte[] req = new byte[buf.readableBytes()]; buf.readBytes(req); String body = new String(req, &quot;UTF-8&quot;); System.out.println(&quot;服务端数据为 :&quot; + body);&#125; // • 发生异常时被调用@Overridepublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; System.out.println(&quot;client exceptionCaught..&quot;); // 释放资源 ctx.close();&#125;&#125; 3.总结在使用Handler的过程中，需要注意：1、ChannelInboundHandler之间的传递，通过调用 ctx.fireChannelRead(msg) 实现；调用ctx.write(msg) 将传递到ChannelOutboundHandler。2、ctx.write()方法执行后，需要调用flush()方法才能令它立即执行。3、流水线pipeline中outhandler不能放在最后，否则不生效4、Handler的消费处理放在最后一个处理。","tags":[{"name":"java","slug":"java","permalink":"http://yoursite.com/tags/java/"},{"name":"netty","slug":"netty","permalink":"http://yoursite.com/tags/netty/"}]},{"title":"第四章 不使用异常来处理错误","date":"2017-04-16T04:47:25.380Z","path":"2017/04/16/functionalProgrammingInScala/第四章 不使用异常来处理错误/","text":"1.异常的优点与劣势&emsp;为什么异常破坏了引用透明,会有怎样的问题?让我们看一个简单的例子,我们定义一个会抛出异常的函数,并且调用他123456789def failingFn(i: Int): Int = &#123; val y: Int = throw new Exception(&quot;fail!&quot;) try&#123; val x = 42 + 5 x+y &#125;catch &#123; case e: Exception =&gt;43 &#125;&#125; 在REPL下调用failingFn会预期错误123scala&gt; failingFn(12)java.lang.Exception: fail! at .failingFn(&lt;console&gt;:8) &emsp;可以证明y不是引用透明的,回忆一下引用透明的例子,表达式可以被他引用的值替代,这种替代保持程序的含义,如果我们对x+y表达式中的y替代为throw newException(“fail!”) 会产生不同的结果,因为try代码块内部的异常会被捕获并返回4312345678def failingFn(i: Int): Int = &#123; try&#123; val x = 42 + 5 x+((throw new Exception(&quot;fail!&quot;)): Int) &#125;catch &#123; case e: Exception =&gt;43 &#125;&#125; &emsp;在REPL下展示12scala&gt; failingFn(12)res2: Int = 43 &emsp;理解这种引用透明的另一种方式是引用透明的表达式不依赖上下文,可以本地推导,而那些非引用透明的表达式是依赖上下文的,并且需要全局推导,举例来说,引用透明的表达式42+5如果嵌入一个更大的表达式中,并不会对这个更大的表达式产生依赖,他总是等于47,但throw new Exception(“fail!”) 表达式是取决于上下文的,他可能呈现不同的含义,取决于他嵌入的try代码块&emsp;异常存在的两个主要问题: 正如我们所讨论的,异常破坏了引用透明并引入了上下文依赖,让代换模型的简单推导无法适用,并可能写成令人困惑的代码,源自坊间的一个习惯是建议异常应该只用于错误处理而非流程控制 异常不是类型安全的,failingFn,Int =&gt; Int 类型没有告诉我们可能会发生什么样的异常,编译器当然不会强迫failingFn的调用者决定如何处理这些异常,如果我们忘记检测failingFn的异常,那么将会直到运行时才会被检测到 检测异常 &emsp;Java的异常检测最低限度的强制了是处理还是抛出异常,但他们导致了调用者对签名模板的修改,更重要的是他们不适用于高阶函数,因为高阶函数不可能感知由他的参数引起的特定的异常,例如,考虑对List定义的map函数:1def map[A, B](l: List[A])(f: A=&gt;B): List[B] &emsp;这个函数很清晰,高度泛化,与使用检测异常不一样的是—–我们不能对每一个f抛出的异常的检测都有一个版本的map&emsp;我们希望其他选择能刨除异常的缺点,但我们又不希望他会失去异常最主要的一个好处—–整合集中的错误处理逻辑,而非被迫在整个代码库发布这个逻辑,这里用的技术基于一个早已存在的理念:替代抛出异常,返回一个值来指示异常情况发生,这种理念或许对使用过C的返回码处理异常的人很熟悉,但这里不是使用错误码,我们引入了一种新的泛型类型来描述这些”可能存在定义的值”,并使用高阶函数来封装这种处理和传播异常的通用模式,不像C风格的错误码,我们使用的错误处理策略是完全类型安全的,并且能得到类型检测的帮助,以最小的语法噪音让我们有效的处理错误 2.异常的其他选择&emsp;这里有一个函数的实现:计算列表的平均值,他没有定义空列表的情况:1234def mean(xs: Seq[Double]): Double = if (xs.isEmpty) throw new ArithmeticException(&quot;mean of mepty list&quot;) else xs.sum / xs.length &emsp;mean函数是一个部分函数:它对一些输入没有定义,如果一个函数对那些非隐式的输入类型做了一些假设,那他是一个典型的部分函数,这种情况下可以抛出异常,但我们还有其他的选择,来看下对mean使用这种选择的例子:&emsp;第一种方案:返回某个伪造的Double类型的值,可以对所有情况简单的返回xs.sum / xs.length ,对输入为空的情况,0.0/0.0 返回Double.NaN,或其他的报警值,但是我们有以下几个理由拒绝这种方案: 他允许错误”无声”的传播—–调用者可能忘了检测这种情况也不会被编译器警告,可能导致后续的代码不能正常工作 除了易错性,使用显示的if声明来检测是否调用者收到了”真正”的结果(因为返回的不同伪造的值),还导致在调用点产生一堆代码模板,如果调用多个函数,代码模板将被放大,每个使用错误码的地方都得检查并以某种方式聚合起来 他不适用于多态代码,对某些输出类型,甚至没有我们想要的报警值,例如max函数,按照自定义的比较从列表中找出最大的值,def maxA(greater: (A,A)=&gt;Boolean):A , 如果输入为空,无法发明一个A类型的值,null也不能用在这里,因为null只对非基础数据类型有效,而A可能是一个基础类型数据,如Int或Double 他需要一个特定的策略或调用约定—–合理使用mean函数需要调用者做一些事情,像这种特定策略的方法,将很难把它传递给高阶函数,因为高阶函数对待所有参数都是一致的 &emsp;第二种方案:针对那些输入不知道怎么处理的情况,强迫调用者提供一个参数告诉我们该如何处理123def mean(xs: Seq[Double], onEmpty: Double): Double = if (xs.isEmpty) onEmpty else xs.sum / xs.length &emsp;这样让mean函数成为了一个完整函数,但有一些缺点——-他需要调用者知道如何处理未定义的情况,限制他们返回一个Double(因为onEmpty是Double类型的) 3.Option数据类型&emsp;解决方案是在返回值类型时明确表示函数并不总是有答案,可以认为这是用于推迟调用者的错误处理策略,我们引入一种新的类型Option,如之前提到过的,这个类型在scala标准库中存在,但我们出于教学的目的在这里重新创建一个123sealed trait Option[+A]case class Some[+A](get: A) extends Option[A]case object None extends Option[Nothing] &emsp;Option有两种情况,已被定义的情况对应的是Some,未被定义的情况对应的是None&emsp;我们可以用Option来定义mean函数,比如:123def mean(xs: Seq[Double]): Option[Double] = if (xs.isEmpty) None else Some(xs.sum / xs.length) 3.1.Option的使用模式&emsp;部分函数在编程中大量存在,Option在函数式编程中常被用于处理部分函数,你会看到Option的使用贯穿整个scala标准库,例如: 在Map中查找一个key,返回Option 在List和其他可迭代集合中定义的headOption和lastOption返回包含第一个或最后一个元素的Option,如果这个集合不为空的话 基础函数中的Option &emsp;Option是一个最多只包含一个元素的List,第3章中把所有List函数都放在了List伴生对象中,这里的做法稍微不同,尽可能的把函数都放在Option接口里,这样就可以使用语法obj.fn(arg1)或 obj fn arg1替代fn(obj,arg1) ,下面是Option的一些函数12345678910111213141516171819202122232425262728293031323334353637383940414243sealed trait Option[+A] &#123; def map[B](f: A =&gt; B): Option[B] = this match &#123; case None =&gt; None case Some(a) =&gt; Some(f(a)) &#125; def getOrElse[B&gt;:A](default: =&gt; B): B = this match &#123; //B&gt;:A表示B类型参数必须是A的父类型 case None =&gt; default case Some(a) =&gt; a &#125; def flatMap[B](f: A =&gt; Option[B]): Option[B] = map(f) getOrElse None /* Of course, we can also implement `flatMap` with explicit pattern matching. */ def flatMap_1[B](f: A =&gt; Option[B]): Option[B] = this match &#123; case None =&gt; None case Some(a) =&gt; f(a) &#125; def orElse[B&gt;:A](ob: =&gt; Option[B]): Option[B] = this map (Some(_)) getOrElse ob /* Again, we can implement this with explicit pattern matching. */ def orElse_1[B&gt;:A](ob: =&gt; Option[B]): Option[B] = this match &#123; case None =&gt; ob case _ =&gt; this &#125; def filter(f: A =&gt; Boolean): Option[A] = this match &#123; case Some(a) if f(a) =&gt; this case _ =&gt; None &#125; /* This can also be defined in terms of `flatMap`. */ def filter_1(f: A =&gt; Boolean): Option[A] = flatMap(a =&gt; if (f(a)) Some(a) else None)&#125; &emsp;上面的代码中有一些新的语法,在getOrElse(还有orElse中的相似标注)中的default:=&gt;B类型表示参数类型是B,但不立即求值,而是函数需要时才求值 基础Option函数的使用场景 &emsp;让我们从map函数开始,map函数可用于转换Option内部的结果(如果存在结果的话),假设没有错误发生,我们可以考虑继续下一个运算,这也是将错误处理延迟到后续代码的一种方式123case class Employee(name: String, department: String)def lookupByName(name: String): Option[Employee] = &#123;//..&#125;val joeDepartment: Option[String] = lookupByName(&quot;Joe&quot;).map(_.department) &emsp;这里lookupByName(“Joe”)返回一个Option[Employee] ,我们为了了解部门信息用map转成Option[String], 注意不需要显示的检测lookupByName(“Joe”)的结果,继续下一步运算,就当做在map的参数内部没有错误发生,如果employeesByName.get(“Joe”)返回None,他会终止后续的运算,这种情况map根本不会调用_.department(因为None map xx, 返回的就是None, 参见Option中的map方法) 12345678lookupByName(&quot;Joe&quot;).map(_.department)//如果Joe是员工,返回Joe的部门,否则返回NonelookupByName(&quot;Joe&quot;).flatMap(_.manager)//如果Joe是一个经理,返回Some(manager),否则返回None(此时Joe是一个员工或者不是经理)lookupByName(&quot;Joe&quot;).map(_.department).getOrElse(&quot;Default Dept.&quot;)//如果Joe的部门存在返回的是部门,否则返回的是缺省的部门 练习 &emsp;根据flatMap实现一个variance(方差)函数,如果一个序列的平均值是m, 那么方差的公式为: 1/n[(x1-m)2+…..(xn-m)2] 1234567891011 def mean(xs: Seq[Double]): Option[Double] = if (xs.isEmpty) None else Some(xs.sum / xs.length) def variance(xs: Seq[Double]): Option[Double] = mean(xs) flatMap (m =&gt; mean(xs.map(x =&gt; math.pow(x - m, 2)))) //第一次mean(xs)是计算序列的平均值m,//xs.map是对序列的每一个元素求math.pow(x - m, 2) ,//第二个mean是平方差之后求均值//不明白为什么要用flatMap,用map不行吗??? &emsp;用flatMap可以对多个阶段构造成一个运算,每个阶段都可能失败,运算中遇到第一个失败便会马上终止,因为None.flatMap(f)会立即返回None,不会再运行f&emsp;如果成功的结果不匹配给定的预判,可以用filter将成功的结果转换成失败的结果,一个通用的模式是调用map,flatMap(或filter)来转换Option,在最后用getOrElse,不会再运行f12345val dept: String = lookupByName(&quot;Joe&quot;). map(_.department). filter(_ != &quot;Accounting&quot;). getOrElse(&quot;Default Dept.&quot;) //用于将Option[String]转成一个String &emsp;orElse与getOrElse相似,不同之处在于如果第一个Option没有定义将返回另一个,当我们需要将可能的失败串联起来的时候,如果第一个失败尝试第二个,这会很有用 &emsp;一个常见的用法是o.getOrElse(throw new Exception(“fail”)) , 将结果是None的Option转回一个异常,一般的经验则是在没有合理的方案能捕获异常时将其抛出,如果异常是一种可恢复的错误,使用Option会更加的灵活&emsp;如你所见,将错误作为一个普通的值返回便于我们使用高阶函数实现与使用异常相似的某种错误逻辑的合并,注意,不必在运算的每个阶段都检测None,可以进行一些转换,等准备好了再检测和处理None,除此之外还获得了一个好处:类型安全,因为Option[A]与A类型是不同的类型,编译器不会忘记显示的让我们推迟处理None的问题(因为最终我们想要得到的是A类型,所以最后,我们肯定要将Option[A]转成A) 3.2.Option的组合,提升及对面向异常的API的包装&emsp;一旦开始使用Option,可能很容易产生一个看法:他对当前整个代码库都会造成影响,想象一下有多少方法的调用者接收或返回Option,不得不修改成处理Some和None的情况,但这并不会发生,因为我们可以把普通函数提升(lift)为一个对Option操作的函数&emsp;举个例子,map函数让我们用一个类型为A=&gt;B的函数对类型为Option[A]的值进行操作,并返回Option[B],从另一个视角可以看做是map把一个A=&gt;B的函数f变成类型为Option[A]=&gt;Option[B]的函数&emsp;也就说任何已经存在的普通函数都可以转换成(通过提升)在一个Option值的上下文里进行操作的函数,举个例子:12val abs0: Option[Double] =&gt; Option[Double] = lift(math.abs)//lift将math.abs转换成为Option[Double] =&gt; Option[Double]类型的abs0,这就是对已经存在的普通函数转换成一个Option值的函数 &emsp;math对象包含abs,sqrt,exp等各种独立的数学函数,我们不需要重写math.abs函数来处理存在可选性(option)的值,只需要将其提升到Option上下文,可以对任何函数都这么干 &emsp;看另外一个例子,假设我们对一个汽车保险公司的网站实现一段逻辑,用户可以在一个页面提交请求一个即时在线报价,我们将从表单里解析信息,最后调用换算函数123456/*age:车龄numberOfSpeedingTickets:超速罚款单号码功能:根据两个关键因子计算每年汽车保险费的绝密配方*/def insurnaceRateQuote(age: Int, numberOfSpeedingTickets: Int): Double &emsp;我们希望能够调用刚刚定义的这个函数,不过用户从页面表单上锁提交的年龄和超速罚款单号码都是字符串,需要将他们解析为整数,而解析可能会失败,对于给定的字符串s,可以用s.toInt解析为整数,如果他不是一个有效的整数将抛出NumberFormatException&emsp;让我们把这个基于异常的API转换成Option,看看是否可以实现一个parseInsuranceRateQuote函数,它以字符串方式接受年龄和超速罚款号码,如果解析成功在试图调用insuranceRateQuote函数12345678def parseInsuranceRateQuote(age: String, numberOfSpeedingTickets: String): Option[Double] = &#123; val optAge: Option[Int] = Try(age.toInt) val optTickets: Option[Int] = Try(numberOfSpeedingTickets.toInt) insurnaceRateQuote(optAge, optTickets) //编译不通过,因为参数类型不匹配&#125;def Try[A](a: =&gt;A):Option[A] = try Some(a)//接收一个非严格求值的A参数,我们可以在对a求值时捕获任何异常并转换为None catch &#123;case e: Exception =&gt; None &#125;//注意:这会丢弃掉错误信息,我们会在本章的后面改进 &emsp;Try函数是一个通用目的的函数,用于将一个基于异常的API转成为一个面向Option的API,参数a的类型=&gt;表示他是一个非严格(或惰性)求值参数,在第5章有讲到&emsp;但是这里有个问题—-我们在解析optAge和optTickets为Option[Int]之后,如何调用接收两个Int参数的insurnaceRateQuote函数?需要把insurnaceRateQuote重写为接收Option[Int]吗?不,修改insurnaceRateQuote会让关注点混乱,会强迫他意识到之前的运算可能会失败,更不必说我们没法去修改——他可能定义在一个我们无法修改的独立模块里,我们更偏向提升insurnaceRateQuote为对两个存在可选值的上下文进行操作的函数 练习 &emsp;写一个泛型函数map2,使用一个二元函数来组合两个Option值,如果两个Option都为None,也返回None,签名如下12345 def map2[A,B,C](a: Option[A], b: Option[B])(f: (A, B) =&gt; C): Option[C] = a flatMap (aa =&gt; b map (bb =&gt; f(aa, bb)))//aa = this.get 参看下面的Option中的flatMap//bb = this.get 参看下面的Option中的map //所以aa,bb就是Some(x) 中的x,所以f(aa,bb)此时是成立的 Option中的flatMap12@inline final def flatMap[B](f: A =&gt; Option[B]): Option[B] = if (isEmpty) None else f(this.get) Option中的map12@inline final def map[B](f: A =&gt; B): Option[B] = if (isEmpty) None else Some(f(this.get)) &emsp;现在可以用map2来实现parseInsuranceRateQuote12345def parseInsuranceRateQuote(age: String, numberOfSpeedingTickets: String): Option[Double] = &#123; val optAge: Option[Int] = Try(age.toInt) val optTickets: Option[Int] = Try(numberOfSpeedingTickets.toInt) map2(optAge, optTickets)(insurnaceRateQuote)&#125; &emsp;有了map2函数,意味着我们不需要修改任何已存在函数,让他们感知Option,可以提升他们操作Option上下文,同理,你可以定义map3,map4,map5 练习: &emsp;写一个sequence函数,将一个Option列表结合为一个Option,这个结果Option包含原Option列表中的所有元素值,如果原Option列表中出现一个None,函数结果也应该返回None,否则结果应该返回所有(使用Some包装的)元素值的列表,签名如下: 123456789 def sequence[A](a: List[Option[A]]): Option[List[A]] = a match &#123; case Nil =&gt; Some(Nil) case h :: t =&gt; h flatMap (hh =&gt; sequence(t) map (hh :: _)) &#125;/*sequence(t) 返回的是一个Option[List[A]],那么map遍历的时候取到的是List[A],也就是_ ,所以hh::_是返回的组合的List*/ 4.Either数据类型&emsp;Option不会告诉我们在异常条件下发生了什么错误,他只是给我们一个None,表示没有可用的值,但有时我们想要知道更多,比如想要一个字符串给出更多消息或者当异常发生时想知道实际错误是什么&emsp;可以创建一个数据类型,能对我们想要的失败信息进行编码,有些时候只需要知道是否发生了失败,这种情况可以使用Option,而有些时候则想知道更多信息,这一节我们将示范一个对Option的简单扩展:Either数据类型,他可以跟踪失败原因,看一下他的定义:123sealed trait Either[+E, +A]case class Left[+E](value: E) extends Either[E, Nothing]case class Right[A](value: A) extends Either[Nothing, A] &emsp;就像Option,Either只有两种情况,他们的实质区别是Either的两种情况都有值,当我们用来表示成功或失败时,习惯上Right构造器用于表示成功(right有双关的意思,既表示右边,又表示正确), Left构造器用于表示失败,建议给Left的类型参数命名为E(代表错误) &emsp;再看一下mean函数的例子,这次在失败的情况下返回一个字符串12345def mean(xs: IndexedSeq[Double]): Either[String, Double] = if (xs.isEmpty) Left(&quot;mean of empty list&quot;) else Right(xs.sum / xs.length) &emsp;有时我们想要对错误包含更多的信息,比如堆栈调用信息,以便在源码中定位错误,这种情况下也可以对Either的Left返回一个异常123def safeDiv(x: Int, y: Int): Either[Exception, Int] = try Right(x/y) catch &#123;case e: Exception =&gt; Left(e)&#125; 练习: &emsp;实现Either版本的map,flatMap,orElse和map2函数1234567891011121314151617181920sealed trait Either[+E,+A] &#123; def map[B](f: A =&gt; B): Either[E, B] = this match &#123; case Right(a) =&gt; Right(f(a)) case Left(e) =&gt; Left(e) &#125; def flatMap[EE &gt;: E, B](f: A =&gt; Either[EE, B]): Either[EE, B] = this match &#123; case Left(e) =&gt; Left(e) case Right(a) =&gt; f(a) &#125; def orElse[EE &gt;: E, AA &gt;: A](b: =&gt; Either[EE, AA]): Either[EE, AA] = this match &#123; case Left(_) =&gt; b case Right(a) =&gt; Right(a) &#125; def map2[EE &gt;: E, B, C](b: Either[EE, B])(f: (A, B) =&gt; C): Either[EE, C] = for &#123; a &lt;- this; b1 &lt;- b &#125; yield f(a,b1)&#125; 练习 &emsp;对Either实现sequence和traverse,如果遇到错误返回第一个错误12345678910 def traverse[E,A,B](es: List[A])(f: A =&gt; Either[E, B]): Either[E, List[B]] = es match &#123; case Nil =&gt; Right(Nil) case h::t =&gt; (f(h) map2 traverse(t)(f))(_ :: _)//map2是取两个参数,然后应用到函数(_::_)中//这里是B和List[B] ,所以是B::List[B] &#125; def sequence[E,A](es: List[Either[E,A]]): Either[E,List[A]] = traverse(es)(x =&gt; x) 练习 &emsp;最后一个例子,这儿有一个map2的应用,用于mkPerson函数在构造一个有效的Person之前校验name和age1234567891011def mkName(name: String,): Either[String, Name] = if (name==&quot;&quot;||name==null) Left(&quot;Name is empty&quot;) else Right(new Name(name))def mkAge(age: Int): Either[String, Age] = if (age&lt;0) Left(&quot;Age is out of range &quot;) else Right(new Age(age))def mkPerson(name: String, age: Int): Either[String, Person] = mkName(name).map2(mkAge(age))(Person(_, _))//map2就是将Name和Age传给Person(_, _)","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala函数式编程","slug":"scala函数式编程","permalink":"http://yoursite.com/tags/scala函数式编程/"}]},{"title":"第五章 严格求值和惰性求值","date":"2017-04-16T04:47:25.377Z","path":"2017/04/16/functionalProgrammingInScala/第五章 严格求值和惰性求值/","text":"&emsp;手动追踪下面的程序是如何求值的?12345List(1,2,3,4).map(_ + 10).filter(_%2 == 0).map(_ * 3)List(11,12,13,14).filter(_%2 == 0).map(_ * 3)List(12,14).map(_ * 3)List(36, 42) &emsp;从上面可以看出在进行map,filter,map的过程中,会有中间结果产生(如:List(11,12,13,14), List(12,14)) ,那么我们如何避免产生临时数据结构(中间结果)呢?我们可以使用非严格求值来实现这种自动循环的融合物,非严格求值又叫惰性求值 1.严格和非严格函数 非严格求值的定义 &emsp;非严格求值是函数的一种属性,称一个函数是非严格求值的意思是这个函数可以选择不对他的一个或多个参数求值,相反,严格求值总会对他的参数求值,严格求值函数在大多数编程语言中是一种常态,并且大部分编程语言也只支持严格求值,在scala中除非明确声明,否则任何函数都是严格求值的&emsp;当你调用square(41.0+1.0) 时,square函数将接收到完成求值的42.0作为参数,因为他是严格求值的,如果你调用square(sys.error(“fail)),将在square方法执行之前得到一个异常,因为sys.error(“fail)会先被求值 scala中非严格求值的例子 &emsp;在很多语言包括scala语言中都会出现的短路Boolean函数&amp;&amp;和||,就是非严格求值的例子,&amp;&amp;接收2个Boolean参数,但只有第一个参数是true的情况下,才对第二个参数求值1false &amp;&amp; &#123;println(&quot;!!&quot;);true&#125; //不打印任何内容 &emsp;||函数只有在第一个参数为false的情况下才对第二个参数求值&emsp;另一个非严格求值的例子是scala中的if控制结构:1val result = if(input.isEmpty) sys.error(&quot;empty input&quot;) else input &emsp;尽管if是scala的内置结构,也可以看做是一个接收3个参数的函数,一个Boolean类型的条件参数,一个返回类型为A的表达式在条件为true时执行,另一个返回类型也为A的表达式在条件为false时执行,这个if函数也是非严格求值的,因为他不会对所有的参数都求值,更精确的说,if函数对条件参数是严格求值的,因为他总是要对条件判断来决定选择哪个分支,对两个true和false的分支参数是非严格求值的,因为他只对其中满足条件的一个参数求值 非严格求值语法的由来 &emsp;在scala中我们可以通过接收某个未求值的参数来写非严格求值函数,先展示这种方式,然后再展示更好的scala内置的语法,下面是一个非严格求值的if函数:123456def if2[A](cond: Boolean, onTrue: ()=&gt;A, onFalse: ()=&gt;A): A = if (cond) onTrue() else onFalse()if2(a&lt;22, ()=&gt;println(&quot;a&quot;), //函数字面量语法,创建一个()=&gt;A的函数 ()=&gt;println(&quot;b&quot;) &emsp;()=&gt;A类型的值表示一个函数,接收0个参数并返回一个A类型,实际上,()=&gt;A类型是Function0[A]类型的语法别名,通常一个表达式的未求值形式称为thunk,我们可以强制对thunk求值得到结果,通过传入一个空参数列表调用函数,如onTrue()或onFalse(),同样if2的调用者必须显式的创建thunk,语法与之前我们见过的函数字面量相同 &emsp;总体来说,这种语法也很清晰—-我们在每一个非严格求值参数的地方传入一个无参函数,然后在方法体里显式的调用这个函数获取结果,但这是一个非常常见的例子,所以scala提供了更好的语法12def if2[A](cond: Boolean, onTrue: =&gt;A, onFalse: =&gt;A): A = if(cond) onTrue else onFalse &emsp;我们传递的未求值的参数有一个箭头=&gt;紧接在类型前边,在方法体中我们不需要对使用=&gt;标注的参数做任何事情,就像往常一样只引用标识符也不需要对这个函数做任何特殊调用,只需要使用正常的函数调用语法,scala会负责为我们将表达式包装为thunk1if2(false, sys.error(&quot;fail&quot;), 3) &emsp;使用上述两种语法中的任何一种,给一个函数传递一个未求值的参数,在方法体中引用的地方会被求值一次,即:scala不会(默认)缓存一个参数的求值结果:12345678def maybeTwice(b: Boolean, i: =&gt; Int) = if (b) i+i else 0val x = maybeTwice(true, &#123;println(&quot;hi&quot;); 1+41&#125;)//打印结果hihix:Int = 84 &emsp;这里i在maybeTwice方法体中引用了两次,也就是说i没有在第一次引用之后,没有被缓存,在第二次引用时,又去计算了一遍&emsp;如果我们希望只求值一次,可以通过lazy关键字显示的缓存这个值12345678910def maybeTwice2(b: Boolean, i: =&gt; Int) = &#123; lazy val j = i if (b) j+j else 0&#125;val x = maybeTwice(true, &#123;println(&quot;hi&quot;); 1+41&#125;)//打印结果hix:Int = 84 &emsp;对一个val声明的变量添加修饰符,将导致scala延迟对这个变量求值,直到他第一个被引用,他也会缓存结果,在后续引用的地方不会触发重复求值&emsp;最后一个术语:scala中非严格求值的函数接收的参数是传名参数,而非传值参数 2.一个扩展例子:惰性列表&emsp;我们将使用惰性列表或Stream作为一个例子探索惰性化在函数式编程中是如何用于提升效率和模块化的,我们将看到基于流(Stream)的转换链怎么通过使用惰性化融合成一次操作,这里是一个简单的Stream的定义1234567891011121314151617sealed trait Stream[+A]case object Empty extends Stream[Nothing]case class Cons[+A](h: ()=&gt;A, t: ()=&gt; Stream[A]) extends Stream[A]//一个非空的Stream由head和tail组成,head和tail都是非严格求值的,因为技术限制,这些参数都是必须明确强制求值的thunk,而非传名参数object Stream&#123; def cons[A](hd: =&gt;A, tl: =&gt; Stream[A]): Stream[A] = &#123; //用于创建空Stream的智能构造器 lazy val head = hd //对惰性列表求值的head和tail做缓存,避免重复及求值 lazy val tail = tl Cons(()=&gt;head, ()=&gt;tail) &#125; def empty[A]: Stream[A] = Empty def apply[A](as: A*): Stream[A] = if(as.isEmpty) empty else cons(as.head, apply(as.tail:_*))&#125; &emsp;下面是一个以可选的方式抽取Stream的head的函数1234def headOption: Option[A] = this match&#123; case Empty =&gt; None case Cons(h,t) =&gt; Some(h()) //对h,thunk显示的调用h()强制求值&#125; &emsp;注意,我们必须显式的调用h()来对h强制求值,不这样做代码会跟List一样,但Stream直到这部分真正需要才求值( 我们不会对Cons的tail求值)的能力很有用 2.1.对stream保持记忆,避免重复运算&emsp;我们希望能缓存Cons节点的值,一旦他们被强制求值,如果我们直接用Cons数据构造器,比如下面的代码,实际运算了2次expensive(x)123val x = Cons(()=&gt;expensive(x), t1)val h1 = x.headOptionval h2 = x.headOption &emsp;我们通常定义更智能的构造器来避免这个问题,他是一种构造器类型的函数,同时能够保证一些附加的不变式或提供与真正构造签名稍微不同的用于模式匹配的能力,智能构造器的写法习惯上跟普通的数据构造器相似,但首字母为小写,这里智能的cons构造器负责将传名参数记录到head和tail,这是一个常见的技巧,他确保thunk只运行一次,只在第一次使用时被强制求值,后续的调用会返回已经缓存的lazy val12345def cons[A](hd: =&gt;A, tl: =&gt;Stream[A]): Stream[A] = &#123; lazy val head = hd lazy val tail = tl Cons(()=&gt;head, ()=&gt;tail)&#125; &emsp;空的智能构造器仅仅返回空(Empty),但Empty被标识为Stream[A]在某些情况下是一个更好的类型接口,我们看一下智能构造器在Stream.apply函数中是如何被使用的:1234def apply[A](as: A*): Stream[A] = &#123; if (as.isEmpty) empty else cons(as.head, apply(as.tail:_*))&#125; &emsp;此外,scala负责包装thunk中传给cons的参数,所以as.head和apply(as.tail:_*)表达式不会求值,知道Stream中被强制求值 2.2.用于检测Stream的helper函数&emsp;练习:写一个可将Stream转换成List的函数,他会被强制求值 12345678def toList: List[A] = &#123; @annotation.tailrec def go(s: Stream[A], acc: List[A]): List[A] = s match &#123; case Cons(h,t) =&gt; go(t(), h() :: acc) case _ =&gt; acc &#125; go(this, List()).reverse&#125; &amp;emsp:练习:写一个函数take(n)返回Stream中的前n个元素,写一个函数drop(n)返回Stream中第n个元素之后的所有元素123456789101112def take(n: Int): Stream[A] = this match &#123; case Cons(h, t) if n &gt; 1 =&gt; cons(h(), t().take(n - 1)) case Cons(h, _) if n == 1 =&gt; cons(h(), empty) case _ =&gt; empty&#125;@annotation.tailrecfinal def drop(n: Int): Stream[A] = this match &#123; case Cons(_, t) if n &gt; 0 =&gt; t().drop(n - 1) case _ =&gt; this&#125; &emsp;写一个函数takeWhile返回Stream中从起始元素连续满足给定断言的所有元素1234def takeWhile(f: A =&gt; Boolean): Stream[A] = this match &#123; case Cons(h,t) if f(h()) =&gt; cons(h(), t() takeWhile f) case _ =&gt; empty&#125; 3.把函数的描述与求值分离&emsp;函数式编程的主题之一是关注分离,我们希望将计算的描述与实际运行分离,在前几章我们已经以不同的方式接触过这一主题了,比如一等函数,捕获函数体内部的运算逻辑,只有在接收到参数时才执行他,我们使用Option捕获实际发生的错误,而决定对他做什么是一个分离的关注点,对Stream,可以构建一个产生一系列元素的计算逻辑知道实际需要这些元素时才运行&emsp;一般而言,惰性化让我们对一个表达式分离了他的描述和求值,这给我们带来一种强大的能力—–可以选择描述一个比我们所需要的更大的表达式,只对这个表达式的一部分求值,用一个例子来看,函数exists检查Stream中是否存在匹配一个Boolean函数1234def exists(p: A=&gt;Boolean): Boolean = this match &#123; case Cons(h, t) =&gt; p(h() || t().exists(p) case _ =&gt; false&#125; &emsp;注意||对他的第二个参数是非严格求值,如果p(h())返回true,那么exists提前终止遍历也返回true,另外,Stream的tail部分也是一个lazy val,不仅仅给是提前终止,Stream的tail压根就没有被求值,所以不管tail里是什么代码,他根本没执行 &emsp;exists函数使用显示的递归实现,回顾一下第3章中的List我们可以以foldRight方式实现一个更通用的递归,可以以惰性的方式对Stream做相同的事情:1234def foldRight[B](z: =&gt;B)(f: (A,=&gt;B) =&gt;B): B = this match &#123; //f参数的类型声明中B类型前箭头=&gt;表示第二个参数是传名参数,f不会对他进行求值 case Cons(h,t) =&gt; f(h(), t().foldRight(z)(f)) //如果f不对第二个参数求值,递归就不会发生 case _ =&gt; z&#125; &emsp;这看上去与List中的foldRight非常相似,但注意我们的组合函数f对他的第二个参数是非严格求值的,如果f选择不对第二个参数求值,这会提前终止遍历,我们看一下使用foldRight实现的exists:1def exists(p: A=&gt;Boolean): Boolean = foldRight(false)((a,b) =&gt; p(a) || b) &emsp;这里b是对Stream的tail进行fold的未求值的递归步骤,如果p(a)返回true,b不会被求值,计算提前终止 练习: &emsp;实现一个forAll函数,检查Stream中所有元素是否与给定的断言匹配,遇到不匹配的值应该立即终止遍历12def forAll(f: A =&gt; Boolean): Boolean = foldRight(true)((a,b) =&gt; f(a) &amp;&amp; b) //调用了前面的foldRight &emsp;使用foldRight实现takeWhile1234def takeWhile_1(f: A =&gt; Boolean): Stream[A] = foldRight(empty[A])((h,t) =&gt; if (f(h)) cons(h,t) else empty) &emsp;使用foldRight实现headOption12def headOption: Option[A] = foldRight(None: Option[A])((h,_) =&gt; Some(h)) &emsp;使用foldRight实现map, filter, append和flatMap,其中append方法参数应该是非严格求值的12345678910111213def map[B](f: A =&gt; B): Stream[B] = foldRight(empty[B])((h,t) =&gt; cons(f(h), t))def filter(f: A =&gt; Boolean): Stream[A] = foldRight(empty[A])((h,t) =&gt; if (f(h)) cons(h, t) else t)def append[B&gt;:A](s: =&gt; Stream[B]): Stream[B] = foldRight(s)((h,t) =&gt; cons(h,t))def flatMap[B](f: A =&gt; Stream[B]): Stream[B] = foldRight(empty[B])((h,t) =&gt; f(h) append t) 跟踪Stream程序 &emsp;让我们看一个简单的程序,跟踪一段本章开头用于启发的例子:Stream(1,2,3,4),map(+10).filter( % 2 ==0) ,我们将转换这个表达式为一个List来强制求值,花几分钟跟踪并理解发生了什么,如下:12345678910Stream(1,2,3,4).map(_ + 10).filter(_ % 2==0).toListcons(11, Stream(2,3,4).map(_ + 10)).filter(_ % 2 ==0).toList //对第1个元素应用mapStream(2,3,4).map(_ + 10).filter(_ % 2==0).toList //对第1个元素应用filtercons(12, Stream(3,4).map(_ + 10)).filter(_ % 2==0).toList //对第2个元素应用map12::Stream(3,4).map(_ + 10).filter(_ % 2 ==0).toList //对第2个元素应用filter生成第1个元素结果12::cons(13, Stream(4).map(_ + 10).filter(_ % 2==0).toList12::Stream(4).map(_ + 10).filter(_ % 2==0).toList //将13%==0过滤掉了12::cons(14, Stream(),map(_ + 10)).filter(_ % 2==0).toList12::14::Stream().map(_ + 10).filter(_ % 2==0).toList12:14:List() //map和filter没有更多要做的了,空Stream变成了空List &emsp;跟踪过程中要注意的是filter和map转换时如何交错的—-计算在用于生成单个元素的map函数与用于测试元素是否被2整除的filter函数之间交替进行,注意我们不会完全实例化map运算结果的中间Stream,就跟我们使用一个专用的循环处理交错逻辑一样,因为这个原因,人们有时也把Stream描述为”一等函数”, 这些逻辑可以使用高阶函数,如map,filter等组合&emsp;既然中间Stream不会实例化,很容易以全新的方式复用已存在的组合子,而不用担心我们对Stream处理超出需要的部分,举个例子,可以复用filter来定义find,一个方法返回第一个匹配的元素,如果存在的话,虽然filter转换了整个stream,但转换时惰性的,所以find一遇到匹配的元素就立刻终止:12def find(p: A=&gt; Boolean): Option[A] = filter(p).headOption &emsp;stream转换的增量性质对内存使用也有重要的影响,因为不再生成中间Stream,转换只需要处理当前元素所够用的内存,比如,Stream(1,2,3,4).map( + 10).filter( % 2 ==0) 这个转换中垃圾收集会回收分配给由map产生的值11和13的内存空间 4.无限流与共递归&emsp;我们所写的函数也适用于无限流,下面是一个由1组成的无限流的例子123456val ones: Stream[Int] = Stream.cons(1, ones) //指向自身的//使用ones.take(3).toList //List(1,1,1)ones.exists(_ % 2 !=0) //true &emsp;再来几个例子123ones.map(_ + 1).exists(_ % 2 ==0)ones.takeWhile(_ == 1)ones.forAll(_ != 1) &emsp;在每个例子里,我们立即取得一个结果,不过要小心,因为很容易写出永不结束或线程栈不安全的表达式,比如ones.forAll(_ == 1)将一直检测后续的元素,因为不会遇到一个可以让他终止遍历并给出确切答案的元素(最终会导致堆栈溢出而非无限循环) 练习 &emsp;对ones稍微泛化一下,定义一个constant函数,根据给定值返回一个无限流1234def constant[A](a: A): Stream[A] = &#123; lazy val tail: Stream[A] = Cons(() =&gt; a, () =&gt; tail) //后面又指向了tail变量,所以是无限流 tail&#125; &emsp;写一个函数生成一个整数无限流,从n开始,然后n+1,n+2,等等12def from(n: Int): Stream[Int] = cons(n, from(n+1)) //tail又是指向的是from,不过此时传递过去的是n+1,所以无限循环的 &emsp;写一个fibs函数生成斐波那契数列的无限流: 0, 1, 1, 2, 3, 5, 8等等12345val fibs = &#123; def go(f0: Int, f1: Int): Stream[Int] = cons(f0, go(f1, f0+f1)) go(0, 1)&#125; &emsp;写一个更加通用的构造流的函数unfold,他接收一个初始化状态,以及一个在生成的stream中用于产生下一状态和下一个值的函数12345def unfold[A, S](z: S)(f: S =&gt; Option[(A, S)]): Stream[A] = f(z) match &#123; //用于判断还有没有下一个状态 case Some((h,s)) =&gt; cons(h, unfold(s)(f)) case None =&gt; empty &#125; &emsp;unfold函数是一个被称作共递归函数的例子,递归函数由不断的对更小范围的输入参数进行递归调用而结束,共递归函数只要保持生产数据不需要结束,这意味着我们总是可以在一个有限的时间段里对更多的结果求值,unfold函数生产能力随f函数的结束而结束,因为我们只需要再次运行f函数生成stream中的下一个元素,共递归有时也被称为守护递归,生产能力有时也被称为共结束 练习 &emsp;根据unfold函数来写fibs,from, constant和ones12345678val fibsViaUnfold = unfold((0,1)) &#123; case (f0,f1) =&gt; Some((f0,(f1,f0+f1))) &#125;def fromViaUnfold(n: Int) = unfold(n)(n =&gt; Some((n,n+1)))def constantViaUnfold[A](a: A) = unfold(a)(_ =&gt; Some((a,a))) &emsp;使用unfold实现map, take, takeWhile, zipWith以及zipAll,zipAll函数应该继续遍历只要Stream还有更多元素—-它使用Option表示Stream是否已经彻底遍历完了12345678910111213141516171819202122232425def mapViaUnfold[B](f: A =&gt; B): Stream[B] = unfold(this) &#123; case Cons(h,t) =&gt; Some((f(h()), t())) case _ =&gt; None &#125;def takeViaUnfold(n: Int): Stream[A] = unfold((this,n)) &#123; case (Cons(h,t), 1) =&gt; Some((h(), (empty, 0))) case (Cons(h,t), n) if n &gt; 1 =&gt; Some((h(), (t(), n-1))) case _ =&gt; None &#125;def takeWhileViaUnfold(f: A =&gt; Boolean): Stream[A] = unfold(this) &#123; case Cons(h,t) if f(h()) =&gt; Some((h(), t())) case _ =&gt; None &#125;def zipWith[B,C](s2: Stream[B])(f: (A,B) =&gt; C): Stream[C] = unfold((this, s2)) &#123; case (Cons(h1,t1), Cons(h2,t2)) =&gt; Some((f(h1(), h2()), (t1(), t2()))) case _ =&gt; None &#125; &emsp;使用已写过的函数实现startWith函数,他检查一个Stream是否是另一个Stream的前缀,比如Stream(1,2,3) startWith Stream(1,2) 返回true1234def startsWith[A](s: Stream[A]): Boolean = zipAll(s).takeWhile(!_._2.isEmpty) forAll &#123; case (h,h2) =&gt; h == h2 &#125; &emsp;使用unfold实现tails函数,对一个给定的Stream,tails返回这个Stream输入序列的所有后缀,比如给定Stream(1,2,3)返回Stream(Stream(1,2,3), Stream(2, 3), Stream(3), Stream())12345def tails: Stream[Stream[A]] = unfold(this) &#123; case Empty =&gt; None case s =&gt; Some((s, s drop 1)) &#125; append Stream(empty) &emsp;现在可以用之前写过的函数来实现hasSubsequence12def hasSubsequence[A](s: Stream[A]): Boolean = tails exists (_ startsWith s)","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala函数式编程","slug":"scala函数式编程","permalink":"http://yoursite.com/tags/scala函数式编程/"}]},{"title":"第二章 在scala中使用函数式编程","date":"2017-04-16T04:47:25.376Z","path":"2017/04/16/functionalProgrammingInScala/第二章 在scala中使用函数式编程/","text":"1.scala语言介绍:一个例子1234567891011121314object MyModule&#123;//声明一个单例对象,即:同时声明一个类和他的唯一实例 def abs(n: Int): Int = if (n&lt;0) -n else n private def formatAbs(x: Int) = &#123; val msg = &quot;The absolute value of %d is %d&quot; //%d表示数字 msg.format(x, abs(x)) &#125; def main(args: Array[String]): Unit = &#123; println(formatAbs(-42)) &#125;&#125; object关键字 &emsp;object关键字创建一个新的单例类型,就像一个class只有一个被命名的实例,如果你熟悉Java,在Scala中声明一个object有些像创建了一个匿名类的实例,scala没有等同于Java中static关键字的概念,object在scala中经常用到,就像你在Java中用一个带有静态成员的class一样 abs方法 &emsp;abs方法是一个接收整型类型参数并返回其绝对值的纯函数123def abs(n: Int): Int = if (n&lt;0) -n else n &emsp;在def关键字之后紧跟着是方法名,然后是小括号,里面是方法参数,在这个例子中,abs方法只有一个Int类型的单个参数,在参数列表闭合括号之后是一个可选的类型(:Int) , 他表示返回值是Int类型(前面的冒号表示存在某种类型)&emsp;在等号(=)之后的部分称之为方法体,有时我们将等号前面的部分称之为”左右边的”或”签名”,等号后面的部分称之为”右手边的”或”定义”,注意一般不使用return关键字,一个方法的返回值就是右手边的求值结果 formatAbs方法 1234private def formatAbs(x: Int) = &#123; val msg = &quot;The absolute value of %d is %d&quot; //%d表示数字 msg.format(x, abs(x))&#125; &emsp;这个方法定义为private,意味着无法再MyModule对象之外调用,他接收一个Int类型,返回一个String,注意返回值类型没有声明,因为scala有能力判断一个方法的返回值类型,所以我们省略了,但是为了保持良好的代码风格,建议显示的声明返回值类型&emsp;记住,一个方法只是简单的返回他右手边的值,所以不需要return关键字,如果右手边是一个代码块,那么返回的是代码块中最后一个表达式的值,此例中返回的是msg.format(x,abs(x))的返回值 main方法 123def main(args: Array[String]): Unit = &#123; println(formatAbs(-42))&#125; &emsp;以main命名的方法是一个特殊方法,当程序运行时scala会查找以main命名的特定签名的方法,他接收一个字符串数组作为参数,返回值类型为Unit,main方法的返回值没有任何意义,所以他是一个特殊的Unit类型,该类型只有唯一的值,文法上写为(),一对小括号,通常返回Unit类型的方法暗示他包含副作用(因为通常的纯函数是一个输入对应有一个输出),在此例中,println方法的返回值是Unit,也正是main方法需要返回值的类型 2.运行程序&emsp;运行scala程序的最简单的方式是从命令行直接调用scala编译器,先把代码放到一个名为MyModule.scala之类的文件里,然后使用scalac编程成java字节码文件1scalac MyModule.scala &emsp;这将生成一些以.class后缀结尾的文件,这些文件包含可运行在java虚拟机上的编译过的代码,该代码可以使用scala命令行工具来执行1scala MyModule &emsp;实际上scala代码并不需要先通过scalac编译,像之前写的简单程序可以直接通过命令行传递给scala解析器来运行:1scala MyModule.scala 3.模块,对象和命名空间&emsp;scala中的每一个值都可以当成一个对象,每个对象都有零个或多个成员,对象的主要目的是给成员一个命名空间,有时我们也称为模块,一个成员可以是以def地鞥一的方法,或以val或object声明的对象&emsp;访问对象中的成员使用”.”符号,也就是一个命名空间后面跟着一个点,在后面跟着成员的名字,如MyModule.abs(-42)&emsp;注意:即使2+1这样的表达式也是调用对象成员,这里是对象2调用其+方法成员,他是表达式2.+(1)的语法糖,把1传给对象2的+方法,scala中没有操作符的概念,+在scala中是一个方法,若方法只是包含一个参数,可以使用中缀方式来调用,即省略点和小括号,比如:MyModule.abs(-42) 可以写成为 MyModule abs -42,结果是一样的&emsp;可以将一个对象的成员导入到当前作用域,这样就可以不受约束的使用它们了:12345scala&gt; import MyModule.absimport MyModule.absscala&gt; abs(-42)res0: Int = 42 &emsp;也可以使用下划线来导入一个对象的所有成员(非私有)1scala&gt; import MyModule._ 4.高阶函数:把函数传给函数&emsp;你需要了解的第一个新的概念:函数也是值,就像其他类型的值,比如整型,字符串,列表;函数也可以赋值给另一个变量,存储在一个数据结构里,像参数一样传递给另一个函数,把一个函数当做参数传递给另一个函数在纯函数式编程离很有用,他被称之为高阶函数 4.1.迂回做法:使用循环方式&emsp;首先我们来写一个阶乘1234567def factorial(n: Int): Int = &#123; def go(n: Int, acc: Int): Int = //一个内部函数或一个局部定义函数,在scala中吧一个函数定义在另一个函数体内很常见,在函数式编程中,认为他跟局部整数或局部的字符串没有什么不同 if (n&lt;=0) acc else go(n-1, n*acc) go(n, 1)&#125; &emsp;scala会检测到上述的尾递归现象,只要递归发生在尾部,编译器优化成类似while循环的字节码,这种尾递归在每次迭代时不消耗栈帧的调用消耗(栈调用再输入很大的情况下可能会StackOverflowError) scala中的尾递归 &emsp;我们所说的尾递归是指调用者在一个递归调用后不做其他的事情,只是返回这个调用结果,比如之前的递归调用go(n-1, nacc) ,他是一个尾递归,因为他没有做其他事情,直接返回了这个递归调用的结果,另一种情况是:1+go(n-1, accn) ,这里的go不是尾递归,因为这个方法的结果还要参与其他的运算(即结果还要再与1相加)&emsp;如果递归调用时一个函数的尾递归,scala会自动编译为循环迭代,这样不会每次都进行栈的操作,默认情况下,scala不会告诉你尾递归是否消除成功,可以通过tailrec注释来告诉编译器,如果编译器不会消除尾部调用会给出编译错误,语法如下:12345678def factorial(n: Int): Int = &#123; @annotation.tailrec def go(n: Int, acc: Int): Int = if (n&lt;=0) acc else go(n-1, n*acc) go(n, 1)&#125; 4.2.第一个高阶函数&emsp;下面是引入了阶乘函数1234567891011121314151617object MyModule&#123;//声明一个单例对象,即:同时声明一个类和他的唯一实例 //..省略这里的abs和factorial定义 private def formatAbs(x: Int) = &#123; val msg = &quot;The absolute value of %d is %d&quot; //%d表示数字 msg.format(x, abs(x)) &#125; private def formatFactorial(x: Int) = &#123; val msg = &quot;The factorial of %d is %d&quot; //%d表示数字 msg.format(x, factorial(x) &#125; def main(args: Array[String]): Unit = &#123; println(formatAbs(-42)) println(formatFactorial(7)) &#125;&#125; &emsp;formatAbs和formatFactorial这两个函数几乎是相同的,可以将他们泛化为一个formatResult函数,他接收一个函数参数1234def formatResult(name: String, n: Int, f: Int=&gt;Int): Unit =&#123; val msg = &quot;The %s of %d is %d.&quot; msg.format(name, n, f(n))&#125; &emsp;formatResult是一个高阶函数,他接收一个函数f作为参数,我们给f参数声明一个类型,就像其他参数那样,他的类型是Int=&gt;Int,表示f接收一个整数参数并返回一个整型结果 变量命名约定 &emsp;对于高阶函数,以f,g或h来命名是一种习惯做法,在函数式编程中,我们倾向于使用短的变量名,甚至单个字母命名,因为高阶函数的参数通常没法表示参数到底执行什么,无法体现他们的含义(所以就没有必要体现),许多函数式程序员觉得短名称让代码更易读,因为代码结构第一眼看上去更简单 5.多态函数:基于类型的抽象&emsp;目前我们定义的函数都是单态的:函数只操作一种数据类型,比如abs和factorial的指定参数类型是Int,高阶函数formatResult也是固定的操作Int=&gt;Int类型的参数,通常,特别是在写高阶函数时,希望写出的这段代码能够使用于任何类型,他们被称为”多态”,这儿的多态与面向对象中的多态稍有差别,面向对象的多态通常意味着某种形式的子类型或继承类型,这个例子中没有接口或子类型,这里所用的多态形式有时也称为参数化多态 5.1.一个多态函数的例子&emsp;下面的单态函数findFirst返回数组里第一个匹配到key的索引,或在匹配不到的情况下返回-1,如下是从一个字符串数组中查找一个字符串的特例123456789def findFirst(ss: Array[String], key: String): Int = &#123; @annotation.tailrec def loop(n: Int): Int = if (n&gt;=ss.length) -1 //如果n到了数组的结尾,返回-1,表示这个key在数组里不存在 else if(ss(n)==key) n //ss(n)抽取数组ss里的第n个元素,如果第n个元素等于key返回n,表示这个元素出现在数组的索引 else loop(n+1) loop(0) //从数组的第一个元素开始启动loop&#125; &emsp;上诉的代码不是我们关注的细节,重要的是不管是从Array[String]中查找一个String,还是从Array[Int]中查找一个Int,或从任何Array[A]中查找一个A,他们看起来几乎都是相同的,我们可以写一个更泛化的适用任何类型A的findFirst函数,他接收一个函数参数,用来对A进行判定123456789def findFirst[A](as: Array[A], p: A=&gt;Boolean): Int = &#123; @annotation.tailrec def loop(n: Int): Int = if (n&gt;=as.length) -1 else if(p(as(n))) n else loop(n+1) loop(0) &#125; &emsp;在参数列表中引入的类型变量,可以在其他类型签名中引用(类似于参数列表中的参数变量可在函数体中引用),在findFirst函数中类型变量A被两个地方引用:一处是数组元素要求是类型A(声明为Array[A]) , 另一处是函数p必须接受类型A(声明为A=&gt;Boolean),这两处类型签名中引用相同的类型变量,意味着他们的类型必须相同,当我们调用findFirst时编译器会强制检测,如果在Array[Int]中查找一个String,可能会造成类型匹配错误 5.2.对高阶函数传入匿名函数&emsp;在使用高阶函数时,不必非要提供一些有名函数,可以传入匿名函数或函数字面量,如下测试findFirst函数12scala&gt; findFirst(Array(7,9,13), (x:Int)=&gt;x==9)res1: Int = 1 &emsp;表达式Array(7,9,13)是一段”数组字面量”,他用3个整数构造一个数组,注意构造数组时并没有使用new关键字&emsp;语法(x:Int)=&gt;x==9 是一段”函数字面量’或”匿名函数”,不必先定义一个有名称的方法,可以利用语法的便利,在调用的时候再定义&emsp;通常函数的参数声明在=&gt;箭头的左边,可以在箭头右边的函数体内使用它们,比如写一个比较两个整数是否相等的函数12scala&gt; (x:Int, y:Int) =&gt; x==yres2: (Int, Int) =&gt; Boolean = &lt;function2&gt; &emsp;在REPL结果中的符号表示res2接收2个参数的函数,如果scala可以从上下文推断输入参数的类型,函数参数可以省略掉类型符号,例如:(x,y)=&gt;x&lt;y 在scala中函数也是值 &emsp;当我们定义一个函数字面量的时候,实际上定义了一个包含一个apply方法的scala对象,scala对这个方法名有特别的规则,一个有apply方法的对象可以把他当成方法一样调用,我们定义函数字面量(a,b)=&gt;a","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala函数式编程","slug":"scala函数式编程","permalink":"http://yoursite.com/tags/scala函数式编程/"}]},{"title":"第三章 函数式数据结构","date":"2017-04-16T04:47:25.374Z","path":"2017/04/16/functionalProgrammingInScala/第三章 函数式数据结构/","text":"1.定义函数式数据结构123456789101112131415161718192021package fpinscala.datastructuressealed trait List[+A]case object Nil extends List[Nothing]case class Cons[+A](head: A, tail: List[A]) extends List[A]object List&#123; def sum(ints: List[Int]): Int = ints match &#123; case Nil =&gt; 0 case Cons(x,xs) =&gt; x+sum(xs) &#125; def product(ds: List[Double]): Double = ds match &#123; case Nil =&gt; 1.0 case Cons(0.0, _) =&gt; 0.0 case Cons(x, xs) =&gt; x*product(xs) &#125; def apply[A](as: A*): List[A] = if (as.isEmpty) Nil else Cons(as.head, apply(as.tail:_*))&#125; &emsp;List有两种实现,或者是说构造器(每种都由case关键字引入),表示List有两种可能的形式,如上面的代码所示,List如果为空,使用数据构造器Nil表示,如果非空,使用数据构造器Cons表示,一个非空List由初始元素head和后续紧跟的也是List结构的tail组成 关于型变 &emsp;在trait List[+A]声明里,类型参数A前面的+是一个型变的符号,标志着A是协变的或正向的参数,意味着假设Dog是Animal的子类,那么List[Dog] 是List[Animal]的子类,(多数情况下,所有类型X和Y,如果X是Y的子类型,那么List[X]是List[Y]的子类型),我们可以在A的前面去掉+号,那么标记List的参数类型是非协变的&emsp;注意:Nil继承List[Nothing] ,Nothing是所有类型的子类型,也就是说使用型变符号后Nil可以当成是List[Int],或List[Double]等任何List的具体类型 2.模式匹配123456789def sum(ints: List[Int]): Int = ints match &#123; case Nil =&gt; 0 case Cons(x,xs) =&gt; x+sum(xs)&#125;def product(ds: List[Double]): Double = ds match &#123; case Nil =&gt; 1.0 case Cons(0.0, _) =&gt; 0.0 case Cons(x, xs) =&gt; x*product(xs)&#125; &emsp;模式匹配有点像一个别致的switch声明,他可以侵入到表达式的数据结构内部,对这个结构进行检验和提取子表达式,他由一个表达式引入,例如ds(目标或被检验者)后边跟着一个关键字match和一个用花括号封装起来的一系列case语句,每一条case语句由=&gt;箭头左边的模式(如Cons(x,xs)) 和=&gt;箭头右边的结果(如 x*product(xs))组成,如果目标匹配其中的一种模式,他的结果就是整个match表达式的结果,如果目标与多个模式都匹配,scala选择第一个匹配的模式 scala中的伴生对象&emsp;除了经常声明数据类型和数据构造器之外,我们也经常声明伴生对象,他只是与数据类型同名的一个单例,通常在里面定义一些用于创建或处理数据类型的便捷方法,例如我们想让函数def fillA: List[A]创建一个有n个拷贝元素的List,很适合把它放在List伴生对象里,用伴生对象是scala中的一种惯例 来看几个模式匹配的例子:1List(1,2,3) match &#123;case _ =&gt; 42&#125; &emsp;结果是42,这里使用变量模式,匹配任何表达式,我们可以用x或foo替代,但通常使用_指示一个在匹配结果里不关心的变量 1List(1,2,3) match &#123;case Cons(h, _) =&gt; h&#125; &emsp;结果是1,这里使用了一个数据构造器模式结合一个变量模式捕获目标的子表达式 1List(1,2,3) match &#123;case Cons(_, t) =&gt; t&#125; &emsp;结果是List(2,3) 1List(1,2,3) match &#123;case Nil =&gt; 42&#125; &emsp;结果是运行时匹配错误(Match-Error),表示没有表达式与目标匹配 &emsp;表达式是否符合模式匹配是由什么决定的?一个模式或许包含只能匹配常量的诸如3或”hi”这样的字面量;或能匹配任何表达式的以小写字母开头的如x,xs这样的变量;或以下划线和只能匹配相应形式的如Cons(x,xs) 和Nil这样的数据构造器,模式IDE组成可以是任意嵌套的—Cons(x1, Cons(x2, Nil))和 Cons(y1, Cons(y2, Cons(y3,_)))都是有效的模式,如果将模式中的变量分配给目标子表达式,使得他在结构上与目标一致,模式与目标就是匹配的,匹配上的话,结果表达式可以访问这些模式中定义的局部变量 &emsp;下面的匹配表达式结果是什么? 123456789 val x = List(1,2,3,4,5) match &#123; case Cons(x, Cons(2, Cons(4, _))) =&gt; x case Nil =&gt; 42 case Cons(x, Cons(y, Cons(3, Cons(4, _)))) =&gt; x + y //匹配 case Cons(h, t) =&gt; h + sum(t) case _ =&gt; 101 &#125;//x = 3 3.函数式数据结构中的数据共享&emsp;当数据不可变时,我们该怎么写一些例如从List中删除元素之类的函数?答案很简单,当我们对一个以存在的列表xs在前面添加一个元素1的时候,返回一个新的元素,即Cons(1, xs) ,既然列表是不可变的,我们不需要真的去复制一份xs,可以直接复用他,这也称为数据共享,共享不可变数据可以让函数实现更高的效率,我们可以返回不可变数据结构而不用担心后续代码修改他,不需要悲观的复制一份数据以避免对其修改或污染&emsp;以同样的方式,删除一个列表的第一个元素,比如:myList = Cons(x, xs) ,只需要返回尾部的xs,并没有真的删除元素,原始列表myList依然可用,丝毫未受影响,我们说函数式数据结构是持久的,意味着已存在的引用不会因为数据结构的操作而改变 &emsp;练习: 实现tail函数,删除一个List的第一个函数,注意这个函数的时间开销是常量级的,如果列表是Nil,在实现的时候会有什么不同的选择?12345def tail[A](l: List[A]): List[A] = l match &#123; case Nil =&gt; sys.error(&quot;tail of empty list&quot;) case Cons(_,t) =&gt; t &#125; &emsp;练习2: 使用相同的思路,实现函数setHead用一个不同的值替代列表中的第一个元素1234def setHead[A](l: List[A], h: A): List[A] = l match &#123; case Nil =&gt; sys.error(&quot;setHead on empty list&quot;) case Cons(_,t) =&gt; Cons(h,t)&#125; 3.1.数据共享的效率&emsp;练习1:把tail泛化为drop函数,用于从列表中删除前n个元素,注意,这个函数的时间开销只需要与drop的元素个数成正比—-不需要复制整个列表123456def drop[A](l: List[A], n: Int): List[A] = if (n &lt;= 0) l else l match &#123; case Nil =&gt; Nil case Cons(_,t) =&gt; drop(t, n-1) &#125; &emsp;练习2:实现dropWhile函数,删除列表中前缀全部符合判定的元素12345def dropWhile[A](l: List[A], f: A =&gt; Boolean): List[A] = l match &#123; case Cons(h,t) if f(h) =&gt; dropWhile(t, f) case _ =&gt; l &#125; &emsp;练习3:利用数据共享的特性将一个列表的所有元素加到另一个列表的后面123456def append[A](a1: List[A], a2: List[A]): List[A] = a1 match &#123; case Nil =&gt; a2 case Cons(h,t) =&gt; Cons(h, append(t, a2)) &#125;//这样之后的结果是:Cons(1.1, Cons(1.2, Cons(1.3, a2) //假设a1(1.1, 1.2, 1.3) &emsp;这个定义只做数据复制,直到第一个列表中没有元素可用,所以他的时间和内存开销只取决于a1的长度,如果我们用两个数组实现相同的函数,被迫要复制两个数组中的所有元素到一个结果集中,这种情况下不可变链表比数组更有效率 &emsp;练习4:不是所有的实现都令人满意,实现一个init函数,返回一个列表,他包含源列表中除了最后一个元素之外的所有元素,比如,传入List(1,2,3,4)返回List(1,2,3),为什么这个函数不能实现同tail一样的常量级时间开销呢?123456def init[A](l: List[A]): List[A] = l match &#123; case Nil =&gt; sys.error(&quot;init of empty list&quot;) case Cons(_,Nil) =&gt; Nil //去掉最后一个元素的关键步 case Cons(h,t) =&gt; Cons(h,init(t)) &#125; &emsp;由上面知道,只有将列表的所有的元素迭代一遍之后,才能得到的结果,所以时间复杂度与列表的大小相关 3.2.改进高阶函数的类型推导&emsp;高阶函数如dropWhile经常传递匿名函数,看一个典型的例子,回忆一下dropWhile的签名:1def dropWhile[A](l: List[A], f: A =&gt; Boolean): List[A] &emsp;当我们传入一个匿名函数来调用它时,我们必须指定他的参数类型,这里是x:12val xs: List[Int] = List(1,2,3,4,5)val ex1 = dropWhile(xs, (x: Int)=&gt; x&lt;4) //ex1的值为List(4,5) &emsp;不幸的是,我们需要声明x类型是Int,dropWhile的第一个参数是一个List[Int],所以函数上的第二个参数必须接受Int类型,scala可以推导这种情况,如果我们把dropWhile的参数分成两组12345def dropWhile[A](l: List[A])( f: A =&gt; Boolean): List[A] = l match &#123; case Cons(h,t) if f(h) =&gt; dropWhile(t)(f) case _ =&gt; l &#125; &emsp;这个版本的dropWhile的语法看起来像dropWhile(xs)(f), 这里dropWhile(xs)返回一个函数,然后对这个函数传入参数调用(换句话说dropWhile是柯里化的),使用这种方式把参数分组的目的是帮助类型推导,现在我们可以使用dropWhile时不加类型标注12val xs: List[Int] = List(1,2,3,4,5)val ex1 = dropWhile(xs)(x=&gt;x&lt;4) //注意变量x没有使用类型标注 &emsp;一般来讲,当函数定义包含多个参数数组时,参数里的类型信息从左到右传递,这里第一个参数组确定A类型参数为Int,所以x=&gt;x&lt;4里的类型标注可以不需要 4.基于list的递归并泛化为高阶函数&emsp;再看一下sum和product的实现,我们已经稍微简化了product的实现,不引入检测0.0的短路逻辑12345678def sum(ints: List[Int]): Int = ints match &#123; case Nil =&gt; 0 case Cons(x,xs) =&gt; x+sum(xs) &#125; def product(ds: List[Double]): Double = ds match &#123; case Nil =&gt; 1.0 case Cons(x, xs) =&gt; x*product(xs) &#125; &emsp;注意:这两个定义很相似,他们操作不同的数据类型(List[Int]与List[Double]), 除此之外的差异还有在List为空时的返回值(sum返回0,product返回1.0), 以及对结果的组合操作(sum是+,而product是*),如果再遇到类似重复的情况,可以把子表达式放到函数参数里来进行泛化,如果一个子表达式引用任何局部变量(+操作引用了模式中的x和xs局部变量,product里也存在相同的情况),把子表达式放入一个接收这些变量作为参数的函数,现在就改一下,函数的参数包含当列表为空时返回的值,以及列表非空时用于将元素添加到结果的函数 &emsp;下面是右折叠的简单运用12345678def foldRight[A,B](as: List[A], z:B)(f: (A,B)=&gt;B): B = as match &#123; case Nil =&gt; z case Cons(x,xs) =&gt; f(x, foldRight(xs,z)(f))&#125;def sum(ints: List[Int]): Int = foldRight(ints, 0)((x,y)=&gt;x+y)def product(ds: List[Double]): Double = foldRight(ds, 1.0)(_ * _)// _ * _ 是对(x,y) =&gt; x*y 更简练的写法 &emsp;来看一下sum函数调用foldRight的执行过程123456789//sum(List(1,2,3))foldRight(Cons(1, Cons(2, Cons(3,Nil))), 0)((x,y) =&gt; x+y)1 + foldRight(Cons(2, Cons(3,Nil)), 0)((x,y) =&gt; x+y)1 + (2 + foldRight(Cons(Cons(3, Nil), 0)((x,y) =&gt; x+y))1 + (2 + 3 + foldRight(Cons(Nil), 0)((x,y) =&gt; x+y))1 + (2 + (3 + (0)))6//注意foldRight在开始迭代之前,必须一路遍历到列表的末尾(在处理过程中不断压栈) 针对匿名函数的下划线 &emsp;匿名函数(x,y)=&gt; x+y ,在x和y类型可以被scala推导的情况下可以缩写为+,对那些函数参数只在函数体中出现一次的函数,这种缩写很适用,匿名函数表达式中的每一个下划线,例如+会引入一个新的(未命名的)函数参数,并对其引用,参数的引入按照从左到右的顺序1234_ + _ // (x,y) =&gt; x+y_ * 2 // x =&gt; x*2_.head // xs =&gt; xs.head_ drop _ // (xs, n) =&gt; xs drop n 4.1.更多与列表相关的函数 练习题1:使用foldRight计算List的长度 12def length[A](l: List[A]): Int = foldRight(l, 0)((_,acc) =&gt; acc + 1) 练习题2:使用尾递归的方式写一个递归函数foldLeft 12345678910@annotation.tailrecdef foldLeft[A,B](l: List[A], z: B)(f: (B, A) =&gt; B): B = l match &#123; //注意函数f的参数的位置变了 case Nil =&gt; z case Cons(h,t) =&gt; foldLeft(t, f(z,h))(f)&#125;def sum3(l: List[Int]) = foldLeft(l, 0)(_ + _) //实现sum函数def product3(l: List[Double]) = foldLeft(l, 1.0)(_ * _) //实现product函数def length2[A](l: List[A]): Int = foldLeft(l, 0)((acc,h) =&gt; acc + 1) //计算列表的长度def reverse[A](l: List[A]): List[A] = foldLeft(l, List[A]())((acc,h) =&gt; Cons(h,acc)) //对列表的元素颠倒顺序 练习题3:写一个函数将List[Int]中的每一个元素都加1 12def add1(l: List[Int]): List[Int] = foldRight(l, Nil:List[Int])((h,t) =&gt; Cons(h+1,t)) 练习题4:写一个函数将List[Double] 中每一个元素转成String, 形成List[String] 12def doubleToString(l: List[Double]): List[String] = foldRight(l, Nil:List[String])((h,t) =&gt; Cons(h.toString,t)) 练习题5:写一个函数,将列表中的每个元素进行修改,并维持列表结构,签名如下:def mapA,B(f: A=&gt;B): List[B] 12def map[A,B](l: List[A])(f: A =&gt; B): List[B] = foldRight(l, Nil:List[B])((h,t) =&gt; Cons(f(h),t)) //f(h)就是对每一个元素进行操作 练习题6:写一个filter函数,从列表中删除所有不满足断言的元素,并用他删除一个List[Int]中的所有奇数 12def filter[A](l: List[A])(f: A =&gt; Boolean): List[A] = foldRight(l, Nil:List[A])((h,t) =&gt; if (f(h)) Cons(h,t) else t) 练习题7:写一个flatMap函数,他跟map函数有些像,除了传入的函数f返回的是列表而非单个结果,这个f所返回的列表会被塞到flatMap最终所返回的列表中,如下签名:def flatMapA,B(f: A=&gt;List[B]): List[B]&emsp; 例如:flatMap[List(1,2,3))(i=&gt;List(i,i)) ,结果是List(1,1,2,2,3,3) 12345def flatMap[A,B](l: List[A])(f: A =&gt; List[B]): List[B] = concat(map(l)(f))def concat[A](l: List[List[A]]): List[A] = foldRight(l, Nil:List[A])(append) 练习题8:写一个函数,接收2个列表,通过对相应元素的相加构造出一个新的列表,比如:List(1,2,3) 和List(4,5,6) ,构造出List(5,7,9) 12345def addPairwise(a: List[Int], b: List[Int]): List[Int] = (a,b) match &#123; case (Nil, _) =&gt; Nil case (_, Nil) =&gt; Nil case (Cons(h1,t1), Cons(h2,t2)) =&gt; Cons(h1+h2, addPairwise(t1,t2))&#125; 练习题9:针对上面的函数进行泛化,不只是针对整数或相加操作,将这个泛化函数命名 为zipWith 12345def zipWith[A,B,C](a: List[A], b: List[B])(f: (A,B) =&gt; C): List[C] = (a,b) match &#123; case (Nil, _) =&gt; Nil case (_, Nil) =&gt; Nil case (Cons(h1,t1), Cons(h2,t2)) =&gt; Cons(f(h1,h2), zipWith(t1,t2)(f))&#125; 标准库中的列表 &emsp;列表(List)存在于scala标准库中,我们写的List和标准库中的List不同的是,Cons在标准库的版本写为::, 他使用右结合,所以1::2::Nil等于1::(2::Nil) ,也等于List(1,2,3) ,进行模式匹配时,case Cons(h,t) 变为了case h::t&emsp;下面是一个其他的List 的方法1234567891011121314def take(n: Int): List[A] //返回一个由当前列表中前n个元素构成的列表def takeWhile(f: A=&gt;Boolean): List[A]//返回当前列表中最长的多个连续元素的列表,这些元素都必须满足断言fdef forall(f: A=&gt;Boolean): Boolean//如果存在所有的元素都满足断言f就返回truedef exists(f: A=&gt;Boolean): Boolean//如果存在任何一个元素满足断言f就返回truescanLeft和scanRight //就像foldLeft和foldRight,但他们都返回部分结果,而非最终的累计值列表 4.2.用简单组件组合list函数时的效率损失 练习:写一个hasSubsequence 来检测List子序列是否包含一个另外指定的一个List,比如List(1,2,3,4) 包含List(1,2),List(1,2,3),List(2,3,4)等,下面是用一种方式实现,第五章,会有改进 123456789101112@annotation.tailrecdef startsWith[A](l: List[A], prefix: List[A]): Boolean = (l,prefix) match &#123; case (_,Nil) =&gt; true case (Cons(h,t),Cons(h2,t2)) if h == h2 =&gt; startsWith(t, t2) case _ =&gt; false&#125;@annotation.tailrecdef hasSubsequence[A](sup: List[A], sub: List[A]): Boolean = sup match &#123; case Nil =&gt; sub == Nil case _ if startsWith(sup, sub) =&gt; true //如果是开头匹配 case Cons(h,t) =&gt; hasSubsequence(t, sub) //如果不是开头匹配,那么去掉开头的元素&#125; 5.树 scala中的元组 1234567891011121314scala&gt; val p = (&quot;Bob&quot;, 42)p: (String, Int) = (Bob,42)scala&gt; p._1res0: String = Bobscala&gt; p._2res1: Int = 42scala&gt; p match &#123;case (a,b) =&gt; b&#125;res2: Int = 42//这个例子中(&quot;Bob&quot;, 42) 是一个类型为(String,Int)的数据对(pair),他其实是Tuple2[String, Int] 的语法糖, 二叉树数据结构 123sealed trait Tree[+A]case class Leaf[A](value: A) extends Tree[A]case class Branch[A](left: Tree[A], right: Tree[A]) extends Tree[A] &emsp;如下是图示 练习1:写一个size函数,统计一颗树中的节点数(叶子节点和分支节点) 1234def size[A](t: Tree[A]): Int = t match &#123; case Leaf(_) =&gt; 1 case Branch(l,r) =&gt; 1 + size(l) + size(r)&#125; 练习2:写一个maxinum返回Tree[Int]中的最大的元素 1234def maximum(t: Tree[Int]): Int = t match &#123; case Leaf(n) =&gt; n case Branch(l,r) =&gt; maximum(l) max maximum(r)&#125; 练习3:写一个depth函数,返回一颗树中从根节点到任何叶子节点最大的路径长度 1234def depth[A](t: Tree[A]): Int = t match &#123; case Leaf(_) =&gt; 0 case Branch(l,r) =&gt; 1 + (depth(l) max depth(r))&#125; 练习4:写一个map函数,类似于List中的同名函数,接受一个函数,对树中的每个元素进行修改 1234def map[A,B](t: Tree[A])(f: A =&gt; B): Tree[B] = t match &#123; case Leaf(a) =&gt; Leaf(f(a)) case Branch(l,r) =&gt; Branch(map(l)(f), map(r)(f)) //map(l)(f) 再次进入map的时候会调用第一个case&#125;","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala函数式编程","slug":"scala函数式编程","permalink":"http://yoursite.com/tags/scala函数式编程/"}]},{"title":"第一章 什么是函数式编程","date":"2017-04-16T04:47:25.373Z","path":"2017/04/16/functionalProgrammingInScala/第一章 什么是函数式编程/","text":"1.函数式编程的好处:一个简单的例子&emsp;函数式编程(FP)基于一个简单又蕴含深远的前提:只用纯函数来构造程序—-换句话说,函数没有副作用,什么是副作用?一个带有副作用的函数不仅是简单的返回一个值,还干了一些其他事情,比如: 修改了一个变量 直接修改数据结构 设置一个对象的成员 抛出一个异常或者一个错误停止 打印到终端或读取用户的输入 读取或写入一个文件 在屏幕上绘画 &emsp;函数式编程更加模块化,由于纯函数的模块化特性,他们很容易被测试,复用,并行化,泛化以及推导,此外,纯函数减少了产生bug的可能性&emsp;在这一章,我们将演示一个简单的带有副作用的程序,并通过去除这些副作用来示范函数式的一些好处 1.1.一段带有副作用的程序&emsp;假设我们要为一家咖啡店的购物编写一段程序,先用一段带有副作用的scala程序来实现1234567class Cafe&#123; def buyCoffee(cc: CreditCard): Coffee = &#123; val cup = new Coffee() cc.charge(cup.price) //副作用,信用卡计费,我需要的是一杯咖啡,返回coffee即可,其他额外的是不需要做 cup //返回coffee &#125;&#125; &emsp;cc.charge(cup.price)这行是一个副作用的例子,信用卡的计费涉及与外部世界的一些交互——-假设需要通过web service联系信用卡公司,授权交易,对卡片计费,并且持久化一些记录以便以后引用,我们的函数不过是返回一杯咖啡,这些其他行为也额外发生了,因此也被称为”副作用”&emsp;副作用导致这段代码很难测试,我们不希望测试逻辑真的是去联系信用卡公司并对卡片计费,缺乏可测试性预示着设计的修改:按理说CreditCard不应该知道如何联系信用卡公司实际执行一次计费,他就是一张信用卡,他应该拥有的是信用卡的属性,所以同样也不应该知道怎么把一次计费持久化到内部系统,我们可以让CreditCard忽略掉这些事情,通过传递一个Payments对象(支付对象)给buyCoffee函数,使代码更加模块化和可测化 1234567class Cafe&#123; def buyCoffee(cc: CreditCard, p: Payment): Coffee = &#123; val cup = new Coffee() p.charge(cc, cup.price) //副作用,信用卡计费,我需要的是一杯咖啡,返回coffee即可,其他额外的是不需要做 cup //返回coffee &#125;&#125; &emsp;虽然当我们调用p.charge(cc, cup.price)的时候仍然有副作用发生,但至少恢复了一些可测试性,Payments可以是一个接口,我们可以写一个合适于测试的mock实现这个接口&emsp;撇开对测试的担心,这里还有另一个问题,buyCoffee方法很难被复用,假设一个叫Alice的顾客,要订购12杯咖啡,最理想的情况是只要复用这个方法,通过循环来调用12次buyCoffee,但是基于当前的程序,会陷入12次对支付系统的调用,对Alice的信用卡执行12次计费,那样所产生的更多的手续费对Alice和咖啡店都不好 &emsp;多次购买的话我们该怎么做呢?如下图,我们编写一个全新的函数buyCoffees,他使用特别的逻辑去批量处理计费,这里可能不是什么大问题,因为buyCoffee的逻辑很简单,但是在别的情况下我们需要重复的逻辑就会很多,而且要为失去代码的可复用性和可组合性而难过 1.2.函数式的解法:去除副作用&emsp;函数式的解法是消除副作用,通过让buyCoffees方法在返回咖啡(Coffee)的时候,把费用(Charge)一并返回,计费的处理包括:发送到信用卡公司,持久化这条记录等,这些过程将在其他地方来做123456class Cafe&#123; def buyCoffee(cc: CreditCard): (Coffee, Charge) = &#123; val cup = new Coffee() (cup, Charge(cc, cup.price)) &#125;&#125; &emsp;我们先来看看”Charge”怎么定义?我们所构造的这个数据类型有信用卡(CreditCard)和金额(amount)组成,并提供了一个combine函数,以便对同一张信用卡合并计费123456789case class Charge(cc: CreditCard, amount: Double)&#123; //对相同的信用卡进行合并计费 def combine(other: Charge) =&#123; if (cc == other.cc) Charge(cc, amount+other.amount) //case类可以不通过new关键字创建,我们只需要在类名后面传递一组参数到主构造器 else throw new Exception(&quot;Can&apos;t combine charge to different cards&quot;) &#125;&#125; &emsp;现在我们来看看buyCoffee方法如何实现购买n杯咖啡,与之前不同,现在可以利用buyCoffees方法实现123456//用同一张信用卡点了多杯咖啡def buyCoffees(cc: CrediCard, n: Int): (List[Coffee], Charge) = &#123; val purchases: List[(Coffee, Charge)] = List.fill(n)(buyCoffee(cc)) //List.fill(n)(x)创建一个对x赋值n份的列表 val (coffees, charges) = purchases.unzip //unzip将数值对儿列表,分成一对儿(pair)列表,所以分解成为了两个list (coffees, charges.reduce((c1,c2)=&gt;c1.combine(c2)) //charges.reduce对整个charge列表规约成一个charge&#125; &emsp;总体上看,这个解决方案有显著的改善,现在我们可以直接复用buyCoffee来定义buyCoffees函数,这两个函数都很简单并容易测试,不需要实现一些Payments接口来进行复杂的mock,事实上,Cafe现在完全忽略了计费是如何处理的,当然我们可以用一个Payments类来做付款处理,但Cafe并不需要了解他&emsp;让Charge成为一等值,还有一些我们没有预期到的好处:我们能更容易的组装业务,举个例子:Alice带着她的笔记本电脑来咖啡店并在这里工作几个小时,中间时不时地会买几杯咖啡,如果咖啡店能把Alice买的咖啡合并成一笔费用以节约信用卡的手续费就太好了,因为Charge是一等值,我们可以用下面的函数把同一张信用卡的费用合并为一个List[Charge]123def coalesce(charges: List[Charge]): List[Charge] =&#123; charges.groupBy(_.cc).values.map(_.reduce(_ combine _)).toList&#125; &emsp;这个函数接收一个计费列表参数,按照信用卡对这个参数进行group,然后对每张卡组合成一个单独的计费 2.(纯)函数究竟是什么?&emsp;对于一个输入类型为A和输出类型为B的函数f(用scala写为:A=&gt;B,读作”A到B”或”A箭头B”),他是一种将所有的A类型值的值a关联到某个确切的值b的运算过程,及b由a来决定,任何内部或外部过程的状态改变都不会影响到f(a)的计算机结果,例如,函数intToString的类型为Int=&gt;String,他负责将整数转换为一个相应的字符串,除此之外应该什么也不用做&emsp;换言之,一个函数在程序的执行过程中除了根据输入参数给出运行结果之外没有其他的影响,就可以说是没有副作用的,将这一类函数称之为”纯函数”&emsp;你可能已经了解过一些纯函数了,比如考虑整数的加法(+)函数,他接收两个整数值并返回一个整数值,对于给定的两个整数值,他的返回值永远是相同的整数值,另一个例子是java,scala和其他字符串作为不可变对象的语言中对字符串求长度的函数,对于给定的字符串,返回的长度总是相同的 引用透明 &emsp;我们可以使用引用透明的概念对纯函数进行形式化,这不仅仅是函数的属性,而且是一般表达式的属性,为了便于讨论,设定一个程序的任何部分的表达式,都可以计算为一个结果,在scala解释器下输入的任何内容都会得到结果,例如:2 + 3是一个表达式,他使用纯函数+对数值2和3进行计算(注意:数值2和3同样也是一个表达式),他没有副作用,这个表达式每次运算的结果都是同一个值5,实际上,如果看到程序里有2+3这样的表达式可以直接替换为5,并不会改变程序的含义&emsp;这意味着任何程序中符合引用透明的表达式都可以由他的结果所取代,而不改变该程序的含义,当调用一个函数时所传入的参数是引用透明的,并且函数调用也是引用透明的,那么这个函数是一个纯函数 引用透明与纯粹性 &emsp;对于程序p,如果他包含的表达式e满足引用透明,所有的e都可以替换为他的运算结果而不会改变程序p的含义,假设存在一个函数f,若表达式f(x)对所有的引用透明的表达式x也是引用透明的,那么这个f是一个纯函数 3.引用透明/纯粹性以及代换模型&emsp;让我们看看如何对原先的buyCoffee例子定义引用透明的12345def buyCoffee(cc: CreditCard): Coffee = &#123; val cup = new Coffee() cc.charge(cup.price) cup&#125; &emsp;无论cc.charge(cup.price)返回什么类型,他都会被buyCoffee丢弃&emsp;因此,buyCoffee(aliceCreditCard)的运算结果将仅仅是一杯咖啡,这相当于new Coffee() , 那么根据我们定义的引用透明,如果buyCoffee要满足纯函数,对于任何p而言,p(buyCoffee(aliceCreditCard))行为需要与p(new Coffee())相同,这显然不成立,表达式new Coffee()不做任何事情,但是buyCoffee(aliceCreditCard)将会连接信用卡公司并授权计费,两个程序显然有差异&emsp;引用透明要求函数不论进行任何操作都可以用它的返回值来代替,这种限制使得推到一个程序的求值变得简单而自然,我们称之为代换模型,如果表达式是引用透明的,那么可以想象计算过程就像在解代数方程,展开表达式的每一部分,使用指示对象替代变量,然后规约到最简单的形式,在这个过程中,每一项都被等价值所替代,计算的过程就是被一个又一个等价值所替代的过程,换句话说:引用透明使得程序具备了等式推理的能力 一个引用透明的例子 12345678scala&gt; val x = &quot;Hello, World&quot;x: String = Hello, Worldscala&gt; val r1 = x.reverser1: String = dlroW ,olleHscala&gt; val r2 = x.reverser2: String = dlroW ,olleH &emsp;假设我们把所有使用x的地方用它所引用的表达式来替换12345scala&gt; val r1 = &quot;Hello, World&quot;.reverser1: String = dlroW ,olleHscala&gt; val r2 = &quot;Hello, World&quot;.reverser2: String = dlroW ,olleH &emsp;这样做并不影响结果,r1和r2的值和以前一样,所以说x是引用透明的,另外,r1和r2也是引用透明的,如果他们出现在某个程序中也可以替代为他们所引用的值而不会对程序造成影响 一个不是引用透明的例子 &emsp;比如,java.lang.StringBuilder类里的append方法,这个方法改变了StringBuilder对象自身,在调用append方法之后,StringBuilder对象之前的状态被破坏1234567891011scala&gt; val x = new StringBuilder(&quot;Hello&quot;)x: StringBuilder = Helloscala&gt; val y = x.append(&quot;, World&quot;)y: StringBuilder = Hello, Worldscala&gt; val r1 = y.toStringr1: String = Hello, Worldscala&gt; val r2 = y.toStringr2: String = Hello, World &emsp;我们将y利用等值替换为append(“, World”),然后r1和r2就不再相同12345scala&gt; val r1 = x.append(&quot;, World&quot;).toStringr1: String = Hello, World, Worldscala&gt; val r2 = x.append(&quot;, World&quot;).toStringr2: String = Hello, World, World, World &emsp;上面的例子说明:StringBuilder.append不是一个纯函数,虽然r1和r2看上去是同一个表达式,但他们引用的StringBuilder是两个不同的值,在r2调用x.append的时候,r1已经改变了x引用的对象(这就是副作用的产生之处)","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"},{"name":"scala函数式编程","slug":"scala函数式编程","permalink":"http://yoursite.com/tags/scala函数式编程/"}]},{"title":"直角坐标系之axis(xAsix/yAxis)","date":"2017-04-16T04:47:25.368Z","path":"2017/04/16/echarts/直角坐标系之axis(xAsix和yAxis)/","text":"下面是对axis(轴)的一些常用选项的介绍 axis.type | string :坐标轴类型[ default: ‘value’ ] 可选： ‘value’ 数值轴，适用于连续数据。 ‘category’ 类目轴，适用于离散的类目数据，为该类型时必须通过 data 设置类目数据。 ‘time’ 时间轴，适用于连续的时序数据，与数值轴相比时间轴带有时间的格式化，在刻度计算上也有所不同，例如会根据跨度的范围来决定使用月，星期，日还是小时范围的刻度。 ‘log’ 对数轴。适用于对数数据。 axis.position | string :x/y轴的位置默认 grid 中的第一个 y 轴在 grid 的左侧（’left’），第二个 y 轴视第一个 y 轴的位置放在另一侧。 可选： ‘left’ ‘right’ axis.name | string : 坐标轴名称。 axis.nameLocation | string : 坐标轴名称显示位置[ default: ‘end’ ] 可选： ‘start’ ‘middle’ ‘end’ axis.nameTextStyle | Object : 坐标轴名称的文字样式 nameTextStyle:{ color:…,//坐标轴名称的颜色 fontStyle:..,//坐标轴名称的文字字体风格 //….} axis.nameGap | number : 坐标轴名称与轴线之间的距离[ default: 15 ] axis.boundaryGap | boolean, Array : 坐标轴两边留白策略，类目轴和非类目轴的设置和表现不一样 类目轴中 boundaryGap 可以配置为 true 和 false。默认为 true，这时候刻度只是作为分隔线，标签和数据点都会在两个刻度之间的带(band)中间。非类目轴，包括时间，数值，对数轴，boundaryGap是一个两个值的数组，分别表示数据最小值和最大值的延伸范围，可以直接设置数值或者相对的百分比，在设置 min 和 max 后无效。 示例： 类目轴 对于boundaryGap，对于柱形图(类目轴)是比较有用的，柱形图和y轴之间留出空白，这样会比较的好看。 非类目轴 axis.min | number, string : 坐标轴刻度最小值[ default: ‘auto’ ] 可以设置成特殊值 ‘dataMin’，此时取数据在该轴上的最小值作为最小刻度。不设置时会自动计算最小值保证坐标轴刻度的均匀分布。在类目轴中，也可以设置为类目的序数（如类目轴 data: [‘类A’, ‘类B’, ‘类C’] 中，序数 2 表示 ‘类C’。也可以设置为负数，如 -3）。 axis.max | number, string : [ default: ‘auto’ ]坐标轴刻度最大值。可以设置成特殊值 ‘dataMax’，此时取数据在该轴上的最大值作为最大刻度。不设置时会自动计算最大值保证坐标轴刻度的均匀分布。在类目轴中，也可以设置为类目的序数（如类目轴 data: [‘类A’, ‘类B’, ‘类C’] 中，序数 2 表示 ‘类C’。也可以设置为负数，如 -3）。 axis.scale | boolean[ default: false ] 只在数值轴中（type: ‘value’）有效。是否是脱离 0 值比例。设置成 true 后坐标刻度不会强制包含零刻度。在双数值轴的散点图中比较有用。在设置 min 和 max 之后该配置项无效。 axis.splitNumber | number : 坐标轴的分割段数[ default: 5 ] 需要注意的是这个分割段数只是个预估值，最后实际显示的段数会在这个基础上根据分割后坐标轴刻度显示的易读程度作调整。在类目轴中无效。 aAxis.data[i] Object类目数据，在类目轴（type: ‘category’）中有效。示例：1234567891011// 所有类目名称列表data: [&apos;周一&apos;, &apos;周二&apos;, &apos;周三&apos;, &apos;周四&apos;, &apos;周五&apos;, &apos;周六&apos;, &apos;周日&apos;]// 每一项也可以是具体的配置项，此时取配置项中的 `value` 为类目名data: [&#123; value: &apos;周一&apos;, // 突出周一 textStyle: &#123; fontSize: 20, color: &apos;red&apos; &#125;&#125;, &apos;周二&apos;, &apos;周三&apos;, &apos;周四&apos;, &apos;周五&apos;, &apos;周六&apos;, &apos;周日&apos;] yAxis.data[i].value | string : 单个类目名称 yAxis.data[i].textStyle | Object : 类目标签的文字样式 axis.axisLine | Object : 坐标轴轴线相关设置。 axis.axisLine.show | boolean : 是否显示坐标轴轴线[ default: true ] axis.axisLine.onZero | boolean :[ default: true ]X 轴或者 Y 轴的轴线是否在另一个轴的 0 刻度上，只有在另一个轴为数值轴且包含 0 刻度时有效。 axis.axisLine.lineStyle | Object axis.Tick axis.Tick.interval axisLable axisLable.interval axisLable.rotate axisLable.margn axisLable.formatter/textStyle splitLine splitArea","tags":[{"name":"echarts","slug":"echarts","permalink":"http://yoursite.com/tags/echarts/"}]},{"title":"echarts上各部分组件图解","date":"2017-04-16T04:47:25.366Z","path":"2017/04/16/echarts/echarts上各部分组件图解/","text":"下图标注了一个普通的echarts图表的各个组成部分,如下:在学习echarts之前,需要了解echarts的整体结构以及各个部分的组件有大概的了解,下面是对echarts的组件进行图解,说明各名词的含义 grid（绘图网格） axis（坐标轴） line（折线图） legend（图例） title（标题） toolbox（工具箱） tooltip（提示） dataZoom（数据区域缩放） dataRange（值域） 名词解析 chart 是指一个完整的图表，如折线图，饼图等“基本”图表类型或由基本图表组合而成的“混搭”图表，可能包括坐标轴、图例等 axis 直角坐标系中的一个坐标轴，坐标轴可分为类目型、数值型或时间型 xAxis 直角坐标系中的横轴，通常并默认为类目型 yAxis 直角坐标系中的纵轴，通常并默认为数值型 grid 直角坐标系中除坐标轴外的绘图网格，用于定义直角系整体布局 legend 图例，表述数据和图形的关联 dataRange 值域选择，常用于展现地域数据时选择值域范围 dataZoom 数据区域缩放，常用于展现大量数据时选择可视范围 roamController 缩放漫游组件，搭配地图使用 toolbox 辅助工具箱，辅助功能，如添加标线，框选缩放等 tooltip 气泡提示框，常用于展现更详细的数据 timeline 时间轴，常用于展现同一系列数据在时间维度上的多份数据 series 数据系列，一个图表可能包含多个系列，每一个系列可能包含多个数据 line 折线图，堆积折线图，区域图，堆积区域图。 bar 柱形图（纵向），堆积柱形图，条形图（横向），堆积条形图。 scatter 散点图，气泡图。散点图至少需要横纵两个数据，更高维度数据加入时可以映射为颜色或大小，当映射到大小时则为气泡图 k K线图，蜡烛图。常用于展现股票交易数据。 pie 饼图，圆环图。饼图支持两种（半径、面积）南丁格尔玫瑰图模式 radar 雷达图，填充雷达图。高维度数据展现的常用图表。 chord 和弦图。常用于展现关系数据，外层为圆环图，可体现数据占比关系，内层为各个扇形间相互连接的弦，可体现关系数据 force 力导布局图。常用于展现复杂关系网络聚类布局。 map 地图。内置世界地图、中国及中国34个省市自治区地图数据、可通过标准GeoJson扩展地图类型。支持svg扩展类地图应用，如室内地图、运动场、物件构造等。 gauge 仪表盘。用于展现关键指标数据，常见于BI类系统。 funnel 漏斗图。用于展现数据经过筛选、过滤等流程处理后发生的数据变化，常见于BI类系统。 evnetRiver 事件河流图。常用于展示具有时间属性的多个事件，以及事件随时间的演化。 funnel 漏斗图。用于展现数据经过筛选、过滤等流程处理后发生的数据变化，常见于BI类系统。 evnetRiver 事件河流图。常用于展示具有时间属性的多个事件，以及事件随时间的演化。 treemap 矩形式树状结构图，简称：矩形树图。用于展示树形数据结构，优势是能最大限度展示节点的尺寸特征。 venn 韦恩图。用于展示集合以及它们的交集。 整理自:echarts2","tags":[{"name":"echarts","slug":"echarts","permalink":"http://yoursite.com/tags/echarts/"}]},{"title":"5 分钟上手 ECharts","date":"2017-04-16T04:47:25.365Z","path":"2017/04/16/echarts/5 分钟上手 ECharts/","text":"1.获取 ECharts的几种方式 从官网下载界面选择你需要的版本下载，根据开发者功能和体积上的需求，我们提供了不同打包的下载，如果你在体积上没有要求，可以直接下载完整版本。开发环境建议下载源代码版本，包含了常见的错误提示和警告。 在 ECharts 的 GitHub 上下载最新的 release 版本，解压出来的文件夹里的 dist 目录里可以找到最新版本的 echarts 库。 通过 npm 获取 echarts，npm install echarts –save，详见“在 webpack 中使用 echarts” cdn 引入，你可以在 cdnjs，npmcdn 或者国内的 bootcdn 上找到 ECharts 的最新版本。 2.引入 EChartsECharts 3 开始不再强制使用 AMD 的方式按需引入，代码里也不再内置 AMD 加载器。因此引入方式简单了很多，只需要像普通的 JavaScript 库一样用 script 标签引入。 12345678&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;!-- 引入 ECharts 文件 --&gt; &lt;script src=&quot;echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;/html&gt; 3.绘制一个简单的图表在绘图前我们需要为 ECharts 准备一个具备高宽的 DOM 容器。1234&lt;body&gt; &lt;!-- 为 ECharts 准备一个具备大小（宽高）的 DOM --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt;&lt;/body&gt; 然后就可以通过 echarts.init 方法初始化一个 echarts 实例并通过 setOption 方法生成一个简单的柱状图，下面是完整代码。12345678910111213141516171819202122232425262728293031323334353637383940&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;meta charset=&quot;utf-8&quot;&gt; &lt;title&gt;ECharts&lt;/title&gt; &lt;!-- 引入 echarts.js --&gt; &lt;script src=&quot;echarts.min.js&quot;&gt;&lt;/script&gt;&lt;/head&gt;&lt;body&gt; &lt;!-- 为ECharts准备一个具备大小（宽高）的Dom --&gt; &lt;div id=&quot;main&quot; style=&quot;width: 600px;height:400px;&quot;&gt;&lt;/div&gt; &lt;script type=&quot;text/javascript&quot;&gt; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById(&apos;main&apos;)); // 指定图表的配置项和数据 var option = &#123; title: &#123; text: &apos;ECharts 入门示例&apos; &#125;, tooltip: &#123;&#125;, legend: &#123; data:[&apos;销量&apos;] &#125;, xAxis: &#123; data: [&quot;衬衫&quot;,&quot;羊毛衫&quot;,&quot;雪纺衫&quot;,&quot;裤子&quot;,&quot;高跟鞋&quot;,&quot;袜子&quot;] &#125;, yAxis: &#123;&#125;, series: [&#123; name: &apos;销量&apos;, type: &apos;bar&apos;, data: [5, 20, 36, 10, 10, 20] &#125;] &#125;; // 使用刚指定的配置项和数据显示图表。 myChart.setOption(option); &lt;/script&gt;&lt;/body&gt;&lt;/html&gt;","tags":[{"name":"echarts","slug":"echarts","permalink":"http://yoursite.com/tags/echarts/"}]},{"title":"第4章 存储系统","date":"2017-04-16T04:47:25.361Z","path":"2017/04/16/bigdata/深入理解spark核心思想与源码分析/第4章 存储系统/","text":"spark为了避免Hadoop读写磁盘的I/O操作成为性能瓶颈,优先将配置信息,计算结果等数据存入内存,这极大的提升了系统的执行效率,正是因为这一关键决策,才让spark能在大数据应用中表现出优秀的计算能力 1.存储系统概述1.1.块管理器BlockManager的实现块管理器BlockManager是spark存储体系中的核心组件,因此本章主要围绕BlockManager展开,Driver Application和Executor都会穿件BlockManager,BlockManager的实现见代码:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293 val diskBlockManager = new DiskBlockManager(this, conf) private val blockInfo = new TimeStampedHashMap[BlockId, BlockInfo] private val futureExecutionContext = ExecutionContext.fromExecutorService( ThreadUtils.newDaemonCachedThreadPool(&quot;block-manager-future&quot;, 128)) // Actual storage of where blocks are kept private var externalBlockStoreInitialized = false private[spark] val memoryStore = new MemoryStore(this, memoryManager) private[spark] val diskStore = new DiskStore(this, diskBlockManager) private[spark] lazy val externalBlockStore: ExternalBlockStore = &#123; externalBlockStoreInitialized = true new ExternalBlockStore(this, executorId) &#125; memoryManager.setMemoryStore(memoryStore) // Note: depending on the memory manager, `maxStorageMemory` may actually vary over time. // However, since we use this only for reporting and logging, what we actually want here is // the absolute maximum value that `maxStorageMemory` can ever possibly reach. We may need // to revisit whether reporting this value as the &quot;max&quot; is intuitive to the user. private val maxMemory = memoryManager.maxStorageMemory private[spark] val externalShuffleServiceEnabled = conf.getBoolean(&quot;spark.shuffle.service.enabled&quot;, false) // Port used by the external shuffle service. In Yarn mode, this may be already be // set through the Hadoop configuration as the server is launched in the Yarn NM. private val externalShuffleServicePort = &#123; val tmpPort = Utils.getSparkOrYarnConfig(conf, &quot;spark.shuffle.service.port&quot;, &quot;7337&quot;).toInt if (tmpPort == 0) &#123; // for testing, we set &quot;spark.shuffle.service.port&quot; to 0 in the yarn config, so yarn finds // an open port. But we still need to tell our spark apps the right port to use. So // only if the yarn config has the port set to 0, we prefer the value in the spark config conf.get(&quot;spark.shuffle.service.port&quot;).toInt &#125; else &#123; tmpPort &#125; &#125; var blockManagerId: BlockManagerId = _ // Address of the server that serves this executor&apos;s shuffle files. This is either an external // service, or just our own Executor&apos;s BlockManager. private[spark] var shuffleServerId: BlockManagerId = _ // Client to read other executors&apos; shuffle files. This is either an external service, or just the // standard BlockTransferService to directly connect to other Executors. private[spark] val shuffleClient = if (externalShuffleServiceEnabled) &#123; val transConf = SparkTransportConf.fromSparkConf(conf, &quot;shuffle&quot;, numUsableCores) new ExternalShuffleClient(transConf, securityManager, securityManager.isAuthenticationEnabled(), securityManager.isSaslEncryptionEnabled()) &#125; else &#123; blockTransferService &#125; // Whether to compress broadcast variables that are stored private val compressBroadcast = conf.getBoolean(&quot;spark.broadcast.compress&quot;, true) // Whether to compress shuffle output that are stored private val compressShuffle = conf.getBoolean(&quot;spark.shuffle.compress&quot;, true) // Whether to compress RDD partitions that are stored serialized private val compressRdds = conf.getBoolean(&quot;spark.rdd.compress&quot;, false) // Whether to compress shuffle output temporarily spilled to disk private val compressShuffleSpill = conf.getBoolean(&quot;spark.shuffle.spill.compress&quot;, true) private val slaveEndpoint = rpcEnv.setupEndpoint( &quot;BlockManagerEndpoint&quot; + BlockManager.ID_GENERATOR.next, new BlockManagerSlaveEndpoint(rpcEnv, this, mapOutputTracker)) // Pending re-registration action being executed asynchronously or null if none is pending. // Accesses should synchronize on asyncReregisterLock. private var asyncReregisterTask: Future[Unit] = null private val asyncReregisterLock = new Object private val metadataCleaner = new MetadataCleaner( MetadataCleanerType.BLOCK_MANAGER, this.dropOldNonBroadcastBlocks, conf) private val broadcastCleaner = new MetadataCleaner( MetadataCleanerType.BROADCAST_VARS, this.dropOldBroadcastBlocks, conf) // Field related to peer block managers that are necessary for block replication @volatile private var cachedPeers: Seq[BlockManagerId] = _ private val peerFetchLock = new Object private var lastPeerFetchTime = 0L /* The compression codec to use. Note that the &quot;lazy&quot; val is necessary because we want to delay * the initialization of the compression codec until it is first used. The reason is that a Spark * program could be using a user-defined codec in a third party jar, which is loaded in * Executor.updateDependencies. When the BlockManager is initialized, user level jars hasn&apos;t been * loaded yet. */ private lazy val compressionCodec: CompressionCodec = CompressionCodec.createCodec(conf)//... 上面的代码中声明的BlockInfo:TimeStampedHashMap[BlockId, BlockInfo],用于BlockManager缓存BlockId以及对应的BlockInfo,从上面的代码可以看到,BlockManager主要由以下部分组成:1.shuffle客户端ShuffleClient2.BlockManagerMaster(对存在于所有Executor上的BlockManager统一管理)3.磁盘块管理器DiskBlockManager4.内存存储MemoryStore5.磁盘存储DiskStore6.Tachyon存储TachyonStore7.非广播block清理器metadataCleaner和广播Block清理器BroadcastCleaner8.压缩算法实现CompressionCodec BlockManager要生效,必须要初始化,他的初始化方法如下:1234567891011121314151617181920212223242526272829303132/** * Initializes the BlockManager with the given appId. This is not performed in the constructor as * the appId may not be known at BlockManager instantiation time (in particular for the driver, * where it is only learned after registration with the TaskScheduler). * * This method initializes the BlockTransferService and ShuffleClient, registers with the * BlockManagerMaster, starts the BlockManagerWorker endpoint, and registers with a local shuffle * service if configured. */def initialize(appId: String): Unit = &#123; blockTransferService.init(this) shuffleClient.init(appId) blockManagerId = BlockManagerId( executorId, blockTransferService.hostName, blockTransferService.port) shuffleServerId = if (externalShuffleServiceEnabled) &#123; logInfo(s&quot;external shuffle service port = $externalShuffleServicePort&quot;) BlockManagerId(executorId, blockTransferService.hostName, externalShuffleServicePort) &#125; else &#123; blockManagerId &#125; master.registerBlockManager(blockManagerId, maxMemory, slaveEndpoint) // Register Executors&apos; configuration with the local shuffle service, if one should exist. if (externalShuffleServiceEnabled &amp;&amp; !blockManagerId.isDriver) &#123; registerWithExternalShuffleServer() &#125;&#125; BlockManager的初始化步骤如下:1.BlockTransferService的初始化和ShuffleClient的初始化,参见本章第2节,ShuffleClient默认是BlockTransferService,当有外部的ShuffleService时,调用外部ShuffleService的初始化方法2.BlockManagerId和ShuffleServerId的创建,当有外部的ShuffleService时,创建新的BlockManagerId,否则ShuffleServiceId默认使用当前BlockManager的BlockManagerId3.向BlockManagerMaster注册BlockManagerId,具体实现见本章3.3节(当有外部的ShuffleService时,还需要向BlockManagerMaster注册ShuffleServerId) 1.2.spark存储体系架构在详细介绍存储体系之前,我们先用图说明Spark存储体系的架构 标号1表示Executor的BlockManager与Driver的BlockManager进行消息通信,例如,注册BlockManager,更新Block信息,获取Block所在的BlockManager,删除Executor等 标号2表示BlockManager的读操作(例如get,doGetLocal,BlockManager内部进行的MemoryStore,DiskStore,Tachyon的getgetBytes,getValues等操作)和写操作(例如doPut,putSingle,putBytes以及BlockManager内部进行的MemoryStore,DiskStore,Tachyon的getgetBytes,putArray,putIterator等操作) 标号3表示当memoryStore的内存不足时,写入DiskStore,而DiskStore实际依赖于DiskBlockManager 标号4表示通过访问远端节点的Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block 标号4表示远端节点的Executor的BlockManager访问本地Executor的BlockManager中的TransportServer提供的RPC服务下载或者上传Block 标号6表示当存储系统选择Tachyon作为存储时,对于BlockManager的读写操作实际调用了TachyonStore的putBytes,putArray,putIterator,getBytes,getValues等 spark目前支持HDFS,Amazon S3两种主流的分布式存储系统,还使用也诞生于UCBerkeley的AMP实验室的Tachyon这种高效的分布式文件系统作为缓存 spark定义了抽象类BlockStore,用于指定所有存储类型的规范,目前BlockStore的具体实现包括MemoryStore,DiskStore和TachyonStore,BlockStore的继承系统如下 2.shuffle服务与客户端有人可能为问?为何要将Netty实现的网络服务组件也放到存储体系里面,这是由于spark是分布式部署的,每个Task最终都运行在不同的机器节点上,map任务的输出结构直接存储到map任务所在机器的存储体系中,reduce任务极有可能不再同一机器上运行,所以需要远程下载map任务的中间输出,因此将ShuffleClient放到存储体系是最合适的 ShuffleClient并不像他的名字一样,是shuffle的客户端,他不光是将shuffle文件上传到其他Executor或者下载到本地的客户端,也提供了可以被其他Executor访问的shuffle服务,从第1节的代码中可以知道:当有外部的ShuffleClient时,新建ExternalShuffleClient,否则默认为BlockTransferService,BlockTransferService只有在其init方法被调用,即被初始化才提供服务,以默认的NettyBlockTransferService的init方法为例,如下代码:123456789101112131415override def init(blockDataManager: BlockDataManager): Unit = &#123; val rpcHandler = new NettyBlockRpcServer(conf.getAppId, serializer, blockDataManager) var serverBootstrap: Option[TransportServerBootstrap] = None var clientBootstrap: Option[TransportClientBootstrap] = None if (authEnabled) &#123; serverBootstrap = Some(new SaslServerBootstrap(transportConf, securityManager)) clientBootstrap = Some(new SaslClientBootstrap(transportConf, conf.getAppId, securityManager, securityManager.isSaslEncryptionEnabled())) &#125; transportContext = new TransportContext(transportConf, rpcHandler) clientFactory = transportContext.createClientFactory(clientBootstrap.toSeq.asJava) server = createServer(serverBootstrap.toList) appId = conf.getAppId logInfo(&quot;Server created on &quot; + server.getPort)&#125; NettyBlockTransferService的初始化步骤如下:1.创建RpcServer2.构造TransportContext3.创建RPC客户端工厂TransportClientFactory4.创建Netty服务器TransportServer 接下来我们逐步讲解Block的RPC服务,构造TransportContext,创建RPC客户端工厂TransportClientFactory,创建Netty服务器TransportServer的实现,此外还会介绍reduce任务是如何拉取map任务中间结果的(即shuffle过程的数传输) 2.1.Block的RPC服务当map任务和reduce任务处于不同节点时,reduce任务需要从远端节点下载map任务的中间输出结果,因此NettyBlockRpcServer提供打开,即下载Block文件的功能,一些情况下,为了容错,需要将Block的数据备份到其他节点上,所以NettyBlockRPCServer还提供了上传Block文件的RPC服务,NettyBlockRPCServer的实现代码如下:123456789101112131415161718192021222324252627282930313233343536class NettyBlockRpcServer( appId: String, serializer: Serializer, blockManager: BlockDataManager) extends RpcHandler with Logging &#123; private val streamManager = new OneForOneStreamManager() override def receive( client: TransportClient, rpcMessage: ByteBuffer, responseContext: RpcResponseCallback): Unit = &#123; val message = BlockTransferMessage.Decoder.fromByteBuffer(rpcMessage) logTrace(s&quot;Received request: $message&quot;) message match &#123; case openBlocks: OpenBlocks =&gt; val blocks: Seq[ManagedBuffer] = openBlocks.blockIds.map(BlockId.apply).map(blockManager.getBlockData) val streamId = streamManager.registerStream(appId, blocks.iterator.asJava) logTrace(s&quot;Registered streamId $streamId with $&#123;blocks.size&#125; buffers&quot;) responseContext.onSuccess(new StreamHandle(streamId, blocks.size).toByteBuffer) case uploadBlock: UploadBlock =&gt; // StorageLevel is serialized as bytes using our JavaSerializer. val level: StorageLevel = serializer.newInstance().deserialize(ByteBuffer.wrap(uploadBlock.metadata)) val data = new NioManagedBuffer(ByteBuffer.wrap(uploadBlock.blockData)) blockManager.putBlockData(BlockId(uploadBlock.blockId), data, level) responseContext.onSuccess(ByteBuffer.allocate(0)) &#125; &#125; override def getStreamManager(): StreamManager = streamManager&#125; 2.2.构造传输上下文TransportContextTransportContext用于维护传输上下文,他的构造器如下:12345678910public TransportContext( TransportConf conf, RpcHandler rpcHandler, boolean closeIdleConnections) &#123; this.conf = conf; this.rpcHandler = rpcHandler; this.encoder = new MessageEncoder(); this.decoder = new MessageDecoder(); this.closeIdleConnections = closeIdleConnections;&#125; TransportContext既可以创建Netty服务,也可以创建Netty访问客户端,TransportContext的组成如下:1.TransportConf:主要控制Netty框架提供的shuffle的I/O交互的客户端和服务端线程数量2.RpcHandler:负责shuffle的I/O服务端在接收到客户端的RPC请求后,提供打开Block或者上传Block的RPC处理,此处即为NettyBlockRPCServer3.decoder:在shuffle的I/O服务器端对客户端传来的ByteBuff进行行解析,防止丢包和解析错误4.encoder:在shuffle的I/O客户端对消息内容进行编码的时候,防止服务端丢包和解析错误 问题:为什么需要MessageEncoder和MessageDecode?因为在基于流的传输里(比如TCP/IP),接收到的数据首先会被存储到一个socket接收缓冲里,不幸的是,基于流的传输并不是一个数据包队列的,而是一个字节队列的,即使发送了2个独立的数据包,操作系统也不会作为2个消息处理,而仅仅认为是一连串的字节,因此不能保证远程写入的数据会被准确的读取,举个例子,假设操作系统的TCP/IP协议栈已经接收到3个数据包,ABC,DEF,GHI,由于基于流传输的协议的这种统一的性质,在应用程序读取数据时很可能性被分成下面的片段:AB,CDEFG,H,I,因此接受方不管是客户端还是服务端,都应该把接收到的数据整理成一个或者多个更有意义并且让程序的逻辑更好理解的数据,这才有了编码和解码 2.3.RPC客户端工厂TransportClientFactoryTransportClientFactory是创建Netty客户端TransportClient的工厂类,TransportClient用于向Netty服务器端发送RPC请求,TransportContext的createClientFactory方法用于创建TransportClientFactory123public TransportClientFactory createClientFactory(List&lt;TransportClientBootstrap&gt; bootstraps) &#123; return new TransportClientFactory(this, bootstraps);&#125; TransportClientFactory的实现如下:1234567891011121314151617public TransportClientFactory( TransportContext context, List&lt;TransportClientBootstrap&gt; clientBootstraps) &#123; this.context = Preconditions.checkNotNull(context); this.conf = context.getConf(); this.clientBootstraps = Lists.newArrayList(Preconditions.checkNotNull(clientBootstraps)); this.connectionPool = new ConcurrentHashMap&lt;SocketAddress, ClientPool&gt;(); this.numConnectionsPerPeer = conf.numConnectionsPerPeer(); this.rand = new Random(); IOMode ioMode = IOMode.valueOf(conf.ioMode()); this.socketChannelClass = NettyUtils.getClientChannelClass(ioMode); // TODO: Make thread pool name configurable. this.workerGroup = NettyUtils.createEventLoop(ioMode, conf.clientThreads(), &quot;shuffle-client&quot;); this.pooledAllocator = NettyUtils.createPooledByteBufAllocator( conf.preferDirectBufs(), false /* allowCache */, conf.clientThreads());&#125; TransportClientFactory由以下部分组成:1.clientBootstraps:用于缓存客户端列表2.connectionPool:用于缓存客户端连接3.numConnectionsPerPeer:节点之前取数据的连接数,可以使用属性spark.shuffle.io.numConnectionsPerPeer来配置,默认为14.socketChannelClass:客户端channel被创建时使用的类,可以使用属性spark.shuffle.io.mode来配置,默认为NioSocketChannel5.workerGroup:根据Netty的规范,客户端只有work组,所以此处创建workerGroup,实际上是NioEventLoopGroup6.pooledAllocator:汇集ByteBuff但对本地线程缓存禁用的分配器 2.4.Netty服务器TransportServerTransportServer提供了Netty实现的服务器端,用于提供RPC服务(比如上传,下载等),创建TransportServer的代码如下在NettyBlockTransferService的init方法中:12345678910111213server = createServer(serverBootstrap.toList)//具体方法实现 /** Creates and binds the TransportServer, possibly trying multiple ports. */ private def createServer(bootstraps: List[TransportServerBootstrap]): TransportServer = &#123; def startService(port: Int): (TransportServer, Int) = &#123; val server = transportContext.createServer(port, bootstraps.asJava) (server, server.getPort) &#125; val portToTry = conf.getInt(&quot;spark.blockManager.port&quot;, 0) Utils.startServiceOnPort(portToTry, startService, conf, getClass.getName)._1 &#125; TransportServer的构造器实现如下:123456789101112131415161718public TransportServer( TransportContext context, String hostToBind, int portToBind, RpcHandler appRpcHandler, List&lt;TransportServerBootstrap&gt; bootstraps) &#123; this.context = context; this.conf = context.getConf(); this.appRpcHandler = appRpcHandler; this.bootstraps = Lists.newArrayList(Preconditions.checkNotNull(bootstraps)); try &#123; init(hostToBind, portToBind); &#125; catch (RuntimeException e) &#123; JavaUtils.closeQuietly(this); throw e; &#125;&#125; 上面的init方法用于对TransportServer初始化,通过使用Netty框架的EventLoopGroup,ServerBootstrap等API创建shuffle的I/O交互的服务器端,init的主要代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private void init(String hostToBind, int portToBind) &#123; IOMode ioMode = IOMode.valueOf(conf.ioMode()); EventLoopGroup bossGroup = NettyUtils.createEventLoop(ioMode, conf.serverThreads(), &quot;shuffle-server&quot;); EventLoopGroup workerGroup = bossGroup; PooledByteBufAllocator allocator = NettyUtils.createPooledByteBufAllocator( conf.preferDirectBufs(), true /* allowCache */, conf.serverThreads()); bootstrap = new ServerBootstrap() .group(bossGroup, workerGroup) .channel(NettyUtils.getServerChannelClass(ioMode)) .option(ChannelOption.ALLOCATOR, allocator) .childOption(ChannelOption.ALLOCATOR, allocator); if (conf.backLog() &gt; 0) &#123; bootstrap.option(ChannelOption.SO_BACKLOG, conf.backLog()); &#125; if (conf.receiveBuf() &gt; 0) &#123; bootstrap.childOption(ChannelOption.SO_RCVBUF, conf.receiveBuf()); &#125; if (conf.sendBuf() &gt; 0) &#123; bootstrap.childOption(ChannelOption.SO_SNDBUF, conf.sendBuf()); &#125; bootstrap.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override protected void initChannel(SocketChannel ch) throws Exception &#123; RpcHandler rpcHandler = appRpcHandler; for (TransportServerBootstrap bootstrap : bootstraps) &#123; rpcHandler = bootstrap.doBootstrap(ch, rpcHandler); &#125; context.initializePipeline(ch, rpcHandler); &#125; &#125;); InetSocketAddress address = hostToBind == null ? new InetSocketAddress(portToBind): new InetSocketAddress(hostToBind, portToBind); channelFuture = bootstrap.bind(address); channelFuture.syncUninterruptibly(); port = ((InetSocketAddress) channelFuture.channel().localAddress()).getPort(); logger.debug(&quot;Shuffle server started on port :&quot; + port);&#125; ServerBootstrap的childHandler方法调用了TransportContext的initializePipeline,initializePipeline中创建了TransportChannelHandler,并将它绑定到SocketChannel的pipeline的handler中 12345678910111213141516171819public TransportChannelHandler initializePipeline( SocketChannel channel, RpcHandler channelRpcHandler) &#123; try &#123; TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler); channel.pipeline() .addLast(&quot;encoder&quot;, encoder) .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder()) .addLast(&quot;decoder&quot;, decoder) .addLast(&quot;idleStateHandler&quot;, new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000)) // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this // would require more logic to guarantee if this were not part of the same event loop. .addLast(&quot;handler&quot;, channelHandler); return channelHandler; &#125; catch (RuntimeException e) &#123; logger.error(&quot;Error while initializing Netty pipeline&quot;, e); throw e; &#125;&#125; 2.5.获取远程shuffle文件NettyBlockTransferService的fetchBlock方法用于获取远程shuffle文件,实际是利用NettyBlockTransferService中创建的Netty服务 123456789101112131415161718192021222324252627282930override def fetchBlocks( host: String, port: Int, execId: String, blockIds: Array[String], listener: BlockFetchingListener): Unit = &#123; logTrace(s&quot;Fetch blocks from $host:$port (executor id $execId)&quot;) try &#123; val blockFetchStarter = new RetryingBlockFetcher.BlockFetchStarter &#123; override def createAndStart(blockIds: Array[String], listener: BlockFetchingListener) &#123; val client = clientFactory.createClient(host, port) new OneForOneBlockFetcher(client, appId, execId, blockIds.toArray, listener).start() &#125; &#125; val maxRetries = transportConf.maxIORetries() if (maxRetries &gt; 0) &#123; // Note this Fetcher will correctly handle maxRetries == 0; we avoid it just in case there&apos;s // a bug in this code. We should remove the if statement once we&apos;re sure of the stability. new RetryingBlockFetcher(transportConf, blockFetchStarter, blockIds, listener).start() &#125; else &#123; blockFetchStarter.createAndStart(blockIds, listener) &#125; &#125; catch &#123; case e: Exception =&gt; logError(&quot;Exception while beginning fetchBlocks&quot;, e) blockIds.foreach(listener.onBlockFetchFailure(_, e)) &#125;&#125; 2.6.上传shuffle文件NettyBlockTransferService的uploadBlock方法用于上传shuffle文件到远程Executor,实际也是利用NettyBlockTransferService中创建的Netty服务,代码如下: 123456789101112131415161718192021222324252627282930313233343536373839override def uploadBlock( hostname: String, port: Int, execId: String, blockId: BlockId, blockData: ManagedBuffer, level: StorageLevel): Future[Unit] = &#123; val result = Promise[Unit]() val client = clientFactory.createClient(hostname, port) // StorageLevel is serialized as bytes using our JavaSerializer. Everything else is encoded // using our binary protocol. val levelBytes = serializer.newInstance().serialize(level).array() // Convert or copy nio buffer into array in order to serialize it. val nioBuffer = blockData.nioByteBuffer() val array = if (nioBuffer.hasArray) &#123; nioBuffer.array() &#125; else &#123; val data = new Array[Byte](nioBuffer.remaining()) nioBuffer.get(data) data &#125; client.sendRpc(new UploadBlock(appId, execId, blockId.toString, levelBytes, array).toByteBuffer, new RpcResponseCallback &#123; override def onSuccess(response: ByteBuffer): Unit = &#123; logTrace(s&quot;Successfully uploaded block $blockId&quot;) result.success((): Unit) &#125; override def onFailure(e: Throwable): Unit = &#123; logError(s&quot;Error while uploading block $blockId&quot;, e) result.failure(e) &#125; &#125;) result.future&#125; NettyBlockTransferService上传Block的步骤如下:1.创建Netty服务的客户端,客户端连接的hostname和port正是我们随机选择的BlockManager的hostname和port2.将Block的存储级别StorageLevel序列化3.将Block的ByteBuff转换为数组,便于序列化4.将appId,execId,blockId,序列化的StorageLevel,转换为数组的Block封装为uploadBlock,并将UploadBlock序列化为字节数组5.最终调用Netty客户端的sendRPC方法将字节数组上传,回调函数RPCResponseCallback根据RPC的结果更改上传状态 3.BlockManagerMaster对BlockManager的管理Driver山的BlockManagerMaster对存在于Executor山的BlockManager统一管理,比如Executor需要向Driver发送注册BlockManager,更新Executor山的Block的最新信息,询问所需要Block目前所在位置以及当Executor运行结束需要将此Executor移除等,但是Driver与Executor却位于不同机器中,该怎么实现呢?Driver上的BlockManagerMaster会持有driverEndpoint,所有Executor也会中的BlockManager中也会持有slaveEndpoint,所以Executor与Driver才可以进行关于BlockManager的交互 3.1.driverEndpointdriverEndpoint只是存在于Driver上,Executor从rpcEnv获取slaveEndpoint,然后给driverEndpoint发送消息,实现和Driver的交互,在BlockManagerMaster.driverEndpoint的各种操作代码如下: 12345678910111213141516//get操作,通过driverEndpoint去获取BlockManager的信息 def getMemoryStatus: Map[BlockManagerId, (Long, Long)] = &#123; driverEndpoint.askWithRetry[Map[BlockManagerId, (Long, Long)]](GetMemoryStatus) &#125; def getStorageStatus: Array[StorageStatus] = &#123; driverEndpoint.askWithRetry[Array[StorageStatus]](GetStorageStatus) &#125;//remove操作,也是通过操作driverEndpoint def removeRdd(rddId: Int, blocking: Boolean) &#123; val future = driverEndpoint.askWithRetry[Future[Seq[Int]]](RemoveRdd(rddId)) //... &#125; ## 3.2.询问Driver并获取恢复方法 在Executor的BlockManageMaster中,所有与Driver上BlockManagerMaster的交互方法最终都调用了askWithRetry,可见他是一个最基础的方法,因为代码如下:1234567891011121314151617181920212223242526272829def askWithRetry[T: ClassTag](message: Any, timeout: RpcTimeout): T = &#123; // TODO: Consider removing multiple attempts var attempts = 0 var lastException: Exception = null while (attempts &lt; maxRetries) &#123; attempts += 1 try &#123; val future = ask[T](message, timeout) val result = timeout.awaitResult(future) if (result == null) &#123; throw new SparkException(&quot;RpcEndpoint returned null&quot;) &#125; return result &#125; catch &#123; case ie: InterruptedException =&gt; throw ie case e: Exception =&gt; lastException = e logWarning(s&quot;Error sending message [message = $message] in $attempts attempts&quot;, e) &#125; if (attempts &lt; maxRetries) &#123; Thread.sleep(retryWaitMs) &#125; &#125; throw new SparkException( s&quot;Error sending message [message = $message]&quot;, lastException)&#125; 其中的ask方法(RpcEndpointRef中是抽象的)在NettyRpcEndpointRef类中你的实现为123override def ask[T: ClassTag](message: Any, timeout: RpcTimeout): Future[T] = &#123; nettyEnv.ask(RequestMessage(nettyEnv.address, this, message), timeout)&#125; 3.3.向BlockManagerMaster注册BlockManagerIdExecutor或者Driver自身的BlockManager在初始化时,需要向Driver的BlockManager注册BlockManager信息,代码如下 1234567/** Register the BlockManager&apos;s id with the driver. */def registerBlockManager( blockManagerId: BlockManagerId, maxMemSize: Long, slaveEndpoint: RpcEndpointRef): Unit = &#123; logInfo(&quot;Trying to register BlockManager&quot;) tell(RegisterBlockManager(blockManagerId, maxMemSize, slaveEndpoint)) logInfo(&quot;Registered BlockManager&quot;)&#125; 从上面的代码可以看到,小心内容包括BlockManagerId,最大内存,slaveEndpoint,消息体带有slaveEndpoint是为了便于接收driverEndpoint回复的消息,这些消息被封装为RegisterBlockManager,并调用刚刚在3.2节介绍的tell方法,根据之前的分析,RegisterBlockManager消息会被driverEndpoint匹配并执行register方法注册BlockManager,并在register方法自行结束后向发送者driverEndpoint发送一个简单的消息true(在spark1.6中没有看到,????) 4.磁盘块管理器DiskBlockManager4.1.DiskBlockManager的构造过程BlockManager在new的时候会创建DiskBlockManager,可以看BlockManager类,DiskBlockManager的构造步骤如下:1.调用createLocalDirs方法创建本地文件目录,然后创建二维数组subDirs,用来缓存一级目录localDirs及二级目录 5.磁盘存储DiskStore6.内存存储MemeoryStore7.Tachyon存储TachyonStore8.块管理BlockManager9.metadataCleaner和broadcastCleaner10.缓存管理器CacheManager11.压缩算法12.磁盘写入实现DiskBlockObjectWriter13.块索引shuffle管理器IndexShuffleBlockManager14.shuffle内存管理器ShuffleMemoryManager","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"第3章 SparkContext的初始化","date":"2017-04-16T04:47:25.359Z","path":"2017/04/16/bigdata/深入理解spark核心思想与源码分析/第3章 SparkContext的初始化/","text":"SparkContext的初始化是Driver应用程序提交执行的前提,本章内容以local模式为主,并按照代码执行顺序讲解 1.SparkContext概述Spark Driver用于提交用户应用程序,实际可以看做Spark的客户端,Spark Driver的初始化始终围绕着SparkContext的初始化,SparkContext可以算得上是所有Spark应用程序的发动机引擎,轿车要想跑起来,发送机首先要启动,SparkContext初始化完毕,才能向Spark集群提交任务,在平坦的公路上,发送机只需要以较低的转速,较低的功率就可以游刃有余,而在山区中,可能需要一台能够提供大功率的发动机才能满足你的需求,这些参数都是通过驾驶员操作油门,档位等传送给发送机的,而SparkContext的配置参数则由SparkConf负责,SparkConf就是你的操作面板 SparkConf的构造很简单,主要是通过ConcurrentHashMap来维护各种Spark的配置属性,SparkConf代码结构如下,spark的配置属性都是以”spark.”开头的字符串 1234567891011121314151617class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging &#123; import SparkConf._ /** Create a SparkConf that loads defaults from system properties and the classpath */ def this() = this(true) private val settings = new ConcurrentHashMap[String, String]() if (loadDefaults) &#123; //加载任何以spark.开头的系统属性 for ((key, value) &lt;- Utils.getSystemProperties if key.startsWith(&quot;spark.&quot;)) &#123; set(key, value) &#125; &#125; //其余代码省略 &#125; 下面是SparkContext的初始化步骤:1.创建Spark执行环境SparkEnv2.创建RDD清理器metadataCleaner3.创建并初始化SparkUI4.Hadoop相关配置文件及Executor环境变量的设置5.创建任务调度TaskScheduler6.创建和启动DAGScheduler7.TaskScheduler的启动8.初始化块管理器BlockManager(BlockManager是存储系统的主要组件之一,后面介绍)9.启动测量系统MetricsSystem10.创建和启动Executor分配管理器ExecutorAllocationManager11.ContextCleaner的创建与启动12.Spark环境更新13.创建DAGSchedulerSource和BlockManagerSource14.将SparkContext标记为激活 12345678910111213141516class SparkContext(config: SparkConf) extends Logging with ExecutorAllocationClient &#123; // The call site where this SparkContext was constructed. private val creationSite: CallSite = Utils.getCallSite() // If true, log warnings instead of throwing exceptions when multiple SparkContexts are active private val allowMultipleContexts: Boolean = config.getBoolean(&quot;spark.driver.allowMultipleContexts&quot;, false) // In order to prevent multiple SparkContexts from being active at the same time, mark this // context as having started construction. // NOTE: this must be placed at the beginning of the SparkContext constructor. SparkContext.markPartiallyConstructed(this, allowMultipleContexts) //省略代码... &#125; 上面的代码中:CallSite存储了线程栈中最靠近栈顶的用户类及最靠近栈低的scala或者spark核心类信息SparkContext默认只有一个实例(有属性spark.driver.allowMultipleContexts来控制,用户需要多个SparkContext实例时,可以将其设置为true),方法markPartiallyConstructed用来确保实例的唯一性,并将当前SparkContext标记为正在构建中 接下来对SparkConf进行复制,然后对各种配置信息进行校验,代码如下:123456789_conf = config.clone()_conf.validateSettings()if (!_conf.contains(&quot;spark.master&quot;)) &#123; throw new SparkException(&quot;A master URL must be set in your configuration&quot;)&#125;if (!_conf.contains(&quot;spark.app.name&quot;)) &#123; throw new SparkException(&quot;An application name must be set in your configuration&quot;)&#125; 从上面的代码看到必须制定属性spark.master和spark.app.name,否则会抛出异常,结束初始化过程,spark.master用于设置部署模式,spark.app.name用于指定应用程序名称 2.创建执行环境SparkEnvSparkEnv是Spark的执行环境对象,其中包括众多与Executor执行相关的对象,由于在local模式下Driver会创建Executor,local-cluster部署模式或者Standalone部署模式下Worker另起的CoarseGrainedExecutorBackend进行中也会创建Executor,所以SparkEnv存在于Driver或者是CoarseGrainedExecutorBackend进程中,创建SparkEnv主要使用SparkEnv的createDriverEnv,SparkEnv.createDriverEnv方法有三个参数:conf,isLocal,listenerBus1234567891011121314151617181920_env = createSparkEnv(_conf, isLocal, listenerBus)SparkEnv.set(_env)---------------------------------//下面是createSparkEnv方法的实现private[spark] def createSparkEnv( conf: SparkConf, isLocal: Boolean, listenerBus: LiveListenerBus): SparkEnv = &#123; SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master))&#125;//createSparkEnv的参数如下:_conf = config.clone()def isLocal: Boolean = (master == &quot;local&quot; || master.startsWith(&quot;local[&quot;))private[spark] val listenerBus = new LiveListenerBus---------------------------------- 上面的代码中的conf是对SparkConf的复制,isLocal标识是否是单机模式,listenerBus采用监听器模式维护各类事件的处理 SparkEnv的方法createDriverEnv最终调用create创建SparkEnv(可以一步一步的点进去看) SparkEnv的构造步骤如下:1.创建安全管理器SecurityManager2.创建基于Akka的分布式消息系统ActorSystem3.创建Map任务输出更踪器mapOutputTracker4.实例化ShuffleManager5.创建ShuffleMemoryManager6.创建块传输服务BlockTransferService7.创建BlockManagerMaster8.创建块管理器BlockManager9.创建广播管理器BroadcastManager10.创建缓存管理器CacheManger11.创建HTTP文件服务器HttpFileServer12.创建测量系统MetricsSystem13.创建SparkEnv 1234567891011121314151617181920212223//在create方法中,有下面的代码:val envInstance = new SparkEnv( executorId, rpcEnv, actorSystem, serializer, closureSerializer, cacheManager, mapOutputTracker, shuffleManager, broadcastManager, blockTransferService, blockManager, securityManager, sparkFilesDir, metricsSystem, memoryManager, outputCommitCoordinator, conf)//返回实例envInstance 2.1.安全管理器SecurityManagerSecurityManager主要对权限,账号进行设置,如果使用Hadoop Yarn作为集群管理器,则需要使用证书生成secret key登录,最后给当前系统设置默认的口令认证实例,此实例采用匿名内部类实现123456789101112131415161718private val secretKey = generateSecretKey()//使用HTTP连接设置口令认证if (authOn) &#123; Authenticator.setDefault( new Authenticator() &#123; override def getPasswordAuthentication(): PasswordAuthentication = &#123; var passAuth: PasswordAuthentication = null val userInfo = getRequestingURL().getUserInfo() if (userInfo != null) &#123; val parts = userInfo.split(&quot;:&quot;, 2) passAuth = new PasswordAuthentication(parts(0), parts(1).toCharArray()) &#125; return passAuth &#125; &#125; )&#125; 2.2.基于Akka的分布式消息系统ActorSystemActorSystem是spark中最基础的设施,spark既使用它发送分布式消息,又用它实现并发编程,消息系统可以实现并发?要解释清楚这个问题,首先应该简单介绍下scala语言的Actor并发编程模型:Scala认为java线程通过共享数据以及通过锁来维护共享数据的一致性是糟糕的做法,容易引起锁的争用,降低并发程序的性能,甚至会引起死锁的问题,在scala中需要自定义类型继承Actor,并且提供act方法,就如同Java里实现Runnable接口,需要实现run方法一样,但是不能直接调用act方法,而是通过发送消息的方式(scala发送消息时异步的)传送数据,如:1Actor!message Akka是Actor编程模型的高级类库,雷雨JDK1.5之后越来越丰富的并发工具包,简化了程序员并发编程的难度,ActorSystem便是Akka提供的用于创建分布式消息通信系统的基础类 正是因为Actor轻量级的并发编程,消息发送以及ActorSystem支持分布式消息发送等特点,Spark选择了ActorSystem SparkEnv中创建ActorSystem时用到了AkkaUtils工具类,AkkaUtils.createActorSystem,如下:1234567AkkaUtils.createActorSystem( actorSystemName + &quot;ActorSystem&quot;, hostname, actorSystemPort, conf, securityManager)._1 createActorSystem方法如下,他会调用:startServiceOnPort1234567891011def createActorSystem( name: String, host: String, port: Int, conf: SparkConf, securityManager: SecurityManager): (ActorSystem, Int) = &#123; val startService: Int =&gt; (ActorSystem, Int) = &#123; actualPort =&gt; doCreateActorSystem(name, host, actualPort, conf, securityManager) &#125; Utils.startServiceOnPort(port, startService, conf, name)&#125; startServiceOnPort中调用startService12345678910def startServiceOnPort[T]( startPort: Int, startService: Int =&gt; (T, Int), conf: SparkConf, serviceName: String = &quot;&quot;): (T, Int) = &#123; //.... val (service, port) = startService(tryPort) //...&#125; //他会调用createActorSystem中的传过来的方法 startService,所以会调用doCreateActorSystem,所以正真启动ActorSystem是由doCreateActorSystem方法完成的 Spark的Driver中Akka的默认访问地址是akka://sparkDriver,Spark的Executor中Akka的默认访问地址是 akka://sparkExecutor,如果不指定ActorSystem的端口,那么所有节点的ActorSystem端口每次启动时随机产生 2.3.map任务输出跟踪器mapOutputTrackermapOutputTracker用于跟踪map阶段任务的输出状态,此状态便于reduce阶段任务获取地址及中间输出结果,每个map任务或者reduce任务都会有唯一的标识,分别为mapId何reduceId,每个reduce任务的输入可能是多个map的输出,reduce回到各个map任务的所在节点上拉取Block,这一过程叫做Shuffle,每批Shuffle过程都有唯一的标识ShuffleId 12345val mapOutputTracker = if (isDriver) &#123; new MapOutputTrackerMaster(conf)&#125; else &#123; new MapOutputTrackerWorker(conf)&#125; 这里先介绍下MapOutputTrackerMaster,在其内部有下面的代码:12protected val mapStatuses = new TimeStampedHashMap[Int, Array[MapStatus]]()private val cachedSerializedStatuses = new TimeStampedHashMap[Int, Array[Byte]]() 其中TimeStampedHashMap[Int, Array[MapStatus]]的int是对应ShuffleId,Array存储各个map任务对应的状态信息MapStatus,由于MapStatus维护了map输出Block的地址BlockManagerId,这样reduce任务知道从何处获取map任务的中间输出123456private[spark] sealed trait MapStatus &#123; /** Location where this task was run. */ def location: BlockManagerId def getSizeForBlock(reduceId: Int): Long&#125; 同时MapOutputTrackerMaster还使用cachedSerializedStatuses:TimeStampedHashMap[Int, Array[Byte]]维护序列化后的各个map任务的输出状态,其中int对应的是ShuffleId,Array存储各个序列化MapStatus生成的字节数组 Driver和Executor处理MapOutputTrackerMaster的方式有所不同如果当前应用程序是Driver,则创建MapOutputTrackerMaster,然后创建MapOutputTrackerMasterActor,并且注册到ActorSystem中如果当前应用程序是Executor,则创建MapOutputTrackerWorker,并从ActorSystem中找到MapOutputTrackerMasterActor 无论是Driver还是Executor,最后都是由mapOutputTracker的属性trackerEndpoint持有MapOutputTrackerMasterActor的引用,如下:12345678910111213141516171819202122val mapOutputTracker = if (isDriver) &#123; new MapOutputTrackerMaster(conf)&#125; else &#123; new MapOutputTrackerWorker(conf)&#125;// Have to assign trackerActor after initialization as MapOutputTrackerActor// requires the MapOutputTracker itselfmapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(MapOutputTracker.ENDPOINT_NAME, new MapOutputTrackerMasterEndpoint(rpcEnv, mapOutputTracker.asInstanceOf[MapOutputTrackerMaster], conf))//def registerOrLookupEndpoint( name: String, endpointCreator: =&gt; RpcEndpoint): RpcEndpointRef = &#123; if (isDriver) &#123; logInfo(&quot;Registering &quot; + name) rpcEnv.setupEndpoint(name, endpointCreator) &#125; else &#123; RpcUtils.makeDriverRef(name, conf, rpcEnv) &#125;&#125; 后面的章节就会知道map任务的状态正是由Executor向持有的MapOutputTrackerMasterActor发送消息,将map任务状态同步到MapOutputTracker的MapStatuses和cachedSerializedStatuses的,Executor究竟是如何找到MapOutputTrackerMasterActor的?registerOrLookupEndpoint方法通过RpcUtils.makeDriverRef找到MapOutputTrackerMasterActor,实际正是利用ActorSystem提供的分布式消息机制实现的 2.4.实例化ShuffleManagerShuffleManager负责管理本地及远程的block数据的shuffle操作,ShuffleManager默认为通过反射方法生成的SortShuffleManager的实例,例如可以修改属性spark.shuffle.manager为hash来显示控制使用HashShuffleManager1234567891011121314151617181920212223242526//下面是几种ShuffleManager的短名称和类名的映射val shortShuffleMgrNames = Map( &quot;hash&quot; -&gt; &quot;org.apache.spark.shuffle.hash.HashShuffleManager&quot;, &quot;sort&quot; -&gt; &quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;, &quot;tungsten-sort&quot; -&gt; &quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;)//从配置文件中获取,是否有配置 spark.shuffle.managerval shuffleMgrName = conf.get(&quot;spark.shuffle.manager&quot;, &quot;sort&quot;)//得到ShuffleManager的类名val shuffleMgrClass = shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase, shuffleMgrName)//根据类名反射val shuffleManager = instantiateClass[ShuffleManager](shuffleMgrClass)//instantiateClass的方法内容def instantiateClass[T](className: String): T = &#123; val cls = Utils.classForName(className) try &#123; cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE) .newInstance(conf, new java.lang.Boolean(isDriver)) .asInstanceOf[T] &#125; //....&#125; SortShuffleManager通过持有的IndexShuffleBlockResolver间接BlockManager中的DiskBlockManager将map结果写入本地,并根据shuffleId,mapId写入索引文件,也能通过MapOutputTrackerMaster中维护的MapStatuses从本地或者其他远程节点读取文件,有人会问,为什么需要shuffle?spark作为并行计算框架,同一个作业会被划分为多个任务在多个节点上并行执行,reduce的输入可能存在于多个节点上,因此需要通过”洗牌”将所有的reduce的输入汇总起来,这个过程就是shuffle 2.5.shuffle线程内存管理器ShuffleMemoryManager在我阅读spark1.6的时候,没有看到作者指定的ShuffleMemoryManager类,在spark1.6的源码中是这样的:12345678//使用传统内存管理器val useLegacyMemoryManager = conf.getBoolean(&quot;spark.memory.useLegacyMode&quot;, false)val memoryManager: MemoryManager = if (useLegacyMemoryManager) &#123; new StaticMemoryManager(conf, numUsableCores) //静态内存管理器 &#125; else &#123; UnifiedMemoryManager(conf, numUsableCores) //统一内存管理器 &#125; 如果配置了传统的内存管理器,代码实现如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344private[spark] class StaticMemoryManager( conf: SparkConf, maxOnHeapExecutionMemory: Long, override val maxStorageMemory: Long, numCores: Int) extends MemoryManager( conf, numCores, maxStorageMemory, maxOnHeapExecutionMemory) &#123; def this(conf: SparkConf, numCores: Int) &#123; this( conf, StaticMemoryManager.getMaxExecutionMemory(conf), StaticMemoryManager.getMaxStorageMemory(conf), numCores) &#125;//....&#125;其中两个重要的方法实现如下: /** * Return the total amount of memory available for the storage region, in bytes. */private def getMaxStorageMemory(conf: SparkConf): Long = &#123; val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory) val memoryFraction = conf.getDouble(&quot;spark.storage.memoryFraction&quot;, 0.6) val safetyFraction = conf.getDouble(&quot;spark.storage.safetyFraction&quot;, 0.9) (systemMaxMemory * memoryFraction * safetyFraction).toLong&#125;/** * Return the total amount of memory available for the execution region, in bytes. */private def getMaxExecutionMemory(conf: SparkConf): Long = &#123; val systemMaxMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory) val memoryFraction = conf.getDouble(&quot;spark.shuffle.memoryFraction&quot;, 0.2) val safetyFraction = conf.getDouble(&quot;spark.shuffle.safetyFraction&quot;, 0.8) (systemMaxMemory * memoryFraction * safetyFraction).toLong&#125; 没有配置传统内存管理器,使用的是统一内存管理器的代码如下:123456789101112131415161718192021222324252627282930def apply(conf: SparkConf, numCores: Int): UnifiedMemoryManager = &#123; val maxMemory = getMaxMemory(conf) new UnifiedMemoryManager( conf, maxMemory = maxMemory, storageRegionSize = (maxMemory * conf.getDouble(&quot;spark.memory.storageFraction&quot;, 0.5)).toLong, numCores = numCores)&#125;//而getMaxMemory的方法实现如下:/** * Return the total amount of memory shared between execution and storage, in bytes. */private def getMaxMemory(conf: SparkConf): Long = &#123; val systemMemory = conf.getLong(&quot;spark.testing.memory&quot;, Runtime.getRuntime.maxMemory) //保留内存 val reservedMemory = conf.getLong(&quot;spark.testing.reservedMemory&quot;, if (conf.contains(&quot;spark.testing&quot;)) 0 else RESERVED_SYSTEM_MEMORY_BYTES) val minSystemMemory = reservedMemory * 1.5 if (systemMemory &lt; minSystemMemory) &#123; throw new IllegalArgumentException(s&quot;System memory $systemMemory must &quot; + s&quot;be at least $minSystemMemory. Please use a larger heap size.&quot;) &#125; val usableMemory = systemMemory - reservedMemory //内存百分比 val memoryFraction = conf.getDouble(&quot;spark.memory.fraction&quot;, 0.75) (usableMemory * memoryFraction).toLong&#125; 2.6.块传输服务BlockTransferServiceBlockTransferService使用的是NettyBlockTransferService,他使用Netty提供的异步事件驱动的网络应用框架,提供web服务及客户端,获取远程节点上Block的集合 1val blockTransferService = new NettyBlockTransferService(conf, securityManager, numUsableCores) NettyBlockTransferService的具体实现在第4章有详细介绍 2.7.BlockManagerMaster介绍BlockManagerMaster负责对Block的管理和协调,具体操作依赖于BlockManagerMasterEndpoint,Driver和Executor处理BlockManagerMaster的方式不同如果当前应用程序是Driver,则创建BlockManagerMasterEndpoint,并且注册到ActorSystem中,如果当前应用程序是Executor,则从ActorSystem中找到BlockManagerMasterEndpoint. 无论是Driver还是Executor,最后BlockManagerMaster的属性driverEndpoint将持有对BlockManagerMasterEndpoint的引用,BlockManagerMaster的创建代码如下: 12345val blockManagerMaster = new BlockManagerMaster( registerOrLookupEndpoint(BlockManagerMaster.DRIVER_ENDPOINT_NAME, new BlockManagerMasterEndpoint(rpcEnv, isLocal, conf, listenerBus)), conf, isDriver) registerOrLookupEndpoint在2.3节有介绍,不再详述 2.8.创建块管理器BlockManagerBlockManager负责对Block的管理,只有在BlockManager的初始化initialize被调用后,他才是有效的,具体实现见第4章 123// NB: blockManager is not valid until initialize() is called later.val blockManager = new BlockManager(executorId, rpcEnv, blockManagerMaster, serializer, conf, memoryManager, mapOutputTracker, shuffleManager, blockTransferService, securityManager, numUsableCores) 2.9.创建广播管理器BroadcastManagerBroadcastManager用于将配置信息和序列化后的RDD,Job以及ShuffleDependency等信息在本地存储,如果为了容灾,也会复制到其他节点上,实现代码如下: 1val broadcastManager = new BroadcastManager(isDriver, conf, securityManager) BroadcastManager必须在其初始化方法initialize被调用后才能生效,initialize方法实际利用反射生成广播工厂实例broadcastFactory(可以配置属性:spark.broadcast.factory 1234567891011121314151617// Called by SparkContext or Executor before using Broadcastprivate def initialize() &#123; synchronized &#123; if (!initialized) &#123; val broadcastFactoryClass = conf.get(&quot;spark.broadcast.factory&quot;, &quot;org.apache.spark.broadcast.TorrentBroadcastFactory&quot;) broadcastFactory = Utils.classForName(broadcastFactoryClass).newInstance.asInstanceOf[BroadcastFactory] // Initialize appropriate BroadcastFactory and BroadcastObject broadcastFactory.initialize(isDriver, conf, securityManager) initialized = true &#125; &#125;&#125; BroadcastManager的newBroadcast实际代理了工厂的newBroadcast方法来生成广播对象,代理unbroadcast来生成非广播对象 1234567def newBroadcast[T: ClassTag](value_ : T, isLocal: Boolean): Broadcast[T] = &#123; broadcastFactory.newBroadcast[T](value_, isLocal, nextBroadcastId.getAndIncrement())&#125;def unbroadcast(id: Long, removeFromDriver: Boolean, blocking: Boolean) &#123; broadcastFactory.unbroadcast(id, removeFromDriver, blocking)&#125; 2.10.创建缓存管理器CacheManagerCacheManager用于缓存RDD某个分区计算后的中间结果,缓存计算结果发生在迭代计算的时候,在6.1节会讲到,而CacheManager将在4.10节详细描述,创建CacheManager的代码如下:1val cacheManager = new CacheManager(blockManager) 2.11.HTTP文件服务器httpFileServer在spark1.6中是下面的代码实现 12345678// Set the sparkFiles directory, used when downloading dependencies. // In local mode, this is a temporary directory; // in distributed mode, this is the executor&apos;s current working directory.val sparkFilesDir: String = if (isDriver) &#123; Utils.createTempDir(Utils.getLocalDir(conf), &quot;userFiles&quot;).getAbsolutePath&#125; else &#123; &quot;.&quot;&#125; HttpFileServer的初始化过程代码如下:123456789101112def initialize() &#123; baseDir = Utils.createTempDir(Utils.getLocalDir(conf), &quot;httpd&quot;) fileDir = new File(baseDir, &quot;files&quot;) jarDir = new File(baseDir, &quot;jars&quot;) fileDir.mkdir() jarDir.mkdir() logInfo(&quot;HTTP File server directory is &quot; + baseDir) httpServer = new HttpServer(conf, baseDir, securityManager, requestedPort, &quot;HTTP file server&quot;) httpServer.start() serverUri = httpServer.uri logDebug(&quot;HTTP file server started at: &quot; + serverUri)&#125; 包括下面的步骤:1.使用Utils工具类创建文件服务器的根目录及临时目录(临时目录在运行环境关闭时会删除)2.创建存放jar包及其他文件的文件目录3.创建并启动Http服务 httpServer的构造和start方法的实现如下:1234567891011def start() &#123; if (server != null) &#123; throw new ServerStateException(&quot;Server is already started&quot;) &#125; else &#123; logInfo(&quot;Starting HTTP Server&quot;) val (actualServer, actualPort) = Utils.startServiceOnPort[Server](requestedPort, doStart, conf, serverName) server = actualServer port = actualPort &#125;&#125; 用到了 Utils.startServiceOnPort方法,因此会回调doStart方法,在doStart方法总内嵌了Jetty所提供的HTTP服务 2.12.创建测量系统MetricsSystemMetricsSystem是spark的测量系统,创建代码如下:1234567891011121314val metricsSystem = if (isDriver) &#123; // Don&apos;t start metrics system right now for Driver. // We need to wait for the task scheduler to give us an app ID. // Then we can start the metrics system. MetricsSystem.createMetricsSystem(&quot;driver&quot;, conf, securityManager)&#125; else &#123; // We need to set the executor ID before the MetricsSystem is created because sources and // sinks specified in the metrics configuration file will want to incorporate this executor&apos;s // ID into the metrics they report. conf.set(&quot;spark.executor.id&quot;, executorId) val ms = MetricsSystem.createMetricsSystem(&quot;executor&quot;, conf, securityManager) ms.start() ms&#125; 上面调用的createMetricsSystem方法实际创建了MetricsSystem,如下:1234def createMetricsSystem( instance: String, conf: SparkConf, securityMgr: SecurityManager): MetricsSystem = &#123; new MetricsSystem(instance, conf, securityMgr)&#125; 构造MetricsSystem的过程最重要的是调用了MetricsConfig.initialize()方法1234567891011121314151617181920212223def initialize() &#123; // Add default properties in case there&apos;s no properties file setDefaultProperties(properties) loadPropertiesFromFile(conf.getOption(&quot;spark.metrics.conf&quot;)) // Also look for the properties in provided Spark configuration val prefix = &quot;spark.metrics.conf.&quot; conf.getAll.foreach &#123; case (k, v) if k.startsWith(prefix) =&gt; properties.setProperty(k.substring(prefix.length()), v) case _ =&gt; &#125; propertyCategories = subProperties(properties, INSTANCE_REGEX) if (propertyCategories.contains(DEFAULT_PREFIX)) &#123; val defaultProperty = propertyCategories(DEFAULT_PREFIX).asScala for((inst, prop) &lt;- propertyCategories if (inst != DEFAULT_PREFIX); (k, v) &lt;- defaultProperty if (prop.get(k) == null)) &#123; prop.put(k, v) &#125; &#125;&#125; 从以上实现可以看出,MetricsConfig的initialize方法主要负责加载metrics.properties文件中的属性配置,并对属性进行初始化转换,变成Map 2.13.创建SparkEnv当所有的基础组件准备好后,最终使用下面的代码创建执行环境SparkEnv12345678910111213141516171819val envInstance = new SparkEnv( executorId, rpcEnv, actorSystem, serializer, closureSerializer, cacheManager, mapOutputTracker, shuffleManager, broadcastManager, blockTransferService, blockManager, securityManager, sparkFilesDir, metricsSystem, memoryManager, outputCommitCoordinator, conf) 3.创建metadtaCleaner我们回到SparkContext类中,如下代码:12345// Create the Spark execution environment (cache, map output tracker, etc)_env = createSparkEnv(_conf, isLocal, listenerBus)SparkEnv.set(_env)_metadataCleaner = new MetadataCleaner(MetadataCleanerType.SPARK_CONTEXT, this.cleanup, _conf) 我们发现在创建完SparkEnv之后,接下来是创建MetadataCleaner sparkContext为了保持对所有持久化的RDD的跟踪,使用类型是TimeStampedWeakValueHashMap的persistentRDDs缓存,metadataCleaner的功能是清除过期的持久化RDD,创建metadataCleaner的代码如下:1_metadataCleaner = new MetadataCleaner(MetadataCleanerType.SPARK_CONTEXT, this.cleanup, _conf) 注意上面的代码中会传给MetadataCleaner构造器一个函数cleanup,cleanup的代码如下:1234/** Called by MetadataCleaner to clean up the persistentRdds map periodically */private[spark] def cleanup(cleanupTime: Long) &#123; persistentRdds.clearOldValues(cleanupTime)&#125; 我们来看下MetadataCleaner构造器的代码:1234567891011121314151617181920212223242526272829303132333435363738394041/** * Runs a timer task to periodically clean up metadata (e.g. old files or hashtable entries) */private[spark] class MetadataCleaner( cleanerType: MetadataCleanerType.MetadataCleanerType, cleanupFunc: (Long) =&gt; Unit,//cleanup方法会传递过来 conf: SparkConf) extends Logging&#123; val name = cleanerType.toString private val delaySeconds = MetadataCleaner.getDelaySeconds(conf, cleanerType) private val periodSeconds = math.max(10, delaySeconds / 10) private val timer = new Timer(name + &quot; cleanup timer&quot;, true) //将cleanup方法封装成一个task,然后去定时执行 private val task = new TimerTask &#123; override def run() &#123; try &#123; cleanupFunc(System.currentTimeMillis() - (delaySeconds * 1000)) logInfo(&quot;Ran metadata cleaner for &quot; + name) &#125; catch &#123; case e: Exception =&gt; logError(&quot;Error running cleanup task for &quot; + name, e) &#125; &#125; &#125; if (delaySeconds &gt; 0) &#123; logDebug( &quot;Starting metadata cleaner for &quot; + name + &quot; with delay of &quot; + delaySeconds + &quot; seconds &quot; + &quot;and period of &quot; + periodSeconds + &quot; secs&quot;) /启动定时任务 timer.schedule(task, delaySeconds * 1000, periodSeconds * 1000) &#125; def cancel() &#123; timer.cancel() &#125;&#125; 从上面的代码可以看出MetadataCleaner实质上是一个用TimerTask实现的定时器,不断的调用cleanupFunc这样的函数,在构造metadataCleaner时的函数参数是cleanup,用于清理persistentRdds的过期内容 4.SparkUI详解任何系统都需要提供监控系统,用浏览器能访问具有样式及布局并提供丰富监控数据的页面无疑是一种简单,高效的方式. 在大型分布式系统中,采用事件监听机制是最常见的,为何要使用事件监听机制,加入SparkUI采用scala的函数调用方式,那么随着整个集群规模的增加,对函数的调用会越来越多,最终会受到Driver所在JVM的线程数量限制而影响监控数据的更新,甚至出现监控数据无法及时显示给用户的情况,由于函数调用多数情况下是同步调用,这就导致现场被阻塞,在分布式环境中,还可能因为网络问题,导致线程被长时间占用,将函数调用更换为发送事件,事件的处理是异步的,当前线程可以继续执行后续逻辑,线程池中的线程还可以被重用,这样整个系统的并发度会大大增加,发送的事件会存入缓存,由定时调度器取出后,分配给监听此事件的监听器对监控数据进行更新 SparkUI就是这样的服务,他的架构如下图: 我们首先简单介绍图中的各个组件: DAGScheduler:主要的产生各类SparkListenerEvent的源头,他将各种SparkListenEvent发送到listenBus的事件队列中 listenBus通过定时器将SparkListenerEvent事件匹配到具体的SparkListener,改变SparkListener中的统计监控数据,最终由SparkUI的界面展示 图中还可以看到Spark里面定义了很多监听器SparkListener的时间,包括JobProgressListener,EnvironmentListener,StorageListener,ExecutorsListenter,他们的继承体系如下图: 4.1.listenerBus详解listenerBus的类型是LiveListenerBus,LiveListenerBus实现了监听器模型,通过监听事件触发对各种监听器状态信息的修改,达到UI界面的数据刷新效果,LiveListenerBus由以下部分组成: 事件阻塞队列:类型为LinkedBlockingQueue[SparkListenerEvent],固定大小为10000 监听器数组:类型为CopyOnWriteArrayList,存放各类监听器SparkListener 123456789101112131415161718192021222324252627282930313233343536373839private[spark] class LiveListenerBus extends AsynchronousListenerBus[SparkListener, SparkListenerEvent](&quot;SparkListenerBus&quot;) with SparkListenerBus &#123;//...&#125;private[spark] abstract class AsynchronousListenerBus[L &lt;: AnyRef, E](name: String) extends ListenerBus[L, E] &#123; self =&gt; private var sparkContext: SparkContext = null private val EVENT_QUEUE_CAPACITY = 10000 //事件阻塞队列 private val eventQueue = new LinkedBlockingQueue[E](EVENT_QUEUE_CAPACITY)//...&#125;//private[spark] trait ListenerBus[L &lt;: AnyRef, E] extends Logging &#123; //监听器数组 private[spark] val listeners = new CopyOnWriteArrayList[L] /** * Add a listener to listen events. This method is thread-safe and can be called in any thread. */ final def addListener(listener: L) &#123; listeners.add(listener) &#125;//....&#125; 事件匹配监听器的线程:此Thread不断拉取LinkedBlockingQueue中的事件,遍历监听器,调用监听器的方法,任何事件都会在LinkedBlockingQueue中存在一段时间,然后Thread处理了此事件后,会将其清除,因此使用listenerBus这个名字再合适不过了,到站就下车,listenerBus的实现如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980private val eventLock = new Semaphore(0)private val listenerThread = new Thread(name) &#123; setDaemon(true) override def run(): Unit = Utils.tryOrStopSparkContext(sparkContext) &#123; AsynchronousListenerBus.withinListenerThread.withValue(true) &#123; while (true) &#123; eventLock.acquire() self.synchronized &#123; processingEvent = true &#125; try &#123; val event = eventQueue.poll if (event == null) &#123; // Get out of the while loop and shutdown the daemon thread if (!stopped.get) &#123; throw new IllegalStateException(&quot;Polling `null` from eventQueue means&quot; + &quot; the listener bus has been stopped. So `stopped` must be true&quot;) &#125; return &#125; postToAll(event) &#125; finally &#123; self.synchronized &#123; processingEvent = false &#125; &#125; &#125; &#125; &#125;&#125;/** * Start sending events to attached listeners. * * This first sends out all buffered events posted before this listener bus has started, then * listens for any additional events asynchronously while the listener bus is still running. * This should only be called once. * * @param sc Used to stop the SparkContext in case the listener thread dies. */def start(sc: SparkContext) &#123; if (started.compareAndSet(false, true)) &#123; sparkContext = sc listenerThread.start() &#125; else &#123; throw new IllegalStateException(s&quot;$name already started!&quot;) &#125;&#125;def post(event: E) &#123; if (stopped.get) &#123; // Drop further events to make `listenerThread` exit ASAP logError(s&quot;$name has already stopped! Dropping event $event&quot;) return &#125; val eventAdded = eventQueue.offer(event) if (eventAdded) &#123; eventLock.release() &#125; else &#123; onDropEvent(event) &#125;&#125;def listenerThreadIsAlive: Boolean = listenerThread.isAlive def stop() &#123; if (!started.get()) &#123; throw new IllegalStateException(s&quot;Attempted to stop $name that has not yet started!&quot;) &#125; if (stopped.compareAndSet(false, true)) &#123; // Call eventLock.release() so that listenerThread will poll `null` from `eventQueue` and know // `stop` is called. eventLock.release() listenerThread.join() &#125; else &#123; // Keep quiet &#125;&#125; LiveListenerBus中调用的postToAll方法实际定义在父类SparkListenerBus中,如下:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758//在类ListenerBus中: final def postToAll(event: E): Unit = &#123; //这里是遍历所有的listeners val iter = listeners.iterator while (iter.hasNext) &#123; val listener = iter.next() try &#123; onPostEvent(listener, event) &#125; catch &#123; case NonFatal(e) =&gt; logError(s&quot;Listener $&#123;Utils.getFormattedClassName(listener)&#125; threw an exception&quot;, e) &#125; &#125; &#125;//而onPostEvent的实现是在SparkListenerBus实现的,如下 override def onPostEvent(listener: SparkListener, event: SparkListenerEvent): Unit = &#123; event match &#123; case stageSubmitted: SparkListenerStageSubmitted =&gt; listener.onStageSubmitted(stageSubmitted) case stageCompleted: SparkListenerStageCompleted =&gt; listener.onStageCompleted(stageCompleted) case jobStart: SparkListenerJobStart =&gt; listener.onJobStart(jobStart) case jobEnd: SparkListenerJobEnd =&gt; listener.onJobEnd(jobEnd) case taskStart: SparkListenerTaskStart =&gt; listener.onTaskStart(taskStart) case taskGettingResult: SparkListenerTaskGettingResult =&gt; listener.onTaskGettingResult(taskGettingResult) case taskEnd: SparkListenerTaskEnd =&gt; listener.onTaskEnd(taskEnd) case environmentUpdate: SparkListenerEnvironmentUpdate =&gt; listener.onEnvironmentUpdate(environmentUpdate) case blockManagerAdded: SparkListenerBlockManagerAdded =&gt; listener.onBlockManagerAdded(blockManagerAdded) case blockManagerRemoved: SparkListenerBlockManagerRemoved =&gt; listener.onBlockManagerRemoved(blockManagerRemoved) case unpersistRDD: SparkListenerUnpersistRDD =&gt; listener.onUnpersistRDD(unpersistRDD) case applicationStart: SparkListenerApplicationStart =&gt; listener.onApplicationStart(applicationStart) case applicationEnd: SparkListenerApplicationEnd =&gt; listener.onApplicationEnd(applicationEnd) case metricsUpdate: SparkListenerExecutorMetricsUpdate =&gt; listener.onExecutorMetricsUpdate(metricsUpdate) case executorAdded: SparkListenerExecutorAdded =&gt; listener.onExecutorAdded(executorAdded) case executorRemoved: SparkListenerExecutorRemoved =&gt; listener.onExecutorRemoved(executorRemoved) case blockUpdated: SparkListenerBlockUpdated =&gt; listener.onBlockUpdated(blockUpdated) case logStart: SparkListenerLogStart =&gt; // ignore event log metadata &#125; &#125; 其实上的过程就是将对应的事件发送到对应的Listener进行处理 4.2.构造JobProgressListener我们以JobProgressListener为例来讲解SparkListener,JobProgressListener是SparkContext中一个重要的组成部分(在SparkContext代码中可以看到),通过监听listenBus中的事件更新任务进度,SparkStatusTracker和SparkUI实际上也是通过JobProgressListener来实现任务状态跟踪的,在SparkContext中创建JobProgressListener的代码如下:123456 // &quot;_jobProgressListener&quot; should be set up before creating SparkEnv because when creating // &quot;SparkEnv&quot;, some messages will be posted to &quot;listenerBus&quot; and we should not miss them. _jobProgressListener = new JobProgressListener(_conf) listenerBus.addListener(jobProgressListener)_statusTracker = new SparkStatusTracker(this) JobProgressListener的作用是通过HashMap,ListBuffer等数据结构存储JobId及对应的JobUIData信息,并按照激活,完成,失败等job状态统计,对stageId,stageInfo等信息按照激活,完成,忽略,失败等Stage状态统计,并且存储StageId与jobId的一对多关系,这些统计信息最终会被JobPage和StagePage等页面访问和渲染,JobProgressListener的数据结构如下:1234567891011121314151617181920212223242526272829303132333435363738394041class JobProgressListener(conf: SparkConf) extends SparkListener with Logging &#123; // Define a handful of type aliases so that data structures&apos; types can serve as documentation. // These type aliases are public because they&apos;re used in the types of public fields: type JobId = Int type JobGroupId = String type StageId = Int type StageAttemptId = Int type PoolName = String type ExecutorId = String // Application: @volatile var startTime = -1L @volatile var endTime = -1L // Jobs: val activeJobs = new HashMap[JobId, JobUIData] val completedJobs = ListBuffer[JobUIData]() val failedJobs = ListBuffer[JobUIData]() val jobIdToData = new HashMap[JobId, JobUIData] val jobGroupToJobIds = new HashMap[JobGroupId, HashSet[JobId]] // Stages: val pendingStages = new HashMap[StageId, StageInfo] val activeStages = new HashMap[StageId, StageInfo] val completedStages = ListBuffer[StageInfo]() val skippedStages = ListBuffer[StageInfo]() val failedStages = ListBuffer[StageInfo]() val stageIdToData = new HashMap[(StageId, StageAttemptId), StageUIData] val stageIdToInfo = new HashMap[StageId, StageInfo] val stageIdToActiveJobIds = new HashMap[StageId, HashSet[JobId]] val poolToActiveStages = HashMap[PoolName, HashMap[StageId, StageInfo]]() // Total of completed and failed stages that have ever been run. These may be greater than // `completedStages.size` and `failedStages.size` if we have run more stages or jobs than // JobProgressListener&apos;s retention limits. var numCompletedStages = 0 var numFailedStages = 0 var numCompletedJobs = 0 var numFailedJobs = 0//... JobProgressListener实现了onJobStart,onJobEnd,onStageCompleted,onStageSubmitted,onTashStart,onTaskEnd等方法,这些方法正是在listenBus的驱动下,改变JobProgressListener中的各种Job,Stage相关的数据 4.3.SparkUI的创建与初始化在SparkContext中接下来的是SparkUI的创建 123456789101112_ui = if (conf.getBoolean(&quot;spark.ui.enabled&quot;, true)) &#123; Some(SparkUI.createLiveUI(this, _conf, listenerBus, _jobProgressListener, _env.securityManager, appName, startTime = startTime)) &#125; else &#123; // For tests, do not enable the UI None &#125;// Bind the UI before starting the task scheduler to communicate// the bound port to the cluster manager properly_ui.foreach(_.bind()) 可以看到如果不需要提供SparkUI服务,可以将属性spark.ui.enabled修改为false,其中createLiveUI实际是调用create方法,如下:1234567891011def createLiveUI( sc: SparkContext, conf: SparkConf, listenerBus: SparkListenerBus, jobProgressListener: JobProgressListener, securityManager: SecurityManager, appName: String, startTime: Long): SparkUI = &#123; create(Some(sc), conf, listenerBus, securityManager, appName, jobProgressListener = Some(jobProgressListener), startTime = startTime)&#125; 而create方法的实现如下:12345678910111213141516171819202122232425262728293031323334 private def create( sc: Option[SparkContext], conf: SparkConf, listenerBus: SparkListenerBus, securityManager: SecurityManager, appName: String, basePath: String = &quot;&quot;, jobProgressListener: Option[JobProgressListener] = None, startTime: Long): SparkUI = &#123; val _jobProgressListener: JobProgressListener = jobProgressListener.getOrElse &#123; val listener = new JobProgressListener(conf) listenerBus.addListener(listener) listener &#125; val environmentListener = new EnvironmentListener val storageStatusListener = new StorageStatusListener val executorsListener = new ExecutorsListener(storageStatusListener) val storageListener = new StorageListener(storageStatusListener) val operationGraphListener = new RDDOperationGraphListener(conf) listenerBus.addListener(environmentListener) listenerBus.addListener(storageStatusListener) listenerBus.addListener(executorsListener) listenerBus.addListener(storageListener) listenerBus.addListener(operationGraphListener)创建SparkUI new SparkUI(sc, conf, securityManager, environmentListener, storageStatusListener, executorsListener, _jobProgressListener, storageListener, operationGraphListener, appName, basePath, startTime) &#125; 在上述的代码中可以看到,在create方法里除了JobProgressListener是外部传入的之外,又增加了一些SparkListener,例如,用于对JVM参数,Spark属性,java系统属性,classpath等进行监控的EnvironmentListener;用于维护Executor的存储状态的StorageStatusListener;用于准备将Executor的信息展示在ExecutorsTab的ExecutorsListener;用于准备将Executor相关存储信息展示在BlockManagerUI的StorageListener等, 最后创建SparkUI,SparkUI服务默认是可以被杀掉的,通过修改属性spark.ui.killEnabled为false,可以保证不被杀死,initialize方法会组织前端页面各个Tab和Page的展示及布局: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546private[spark] class SparkUI private ( val sc: Option[SparkContext], val conf: SparkConf, securityManager: SecurityManager, val environmentListener: EnvironmentListener, val storageStatusListener: StorageStatusListener, val executorsListener: ExecutorsListener, val jobProgressListener: JobProgressListener, val storageListener: StorageListener, val operationGraphListener: RDDOperationGraphListener, var appName: String, val basePath: String, val startTime: Long) extends WebUI(securityManager, SparkUI.getUIPort(conf), conf, basePath, &quot;SparkUI&quot;) with Logging with UIRoot &#123; val killEnabled = sc.map(_.conf.getBoolean(&quot;spark.ui.killEnabled&quot;, true)).getOrElse(false) val stagesTab = new StagesTab(this) var appId: String = _ /** Initialize all components of the server. */ def initialize() &#123; attachTab(new JobsTab(this)) attachTab(stagesTab) attachTab(new StorageTab(this)) attachTab(new EnvironmentTab(this)) attachTab(new ExecutorsTab(this)) attachHandler(createStaticHandler(SparkUI.STATIC_RESOURCE_DIR, &quot;/static&quot;)) attachHandler(createRedirectHandler(&quot;/&quot;, &quot;/jobs/&quot;, basePath = basePath)) attachHandler(ApiRootResource.getServletHandler(this)) // This should be POST only, but, the YARN AM proxy won&apos;t proxy POSTs attachHandler(createRedirectHandler( &quot;/stages/stage/kill&quot;, &quot;/stages/&quot;, stagesTab.handleKillRequest, httpMethods = Set(&quot;GET&quot;, &quot;POST&quot;))) &#125; //初始化 initialize()///....&#125; 4.4.Spark UI的页面布局与展示SparkUI究竟是如何实现页面布局及展示的?JobsTab展示所有的Job的进度,状态信息,这里我们以他为例来说明,JobsTab会复用SparkUI的killEnabled,SparkContext,jobProgressListener,包括AllJobsPage和JobPage两个页面,代码如下:12345678910111213private[ui] class JobsTab(parent: SparkUI) extends SparkUITab(parent, &quot;jobs&quot;) &#123; val sc = parent.sc val killEnabled = parent.killEnabled val jobProgresslistener = parent.jobProgressListener val executorListener = parent.executorsListener val operationGraphListener = parent.operationGraphListener def isFairScheduler: Boolean = jobProgresslistener.schedulingMode.exists(_ == SchedulingMode.FAIR) attachPage(new AllJobsPage(this)) attachPage(new JobPage(this))&#125; AllJobsPage由render方法渲染,利用JobProgressListener中的统计监控数据生成:激活,完成,失败等job的状态摘要信息,并调用jobsTable方法生成表格等html元素,最终使用UIUtils的headerSparkPage封装好css,js,header及页面布局等,代码如下: 下面是AllJobsPage.render方法1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495def render(request: HttpServletRequest): Seq[Node] = &#123; val listener = parent.jobProgresslistener listener.synchronized &#123; val startTime = listener.startTime val endTime = listener.endTime val activeJobs = listener.activeJobs.values.toSeq val completedJobs = listener.completedJobs.reverse.toSeq val failedJobs = listener.failedJobs.reverse.toSeq val activeJobsTable = jobsTable(activeJobs.sortBy(_.submissionTime.getOrElse(-1L)).reverse) val completedJobsTable = jobsTable(completedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val failedJobsTable = jobsTable(failedJobs.sortBy(_.completionTime.getOrElse(-1L)).reverse) val shouldShowActiveJobs = activeJobs.nonEmpty val shouldShowCompletedJobs = completedJobs.nonEmpty val shouldShowFailedJobs = failedJobs.nonEmpty val completedJobNumStr = if (completedJobs.size == listener.numCompletedJobs) &#123; s&quot;$&#123;completedJobs.size&#125;&quot; &#125; else &#123; s&quot;$&#123;listener.numCompletedJobs&#125;, only showing $&#123;completedJobs.size&#125;&quot; &#125; val summary: NodeSeq = &lt;div&gt; &lt;ul class=&quot;unstyled&quot;&gt; &lt;li&gt; &lt;strong&gt;Total Uptime:&lt;/strong&gt; &#123; if (endTime &lt; 0 &amp;&amp; parent.sc.isDefined) &#123; UIUtils.formatDuration(System.currentTimeMillis() - startTime) &#125; else if (endTime &gt; 0) &#123; UIUtils.formatDuration(endTime - startTime) &#125; &#125; &lt;/li&gt; &lt;li&gt; &lt;strong&gt;Scheduling Mode: &lt;/strong&gt; &#123;listener.schedulingMode.map(_.toString).getOrElse(&quot;Unknown&quot;)&#125; &lt;/li&gt; &#123; if (shouldShowActiveJobs) &#123; &lt;li&gt; &lt;a href=&quot;#active&quot;&gt;&lt;strong&gt;Active Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;activeJobs.size&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowCompletedJobs) &#123; &lt;li id=&quot;completed-summary&quot;&gt; &lt;a href=&quot;#completed&quot;&gt;&lt;strong&gt;Completed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;completedJobNumStr&#125; &lt;/li&gt; &#125; &#125; &#123; if (shouldShowFailedJobs) &#123; &lt;li&gt; &lt;a href=&quot;#failed&quot;&gt;&lt;strong&gt;Failed Jobs:&lt;/strong&gt;&lt;/a&gt; &#123;listener.numFailedJobs&#125; &lt;/li&gt; &#125; &#125; &lt;/ul&gt; &lt;/div&gt; var content = summary val executorListener = parent.executorListener content ++= makeTimeline(activeJobs ++ completedJobs ++ failedJobs, executorListener.executorIdToData, startTime) if (shouldShowActiveJobs) &#123; content ++= &lt;h4 id=&quot;active&quot;&gt;Active Jobs (&#123;activeJobs.size&#125;)&lt;/h4&gt; ++ activeJobsTable &#125; if (shouldShowCompletedJobs) &#123; content ++= &lt;h4 id=&quot;completed&quot;&gt;Completed Jobs (&#123;completedJobNumStr&#125;)&lt;/h4&gt; ++ completedJobsTable &#125; if (shouldShowFailedJobs) &#123; content ++= &lt;h4 id =&quot;failed&quot;&gt;Failed Jobs (&#123;failedJobs.size&#125;)&lt;/h4&gt; ++ failedJobsTable &#125; val helpText = &quot;&quot;&quot;A job is triggered by an action, like count() or saveAsTextFile().&quot;&quot;&quot; + &quot; Click on a job to see information about the stages of tasks inside it.&quot; UIUtils.headerSparkPage(&quot;Spark Jobs&quot;, content, parent, helpText = Some(helpText)) &#125;&#125; jobsTable用来生成表格数据,jobsTable(job),将传过来的job信息生成对应的表格,如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960 private def jobsTable(jobs: Seq[JobUIData]): Seq[Node] = &#123; val someJobHasJobGroup = jobs.exists(_.jobGroup.isDefined) val columns: Seq[Node] = &#123; &lt;th&gt;&#123;if (someJobHasJobGroup) &quot;Job Id (Job Group)&quot; else &quot;Job Id&quot;&#125;&lt;/th&gt; &lt;th&gt;Description&lt;/th&gt; &lt;th&gt;Submitted&lt;/th&gt; &lt;th&gt;Duration&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Stages: Succeeded/Total&lt;/th&gt; &lt;th class=&quot;sorttable_nosort&quot;&gt;Tasks (for all stages): Succeeded/Total&lt;/th&gt; &#125;//表格中每行数据又是通过makeRow方法渲染的 def makeRow(job: JobUIData): Seq[Node] = &#123; val (lastStageName, lastStageDescription) = getLastStageNameAndDescription(job) val duration: Option[Long] = &#123; job.submissionTime.map &#123; start =&gt; val end = job.completionTime.getOrElse(System.currentTimeMillis()) end - start &#125; &#125; val formattedDuration = duration.map(d =&gt; UIUtils.formatDuration(d)).getOrElse(&quot;Unknown&quot;) val formattedSubmissionTime = job.submissionTime.map(UIUtils.formatDate).getOrElse(&quot;Unknown&quot;) val basePathUri = UIUtils.prependBaseUri(parent.basePath) val jobDescription = UIUtils.makeDescription(lastStageDescription, basePathUri) val detailUrl = &quot;%s/jobs/job?id=%s&quot;.format(basePathUri, job.jobId) &lt;tr id=&#123;&quot;job-&quot; + job.jobId&#125;&gt; &lt;td sorttable_customkey=&#123;job.jobId.toString&#125;&gt; &#123;job.jobId&#125; &#123;job.jobGroup.map(id =&gt; s&quot;($id)&quot;).getOrElse(&quot;&quot;)&#125; &lt;/td&gt; &lt;td&gt; &#123;jobDescription&#125; &lt;a href=&#123;detailUrl&#125; class=&quot;name-link&quot;&gt;&#123;lastStageName&#125;&lt;/a&gt; &lt;/td&gt; &lt;td sorttable_customkey=&#123;job.submissionTime.getOrElse(-1).toString&#125;&gt; &#123;formattedSubmissionTime&#125; &lt;/td&gt; &lt;td sorttable_customkey=&#123;duration.getOrElse(-1).toString&#125;&gt;&#123;formattedDuration&#125;&lt;/td&gt; &lt;td class=&quot;stage-progress-cell&quot;&gt; &#123;job.completedStageIndices.size&#125;/&#123;job.stageIds.size - job.numSkippedStages&#125; &#123;if (job.numFailedStages &gt; 0) s&quot;($&#123;job.numFailedStages&#125; failed)&quot;&#125; &#123;if (job.numSkippedStages &gt; 0) s&quot;($&#123;job.numSkippedStages&#125; skipped)&quot;&#125; &lt;/td&gt; &lt;td class=&quot;progress-cell&quot;&gt; &#123;UIUtils.makeProgressBar(started = job.numActiveTasks, completed = job.numCompletedTasks, failed = job.numFailedTasks, skipped = job.numSkippedTasks, total = job.numTasks - job.numSkippedTasks)&#125; &lt;/td&gt; &lt;/tr&gt; &#125; &lt;table class=&quot;table table-bordered table-striped table-condensed sortable&quot;&gt; &lt;thead&gt;&#123;columns&#125;&lt;/thead&gt; &lt;tbody&gt; &#123;jobs.map(makeRow)&#125; &lt;/tbody&gt; &lt;/table&gt; &#125; 4.5.SparkUI的启动SparkUI创建好后,需要调用父类WebUI的bind方法,绑定服务和端口,bind方法中主要的代码实现如下:1serverInfo = Some(startJettyServer(&quot;0.0.0.0&quot;, port, handlers, conf, name)) 最终启动了Jetty提供的服务,默认端口是4040 5.Hadoop相关配置及Executor环境变量5.1.Hadoop相关配置信息默认情况下,Spark使用HDFS作为分布式文件系统,所以需要获取Hadoop相关配置信息的,在SparkContext的相关代码如下:1_hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf) 获取的配置信息包括: 将Amazon S3文件系统的AccessKeyId和SecretAccessKey加载到Hadoop的Configuration 将SparkConf中所有以spark.hadoop.开头的属性都复制到Hadoop的Configuration 将SparkConf的属性spark.buff.size复制为Hadoop的Configuration的配置io.file.buffer.size 12345678910111213141516171819202122232425262728293031323334def newConfiguration(conf: SparkConf): Configuration = &#123; val hadoopConf = new Configuration() // Note: this null check is around more than just access to the &quot;conf&quot; object to maintain // the behavior of the old implementation of this code, for backwards compatibility. if (conf != null) &#123; // Explicitly check for S3 environment variables if (System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;) != null &amp;&amp; System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;) != null) &#123; val keyId = System.getenv(&quot;AWS_ACCESS_KEY_ID&quot;) val accessKey = System.getenv(&quot;AWS_SECRET_ACCESS_KEY&quot;) hadoopConf.set(&quot;fs.s3.awsAccessKeyId&quot;, keyId) hadoopConf.set(&quot;fs.s3n.awsAccessKeyId&quot;, keyId) hadoopConf.set(&quot;fs.s3a.access.key&quot;, keyId) hadoopConf.set(&quot;fs.s3.awsSecretAccessKey&quot;, accessKey) hadoopConf.set(&quot;fs.s3n.awsSecretAccessKey&quot;, accessKey) hadoopConf.set(&quot;fs.s3a.secret.key&quot;, accessKey) &#125; // Copy any &quot;spark.hadoop.foo=bar&quot; system properties into conf as &quot;foo=bar&quot; conf.getAll.foreach &#123; case (key, value) =&gt; if (key.startsWith(&quot;spark.hadoop.&quot;)) &#123; hadoopConf.set(key.substring(&quot;spark.hadoop.&quot;.length), value) &#125; &#125; val bufferSize = conf.get(&quot;spark.buffer.size&quot;, &quot;65536&quot;) hadoopConf.set(&quot;io.file.buffer.size&quot;, bufferSize) &#125; hadoopConf&#125; 注意:如果指定了SPARK_YARN_MODE属性,则会使用YarnSparkHadoopUtil,否则默认为SparkHadoopUtil 123456789101112131415161718192021222324252627282930object SparkHadoopUtil &#123;//2种模式 private lazy val hadoop = new SparkHadoopUtil private lazy val yarn = try &#123; Utils.classForName(&quot;org.apache.spark.deploy.yarn.YarnSparkHadoopUtil&quot;) .newInstance() .asInstanceOf[SparkHadoopUtil] &#125; catch &#123; case e: Exception =&gt; throw new SparkException(&quot;Unable to load YARN support&quot;, e) &#125; val SPARK_YARN_CREDS_TEMP_EXTENSION = &quot;.tmp&quot; val SPARK_YARN_CREDS_COUNTER_DELIM = &quot;-&quot;//SPARK_YARN_MODE选择对应的Utils def get: SparkHadoopUtil = &#123; // Check each time to support changing to/from YARN val yarnMode = java.lang.Boolean.valueOf( System.getProperty(&quot;SPARK_YARN_MODE&quot;, System.getenv(&quot;SPARK_YARN_MODE&quot;))) if (yarnMode) &#123; yarn // &#125; else &#123; hadoop &#125; &#125;&#125; 5.2.Executor环境变量对Executor的环境变量的处理,如下: 123456789101112131415161718192021_executorMemory = _conf.getOption(&quot;spark.executor.memory&quot;) .orElse(Option(System.getenv(&quot;SPARK_EXECUTOR_MEMORY&quot;))) .orElse(Option(System.getenv(&quot;SPARK_MEM&quot;)) .map(warnSparkMem)) .map(Utils.memoryStringToMb) .getOrElse(1024)// Convert java options to env vars as a work around// since we can&apos;t set env vars directly in sbt.for &#123; (envKey, propKey) &lt;- Seq((&quot;SPARK_TESTING&quot;, &quot;spark.testing&quot;)) value &lt;- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))&#125; &#123; executorEnvs(envKey) = value&#125;Option(System.getenv(&quot;SPARK_PREPEND_CLASSES&quot;)).foreach &#123; v =&gt; executorEnvs(&quot;SPARK_PREPEND_CLASSES&quot;) = v&#125;// The Mesos scheduler backend relies on this environment variable to set executor memory.// TODO: Set this only in the Mesos scheduler.executorEnvs(&quot;SPARK_EXECUTOR_MEMORY&quot;) = executorMemory + &quot;m&quot;executorEnvs ++= _conf.getExecutorEnvexecutorEnvs(&quot;SPARK_USER&quot;) = sparkUser executorEnvs包含的环境变量将在7.2.2节中介绍的注册应用的过程中发送给Master,Master给Worker发送调度后,Worker最终使用executorEnvs提供的信息启动Executor,可以通过配置spark.executor.memory指定Executor占用的内存大小,也可以配置系统变量SPARK_EXECUTOR_MEMORY或者SPARK_MEM对其大小进行设置 6.创建任务调度器TaskSchedulerTaskScheduler也是SparkContext的重要组成部分,负责任务的提交,并且请求集群管理器对任务调度,TaskScheduler也可以看做任务调度的客户端,创建TaskScheduler的代码如下:123val (sched, ts) = SparkContext.createTaskScheduler(this, master)_schedulerBackend = sched_taskScheduler = ts createTaskScheduler方法会根据master的配置匹配部署模式,创建TaskSchedulerImpl,并生成不同的SchedulerBackend,本章为了使读者更容易理解Spark的初始化流程,故以local模式为例,其余模式将在第7章详解,master匹配local模式的代码如下:123456789101112131415private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = &#123; import SparkMasterRegex._ // When running locally, don&apos;t try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 master match &#123; case &quot;local&quot; =&gt; val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler)//... 6.1.创建TaskSchedulerImplTaskSchedulerImpl的构造过程如下:1.从SparkConf中读取配置信息,包括每个任务分配的CPU数,调度模式(调度模式有fair和fifo两种,默认为fifo,可以修改属性spark.scheduler.mode来改变)等2.创建TaskSchedulerGetter,他的作用是通过线程池(Executor.newFixedThreadPool创建的,默认4个线程,线程名字以task-result-getter开头,线程工厂默认是Executors.defaultThreadFactory)对Worker上的Executor发送的Task的执行结果进行处理 123456789101112131415161718192021// Listener object to pass upcalls intovar dagScheduler: DAGScheduler = nullvar backend: SchedulerBackend = nullval mapOutputTracker = SparkEnv.get.mapOutputTrackervar schedulableBuilder: SchedulableBuilder = nullvar rootPool: Pool = null// default scheduler is FIFOprivate val schedulingModeConf = conf.get(&quot;spark.scheduler.mode&quot;, &quot;FIFO&quot;)val schedulingMode: SchedulingMode = try &#123; SchedulingMode.withName(schedulingModeConf.toUpperCase)&#125; catch &#123; case e: java.util.NoSuchElementException =&gt; throw new SparkException(s&quot;Unrecognized spark.scheduler.mode: $schedulingModeConf&quot;)&#125;// This is a var so that we can reset it for testing purposes.private[spark] var taskResultGetter = new TaskResultGetter(sc.env, this) TaskSchedulerImpl的调度模式有fair和fifo两种,任务的最终调度实际都是落实到接口SchedulerBackend的具体实现上的,为方便分析,我们先来看看local模式中SchedulerBackend的实现LocalBackend,LocalBackend依赖localEndpoint与ActorSystem进行消息通信,LocalBackend的实现如下:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private[spark] class LocalBackend( conf: SparkConf, scheduler: TaskSchedulerImpl, val totalCores: Int) extends SchedulerBackend with ExecutorBackend with Logging &#123; private val appId = &quot;local-&quot; + System.currentTimeMillis private var localEndpoint: RpcEndpointRef = null //.... override def start() &#123; val rpcEnv = SparkEnv.get.rpcEnv val executorEndpoint = new LocalEndpoint(rpcEnv, userClassPath, scheduler, this, totalCores) localEndpoint = rpcEnv.setupEndpoint(&quot;LocalBackendEndpoint&quot;, executorEndpoint) listenerBus.post(SparkListenerExecutorAdded( System.currentTimeMillis, executorEndpoint.localExecutorId, new ExecutorInfo(executorEndpoint.localExecutorHostname, totalCores, Map.empty))) launcherBackend.setAppId(appId) launcherBackend.setState(SparkAppHandle.State.RUNNING) &#125; override def stop() &#123; stop(SparkAppHandle.State.FINISHED) &#125; override def reviveOffers() &#123; localEndpoint.send(ReviveOffers) &#125; override def defaultParallelism(): Int = scheduler.conf.getInt(&quot;spark.default.parallelism&quot;, totalCores) override def killTask(taskId: Long, executorId: String, interruptThread: Boolean) &#123; localEndpoint.send(KillTask(taskId, interruptThread)) &#125; override def statusUpdate(taskId: Long, state: TaskState, serializedData: ByteBuffer) &#123; localEndpoint.send(StatusUpdate(taskId, state, serializedData)) &#125; override def applicationId(): String = appId private def stop(finalState: SparkAppHandle.State): Unit = &#123; localEndpoint.ask(StopExecutor) try &#123; launcherBackend.setState(finalState) &#125; finally &#123; launcherBackend.close() &#125; &#125;&#125; 6.2.TaskSchedulerImpl的初始化12345678910111213141516 private def createTaskScheduler( sc: SparkContext, master: String): (SchedulerBackend, TaskScheduler) = &#123; import SparkMasterRegex._ // When running locally, don&apos;t try to re-execute tasks on failure. val MAX_LOCAL_TASK_FAILURES = 1 master match &#123; case &quot;local&quot; =&gt; val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true) val backend = new LocalBackend(sc.getConf, scheduler, 1) scheduler.initialize(backend) (backend, scheduler)//.... 创建完TaskSchedulerImpl和LocalBackend后对TaskSchedulerImpl调用方法initialize进行初始化,以默认的fifo调度为例,TaskSchedulerImpl的初始化过程如下:1.使TaskSchedulerImpl持有LocalBackend的引用2.创建Pool,Pool中缓存了调度队列,调度算法及TaskSetManager集合等信息3.创建FIFOSchedulableBuilder,FIFOSchedulableBuilder用来操作Pool中的调度队列 initialize方法的实现如下: 1234567891011121314def initialize(backend: SchedulerBackend) &#123; this.backend = backend // temporarily set rootPool name to empty rootPool = new Pool(&quot;&quot;, schedulingMode, 0, 0) schedulableBuilder = &#123; schedulingMode match &#123; case SchedulingMode.FIFO =&gt; new FIFOSchedulableBuilder(rootPool) case SchedulingMode.FAIR =&gt; new FairSchedulableBuilder(rootPool, conf) &#125; &#125; schedulableBuilder.buildPools()&#125; 7.创建和启动DAGSchedulerDAGScheduler主要在任务正式交给TaskSchedulerImpl提交之前做一些准备工作,包括:创建Job,将DAG中的RDD划分到不同的stage,提交stage等等,创建DAGScheduler的代码在SparkContext中如下:1_dagScheduler = new DAGScheduler(this) DAGScheduler的数据结构主要维护jobId和stageId的关系,Stage,ActiveJob,以及缓存的RDD的Partitions的位置信息,代码如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445private[scheduler] val nextJobId = new AtomicInteger(0)private[scheduler] def numTotalJobs: Int = nextJobId.get()private val nextStageId = new AtomicInteger(0)private[scheduler] val jobIdToStageIds = new HashMap[Int, HashSet[Int]]private[scheduler] val stageIdToStage = new HashMap[Int, Stage]private[scheduler] val shuffleToMapStage = new HashMap[Int, ShuffleMapStage]private[scheduler] val jobIdToActiveJob = new HashMap[Int, ActiveJob]// Stages we need to run whose parents aren&apos;t doneprivate[scheduler] val waitingStages = new HashSet[Stage]// Stages we are running right nowprivate[scheduler] val runningStages = new HashSet[Stage]// Stages that must be resubmitted due to fetch failuresprivate[scheduler] val failedStages = new HashSet[Stage]private[scheduler] val activeJobs = new HashSet[ActiveJob]/** * Contains the locations that each RDD&apos;s partitions are cached on. This map&apos;s keys are RDD ids * and its values are arrays indexed by partition numbers. Each array value is the set of * locations where that RDD partition is cached. * * All accesses to this map should be guarded by synchronizing on it (see SPARK-4454). */private val cacheLocs = new HashMap[Int, IndexedSeq[Seq[TaskLocation]]]// For tracking failed nodes, we use the MapOutputTracker&apos;s epoch number, which is sent with// every task. When we detect a node failing, we note the current epoch number and failed// executor, increment it for new tasks, and use this to ignore stray ShuffleMapTask results.//// TODO: Garbage collect information about failure epochs when we know there are no more// stray messages to detect.private val failedEpoch = new HashMap[String, Long]private [scheduler] val outputCommitCoordinator = env.outputCommitCoordinator// A closure serializer that we reuse.// This is only safe because DAGScheduler runs in a single thread.private val closureSerializer = SparkEnv.get.closureSerializer.newInstance()private[scheduler] val eventProcessLoop = new DAGSchedulerEventProcessLoop(this) 上面的代码中有一个DAGSchedulerEventProcessLoop,他继承了EventLoop,EventLoop的实现如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172private[spark] abstract class EventLoop[E](name: String) extends Logging &#123; private val eventQueue: BlockingQueue[E] = new LinkedBlockingDeque[E]() private val stopped = new AtomicBoolean(false) private val eventThread = new Thread(name) &#123; setDaemon(true) override def run(): Unit = &#123; try &#123; while (!stopped.get) &#123; val event = eventQueue.take() try &#123; onReceive(event) &#125; catch &#123; case NonFatal(e) =&gt; &#123; try &#123; onError(e) &#125; catch &#123; case NonFatal(e) =&gt; logError(&quot;Unexpected error in &quot; + name, e) &#125; &#125; &#125; &#125; &#125; catch &#123; case ie: InterruptedException =&gt; // exit even if eventQueue is not empty case NonFatal(e) =&gt; logError(&quot;Unexpected error in &quot; + name, e) &#125; &#125; &#125; def start(): Unit = &#123; if (stopped.get) &#123; throw new IllegalStateException(name + &quot; has already been stopped&quot;) &#125; // Call onStart before starting the event thread to make sure it happens before onReceive onStart() eventThread.start() &#125; def stop(): Unit = &#123; if (stopped.compareAndSet(false, true)) &#123; eventThread.interrupt() var onStopCalled = false try &#123; eventThread.join() // Call onStop after the event thread exits to make sure onReceive happens before onStop onStopCalled = true onStop() &#125; catch &#123; case ie: InterruptedException =&gt; Thread.currentThread().interrupt() if (!onStopCalled) &#123; // ie is thrown from `eventThread.join()`. Otherwise, we should not call `onStop` since // it&apos;s already called. onStop() &#125; &#125; &#125; else &#123; // Keep quiet to allow calling `stop` multiple times. &#125; &#125; /** * Put the event into the event queue. The event thread will process it later. */ def post(event: E): Unit = &#123; eventQueue.put(event) &#125; 他会有一个线程从队列中循环取Event,然后调用onReceive(event)去处理事件,onReceive在DAGSchedulerEventProcessLoop类中实现,最终是match去匹配处理不同的事件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849override def onReceive(event: DAGSchedulerEvent): Unit = &#123; val timerContext = timer.time() try &#123; doOnReceive(event) &#125; finally &#123; timerContext.stop() &#125;&#125;private def doOnReceive(event: DAGSchedulerEvent): Unit = event match &#123; case JobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) =&gt; dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties) case MapStageSubmitted(jobId, dependency, callSite, listener, properties) =&gt; dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties) case StageCancelled(stageId) =&gt; dagScheduler.handleStageCancellation(stageId) case JobCancelled(jobId) =&gt; dagScheduler.handleJobCancellation(jobId) case JobGroupCancelled(groupId) =&gt; dagScheduler.handleJobGroupCancelled(groupId) case AllJobsCancelled =&gt; dagScheduler.doCancelAllJobs() case ExecutorAdded(execId, host) =&gt; dagScheduler.handleExecutorAdded(execId, host) case ExecutorLost(execId) =&gt; dagScheduler.handleExecutorLost(execId, fetchFailed = false) case BeginEvent(task, taskInfo) =&gt; dagScheduler.handleBeginEvent(task, taskInfo) case GettingResultEvent(taskInfo) =&gt; dagScheduler.handleGetTaskResult(taskInfo) case completion @ CompletionEvent(task, reason, _, _, taskInfo, taskMetrics) =&gt; dagScheduler.handleTaskCompletion(completion) case TaskSetFailed(taskSet, reason, exception) =&gt; dagScheduler.handleTaskSetFailed(taskSet, reason, exception) case ResubmitFailedStages =&gt; dagScheduler.resubmitFailedStages()&#125; 8.TaskScheduler的启动在3.6节介绍了任务调度器TaskScheduler的创建,要想TaskScheduler发挥作用,必须要启动它,如下代码:123// start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&apos;s constructor_taskScheduler.start() TaskScheduler在启动的时候,实际调用了Backend的start方法,以TaskSchedulerImpl为例123456789101112override def start() &#123; backend.start() if (!isLocal &amp;&amp; conf.getBoolean(&quot;spark.speculation&quot;, false)) &#123; logInfo(&quot;Starting speculative execution thread&quot;) speculationScheduler.scheduleAtFixedRate(new Runnable &#123; override def run(): Unit = Utils.tryOrStopSparkContext(sc) &#123; checkSpeculatableTasks() &#125; &#125;, SPECULATION_INTERVAL_MS, SPECULATION_INTERVAL_MS, TimeUnit.MILLISECONDS) &#125;&#125; 以LocalBackend为例,启动LocalBackend时向rpcEnv注册了LocalEndpoint,下面是LocalBackend的start方法1234567891011override def start() &#123; val rpcEnv = SparkEnv.get.rpcEnv val executorEndpoint = new LocalEndpoint(rpcEnv, userClassPath, scheduler, this, totalCores) localEndpoint = rpcEnv.setupEndpoint(&quot;LocalBackendEndpoint&quot;, executorEndpoint) listenerBus.post(SparkListenerExecutorAdded( System.currentTimeMillis, executorEndpoint.localExecutorId, new ExecutorInfo(executorEndpoint.localExecutorHostname, totalCores, Map.empty))) launcherBackend.setAppId(appId) launcherBackend.setState(SparkAppHandle.State.RUNNING)&#125; 8.1.创建LocalEndpointLocalEndpoint的创建过程主要是构建本地的Executor,如下: 123456789101112131415161718192021222324252627282930313233private[spark] class LocalEndpoint( override val rpcEnv: RpcEnv, userClassPath: Seq[URL], scheduler: TaskSchedulerImpl, executorBackend: LocalBackend, private val totalCores: Int) extends ThreadSafeRpcEndpoint with Logging &#123; private var freeCores = totalCores val localExecutorId = SparkContext.DRIVER_IDENTIFIER val localExecutorHostname = &quot;localhost&quot; private val executor = new Executor( localExecutorId, localExecutorHostname, SparkEnv.get, userClassPath, isLocal = true) override def receive: PartialFunction[Any, Unit] = &#123; case ReviveOffers =&gt; reviveOffers() case StatusUpdate(taskId, state, serializedData) =&gt; scheduler.statusUpdate(taskId, state, serializedData) if (TaskState.isFinished(state)) &#123; freeCores += scheduler.CPUS_PER_TASK reviveOffers() &#125; case KillTask(taskId, interruptThread) =&gt; executor.killTask(taskId, interruptThread) &#125;//...&#125; Executor的构建代码如下:123456789101112131415161718192021222324252627282930// Start worker thread poolprivate val threadPool = ThreadUtils.newDaemonCachedThreadPool(&quot;Executor task launch worker&quot;)private val executorSource = new ExecutorSource(threadPool, executorId)if (!isLocal) &#123; env.metricsSystem.registerSource(executorSource) env.blockManager.initialize(conf.getAppId)&#125;// Whether to load classes in user jars before those in Spark jarsprivate val userClassPathFirst = conf.getBoolean(&quot;spark.executor.userClassPathFirst&quot;, false)// Create our ClassLoader// do this after SparkEnv creation so can access the SecurityManagerprivate val urlClassLoader = createClassLoader()private val replClassLoader = addReplClassLoaderIfNeeded(urlClassLoader)// Set the classloader for serializerenv.serializer.setDefaultClassLoader(replClassLoader)// Akka&apos;s message frame size. If task result is bigger than this, we use the block manager// to send the result back.private val akkaFrameSize = AkkaUtils.maxFrameSizeBytes(conf)// Limit of bytes for total size of results (default is 1GB)private val maxResultSize = Utils.getMaxResultSize(conf)// Maintains the list of running tasks.private val runningTasks = new ConcurrentHashMap[Long, TaskRunner] Executor的构建,主要包括以下步骤:1.创建并注册ExecutorSource,ExecutorSource是做什么的呢?在8.2节会有介绍2.获取SparkEnv,如果是非local模式,Worker上的CoarseGrainedExecutorBackend向Driver的CoarseGrainedExecutorBackend注册Executor时,则需要新建SparkEnv3.创建并注册ExecutorActor,Executor负责接收发送给Executor的消息(在spark1.6中没有找到)4.urlClassLoader的创建,为什么需要创建这个urlClassLoader?在非local模式中,Driver或者Worker上都会有多个Executor,每个Executor都设置自身的urlClassLoader,用于加载任务上传的jar包中的类,有效的对任务的类加载环境进行隔离5.创建Executor执行的Task的线程池,此线程池用于执行任务6.启动Executor的心跳线程,此线程用于向Driver发送心跳 此外还包括Akka发送消息的帧大小,结果总大小的字节限制,正在运行的task的列表,设置serializer的默认ClassLoader为创建的ClassLoader等 8.2.ExecutorSource的创建与注册ExecutorSource用于测量系统,通过metriRegistry的register方法注册计量,这些计量信息包括threadpool.activeTasks,threadpool.completeTasks,threadpool.currentPool_size,threadpool.maxPool_size等,详见代码:12345678910111213141516171819202122232425262728293031323334353637class ExecutorSource(threadPool: ThreadPoolExecutor, executorId: String) extends Source &#123; private def fileStats(scheme: String) : Option[FileSystem.Statistics] = FileSystem.getAllStatistics.asScala.find(s =&gt; s.getScheme.equals(scheme)) private def registerFileSystemStat[T]( scheme: String, name: String, f: FileSystem.Statistics =&gt; T, defaultValue: T) = &#123; metricRegistry.register(MetricRegistry.name(&quot;filesystem&quot;, scheme, name), new Gauge[T] &#123; override def getValue: T = fileStats(scheme).map(f).getOrElse(defaultValue) &#125;) &#125; override val metricRegistry = new MetricRegistry() override val sourceName = &quot;executor&quot; // Gauge for executor thread pool&apos;s actively executing task counts metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;activeTasks&quot;), new Gauge[Int] &#123; override def getValue: Int = threadPool.getActiveCount() &#125;) // Gauge for executor thread pool&apos;s approximate total number of tasks that have been completed metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;completeTasks&quot;), new Gauge[Long] &#123; override def getValue: Long = threadPool.getCompletedTaskCount() &#125;) // Gauge for executor thread pool&apos;s current number of threads metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;currentPool_size&quot;), new Gauge[Int] &#123; override def getValue: Int = threadPool.getPoolSize() &#125;) // Gauge got executor thread pool&apos;s largest number of threads that have ever simultaneously // been in th pool metricRegistry.register(MetricRegistry.name(&quot;threadpool&quot;, &quot;maxPool_size&quot;), new Gauge[Int] &#123; override def getValue: Int = threadPool.getMaximumPoolSize() &#125;) 创建完ExecutorSource后,调用MetricsSystem的registerSource方法将ExecutorSource注册到MetricsSystem12345678910111213141516if (!isLocal) &#123; env.metricsSystem.registerSource(executorSource) env.blockManager.initialize(conf.getAppId)&#125;def registerSource(source: Source) &#123; sources += source try &#123; val regName = buildRegistryName(source) registry.register(regName, source.metricRegistry) &#125; catch &#123; case e: IllegalArgumentException =&gt; logInfo(&quot;Metrics already registered&quot;, e) &#125;&#125; 8.4.spark自身ClassLoader的创建获取要创建的ClassLoader的父加载器currentLoader,然后根据currentJars生成URL数组,spark.files.userClassPathFirst属性指定加载类时是否先从用户的classpath下加载,最后创建MutableURLClassLoader或者ChildExecutorURLClassLoader 123456789101112131415161718192021private def createClassLoader(): MutableURLClassLoader = &#123; // Bootstrap the list of jars with the user class path. val now = System.currentTimeMillis() userClassPath.foreach &#123; url =&gt; currentJars(url.getPath().split(&quot;/&quot;).last) = now &#125; val currentLoader = Utils.getContextOrSparkClassLoader // For each of the jars in the jarSet, add them to the class loader. // We assume each of the files has already been fetched. val urls = userClassPath.toArray ++ currentJars.keySet.map &#123; uri =&gt; new File(uri.split(&quot;/&quot;).last).toURI.toURL &#125; if (userClassPathFirst) &#123; new ChildFirstURLClassLoader(urls, currentLoader) &#125; else &#123; new MutableURLClassLoader(urls, currentLoader) &#125;&#125; 8.5.启动Executor的心跳线程Executor的心跳由startDriverHeadrtbeater启动,在Executor类中的代码如下:123456789101112131415161718192021// Executor for the heartbeat task.private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;driver-heartbeater&quot;)// must be initialized before running startDriverHeartbeat()private val heartbeatReceiverRef = RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)startDriverHeartbeater()private def startDriverHeartbeater(): Unit = &#123; val intervalMs = conf.getTimeAsMs(&quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;) // Wait a random interval so the heartbeats don&apos;t end up in sync val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf[Int] val heartbeatTask = new Runnable() &#123; override def run(): Unit = Utils.logUncaughtExceptions(reportHeartBeat()) &#125; heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)&#125; Executor心跳线程的间隔由属性spark.executor.heartbeatInterval配置,默认是10s,此外还有一个延迟时间,最终是调用run方法中的reportHeartBeat方法,该方法代码如下:1234567891011121314151617181920212223242526272829303132333435363738394041/** Reports heartbeat and metrics for active tasks to the driver. */private def reportHeartBeat(): Unit = &#123; // list of (task id, metrics) to send back to the driver val tasksMetrics = new ArrayBuffer[(Long, TaskMetrics)]() val curGCTime = computeTotalGcTime() for (taskRunner &lt;- runningTasks.values().asScala) &#123; if (taskRunner.task != null) &#123; taskRunner.task.metrics.foreach &#123; metrics =&gt; metrics.updateShuffleReadMetrics() metrics.updateInputMetrics() metrics.setJvmGCTime(curGCTime - taskRunner.startGCTime) metrics.updateAccumulators() if (isLocal) &#123; // JobProgressListener will hold an reference of it during // onExecutorMetricsUpdate(), then JobProgressListener can not see // the changes of metrics any more, so make a deep copy of it val copiedMetrics = Utils.deserialize[TaskMetrics](Utils.serialize(metrics)) tasksMetrics += ((taskRunner.taskId, copiedMetrics)) &#125; else &#123; // It will be copied by serialization tasksMetrics += ((taskRunner.taskId, metrics)) &#125; &#125; &#125; &#125; val message = Heartbeat(executorId, tasksMetrics.toArray, env.blockManager.blockManagerId) try &#123; val response = heartbeatReceiverRef.askWithRetry[HeartbeatResponse]( message, RpcTimeout(conf, &quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;)) if (response.reregisterBlockManager) &#123; logInfo(&quot;Told to re-register on heartbeat&quot;) env.blockManager.reregister() &#125; &#125; catch &#123; case NonFatal(e) =&gt; logWarning(&quot;Issue communicating with driver in heartbeater&quot;, e) &#125;&#125; 这个心跳的作用有两个1.更新正在处理的任务的测量信息2.通知BlockManagerMaster,此Executor上的BlockManager依然活着 下面是对心跳线程的实现详细分析初始化TaskSchedulerImpl后会创建心跳接收器HeartbeatReceiver,HeartbeatReceiver接收所有分配给当前DriverApplication的Executor的心跳,并将Task,Task计量信息,心跳等交给TaskSchedulerImpl和DAGScheduler作进一步处理,创建心跳接收器的代码在SparkContext中如下1234// We need to register &quot;HeartbeatReceiver&quot; before &quot;createTaskScheduler&quot; because Executor will// retrieve &quot;HeartbeatReceiver&quot; in the constructor. (SPARK-6640)_heartbeatReceiver = env.rpcEnv.setupEndpoint( HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this)) HeartbeatReceiver在接收到心跳消息后,会调用TaskScheduler的executorHeartbeatReceived方法1234567891011121314override def executorHeartbeatReceived( execId: String, taskMetrics: Array[(Long, TaskMetrics)], // taskId -&gt; TaskMetrics blockManagerId: BlockManagerId): Boolean = &#123; val metricsWithStageIds: Array[(Long, Int, Int, TaskMetrics)] = synchronized &#123; taskMetrics.flatMap &#123; case (id, metrics) =&gt; taskIdToTaskSetManager.get(id).map &#123; taskSetMgr =&gt; (id, taskSetMgr.stageId, taskSetMgr.taskSet.stageAttemptId, metrics) &#125; &#125; &#125; dagScheduler.executorHeartbeatReceived(execId, metricsWithStageIds, blockManagerId)&#125; 这段程序通过遍历taskMetrices,依据taskIdToTaskSetId和activeTaskSets找到TaskSetManager,然后将taskId,TaskSetManager.stageId,TaskSetManager.taskSet.attempt,TaskMetrices封装到Array[(Long, Int, Int, TaskMetrics)]的数组中,最后调用dagScheduler的executorHeartbeatReceived方法12345678def executorHeartbeatReceived( execId: String, taskMetrics: Array[(Long, Int, Int, TaskMetrics)], // (taskId, stageId, stateAttempt, metrics) blockManagerId: BlockManagerId): Boolean = &#123; listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, taskMetrics)) blockManagerMaster.driverEndpoint.askWithRetry[Boolean]( BlockManagerHeartbeat(blockManagerId), new RpcTimeout(600 seconds, &quot;BlockManagerHeartbeat&quot;))&#125; dagScheduler将executorId,metricsWithStageIds封装为SparkListenerExecutorMetricsUpdate事件,并post到listenerBus中,此事件用于更新stage的各种测量数据,最后给BlockManagerMaster持有的driverEndpoint发送BlockManagerHeartbeat,在local模式下Executor的心跳通信过程如下图: 在非local模式下,Executor发送心跳的过程是一样的,主要的区别是Executor进程与Driver不再同一个进程,甚至不再同一个节点上 9.启动测量系统MetricsSystemMetricsSystem使用codahale提供的第三方测量仓库Metrics,MetricsSystem中有三个概念:1.Instance:指定了谁在使用测量系统2.Source:指定了从哪里收集测量数据3.Sink:指定了往哪里输出测量数据 Spark按照Instance的不同,区分为Master,Worker,Application,Driver和Executor Spark目前提供的Sink有ConsoleSink,CsvSink,jmxSink,MetricsServlet,GraphiteSink等 Spark中使用MetriceServlet作为模式的Sink MetricsSystem的启动代码在SparkContext中如下:1234def metricsSystem: MetricsSystem = if (_env != null) _env.metricsSystem else nullmetricsSystem.start() MetricsSystem的启动过程包括以下的步骤:1.注册Source2.注册Sinks3.给Sinks增加Jetty的ServletContextHandler MetricsSystem启动完毕后,会遍历与Sinks有关的ServletContextHandler,并调用attachHandle将他们绑定到SparkUI上 12// Attach the driver metrics servlet handler to the web ui after the metrics system is started.metricsSystem.getServletHandlers.foreach(handler =&gt; ui.foreach(_.attachHandler(handler))) start方法的实现如下: 1234567def start() &#123; require(!running, &quot;Attempting to start a MetricsSystem that is already running&quot;) running = true registerSources() registerSinks() sinks.foreach(_.start)&#125; 9.1.注册SourcesregisterSources方法用于注册Sources,告诉测量系统从哪里收集测量数据,代码实现如下:123456789101112131415private def registerSources() &#123; val instConfig = metricsConfig.getInstance(instance) val sourceConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SOURCE_REGEX) // Register all the sources related to instance sourceConfigs.foreach &#123; kv =&gt; val classPath = kv._2.getProperty(&quot;class&quot;) try &#123; val source = Utils.classForName(classPath).newInstance() registerSource(source.asInstanceOf[Source]) &#125; catch &#123; case e: Exception =&gt; logError(&quot;Source class &quot; + classPath + &quot; cannot be instantiated&quot;, e) &#125; &#125;&#125; 注册Sources的过程分为以下步骤1.从metricsConfig获取Driver的Properties,默认为创建MetricsSystem的过程中解析的2.用正则匹配Driver的Properties中以source.开头的属性,然后将属性中的Source反射得到的实例加入ArrayBuffer[Source]3.将每个source的metricRegistry注册到ConcurrentMapmetrics 9.2.注册SinksregisterSinks方法用于注册Sinks,即告诉测量系统MetricsSystem往哪里输出测量数据,他的实现代码如下: 1234567891011121314151617181920212223242526private def registerSinks() &#123; val instConfig = metricsConfig.getInstance(instance) val sinkConfigs = metricsConfig.subProperties(instConfig, MetricsSystem.SINK_REGEX) sinkConfigs.foreach &#123; kv =&gt; val classPath = kv._2.getProperty(&quot;class&quot;) if (null != classPath) &#123; try &#123; val sink = Utils.classForName(classPath) .getConstructor(classOf[Properties], classOf[MetricRegistry], classOf[SecurityManager]) .newInstance(kv._2, registry, securityMgr) if (kv._1 == &quot;servlet&quot;) &#123; metricsServlet = Some(sink.asInstanceOf[MetricsServlet]) &#125; else &#123; sinks += sink.asInstanceOf[Sink] &#125; &#125; catch &#123; case e: Exception =&gt; &#123; logError(&quot;Sink class &quot; + classPath + &quot; cannot be instantiated&quot;) throw e &#125; &#125; &#125; &#125;&#125; 注册Sinks的步骤如下:1.从Driver的Properties中用正则匹配以sink.开头的属性,如1&#123;sink.servlet.class=org.apache.spark.metrics.sink.MetricsServlet，sink.servlet.path=/metrics/json&#125; 将其转换为Map（servlet-&gt;{class=org.apache.spark.metrics.sink.MetricsServlet，path=/metrics/json}）2.将子属性class对应的类metricsServlet反射得到MetricsServlet实例,如果属性的key是servlet,将其设置为metricsServlet,如果是sink,则加入到ArrayBuffer[Sink]中 9.3.给Sinks增加Jetty的ServletContextHandler为了能够在SparkUI(网页)访问到测量数据,所以需要给Sinks增加Jetty的ServletContextHandler,这里主要用到MetricsSystem的getServletHandlers方法,实现如下:1234567/** * Get any UI handlers used by this metrics system; can only be called after start(). */def getServletHandlers: Array[ServletContextHandler] = &#123; require(running, &quot;Can only call getServletHandlers on a running MetricsSystem&quot;) metricsServlet.map(_.getHandlers(conf)).getOrElse(Array())&#125; 可以看到调用了metricsServlet的getHandlers,其实现如下123456def getHandlers(conf: SparkConf): Array[ServletContextHandler] = &#123; Array[ServletContextHandler]( createServletHandler(servletPath, new ServletParams(request =&gt; getMetricsSnapshot(request), &quot;text/json&quot;), securityMgr, conf) )&#125; 最终生成处理/metrics/json请求的ServletContextHandler,而请求的真正处理由getMetricsSnapshot方法,利用fastjson解析,生成的ServletContextHandler通过SparkUI的attachHandler方法,也被绑定到SparkUI,最红我们可以使用以下这些地址来访问测量数据:123http://localhost:4040/metrics/applications/jsonhttp://localhost:4040/metrics/jsonhttp://localhost:4040/metrics/master/json 10.创建和启动ExecutorAllocationManagerExecutorAllocationManager用于对已分配的Executor进行管理,创建和启动ExecutorAllocationManager的代码如下12345678_executorAllocationManager = if (dynamicAllocationEnabled) &#123; Some(new ExecutorAllocationManager(this, listenerBus, _conf)) &#125; else &#123; None &#125;_executorAllocationManager.foreach(_.start()) 默认情况下不会创建ExecutorAllocationManager,可以修改属性spark.dynamicAllocation.enabled为true来创建,ExecutorAllocationManager可以设置动态分配最小Executor数量,动态分配最大Executor数量,每个Executor可以运行的Task数量等配置信息,并对配置信息进行校验,start方法将ExecutorAllocationListener加入listenerBus中,ExecutorAllocationListener通过监听listenBus里的事件,动态添加,删除Executor,并且通过Thread不断添加Executor,遍历Executor,将超时的Executor杀掉并移除,ExecutorAllocationListener的实现与其他SparkListener类似,ExecutorAllocationManager的关键代码如下:1234567891011121314151617181920212223242526272829// Clock used to schedule when executors should be added and removedprivate var clock: Clock = new SystemClock()// Listener for Spark events that impact the allocation policyprivate val listener = new ExecutorAllocationListener/** * Register for scheduler callbacks to decide when to add and remove executors, and start * the scheduling task. */def start(): Unit = &#123; listenerBus.addListener(listener) val scheduleTask = new Runnable() &#123; override def run(): Unit = &#123; try &#123; schedule() &#125; catch &#123; case ct: ControlThrowable =&gt; throw ct case t: Throwable =&gt; logWarning(s&quot;Uncaught exception in thread $&#123;Thread.currentThread().getName&#125;&quot;, t) &#125; &#125; &#125; executor.scheduleAtFixedRate(scheduleTask, 0, intervalMillis, TimeUnit.MILLISECONDS)&#125; 11.ContextCleaner的创建与启动ContextCleaner用于清理那些超出应用范围的RDD,ShuffleDependency和Broadcast对象,由于配置属性spark.cleaner.referenceTracking默认是true,所以会构造并请ContextCleaner,代码如下:12345678_cleaner = if (_conf.getBoolean(&quot;spark.cleaner.referenceTracking&quot;, true)) &#123; Some(new ContextCleaner(this)) &#125; else &#123; None &#125;_cleaner.foreach(_.start()) ContextCleaner的组成如下:1.referenceQueue:缓存顶级的AnyRef引用2.referenceBuffer:缓存AnyRef的虚引用3.listeners:缓存清理工作的监听器数组4.cleaningThread:用于具体清理工作的线程 123456789101112131415private[spark] class ContextCleaner(sc: SparkContext) extends Logging &#123; private val referenceBuffer = new ArrayBuffer[CleanupTaskWeakReference] with SynchronizedBuffer[CleanupTaskWeakReference] private val referenceQueue = new ReferenceQueue[AnyRef] private val listeners = new ArrayBuffer[CleanerListener] with SynchronizedBuffer[CleanerListener] private val cleaningThread = new Thread() &#123; override def run() &#123; keepCleaning() &#125;&#125;///....&#125; ContextCleaner的工作原理和listenerBus一样,也采用监听器模式,由线程来处理,此线程实际只是调用keepCleaning方法,该方法的实现如下:1234567891011121314151617181920212223242526272829303132/** Keep cleaning RDD, shuffle, and broadcast state. */private def keepCleaning(): Unit = Utils.tryOrStopSparkContext(sc) &#123; while (!stopped) &#123; try &#123; val reference = Option(referenceQueue.remove(ContextCleaner.REF_QUEUE_POLL_TIMEOUT)) .map(_.asInstanceOf[CleanupTaskWeakReference]) // Synchronize here to avoid being interrupted on stop() synchronized &#123; reference.map(_.task).foreach &#123; task =&gt; logDebug(&quot;Got cleaning task &quot; + task) referenceBuffer -= reference.get task match &#123; case CleanRDD(rddId) =&gt; doCleanupRDD(rddId, blocking = blockOnCleanupTasks) case CleanShuffle(shuffleId) =&gt; doCleanupShuffle(shuffleId, blocking = blockOnShuffleCleanupTasks) case CleanBroadcast(broadcastId) =&gt; doCleanupBroadcast(broadcastId, blocking = blockOnCleanupTasks) case CleanAccum(accId) =&gt; doCleanupAccum(accId, blocking = blockOnCleanupTasks) case CleanCheckpoint(rddId) =&gt; doCleanCheckpoint(rddId) &#125; &#125; &#125; &#125; catch &#123; case ie: InterruptedException if stopped =&gt; // ignore case e: Exception =&gt; logError(&quot;Error in cleaning thread&quot;, e) &#125; &#125;&#125; 12.Spark环境更新在SparkContext的初始化过程中,可能对其环境造成影响,所以需要更新环境,代码如下:12postEnvironmentUpdate()postApplicationStart() SparkContext初始化过程中,如果设置了spark.jars属性,spark.jars指定的jar包将有addJar方法假如HttpFileServer的jarDir变量指定的路径下,spark.files指定的文件将由addFile方法假如HttpFileServer的fileDir变量指定的路径下,如下:123456789101112/** Post the environment update event once the task scheduler is ready */private def postEnvironmentUpdate() &#123; if (taskScheduler != null) &#123; val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths, addedFilePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) &#125;&#125; httpFileServer的addFile和addJar方法,如下:123456789def addFile(file: File) : String = &#123; addFileToDir(file, fileDir) serverUri + &quot;/files/&quot; + Utils.encodeFileNameToURIRawPath(file.getName)&#125;def addJar(file: File) : String = &#123; addFileToDir(file, jarDir) serverUri + &quot;/jars/&quot; + Utils.encodeFileNameToURIRawPath(file.getName)&#125; postEnvironmentUpdate的实现见代码1234567891011private def postEnvironmentUpdate() &#123; if (taskScheduler != null) &#123; val schedulingMode = getSchedulingMode.toString val addedJarPaths = addedJars.keys.toSeq val addedFilePaths = addedFiles.keys.toSeq val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths, addedFilePaths) val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails) listenerBus.post(environmentUpdate) &#125;&#125; 其处理步骤如下:1.通过调用SparkEnv的方法environmentDetails最终影响环境的JVM参数,/Spark属性,系统属性,classpath等2.生成时间SparkListenerEnvironmentUpdate,并post到listenerBus,此事件被EnvironmentListener监听,最终影响EnvironmentPage页面中的额输出内容 environmentDetails的代码实现如下:123456789101112131415161718192021222324252627282930313233343536373839404142434445 def environmentDetails( conf: SparkConf, schedulingMode: String, addedJars: Seq[String], addedFiles: Seq[String]): Map[String, Seq[(String, String)]] = &#123; import Properties._ val jvmInformation = Seq( (&quot;Java Version&quot;, s&quot;$javaVersion ($javaVendor)&quot;), (&quot;Java Home&quot;, javaHome), (&quot;Scala Version&quot;, versionString) ).sorted // Spark properties // This includes the scheduling mode whether or not it is configured (used by SparkUI) val schedulerMode = if (!conf.contains(&quot;spark.scheduler.mode&quot;)) &#123; Seq((&quot;spark.scheduler.mode&quot;, schedulingMode)) &#125; else &#123; Seq[(String, String)]() &#125; val sparkProperties = (conf.getAll ++ schedulerMode).sorted // System properties that are not java classpaths val systemProperties = Utils.getSystemProperties.toSeq val otherProperties = systemProperties.filter &#123; case (k, _) =&gt; k != &quot;java.class.path&quot; &amp;&amp; !k.startsWith(&quot;spark.&quot;) &#125;.sorted // Class paths including all added jars and files val classPathEntries = javaClassPath .split(File.pathSeparator) .filterNot(_.isEmpty) .map((_, &quot;System Classpath&quot;)) val addedJarsAndFiles = (addedJars ++ addedFiles).map((_, &quot;Added By User&quot;)) val classPaths = (addedJarsAndFiles ++ classPathEntries).sorted Map[String, Seq[(String, String)]]( &quot;JVM Information&quot; -&gt; jvmInformation, &quot;Spark Properties&quot; -&gt; sparkProperties, &quot;System Properties&quot; -&gt; otherProperties, &quot;Classpath Entries&quot; -&gt; classPaths) &#125;&#125; postApplicationStart()方法很简单,只是向listenerBus发送了SparkListenerApplicationStart事件,代码如下:12345private def postApplicationStart() &#123; listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId), startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))&#125; 13.创建DAGSchedulerSource和BlockManagerSource在创建DAGSchedulerSource,BlockManagerSource之前首先会调用TaskScheduler的postStartHook方法,其目的是为了等待Backend就绪,如下代码:12345678// Post init_taskScheduler.postStartHook()_env.metricsSystem.registerSource(_dagScheduler.metricsSource)_env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))_executorAllocationManager.foreach &#123; e =&gt; _env.metricsSystem.registerSource(e.executorAllocationManagerSource)&#125; postStartHook方法如下1234567891011121314override def postStartHook() &#123; waitBackendReady()&#125;private def waitBackendReady(): Unit = &#123; if (backend.isReady) &#123; return &#125; while (!backend.isReady) &#123; synchronized &#123; this.wait(100) &#125; &#125;&#125; 14.将SparkContext标记为激活SparkContext初始化的最后将当前SparkContext的状态从contextBeingConstructed(正在构建中)改为activeContext(已激活),代码如下:1234// In order to prevent multiple SparkContexts from being active at the same time, mark this// context as having finished construction.// NOTE: this must be placed at the end of the SparkContext constructor.SparkContext.setActiveContext(this, allowMultipleContexts) setActiveContext的方法实现如下:1234567891011121314/** * Called at the end of the SparkContext constructor to ensure that no other SparkContext has * raced with this constructor and started. */private[spark] def setActiveContext( sc: SparkContext, allowMultipleContexts: Boolean): Unit = &#123; SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized &#123; assertNoOtherContextIsRunning(sc, allowMultipleContexts) contextBeingConstructed = None activeContext.set(sc) &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"第2章 spark设计理念与基本架构","date":"2017-04-16T04:47:25.358Z","path":"2017/04/16/bigdata/深入理解spark核心思想与源码分析/第2章 spark设计理念与基本架构/","text":"spark也是基于map reduce算法模式实现的分布式框架,拥有hadoop MapReduce所具有的优点,并且解决了Hadoop MapReduce中的诸多缺陷. 1.初试spark1.1.hadoop MRv1的局限hadoop1.0版本的采用的是MRv1版本的MapReduce编程模型,MRv1版本的实现都封装在org.apache.hadoop.mapred包中,MRv1的Map和Reduce是通过接口实现的,MRv1包括三个部分: 运行时环境(JobTracker和TaskTracker) 编程模型(MapReduce) 数据处理引擎(Map任务和Reduce任务) MRv1存在的不足:1.可扩展性差:在运行时,JobTracker既负责资源管理又负责任务调度,当集群繁忙时,JobTracker很容易称为瓶颈,最终导致他的可扩展性问题2.可用性差:采用了单节点的Master,没有备用Master及选举操作,这导致一旦Master出现故障,整个集群将不可用3.资源利用率低:TaskTracker使用slot等量划分本节点上的资源,slot代表计算资源(CPU,内存等),一个Task获取到一个slot才有机会运行,hadoop调度器负责各个TaskTracker山的空闲slot分配给Task使用,一些Task并不能充分利用slot,而其他Task也无法使用这些空闲的资源,slot分为Map slot和Reduce slot,分别供MapTask和ReduceTask使用,有时因为作业刚刚启动等原因导致MapTask很多,而ReduceTask任务还没有调度的情况,这时Reduce slot也会被闲置4.不能支持多种MapReduce框架:无法通过可插拔方式将自身的MapReduce框架替换为其他实现,如spark,storm等. MRv1.0的示意图 apache为了解决以上问题,对hadoop进行升级改造,MRv2最终诞生了,MRv2重用了MRv1中的编程模型和数据处理引擎,但是运行时环境被重构了,JobTracker被拆分成了通用的资源调度平台(ResourceManager,RM)和负责各个计算框架的任务调度模型(ApplicationMaster AM),MRv2中MapReduce的核心不再是MapReduce框架,而是YARN,在以Yarn为核心的MRv2中,MapReduce框架是可插拔的,完全可以替换为其他MapReduce实现,比如:spark,storm等,MRv2的示意图如下: Hadoop MRv2虽然解决了MRv1中的一些问题,但是由于对HDFS的频繁操作(包括计算结果持久化,数据备份及Shuffle等)导致磁盘I/O成为系统性能的瓶颈,因此只适用于离线数据处理,而不能提供实时数据处理能力 1.2.spark使用场景hadoop常用于解决高吞吐,批量处理的业务场景,例如离线计算结果用于浏览量统计,如果需要实时查看浏览量统计信息,hadoop显然不符合这样的要求,spark通过内存计算能力极大的提高了大数据处理速度,满足了以上场景的需求,此外,spark还支持sql查询,流式计算,图计算,机器学习等,通过对java,Python,scala,R等语言的支持,极大的方便了用户的使用. 1.3.spark的特点spark看到MRv1的问题,对MapReduce做了大量的优化,总结如下: 1.快速处理能力:随着实时大数据应用越来越多,hadoop作为离线的高吞吐,低响应框架已不能满足这类需求,hadoop MapReduce的Job将中间输出和结果存储在HDFS中,读写HDFS造成磁盘I/O成为瓶颈,spark允许将中间输出和结果存储在内存中,避免了大量的磁盘I/O,同时spark自身的DAG执行引擎也支持数据在内存中计算,spark官网声称性能比hadoop快100倍,即便是内存不足,需要磁盘I/O,其速度也是hadoop的10倍以上.2.易于使用:spark现在支持java,scala,Python和R语言编写的应用程序,大大降低了使用者的门槛,自带80多个高等级操作符,允许在scala,Python,R的shell中进行交互式查询.3.支持查询:spark支持sql及hive sql对输数据查询4.支持流式计算:与MapReduce智能处理离线数据相比,spark还支持实时的流计算,spark依赖spark streaming对数据进行实时的处理,其流式处理能力还要强于storm5.可用性高:spark自身实现了standalone模式部署,次模式下的Master可以有多个,解决了单点故障问题,此模式完全可以使用其他集群管理器替换,如yarn,Mesos,EC2等.6.丰富的数据源支持:spark除了可以访问操作系统自身的文件系统和HDFS,还可以访问Cassandra,HBase,Hive,Tachyon以及任何hadoop的数据源,这极大的方便了已经使用HDFS,HBase的用户顺利迁移到spark上. 2.spark基础知识2.1.基本概念 RDD: resilient distributed dataset 弹性分布式数据集 Task:具体执行任务,Task分为ShuffleMapTask和ResultTask两种,ShuffleMapTask和ResultTask分别类似于Hadoop只能的Map和Reduce Job:用户提交的作业,一个job可能由一到多个Task组成 Stage:Job分成的阶段,以job可能被划分为一到多个Stage Partition:数据分区,即一个RDD的数据可以划分为多少个分区 NarrowDependency:窄依赖,即子RDD依赖于父RDD中固定的Partition,NarrowDependency分为OneToOneDependency和RangeDependency两种 ShuffleDependency:Shuffle依赖,也称为宽依赖,即子RDD对父RDD中的所有Partition都有依赖 DAG: directed acycle graph 有向无环图,用于反映各RDD之间的依赖关系 3.spark的基本设计思想3.1.spark模块设计整个spark主要由以下模块组成: spark core:spark的核心模块功能实现,包括:SparkContext的初始化(Driver Application通过SparkContext提交),部署模式,存储系统,任务提交与执行,计算引擎等 Spark SQL:提供SQL处理能力,便于熟悉关系型数据库操作的工程师进行交互式查询,此外,还为熟悉Hadoop的用户提供了hive sql处理能力 Spark Streaming:提供流式计算处理能力,目前支持Kafka,flume,Twitter,MQTT,ZeroMQ,Kinesis和简单 的TCP套接字等数据源,此外,还提供了窗口操作. GraphX:提供了图计算处理能力,支持分布式,Pregel提供的API可以解决图计算中的常见问题. MLib:提供机器学习相关的统计,分类,回归等领域的多种算法实现,其一致的API接口大大降低了用户的学习成本. Spark SQL, Spark Streaming, GraphX, Mlib的能力都是建立在核心引擎之上,如下: Spark的核心功能 spark Core提供Spark最基础与最核心的功能,主要包括以下功能: SparkContext:通常而言,Driver Application的执行与输出都是通过SparkContext来完成的,在正式提交Application之前,首先需要初始化SparkContext, SparkContext隐藏了网络通信,分布式部署,消息通信,存储能力,计算能力,缓存,测量系统,文件服务,Web服务等内容,应用程序开发者只需要使用SparkContext提供的API完成功能开发,SparkContext内置的DAGScheduler负责创建Job,将DAG中的RDD划分到不同的Stage,提交stage等功能,内置的TaskScheduler负责资源的申请,任务的提交及请求集群对任务的调度等工作 存储系统:spark优先考虑使用各节点的内存作为存储,当内存不足时才会考虑使用磁盘,这极大的减少了磁盘I/O,提升了任务执行的效率,是的Spark适用于实时计算,流式计算等场景,此外,spark还提供了以内存为中心的高容错的分布式文件系统Tachyon供用户进行选择,Tachyon能够为spark提供可靠的内存级的文件共享服务 计算引擎:计算引擎由SparkContext中的DAGScheduler,RDD以及具体节点上的Executor负责执行的Map和Reduce任务组成,DAGScheduler和RDD虽然位于SparkContext内部,但是在任务提交与执行之前会将Job中的RDD组织成有向无环图(DAG),并对stage进行划分,决定了任务执行阶段任务的数量,迭代计算,Shuffle等过程 部署模式:由于单节点不足以提供足够的存储及计算能力,所以作为大数据处理的spark在SparkContext的TaskScheduler组件中提供了对Standalone部署模式的实现和Yarn,Mesos等分布式资源管理系统的支持,通过使用Standalone,Yarn,Mesos等部署模式为Task分配计算资源,提高任务的并发执行效率,除了可用于实际生产环境的Standalone,Yarn,Mesos等部署模式外,spark还提供了Local模式和local-cluster模式便于开发和调试. spark扩展功能 为了扩大应用范围,spark陆续提供了一些扩展功能,主要包括: Spark SQL:sql具有普及率高,学习成本低等特点,为了扩大Spark的应用面,增加了对sql及hive的支持,sparksql的过程可以总结为:首先使用sql语句解析器(SqlParser)将SQL转换为语法树(Tree),并且使用规则执行器(RuleExecutor)将一系列规则(Rule)应用到语法树,最终生成物理执行计划并执行,其中,规则执行器包括语法分析器(Analyzer)和优化器(Optimizer),hive的执行过程与sql类似 Spark Streaming:Sparking streaming与Apache Storm类似,也用于流式计算,Spark streaming支持Kafka,flume,Twitter,MQTT,ZeroMQ,Kinesis和简单的TCp套接字等多种数据源,输入流接收器负责接入数据,是接入数据流的接口规范,Dstream是SparkStreaming中所有数据流的抽象,Dstream可以被组织为Dstream Graph,Dstream本质上由一系列连续的RDD组成 GraphX:Spark提供的分布式图计算框架,GraphX主要遵循整体同步并行计算模式下的Pregel模式实现,GraphX提供了对图的抽象Graph,Graph由顶点(Vertex),边(Edge)及继承了Edge的EdgeTriplet(添加了srcAttr和dstAttr用来保存源顶点和目的顶点的属性)三种结构组成,GraphX目前已经封装了最短路径,网页排名,连接组件,三角关系统计等算法的实现,用户可以选择使用 MLib:Spark提供的机器学习框架,机器学习是一门设计概率论,统计学,逼近论,凸分析,算法复杂度理论等多领域的交叉学科,MLib目前已经提供了基础统计,分类,回归,决策树,随机森林,朴素贝叶斯,保序回归,协同过滤,聚类,维数缩减,特征提取与转型,频繁模式挖掘,语言模型标记语言,管道等多种数理统计,概率论,数据挖掘方面的数学算法. 3.2.spark模型设计1.spark编程模型 Spark应用程序从编写到提交,执行,输出的整个过程如下图: 图中描述的步骤如下: 用户使用SparkContext提供的API(常用的有textFile,sequenceFile,runJob,stop等)编写的Driver Application程序,此外SQLContext,HiveContext及StreamingContext对SparkContext进行封装,并提供了SQL,Hive及流式计算相关的API 使用SparkContext提交的用户应用程序,首先会使用BlockManager和BroadcastManager将任务的Hadoop配置进行广播,然后由DAGScheduler将任务转换为RDD并组织DAG,DAG还将被划分为不同的stage,最后由TaskScheduler借助ActorSystem将任务提交给集群管理器(Cluster Manager) 集群管理器(Cluster Manager)给任务分配资源,即将具体任务分配到Worker上,Worker创建Executor来处理任务的运行,Standalone,Yarn,Mesos,EC2等都可以作为spark的集群管理器 2.RDD计算模型RDD可以看做是对各种数据模型的统一抽象,spark的计算过程主要是RDD的迭代计算的过程,如下图, RDD的迭代过程非常类似于管道,分区数量取决于Partition数量的设定,每个分区的数据只会在一个Task中计算,所有分区可以在多个机器节点的Executor上并行执行 4.spark的基本架构从集群部署的角度来看,spark集群由以下部分组成: Cluster Manager:Spark的集群管理器,主要负责资源的分配与管理,集群管理器分配的资源属于一级分配,他将各个Worker上的内存,CPU等资源分配个应用程序,但是并不负责对Executor的资源分配,目前,Standalone,Yarn,Mesos,EC2等都可以作为Spark的集群管理器 Worker:Spark的工作节点,对Spark应用程序来说,由集群管理器分配得到资源的Worker节点主要负责以下工作,创建Executor,将资源和任务进一步分配给Executor,同步资源信息给Cluster Manager Executor:执行计算任务的一线进程,主要负责任务的执行以及与Worker,Driver APP的信息同步 Driver APP :客户端驱动程序,也可以理解为客户端应用程序,用于将任务程序转换为RDD和DAG,并与Cluster Manager进行通信与调度 这些组成部分之间的整体关系如图:","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"第1章 环境准备","date":"2017-04-16T04:47:25.357Z","path":"2017/04/16/bigdata/深入理解spark核心思想与源码分析/第1章 环境准备/","text":"剖析spark-shell1234567891011function main() &#123; if $cygwin; then stty -icanon min 1 -echo &gt; /dev/null 2&gt;&amp;1 export SPARK_SUBMIT_OPTS=&quot;$SPARK_SUBMIT_OPTS -Djline.terminal=unix&quot; &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot; stty icanon echo &gt; /dev/null 2&gt;&amp;1 else export SPARK_SUBMIT_OPTS &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-submit --class org.apache.spark.repl.Main --name &quot;Spark shell&quot; &quot;$@&quot; fi&#125; 看到脚本spark-shell里执行了spark-submit脚本,打开spark-submit脚本:12exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot; 在脚本spark-submit中执行的是spark-class,并且增加了参数 SparkSubmit,打开spark-class脚本,如下:1234567891011121314151617181920212223242526if [ -n &quot;$&#123;JAVA_HOME&#125;&quot; ]; then RUNNER=&quot;$&#123;JAVA_HOME&#125;/bin/java&quot;else if [ `command -v java` ]; then RUNNER=&quot;java&quot; else echo &quot;JAVA_HOME is not set&quot; &gt;&amp;2 exit 1 fifiCMD=()while IFS= read -d &apos;&apos; -r ARG; do CMD+=(&quot;$ARG&quot;)done &lt; &lt;(&quot;$RUNNER&quot; -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main &quot;$@&quot;)exec &quot;$&#123;CMD[@]&#125;&quot;/*结合spark-submit脚本:exec &quot;$&#123;SPARK_HOME&#125;&quot;/bin/spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;就是:exec &quot;$RUNNER&quot; -cp &quot;$LAUNCH_CLASSPATH&quot; org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit &quot;$@&quot;*/ 读到这里,应该知道spark启动了以SparkSubmit为主类jvm进程,","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"服务器动态上下线程序的工作机制","date":"2017-04-16T04:47:25.354Z","path":"2017/04/16/bigdata/zookeeper/服务器动态上下线程序的工作机制/","text":"需求:客户端能够实时洞察到服务器上下线的变化 原理图如下: 服务器端启动时就去zookeeper注册信息,如图 客户端启动就去getChildren,获取当前在服务器列表,并且注册监听 如果服务器上下线,就会有事件通知 客户端的监听机制再去重新获取服务器列表,并注册监听 服务器端实现 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.itcast.bigdata.zkdist;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.ZooKeeper;public class DistributedServer &#123; private static final String connectString = &quot;mini1:2181,mini2:2181,mini3:2181&quot;; private static final int sessionTimeout = 2000; private static final String parentNode = &quot;/servers&quot;; private ZooKeeper zk = null; /** * 创建到zk的客户端连接 * * @throws Exception */ public void getConnect() throws Exception &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） System.out.println(event.getType() + &quot;---&quot; + event.getPath()); try &#123; zk.getChildren(&quot;/&quot;, true); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 向zk集群注册服务器信息 * * @param hostname * @throws Exception */ public void registerServer(String hostname) throws Exception &#123; String create = zk.create(parentNode + &quot;/server&quot;, hostname.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); System.out.println(hostname + &quot;is online..&quot; + create); &#125; /** * 业务功能 * * @throws InterruptedException */ public void handleBussiness(String hostname) throws InterruptedException &#123; System.out.println(hostname + &quot;start working.....&quot;); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributedServer server = new DistributedServer(); server.getConnect(); // 利用zk连接注册服务器信息 server.registerServer(args[0]); // 启动业务功能 server.handleBussiness(args[0]); &#125;&#125; 客户端实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990package cn.itcast.bigdata.zkdist;import java.util.ArrayList;import java.util.List;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;public class DistributedClient &#123; private static final String connectString = &quot;mini1:2181,mini2:2181,mini3:2181&quot;; private static final int sessionTimeout = 2000; private static final String parentNode = &quot;/servers&quot;; // 注意:加volatile的意义何在？ private volatile List&lt;String&gt; serverList; private ZooKeeper zk = null; /** * 创建到zk的客户端连接 * * @throws Exception */ public void getConnect() throws Exception &#123; zk = new ZooKeeper(connectString, sessionTimeout, new Watcher() &#123; @Override public void process(WatchedEvent event) &#123; // 收到事件通知后的回调函数（应该是我们自己的事件处理逻辑） try &#123; //重新更新服务器列表，并且注册了监听，很重要 getServerList(); &#125; catch (Exception e) &#123; &#125; &#125; &#125;); &#125; /** * 获取服务器信息列表 * * @throws Exception */ public void getServerList() throws Exception &#123; // 获取服务器子节点信息，并且对父节点进行监听 List&lt;String&gt; children = zk.getChildren(parentNode, true); // 先创建一个局部的list来存服务器信息 List&lt;String&gt; servers = new ArrayList&lt;String&gt;(); for (String child : children) &#123; // child只是子节点的节点名 byte[] data = zk.getData(parentNode + &quot;/&quot; + child, false, null); servers.add(new String(data)); &#125; // 把servers赋值给成员变量serverList，已提供给各业务线程使用 serverList = servers; //打印服务器列表 System.out.println(serverList); &#125; /** * 业务功能 * * @throws InterruptedException */ public void handleBussiness() throws InterruptedException &#123; System.out.println(&quot;client start working.....&quot;); Thread.sleep(Long.MAX_VALUE); &#125; public static void main(String[] args) throws Exception &#123; // 获取zk连接 DistributedClient client = new DistributedClient(); client.getConnect(); // 获取servers的子节点信息（并监听），从中获取服务器信息列表 client.getServerList(); // 业务线程启动 client.handleBussiness(); &#125;&#125;","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper选举及数据一致性","date":"2017-04-16T04:47:25.353Z","path":"2017/04/16/bigdata/zookeeper/zookeeper选举及数据一致性/","text":"zab协议ZooKeeper Atomic Broadcast 即ZooKeeper原子消息广播协议，简称为ZAB 选举过程需要依赖此协议 数据写入过程也需要此协议 ab的核心是定义了那些会改变zk服务器数据状态的事务请求的处理方式 所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其它服务器则成ОFollower服务器。 Leader服务器负责将一个客户端事务请求转换成一个proposal(提议),并将该Proposal分发给集群中所有的Follower服务器。之后Leader服务器需要等待所有Follower服务器的反馈，一旦超过半数的Follower服务器进行了正确的反馈后，那ТLeader就会再次向所有的Follower服务器分发Commit消息，要求其将前一个Proposal进行提交 zab协议的三阶段: 发现ͧDiscovery，即选举Leader过程 同步(Synchronization),选举出新的Leader后,Follwer或者Observer从Leader同步最新数据 广播,同步完成之后,就可以接收客户端新的事物请求,并进行广播,实现数据在集群节点的副本存储 服务器角色 Leader 事物请求的唯一调度和处理者,保证集群事物处理的顺序性 集群内部各服务器的调度者 Follower 处理客户端非事物请求,转发事物请求给Leader服务器 参与事物请求Proposal的投票 参与Leader选举投票 Observer 处理客户端非事物请求,转发事物请求给Leader服务器 不参与任何形式的投票,包括选举和事务投票(超过半数确认) 此角色存在通常是为了提高读性能 服务器状态 LOOKING 寻找Leader状态 当服务器处于此状态时,表示当前没有Leader,需要进入选举流程 Following 跟随者状态,表名当前服务器角色是Follower Observing 观察者状态,表明当前服务器角色为Observer Leading Leader状态,表明当前服务器角色为Leader 以上四种状态由:org.apache.zookeeper.server.quorum. ServerState类维护 集群通信 基于tcp协议 为了避免重复创建两个节点之间的tcp连接,zk按照myid数值方向来建立连接,id大的向小的发起连接，在zk实现中，当发现自己的id比发起连接的id还大时,会关闭此连接 多端口 配置文件中第一个端口是通信和数据同步端口,默认是2888第二个端口是投票端口,默认是3888 选举触发的时机 集群启动 寻找Leader状态 当服务器处于此状态时,表示当前没有Leader,需要进入选举流程 崩溃恢复 Leader宕机 网络原因导致过半数节点与Leader心跳中断 影响成为Leader的因素 数据新旧程度 只有拥有最新数据的接单才能有机会成为Leader 通过事务id(zxid)的大小来表示数据的新旧,越大代表数据越新 myid 集群启动时,会在data目录下配置myid文件,里面的数字代表当前zk服务器节点的编号 当zk服务器节点数据一样新时,myid中数字越大的就会选举称为Leader 当集群中已经有Leader时,新加入的节点不会影响原有的集群 投票数量 只有得到集群中多半的投票,才能称为Leader 多半即:(n/2)+1,其中n为集群中的节点数量 zxid(事务id)的构成 主进程周期 也叫epoch 选举的轮次,每多一次选举,则主进程周期加1 zxid总共64位来表示,其高32位代表主进程周期 比较数据新旧的时候,先比较epoch的大小 事务单调递增的计数器 zxid的低32位表示,选举完成后,从0开始 zk的初次启动时的选举过程 第一步:启动myid为1的节点,此时zxid为0,此时没法选举出主节点 第二步:启动myid为2的节点,他的zxid也为0,此时2这个节点成为主节点 第三步:启动myid为3的节点,因为已经有主节点,则3加入集群,2还是leader zk主节点宕机的选举过程 场景说明 3台机器,此时server2为主,并且server2宕机 选举流程 变更状态 当leader宕机后,其他节点的状态变更为Looking 每个server发出一个投自己的票的投票 生成投票信息(myid,zxid) 假定:server1为(1,123),server3为(2,122) server1发给server3,server3发给server1 接收投票 投票处理 server1收到server3,因为server1的zxid(123)比server3的zxid(122)大,所以server3修改自己的投票为(1,123),然后发送给server1 server1收到server3的投票,因为123大于122,因此不改变自己的投票 投票统计 server3统计:自己收到投票(包括自己投的)中,(1,123)是两票 server1统计:自己收到的投票(包括自己投的)中,(1,123)是两票 修改服务器状态 server3,选出的leader是server1,因此自己进入followering,即:follower角色 server1,选出的leader是server1,即自己,因此自己进入leading状态,即:自己是leader角色 同步 同步时机 当leader完成选举后,follower需要与新的leader同步数据 同步准备–leader leader告诉其他follower当前最新数据是什么即zxid leader构建一个newleader的包,包括当前最大的zxid,发送给所有的follower或者Observer leader给每个follower创建一个线程leaderHandler来负责处理每个follower的数据同步请求,同时主线程开始阻塞,只有超过一半的follower同步完成,同步过程才完成,leader才能成为真正的leader 根据同步算法进行操作 同步准备–follower端 选举完成后,尝试与leader建立同步连接,如果一段时间没有连接上就报错超时,重新回到选举状态 向leader发送followerinfo包,带上follower自己最大的zxid 根据不同同步算法进行操作 初始化 minCommittedLog:最小的事务日志id,即zxid(没有被快照存储的日志文件的第一条,每次快照存储完,会重新生成一个事务日志文件) maxCommittedLog:事务日志中最大的事务,即zxid 同步算法 直接差异化同步(diff同步) 仅回滚同步(trunc),即删除多余的事务日志,比如原来的主宕机后重新加入,可能存在他自己写入提交但是别的节点还没来得及提交 先回滚再差异化同步(trunc+diff同步) 全量同步(snap同步) 同步应用的场景场景一 把follower最后的事务zxid称为peerLastZxid 当minCommittedLog&lt;peerLastZxid&lt;maxCommittedLog 同步方案: 直接差异化同步 leader会给follower服务器发送diff指令,意思是:进入差异化数据同步阶段,leader会把proposal同步给follower 实际同步过程会先发送数据修改proposal,然后再发送commit指令数据包 举例说明 某个时刻Leader服务器为proposal队列对应的zxid依次是0x500000001 0x500000002 0x500000003 0x500000004 0x500000005此时follower的peerLastZxid为0x500000003，因此需要把0x500000004 0x500000005同步给follower 差异化同步的消息发送顺序如下: 发送顺序 数据包类型 对应的zxid 1 proposal 0x500000004 2 commit 0x500000004 3 proposal 0x500000005 4 commit 0x500000005 执行的流程如下: follower端收到diff指令,然后进入diff同步阶段 follower收到同步的数据和提交命令,并应用到内存数据库中 同步完成后: leader会发送一个newLeader指令,通知follower已经将最新的数据都同步给follower了 follower收到newLeader指令后反馈一个ack消息,表名自己已经完成同步 单个follower的同步完成,Leader进入集群的”过半策略”等待状态 当有超过一半的follower都同步完成后,leader会向已经完成同步的follower发送uptodate指令,用于通知follower完成数据同步,可以对外提供服务了 follower收到leader的uptodate指令后,会终止数据同步流程,向leader再次反馈ack消息 场景二Leader在提交本地完成，还没有把事务Proposal提交给其它节点前，leader宕机了 假设3个节点的集群，分别是A,B,Cͺ没有宕机前，leader是B，已经发送过0X500000001和0X500000002的数据和事务提交proposal，并且发送了0X500000003的数据修改提议，但是在B节点发送事务提交的proposal之前，B宕机了，由于B是本机发送，所以B的本地事务已经提交，即B最新的数据是0X500000003 在A和C进行选举后，C成为主,并且进行过两次数据修改，对应的Proposal是0X600000001 0X600000002 B机器恢复后加入集群(AC), 重新进行数据同步，对于B来说，peerLastZxidО0X500000003,对于当前的主C来说, minCommittedLog= 0X500000001 maxCommittedLog= 0X600000002 同步方案: B恢复后,并且向已有的集群(AC)注册后,向C发起同步连接请求 B向leader(C)发送followerinfo包,带上follower自己最大的zxid C发现B上没有自己的事务提交记录(0X500000003),则向B发送trunc命令,让B混滚到0X500000002 B完成混滚后,向C发送信息包,确认完成,并说明当前的zxid为0X500000002 C向B发送diff同步命令 B收到Diff命令后进入同步状态,并向C发送ack确认包 C陆续把对应的差异数据和Commit提交proposal发送给B,当数据发送完成后,在发送通知包给B B应用用于内存的数据结构,当收到C通知已经完成同步后,B给回应ACK,并且结束同步 场景三某个节点宕机时间太长,当恢复并且加入集群后,数据的事务日志文件已经生成多个,此时的minCommittedLog比节点宕机时的最大日志还要大 假设B宕机后,几天后才恢复,此时minCommittedLog为0X6000008731，而peerLastZxid为0X500000003 同步方案 采用全量同步(snap) 当leader(C)发现,B的zxid小于minCommittedLog时,向B发送snap指令 B收到指令,进入同步阶段 leader(C)会从内存数据库中获取全量的数据发送给B B收到数据处理完成后,C还会把最新的proposal(全量同步期间产生)通过增量的方式发送给B 广播流程 集群选举完成后,并且完成数据同步后,即可开始对外提供服务,接收读写请求 当leader接收到客户端新的事务请求后,会生成对应的事务proposal,并根据zxid的顺序向所有的follower发送提案,即:proposal 当follower收到leader的事务proposal时,根据接收的先后顺序处理这些proposal,即如果先后收到1,2,3条,则如果处理完了第3条,则代表1,2两条一定已经处理成功 当leader收到follower针对某个事务proposal过半的ack后,则发起事务提交,重新发送一个commit的proposal follower收到commit的proposal后,记录事务提交,并把数据更新到内存数据库 补充说明: 由于只有过半的机器给出反馈,则可能存在某时刻某些节点数据不是最新的 业务上如果需要确认读取到的数据是最新的,则可以读取之前,调用sync方法进行数据同步","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper运维日常","date":"2017-04-16T04:47:25.352Z","path":"2017/04/16/bigdata/zookeeper/zookeeper运维日常/","text":"性能优化 Zk本身是基于java实现的，因此,调大JVM内存是优化点之一,具体数额需要根据业务情况来定,涉及到的JVM参数为-Xmx-Xms-Xmn IO优化:把事务日志与快照存储分磁盘存储,提高IOPS,最好事务日志单独磁盘挂载 加大linux系统的文件句柄数和用户线程数,通过linux的命令ulimit可以查看当前的配置 业务并发高时,可以创建多于1个的客户端会话,可以不同的业务模块采用不同的客户端实例 利用zk进行业务开发时,尽量通过良好的设计减少资源消耗,比如watcher的数量 节点数量,在写少,读多应用场景,采用多一点的节点会提升整体的读并发性能 节点数据最好比默认的10M还小 带宽尽量高,可以通过网络监控查看带宽是否已经是瓶颈 扩容 停机增加相应的节点即可,比较简单 不停机 增加新的节点,id一定要比原来集群的要大 新节点启动后会加入集群,并且同步数据 当用mntr命令查看新的节点数据已经同步成功后做下面的操作 按照之前的id顺序依次再去关闭zk实例,然后修改配置,启动实例 容灾 单机房的容灾靠zk本身的集群机制就能很好的支撑 多机房的容灾由于多半投票机制,zk不支持双机房的容灾,比如5节点,分为2和3,当3这个机房出现故障 ,2不能选举成功因此,多机房容灾主要是考虑三机房的情况跨机房的网络延时较大,做这个容灾要避免大量写应用场景 为了避免服务器的地址变化影响客户端,客户端尽量采用域名的方式 重点监控指标 连接数 注册的watchers数 zk事件通知的延时是否过大 zookeeper事务日志 磁盘IO 可以开启事务日志自动清理 autopurge.snapRetainCount autopurge.purgeInterval=24","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper目录文件说明","date":"2017-04-16T04:47:25.351Z","path":"2017/04/16/bigdata/zookeeper/zookeeper目录文件说明/","text":"1.bin目录文件123456789101112131415161718/*.sh为linux的脚本文件.cmd为Windows的脚本文件*/[root@data-1-1 zookeeper0]# cd bin/[root@data-1-1 bin]# lltotal 48-rwxr-xr-x 1 1000 1000 238 Feb 20 2014 README.txt-rwxr-xr-x 1 1000 1000 1937 Feb 20 2014 zkCleanup.sh-rwxr-xr-x 1 1000 1000 1049 Feb 20 2014 zkCli.cmd -rwxr-xr-x 1 1000 1000 1534 Feb 20 2014 zkCli.sh #客户端启动脚本-rwxr-xr-x 1 1000 1000 1333 Feb 20 2014 zkEnv.cmd-rwxr-xr-x 1 1000 1000 2697 Nov 5 16:53 zkEnv.sh #环境变量启动脚本-rwxr-xr-x 1 root root 2696 Nov 5 16:44 zkEnv.sh.bak-rwxr-xr-x 1 1000 1000 1084 Feb 20 2014 zkServer.cmd-rwxr-xr-x 1 1000 1000 5797 Nov 6 19:35 zkServer.sh #服务端启动脚本-rw-r--r-- 1 root root 5980 Nov 5 16:45 zookeeper.out #日志文件[root@data-1-1 bin]# 2.conf目录 zoo_sample.cfg为样例配置文件，需要修改为自己的名称，一般为zoo.cfg Log4j.properties为日志配置文件123456789[root@data-1-1 zookeeper0]# cd conf/[root@data-1-1 conf]# lltotal 48-rw-rw-r-- 1 1000 1000 535 Feb 20 2014 configuration.xsl-rw-rw-r-- 1 1000 1000 2161 Nov 5 16:55 log4j.properties #log4j日志-rw-r--r-- 1 root root 1085 Nov 6 19:20 zoo.cfg #启动的配置文件-rw-r--r-- 1 root root 21487 Nov 6 23:11 zookeeper.out #日志文件-rw-rw-r-- 1 1000 1000 922 Feb 20 2014 zoo_sample.cfg.bak #启动的配置文件（原）[root@data-1-1 conf]# 3.contrib目录一些用于操作zk的工具包123456789101112[root@data-1-1 zookeeper0]# cd contrib/[root@data-1-1 contrib]# lltotal 32drwxr-xr-x 4 1000 1000 4096 Feb 20 2014 fatjardrwxr-xr-x 3 1000 1000 4096 Feb 20 2014 loggraphdrwxr-xr-x 2 1000 1000 4096 Feb 20 2014 restdrwxr-xr-x 3 1000 1000 4096 Feb 20 2014 zkfusedrwxr-xr-x 4 1000 1000 4096 Feb 20 2014 zkperldrwxr-xr-x 3 1000 1000 4096 Feb 20 2014 zkpythondrwxr-xr-x 4 1000 1000 4096 Feb 20 2014 zktreeutildrwxr-xr-x 7 1000 1000 4096 Feb 20 2014 ZooInspector[root@data-1-1 contrib]# 4.lib目录zk依赖的某些jar包 5.recipeszk某些用法的代码示例 6.dist-maven 目录maven编译后的发布目录 7.dataDir目录在zoo.cfg中的dataDir 选项配置中是数据的配置目录如，默认是：/tmp/zookeeper/那么在zk服务启动额时候，在/tmp/zookeeper/zookeeper_server.pid 是server的进程pid如果将服务停止，那么该文件将会被自动删除","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper的配置文件说明","date":"2017-04-16T04:47:25.349Z","path":"2017/04/16/bigdata/zookeeper/zookeeper的配置文件说明/","text":"1.基本配置最低配置要求中必须配置的参数123456789101112clientPort#监听客户端连接的端口。tickTime#基本事件单元，这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，每隔tickTime时间就会发送一个心跳；最小 的session过期时间为2倍tickTime dataDir#存储内存中数据库快照的位置，如果不设置参数，更新事务的日志将被存储到默认位置。/*应该谨慎的选择日志存放的位置，使用专用的日志存储设备能够大大提高系统的性能，如果将日志存储在比较繁忙的存储设备上，那么将会很大程度上影像系统性能*/ 2.高级配置高级配置参数中可选配置参数1234567891011121314151617dataLogdDir#这个操作让管理机器把事务日志写入“dataLogDir”所指定的目录中，而不是“dataDir”所指定的目录。这将允许使用一个专用的日志设备，帮助我们避免日志和快照的竞争maxClientCnxns/*这个操作将限制连接到Zookeeper的客户端数量，并限制并发连接的数量，通过IP来区分不同的客户端。此配置选项可以阻止某些类别的Dos攻击。将他设置为零或忽略不进行设置将会取消对并发连接的限制。例如，此时我们将maxClientCnxns的值设为1，如下所示：#set maxClientCnxns maxClientCnxns=1启动Zookeeper之后，首先用一个客户端连接到Zookeeper服务器上。之后如果有第二个客户端尝试对Zookeeper进行连接，或者有某些隐式的对客户端的连接操作，将会触发Zookeeper的上述配置*/minSessionTimeout和maxSessionTimeout #即最小的会话超时和最大的会话超时时间。在默认情况下，minSession=2*tickTime；maxSession=20*tickTime 3.集群配置1234567891011121314151617181920212223242526initLimit=10#此配置表示，允许follower(相对于Leaderer言的“客户端”)连接并同步到Leader的初始化连接时间，以tickTime为单位。当初始化连接时间超过该值，则表示连接失败。 syncLimit=5#此配置项表示Leader与Follower之间发送消息时，请求和应答时间长度。如果follower在设置时间内不能与leader通信，那么此follower将会被丢弃。 server.A=B：C：D/*#Exampleserver.0=hadoop:2288:3388server.1=hadoop0:2288:3388server.2=hadoop1:2288:3388A：其中 A 是一个数字，表示这个是服务器的编号；B：是这个服务器的 ip 地址；C：Leader选举的端口；D：Zookeeper服务器之间的通信端口。*/ myid和zoo.cfg/* 除了修改 zoo.cfg 配置文件，集群模式下还要配置一个文件 myid，这个文件在 dataDir 目录下，这个文件里面就有一个数据就是 A 的值，Zookeeper 启动时会读取这个文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是那个 server*/","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper的典型应用场景","date":"2017-04-16T04:47:25.348Z","path":"2017/04/16/bigdata/zookeeper/zookeeper的典型应用场景/","text":"1.数据发布/订阅数据发布/订阅即所谓的配置中心：发布者将数据发布到zk的一个或者一系列节点上，阅者进行数据订阅，当数据有变化时，可以及时得到数据的变化通知 因为应用B对zk的某一个节点进行了监听（watch） ，所以当应用A向该监听的节点发布数据的时候（即节点的数据发生变化），此时会触发监听，然后就会通知应用B（即订阅的过程） 2.负载均衡本质是利用zookeeper的配置管理功能，涉及的步骤为： 服务提供者把自ٜ的域名及IP 端口的映射注册到zk中 服务消费者通过域名从zk中获取到对应的IP及端口，这个IP及端口有多个，只是获取其中一个 当服务提供者宕机时，对应的域名与IP的对应就会减少一个映射 阿里的dubbo服务框架就是基于zk来实现服务路由和负载 3.命令服务&emsp;在分布式系统中，服务命令（Name Service）也是很重要的应用场景，通过zk也可以实现类似于J2EE中的JNDI的效果；分布式环境中，命令服务更多的是资源定位，并不是真正的实体资源，其本质是用到zk的其中配置管理和查找 4.分布式协调和通知 通过watcher和通知机制实现 分布式锁 分布式事务 5.集群管理 当前集群中的机器数量 集群中的机器的运行时状态 集群中节点的上下线操作 集群节点的统一配置 6.Master选举 临时节点 顺序节点 7.分布式锁 排他锁 共享锁 8.分布式队列FIFO","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper的事务日志","date":"2017-04-16T04:47:25.347Z","path":"2017/04/16/bigdata/zookeeper/zookeeper的事务日志/","text":"日志文件的存储路径 存储于datalog或者是dataLogDir配置目录 对应目录下的version-2代表的是日志格式版本号 日志文件命名 文件大小都是64m 后缀都是16进制格式数字,逐渐增大,其本质是本日志文件的第一条zxid号 日志格式 zk提供了工具类org.apache.zookeeper.server. LogFormatter解析日志的内容 第一行是日志格式信息 日志写入 Zk通过类org.apache.zookeeper.server.persistence. FileTxnLog实现对事务日志的管理 通过append方法来添加事务日志 写入过程 确定是否有事务日志文件可写,当第一次创建事务日志文件或者上一个事务日志文件写满后都会关闭这个文件流 确定事务日志是否需要扩容,当文件剩余空间不足4KB时,把文件新增64MB(新增一个日志文件),用0填充剩余的空间 事务序列化 生成checksum 写入事务日志文件流 事务日志刷入磁盘,本质是调用系统的fsync接口 数据快照 zk某个时刻的完整数据 快照文件的后缀为服务器最新的zxid 通过工具类SnapshotFormatter可以查看快照文件的文件内容 快照流程 确定是否需要进行数据快照 snapCount默认是100000,表示达到这个数量的日志才开始进行快照 为了避免集群节点同时进行快照,按照如下公式触发快照:logCount &gt;(snapCount/2+randRoll) //randRoll是为1—snapCount/2之间的随机数 切换事务日志文件 创建新的事务日志文件 创建数据快照异步线程 获取全量数据和会话信息 生成快照数据文件 把数据刷入快照文件","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper的Znode数据模型","date":"2017-04-16T04:47:25.345Z","path":"2017/04/16/bigdata/zookeeper/zookeeper的Znode数据模型/","text":"1.zkdatabase、datatree、datanode的关系 1.1.DataTree整个zk的数据就靠datatree维护，包括数据、目录、权限, DataTree是内存数据存储的核心，是一个树结构，代表了内存中一份完整的数据。DataTree不包含任何与网络、客户端连接及请求处理相关的业务逻辑，是一个独立的组件, 默认初始化三目录 1.2.DataNode 树形结构中的每个节点叫做做Znode 使用路径来引用一个节点，节点的路径是绝对的，没有相对路径，如：/app1是一个节点，/app1/p_1是一个节点 DataNode是数据存储的最小单元，其内部除了保存了结点的数据内容、ACL列表、节点状态之外，还记录了父节点的引用和子节点列表两个属性，其也提供了对子节点列表进行操作的接口。 1.3. ZKDatabaseZookeeper的内存数据库，管理Zookeeper的所有会话、DataTree存储和事务日志。ZKDatabase会定时向磁盘dump快照数据，同时在Zookeeper启动时，会通过磁盘的事务日志和快照文件恢复成一个完整的内存数据库。","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper的ACL","date":"2017-04-16T04:47:25.344Z","path":"2017/04/16/bigdata/zookeeper/zookeeper的ACL/","text":"ACL组成 Scheme:id:permission 比如:world:anyone:crdwa Scheme:验证过程中使用的检验策略 id:权限被赋予的对象,比如ip或者某个用户 permission为权限,上面的crdwa,表示5个权限组合 通过setAcl命令设置节点的权限 节点的acl不具有继承关系 getAcl可以查看节点的acl权限信息 1234[zk: localhost:2181(CONNECTED) 0] getAcl /student&apos;world,&apos;anyone: cdrwa Scheme类型–world scheme:id:permission id为固定值,anyone,表示任何用户 world:anyone:crdwa 表示任何用户都具有crdwa权限 1234567[zk: localhost:2181(CONNECTED) 1] setAcl /student world:anyone:ca[zk: localhost:2181(CONNECTED) 2] getAcl /student&apos;world,&apos;anyone: ca #没有读权限[zk: localhost:2181(CONNECTED) 3] get /studentAuthentication is not valid : /student Scheme类型–auth Scheme:id:permisstion,比如:auth:username:password:crdwa 表示给认证通过的所有用户设置acl权限 同时可以添加多个用户 通过addauth命ј进行认证用户的添加:addauth digest : auth策略的本质就是digest 如果通过addauth创建多组用户和密码，当使用setAcl修改权限时，所有的用户和密码的权限都会跟着修改 通过addauth新创建的用户和密码组需要重新调用setAcl才会入到权限组中去 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#创建节点[zk: localhost:2181(CONNECTED) 4] create /node2 &quot;node2data&quot;Created /node2#获取默认的acl[zk: localhost:2181(CONNECTED) 5] getAcl /node2&apos;world,&apos;anyone: cdrwa#设置acl(不能设置,因为node2u:1111这样的用户是不存在的,所以验证不通过,即不能设置)[zk: localhost:2181(CONNECTED) 6] setAcl /node2 auth:node2u:1111:crdwaAcl is not valid : /node2#添加用户[zk: localhost:2181(CONNECTED) 7] addauth digest node2u:1111#再次设置acl[zk: localhost:2181(CONNECTED) 8] setAcl /node2 auth:node2u:1111:crdwacZxid = 0x2dctime = Thu Feb 23 21:52:48 CST 2017mZxid = 0x2dmtime = Thu Feb 23 21:52:48 CST 2017pZxid = 0x2dcversion = 0dataVersion = 0aclVersion = 1ephemeralOwner = 0x0dataLength = 11numChildren = 0[zk: localhost:2181(CONNECTED) 9]#在另外一个客户端上(因为本地没有验证用户)[zk: localhost:2181(CONNECTED) 0] get /node2Authentication is not valid : /node2#添加验证用户[zk: localhost:2181(CONNECTED) 3] addauth digest node2u:1111[zk: localhost:2181(CONNECTED) 4] get /node2&quot;node2data&quot;cZxid = 0x2dctime = Thu Feb 23 21:52:48 CST 2017mZxid = 0x2dmtime = Thu Feb 23 21:52:48 CST 2017pZxid = 0x2dcversion = 0dataVersion = 0aclVersion = 1ephemeralOwner = 0x0dataLength = 11numChildren = 0[zk: localhost:2181(CONNECTED) 5] Scheme类型–digest scheme:id:permission, 比如: digest:username:password:crdwa 指定某个用户及他的密码可以访问 此处的username:password必须进过SHA-1和BASE64编码 BASE64(SHA1(username:password)) 通过addauth命令进行认证用户的添加 addauth digest : Scheme类型—IP Scheme:id:permission ，比如: ip:127.0.0.1:crdwa 指定某个IP地址可以访问 12345678910111213141516171819202122[zk: localhost:2181(CONNECTED) 8] create /node5 &quot;node5data&quot;Created /node5[zk: localhost:2181(CONNECTED) 9] setAcl /node5 ip:127.0.0.1:crdwacZxid = 0x34ctime = Thu Feb 23 22:06:30 CST 2017mZxid = 0x34mtime = Thu Feb 23 22:06:30 CST 2017pZxid = 0x34cversion = 0dataVersion = 0aclVersion = 1ephemeralOwner = 0x0dataLength = 11numChildren = 0[zk: localhost:2181(CONNECTED) 10] get /node5 ##这是为什么?????Authentication is not valid : /node5[zk: localhost:2181(CONNECTED) 11] setAcl ip:192.168.0.33:crdwa[zk: localhost:2181(CONNECTED) 12] get /node5Authentication is not valid : /node5[zk: localhost:2181(CONNECTED) 13] Scheme类型—super 供运维人员维护节点使用 有权限操作任何节点 启动时,在命令参数中配置 -Dzookeeper.DigestAuthenticationProvider.superDigest=admin:015uTByzA4zSglcmseJsxTo7n3c= 打开zkCli.cmd，在java命令后面增加以上配置 用户名和密码也需要通过sha1和base64编码","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper开源客户端","date":"2017-04-16T04:47:25.343Z","path":"2017/04/16/bigdata/zookeeper/zookeeper开源客户端/","text":"1.原生api的不足 连接的创建是异步的，需要开发人员自行编码实现等待 连接没有自动的超时重连机制 zk本省不提供序列化机制,需要开发人员自行制定,从而实现数据的序列化和反序列化 watcher注册一次只会生效一次,需要不断的重新注册 watcher的使用方式不合符java本身的术语,如果采用监听方式,将更容易理解 不支持递归创建树形节点 2.开源客户端–ZkClient介绍Github上一个开源的zk客户端，由datameer的工程师Stefan Groschupf和Peter Voss一起开发 解决session会话超时重连 watcher反复注册 简化开发API 其他 github地址:https://github.com/sgroschupf/zkclient 特点: 简单 社区不活跃,连API文档都不完善 3.开源客户端—Curator介绍Apache基金会的顶级项目之一 解决session会话超时重连 Watcher反复注册 简化开发api 遵循Fluent风格Api规范 NodeExistsException异常处理 大招:共享锁服务 master选举 分布式计数器等 其他 http://curator.apache.org Curator—API创建会话12345CuratorFrameworkFactory.newClient(connectString, retryPolicy)CuratorFrameworkFactory.newClient(connectString, sessionTimeoutMs, connectionTimeoutMs, retryPolicy) #启动:start()方法CuratorFrameworkFactory.newClient(&quot;localhost:2181,localhost:2182&quot;, retryPolicy).start(); 参数 说明 connectString 逗՚分开的ip:port对 retryPolicy 重试策略，默认四种:Exponential BackoffRetry,RetryNTimes,RetryOneTime,RetryUntilElapsed sessionTimeoutMs 会话超时时间，单位为毫秒，默认60000ms connectionTimeoutMs 连接创建超时时间，单位为毫秒，默认是15000ms 重试策略 实现接口RetryPolicy可以自定义重试策略123public abstract interface org.apache.curator.RetryPolicy &#123; public abstract boolean allowRetry(int retryCount, long elapsedTimeMs, RetrySleeper sleeper);&#125; 参数名 说明 retryCount 已经重试的次数，如果第一次重试，此值为0 elapsedTimeMs 重试花费的时间，单位为毫秒 sleeper 类似于Thread.sleep,用于sleep指定时间 返回值 如果还会继续重试,则返回Ture 四种默认重试策略 ExponentialBackoffRetry ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries) ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs) 当前应该sleep的时间: baseSleepTimeMs * Math.max(1, random.nextInt(1 &lt;&lt; (retryCount + 1))) 参数名 说明 baseSleepTimeMs 初始化sleep时间 maxRetries 最大重试次数 maxSleepMs 最大重试时间 返回值 如果还会继续重试,则返回Ture RetryNTimes RetryNTimes(int n, int sleepMsBetweenRetries) 参数名 说明 n 最大重试次数 sleepMsBetweenRetries 每次重试的时间间隔 RetryOneTime 只重试一次 RetryOneTime(int sleepMsBetweenRetry) #sleepMsBetweenRetry为重试间隔的时间 RetryUntilElapsed RetryUntilElapsed(int maxElapsedTimeMs, int sleepMsBetweenRetries) 重试的时间超过最大时间后，就不再重试 参数名 说明 maxElapsedTimeMs 最大重试时间 sleepMsBetweenRetries 每次间隔重试时间 Fluent风格的API 定义:一种面向对象的开发方式，目的是提高代码的可读性 实现方式:通过方法的级联或者方法链的方式实现 举例 12345678910private CuratorFramework client = null;RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);//以方法链的方式client = CuratorFrameworkFactory.builder() .connectString(&quot;localhost:2181,localhost:2182&quot;) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace(&quot;base&quot;).build();client.start(); 创建节点12345client.create() //返回CreateBuilder .creatingParentsIfNeeded() //递归创建父目录 .withMode(CreateMode.PERSISTENT)//设置节点属性，比如:CreateMode.PERSISTENT，如果是递归创建模式为临时节点，则只有ՙ子节点是临时节点，非ՙ子节点都为持久节点 .withACL(Ids.OPEN_ACL_UNSAFE)//设置acl .forPath(path, data);//指定路径 删除节点123456client.delete() //返回DeleteBuilder .guaranteed() //确保节点被删除 .deletingChildrenIfNeeded() //递归删除所有子节点 .withVersion(version) //特定版本՚号 .inBackground(new DeleteCallBack()) .forPath(path); //指定路径 关于guaranteed：Solves edge cases where an operation may succeed on the server but connection failure occurs before a response can be successfully returned to the client意思是:解决当某个删除操作在服务器端可能成功，但是此时客户端与服务器端的连接中断，而删除的响应没有成功返回到客户端底层的本质:重试 关于异步操作 12345678inBackground()inBackground(Object context)inBackground(BackgroundCallback callback)inBackground(BackgroundCallback callback, Object context)inBackground(BackgroundCallback callback, Executor executor)inBackground(BackgroundCallback callback, Object context, Executor executor)//从参数看跟zk的原生异࠵api相同，多了一个线程池，用于执行回调 异步操作回调 12345678910111213client.delete().guaranteed().deletingChildrenIfNeeded(). withVersion(version).inBackground(new DeleteCallBack()).forPath(path);----------------DeleteCallBack--------------------public class DeleteCallBack implements BackgroundCallback &#123; public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println(event.getPath()+&quot;,data=&quot;+event.getData()); System.out.println(&quot;event type=&quot;+event.getType()); System.out.println(&quot;event code=&quot;+event.getResultCode()); &#125;&#125; 异步操作事件状态:event.getType() 123456789101112131415public final enum CuratorEventType &#123; CREATE; DELETE; EXISTS; GET_DATA; SET_DATA; CHILDREN; SYNC; GET_ACL; SET_ACL; WATCHED; CLOSING;&#125; 异步操作事件状态码:event.getResultCode() 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/*--------------org.apache.zookeeper.KeeperException.Code ---------*/public static enum Code implements CodeDeprecated &#123; /** Everything is OK */ OK (Ok), /** System and server-side errors. * This is never thrown by the server, it shouldn&apos;t be used other than * to indicate a range. Specifically error codes greater than this * value, but lesser than &#123;@link #APIERROR&#125;, are system errors. */ SYSTEMERROR (SystemError), /** A runtime inconsistency was found */ RUNTIMEINCONSISTENCY (RuntimeInconsistency), /** A data inconsistency was found */ DATAINCONSISTENCY (DataInconsistency), /** Connection to the server has been lost */ CONNECTIONLOSS (ConnectionLoss), /** Error while marshalling or unmarshalling data */ MARSHALLINGERROR (MarshallingError), /** Operation is unimplemented */ UNIMPLEMENTED (Unimplemented), /** Operation timeout */ OPERATIONTIMEOUT (OperationTimeout), /** Invalid arguments */ BADARGUMENTS (BadArguments), /** API errors. * This is never thrown by the server, it shouldn&apos;t be used other than * to indicate a range. Specifically error codes greater than this * value are API errors (while values less than this indicate a * &#123;@link #SYSTEMERROR&#125;). */ APIERROR (APIError), /** Node does not exist */ NONODE (NoNode), /** Not authenticated */ NOAUTH (NoAuth), /** Version conflict */ BADVERSION (BadVersion), /** Ephemeral nodes may not have children */ NOCHILDRENFOREPHEMERALS (NoChildrenForEphemerals), /** The node already exists */ NODEEXISTS (NodeExists), /** The node has children */ NOTEMPTY (NotEmpty), /** The session has been expired by the server */ SESSIONEXPIRED (SessionExpired), /** Invalid callback specified */ INVALIDCALLBACK (InvalidCallback), /** Invalid ACL specified */ INVALIDACL (InvalidACL), /** Client authentication failed */ AUTHFAILED (AuthFailed), /** Session moved to another server, so operation is ignored */ SESSIONMOVED (-118), /** State-changing request is passed to read-only server */ NOTREADONLY (-119);&#125; 在回调函数中可以打印错误状态码1234567891011121314public class DeleteCallBack implements BackgroundCallback &#123; public void processResult(CuratorFramework client, CuratorEvent event) throws Exception &#123; System.out.println(event.getPath()+&quot;,data=&quot;+event.getData()); System.out.println(&quot;event type=&quot;+event.getType()); System.out.println(&quot;event code=&quot;+event.getResultCode()); &#125;&#125;//打印结果:/,data=nullevent type=DELETEevent code=-111 //-111表示有子节点，所以删除失败，只有为0(ok)表示删除成功 读取数据123456789101112public void readNode(String path) throws Exception &#123; Stat stat = new Stat(); byte[] data = client.getData().storingStatIn(stat).forPath(path); System.out.println(&quot;读取节点&quot; + path + &quot;的数据:&quot; + new String(data)); System.out.println(stat.toString());&#125;/*client.getData() 返回GetDataBuilderstoringStatIn(org.apache.zookeeper.data.Stat stat) //把服务器端获取的状态数据存储到stat对象Byte[] forPathͧString pathͨ//节点路径*/ 更新数据1234567891011public void updateNode(String path, byte[] data, int version) throws Exception &#123; client.setData().withVersion(version).forPath(path, data);&#125;/*client.setData() //返回SetDataBuilderwithVersion(version) //特定版本号forPath(path, data) //节点路径和dataforPath(path) //节点路径*/ 读取子节点12345678910111213141516public void getChildren(String path) throws Exception &#123; List&lt;String&gt; children = client.getChildren().usingWatcher(new WatcherTest()).forPath(&quot;/curator&quot;); for (String pth : children) &#123; System.out.println(&quot;child=&quot; + pth); &#125;&#125;/*client.getChildren() //返回GetChildrenBuilderstoringStatIn(org.apache.zookeeper.data.Stat stat) //把服务器端获取的状态数据存储到stat对象Byte[] forPathͧString pathͨ//节点路径usingWatcher(org.apache.zookeeper.Watcher watcher) //设置watcher，类似于zk本身的api，也只能使用一次usingWatcher(CuratorWatcher watcher) //设置watcher ，类似于zk本身的api，也只能使用一次*/ 设置watcher watcher之NodeCache 监听数据节点的内容变更 监听节点的创建,即如果指定的节点不存在，则节点创建后，会触发这个监听 构造函数 12345678NodeCache(CuratorFramework client, String path)NodeCache(CuratorFramework client, String path, boolean dataIsCompressed)/*client 客户端实例path 数据节点路径dataIsCompressed 是否进行数据压缩*/ 回调接口12public interface NodeCacheListenervoid nodeChanged() //没有参数，怎么获取事件信息以及节点数据？ 实例代码123456789101112public void addNodeDataWatcher(String path) throws Exception &#123; final NodeCache nodeC = new NodeCache(client, path); nodeC.start(true); nodeC.getListenable().addListener(new NodeCacheListener() &#123;//回调接口 public void nodeChanged() throws Exception &#123; String data = new String(nodeC.getCurrentData().getData());//直接通过NodeCache获取数据 System.out.println(&quot;path=&quot; + nodeC.getCurrentData().getPath() + &quot;:data=&quot; + data); &#125; &#125;);&#125; watcher之PathChildrenCache 监听指定节点的子节点变化情况 包括:新增子节点,子节点数据变更和子节点删除 构造函数 12345678910111213PathChildrenCache(CuratorFramework client, String path, boolean cacheData)PathChildrenCache(CuratorFramework client, String path, boolean cacheData, boolean dataIsCompressed, CloseableExecutorService executorService)PathChildrenCache(CuratorFramework client, String path, boolean cacheData, boolean dataIsCompressed, ExecutorService executorService)PathChildrenCache(CuratorFramework client, String path, boolean cacheData, boolean dataIsCompressed, ThreadFactory threadFactory)PathChildrenCache(CuratorFramework client, String path, boolean cacheData, ThreadFactory threadFactory)/**/ 回调接口123interface PathChildrenCacheListener&#123; void childEvent(CuratorFramework client, PathChildrenCacheEvent event)&#125; 构造函数参数说明 参数名 说明 client 客户端实例 path 数据节点路径 dataIsCompressed 是否进行数据压缩 cacheData 用于配置是否把节点内容缓存起来，如果配置为true，那么客户端在接收到节点列表变更的同时，也能够获取到节点的数据内容ͺ如果为false,则无法取到数据内容 threadFactory 通过଑两个参数构造专门的线程池来处理事件通知 executorService 实例代码1234567891011121314151617181920212223242526272829public void addChildWatcher(String path) throws Exception &#123; final PathChildrenCache cache = new PathChildrenCache(this.client, path, true); cache.start(StartMode.POST_INITIALIZED_EVENT);//ppt中需要讲StartMode System.out.println(cache.getCurrentData().size()); cache.getListenable().addListener(new PathChildrenCacheListener() &#123; public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) throws Exception &#123; if(event.getType().equals(PathChildrenCacheEvent.Type.INITIALIZED))&#123; System.out.println(&quot;客户端子节点cache初始化数据完成&quot;); System.out.println(&quot;size=&quot;+cache.getCurrentData().size()); &#125;else if(event.getType().equals(PathChildrenCacheEvent.Type.CHILD_ADDED))&#123; System.out.println(&quot;添加子节点:&quot;+event.getData().getPath()); System.out.println(&quot;修改子节点数据:&quot;+new String(event.getData().getData())); &#125;else if(event.getType().equals(PathChildrenCacheEvent.Type.CHILD_REMOVED))&#123; System.out.println(&quot;删除子节点:&quot;+event.getData().getPath()); &#125;else if(event.getType().equals(PathChildrenCacheEvent.Type.CHILD_UPDATED))&#123; System.out.println(&quot;修改子节点数据:&quot;+event.getData().getPath()); System.out.println(&quot;修改子节点数据:&quot;+new String(event.getData().getData())); &#125; &#125; &#125;);&#125;/*PathChildrenCache.StartMode 有:BUILD_INITIAL_CACHE //同࠵初始化客户端的cache，及创建cache后，就从服务器端拉入对应的数据NORMAL //异࠵初始化cachePOST_INITIALIZED_EVENT //异࠵初始化，初始化完成触发事件PathChildrenCacheEvent.Type.INITIALIZED*/","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper客户端连接的原理","date":"2017-04-16T04:47:25.341Z","path":"2017/04/16/bigdata/zookeeper/zookeeper客户端连接的原理/","text":"创建zookeeper连接对象时，如何选择哪个服务器进行连接？ 客户端的connectstring:localhost:2181, localhost:2182, localhost:2183 通过类:org.apache.zookeeper.client. StaticHostProvider维护地址列表 通过解析connectstring,进行随机排序,形成最终的地址列表 每次从形成地址列表中选择第一个地址进行连接,如果连接不上再选择第一个地址 如果当前节点是列表的最后一个节点,则再重新选择第一个节点,相当于一个环 通过随机排序,每个zk的客户端就会随机的去连接zk服务器,分布相对均匀 举例: connectstring为:192.168.1.2:2181, 192.168.1.3:2181, 192.168.1.4:2181随机打乱后的顺序为:192.168.1.3:2181, 192.168.1.2:2181, 192.168.1.4:2181则第一次连接时选择1.3这个节点,如果连接不上,则重新选择1.2, 然后是1.4,如果1.4连接不上,则开始连接第一个节点1.3 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/*在StaticHostProvider类中,有如下的两个方法:初始化连接地址列表*/public StaticHostProvider(Collection&lt;InetSocketAddress&gt; serverAddresses) throws UnknownHostException &#123; for (InetSocketAddress address : serverAddresses) &#123; InetAddress ia = address.getAddress(); InetAddress resolvedAddresses[] = InetAddress.getAllByName((ia!=null) ? ia.getHostAddress(): address.getHostName()); for (InetAddress resolvedAddress : resolvedAddresses) &#123; if (resolvedAddress.toString().startsWith(&quot;/&quot;) &amp;&amp; resolvedAddress.getAddress() != null) &#123; this.serverAddresses.add( new InetSocketAddress(InetAddress.getByAddress( address.getHostName(), resolvedAddress.getAddress()), address.getPort())); &#125; else &#123; this.serverAddresses.add(new InetSocketAddress(resolvedAddress.getHostAddress(), address.getPort())); &#125; &#125; &#125; if (this.serverAddresses.isEmpty()) &#123; throw new IllegalArgumentException( &quot;A HostProvider may not be empty!&quot;); &#125; Collections.shuffle(this.serverAddresses); //随机排序&#125;------选择一个地址返回--------------public InetSocketAddress next(long spinDelay) &#123; ++currentIndex; //如果已经是最后一个节点,则从第一个节点开始 if (currentIndex == serverAddresses.size()) &#123; currentIndex = 0; &#125; //当地址列表只有一个地址时,再次获取之前先sleep一定时间再返回,这算是一个重试时间间隔 if (currentIndex == lastIndex &amp;&amp; spinDelay &gt; 0) &#123; try &#123; Thread.sleep(spinDelay); &#125; catch (InterruptedException e) &#123; LOG.warn(&quot;Unexpected exception&quot;, e); &#125; &#125; else if (lastIndex == -1) &#123; // We don&apos;t want to sleep on the first ever connect attempt. lastIndex = 0; &#125; return serverAddresses.get(currentIndex);//返回下一个节点的地址&#125;","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper客户端API使用(zk-API)","date":"2017-04-16T04:47:25.340Z","path":"2017/04/16/bigdata/zookeeper/zookeeper客户端API使用(zk-API)/","text":"1.创建zk实例1234ZooKeeper(String connectString, int sessionTimeout, Watcher watcher)ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, boolean canBeReadOnly)ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd)ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd, boolean canBeReadOnly) 客户端与服务器端会话的建立是一个异步的过程 完成客户端的初始化就返回，此时连接并没有真正的建立，所以主线程中使用：connectedSignal.await()去等待 当连接真正建立的时候，客户端会收到一个事件的通知，会触发Watcher的process，所以在process函数中connectedSignal.countDown();//计数器减1，阻塞结束 Zk构造方法参数说明 参数 说明 connectString 1.指zk服务器列表，host:port,示例：192.168.0.50:2180,192.168.0.51:2181 ,2.也可以在后面跟上目录，表示此客户端的操作都是在此目录下，如：192.168.0.50:2180/zk-book 表示此客户端操作的节点都是在/zk-book根目录下，比如创建/foo/bar，其实完整的路径是/zk-book/foo/bar sessionTimeout 会话超时时间，单位是毫秒，当在这个时间内没有收到心跳检测，会话就会失效 watcher 注ӆ的watcher，null表示不设置 canBeReadOnly 用于标识当前会话是否支持”read-only”模式ͺ sessionPasswd 和 sessionId 分别代表会话ID和会话密钥，这两个参数一起可以唯一确定一个会话，客户端通过这两个参数实现客户端会话复用 2.创建节点12345String create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode) #以同步的方式创建节点void create(String path, byte[] data, List&lt;ACL&gt; acl, CreateMode createMode, AsyncCallback.StringCallback cb, Object ctx) #以异步的方式创建节点 无论是上面的同步或者异步都不支持递归创建节点 当创建的节点存在时，抛出异常NodeExistsException Zk create api参数说明 参数名 说明 path 被创建的节点路径，比如：/zk-book/foo data[] 节点中的数据 acl acl策略 createMode 节点类型，枚举类型，有四种选择：持久(PERSISTENT)持久顺序（PERSISTENT_SEQUENTIAL）临时（EPHEMERAL）临时顺序（EPHEMERAL_SEQUENTIAL） cb 回调函数，需要实现接口StringCallback接口，当服务器端创建完成后，客户端会自动调用这个对象的方法processResult ctx 用于传递一个对象，可以在回调方法执行的时候用，通常用于传递业务的上下文信息 创建节点时的ACL 1.通过接口Ids可以预先定义几种scheme模式 OPEN_ACL_UNSAFE：相当于world:anyone:cdrwa CREATOR_ALL_ACL：相当于auth:用户名:密码,但是需要通过ZooKeeper的addAuthInfo添加对应的用户和密码对 READ_ACL_UNSAFE：相当于world:anyone:r，即所有人拥有读权限 2.自己定义,比如: 123456public List&lt;ACL&gt; getDigestAcl()&#123; List&lt;ACL&gt; acls = new ArrayList&lt;ACL&gt;(); Id digestId = new Id(&quot;digest&quot;, &quot;javaclient2:CGf2ydfsfdsjfsldfsdfsdfs=&quot;); acls.add(new ACL(Perms.ALL, digestId)); return acls;&#125; 3.删除节点12345void delete(String path, int version) #以同步的方式删除void delete(String path, int version, AsyncCallback.VoidCallback cb, Object ctx) #以异步的方式删除，客户端主线程不能退出，否则可能请求没有发到服务器或者是异步回调不成功 参数 说明 Path 被删除的节点的路径 Version 知道节点的数据版本，如果指定的版本不是最新版本，将会报错，它的作用类似于hibernate中的乐观锁,if the given version is -1, it matches any node’s versions(删除指定版本，-1表示删除所有版本) cb 异步回调函数 ctx 传递上下文信息，即操作之前的信息传递到删除之后的异步回调函数里（可以用于保留删除前的状态） 4.获取子节点list1234567891011121314151617List&lt;String&gt; getChildren(String path, boolean watch) #返回path节点的子节点列表void getChildren(String path, boolean watch, AsyncCallback.Children2Callback cb, Object ctx) #以异步的方式返回子节点，返回path指定节点的状态信息statvoid getChildren(String path, boolean watch, AsyncCallback.ChildrenCallback cb, Object ctx) ##以异步的方式返回子节点，不返回path指定节点的状态信息ͧstatͨList&lt;String&gt; getChildren(String path, boolean watch, Stat stat) #返回path指定节点的状态信息stat和子节点列表List&lt;String&gt; getChildren(String path, Watcher watcher)void getChildren(String path, Watcher watcher, AsyncCallback.Children2Callback cb, Object ctx)void getChildren(String path, Watcher watcher, AsyncCallback.ChildrenCallback cb, Object ctx) 参数 说明 path 数据节点的路径，比如：/zk-book/foo，获取该路径下的子节点列表 watcher 设置watcher，如果path对应节点的子节点数量发生变化，将会得到通知，允许为null watch 是否使用默认的watcher（就是在创建zk实例的时候指定的watcher） stat 指定数据节点的状态信息 cb 异步回调函数 ctx 用于传递一个对象，可以在回调方法执行的时候用，通常用于传递业务的上下文信息 5.获取节点数据1234567891011121314151617181920212223242526void getData(String path, boolean watch, AsyncCallback.DataCallback cb, Object ctx)byte[] getData(String path, boolean watch, Stat stat)void getData(String path, Watcher watcher, AsyncCallback.DataCallback cb, Object ctx)byte[] getData(String path, Watcher watcher, Stat stat)############ Stat 类的内部（其实是对状态属性的封装）##############public class Stat implements Record &#123; private long czxid; private long mzxid; private long ctime; private long mtime; private int version; private int cversion; private int aversion; private long ephemeralOwner; private int dataLength; private int numChildren; private long pzxid; public Stat() &#123; &#125;//............&#125; 参数 说明 path 数据节点的路径，比如：/zk-book/foo，获取该路径的数据 watcher 设置watcher后，如果path对应节点的数据发生变化，将会得到通知，允许为null watch 是否使用默认的watcher（就是在创建zk实例的时候指定的watcher） stat 指定数据节点的状态信息 cb 异步回调函数 ctx 用于传递一个对象，可以在回调方法执行的时候用，通常用于传递业务的上下文信息 6.修改数据123Stat setData(String path, byte[] data, int version)void setData(String path, byte[] data, int version, AsyncCallback.StatCallback cb, Object ctx) 参数 说明 path 被修改的节点的路径 data 新的数据 version 知道节点的数据版本，如果指定的版本不是最新版本，将会报错，它的作用类似于hibernate中的乐观锁,if the given version is -1, it matches any node’s versions cb 异步回调函数 ctx 传递上下文信息，即操作之前的信息传递到操作之后的异步回调函数里 7.检查节点是否存在1234567891011121314Stat exists(String path, boolean watch) #如果节点不存在就返回为nullvoid exists(String path, boolean watch, AsyncCallback.StatCallback cb, Object ctx)Stat exists(String path, Watcher watcher)void exists(String path, Watcher watcher, AsyncCallback.StatCallback cb, Object ctx)############################## interface StatCallback extends AsyncCallback &#123; public void processResult(int rc, String path, Object ctx, Stat stat); &#125; 参数 说明 path 数据节点的路径，如：/zk-book/foo 即API调用的目的是检测该节点是否存在 watcher 注册的watcher，用于监听以下三个事件：节点被创建节点被删除节点被更新 watch 是否使用默认的watcher cb 异步回调函数 ctx 传递上下文信息，即操作之前的信息传递到操作之后的异步回调函数里 8.异步函数的实现其实以上所有的异步回调函数都是在AsyncCallback中 9.代码实现（重要）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232package it.com.zk;import java.io.IOException;import java.util.List;import java.util.concurrent.CountDownLatch;import org.apache.zookeeper.AsyncCallback;import org.apache.zookeeper.CreateMode;import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import org.apache.zookeeper.AsyncCallback.StringCallback;import org.apache.zookeeper.AsyncCallback.VoidCallback;import org.apache.zookeeper.Watcher.Event.KeeperState;import org.apache.zookeeper.ZooDefs.Ids;import org.apache.zookeeper.data.Stat;public class WatchExample implements Watcher&#123; private static final int SESSION_TIMEOUT=5000; private ZooKeeper zk; private CountDownLatch connectedSignal=new CountDownLatch(1);//初始化计数器为1 @Override public void process(WatchedEvent event) &#123; if(event.getState()==KeeperState.SyncConnected)&#123;//KeeperState是枚举类型，存放的是状态 connectedSignal.countDown();//计数器减1，阻塞结束 &#125; &#125; private void close() throws InterruptedException &#123; zk.close(); System.out.println(&quot;close........&quot;); &#125;/*-------------------- 连接 start---------------------*/ private void connect(String hosts) throws IOException, InterruptedException &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, this); //因为自身实现了Watcher，所以将自身传过来了 connectedSignal.await();//阻塞等待，直到计数器为0结束(主线程阻塞，登台子线程调用process函数，然后唤醒自己) /* 客户端与服务端会话的建立是一个异步的过程，即 完成ۨ客户端的初始化后就返回，此时连接并没有真正的建立起来 当连接真正建立起来后，客户端会收到一个事件通知（即Watcher函数process的调用） */ &#125;/*--------------------- 连接 end -------------------------------*/ /*--------------------- 创建节点 start ----------------------*/ //创建节点 private void createSync(String groupName, String dataStr) throws KeeperException, InterruptedException &#123; String path=&quot;/&quot;+groupName; if(zk.exists(path, false)== null)&#123;//判断路径是否存在,不存在就创建 String actual_path = zk.create(path, dataStr.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); /* Ids:提供了几种默认的acl，但是也可以通过new Id()来自己指定，Ids内部就是new的 CreateMode:持久节点、持久顺序节点、临时节点、临时顺序节点 */ System.out.println(actual_path); System.out.println(&quot;Created Sync:&quot;+path); &#125; &#125; //异步创建节点 private void createAsync(String groupName, String dataStr) throws KeeperException, InterruptedException &#123; String path=&quot;/&quot;+groupName; if(zk.exists(path, false)== null)&#123;//判断路径是否存在 zk.create(path, dataStr.getBytes(), Ids.CREATOR_ALL_ACL, CreateMode.PERSISTENT, new StringCallback()&#123; @Override public void processResult(int rc, String path, Object ctx, String name) &#123; System.out.println(&quot;rc:&quot;+rc); System.out.println(&quot;path:&quot;+path); System.out.println(&quot;ctx:&quot;+ctx); System.out.println(&quot;name:&quot;+name); &#125; &#125;, &quot;async_&quot;); String actual_path = zk.create(path, dataStr.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(&quot;Created Async success:&quot;+path); &#125; &#125;/*------------------------- 创建节点 end---------------------------------*/ /*----------------------- 删除节点 start ---------------------------------*/ //删除 public void deleteSync(String path) throws InterruptedException, KeeperException&#123; path=&quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 //if the given version is -1, it matches any node&apos;s versions zk.delete(path, -1); System.out.println(&quot;delete sync success......&quot;); &#125; &#125; //删除 public void deleteAsync(String path) throws InterruptedException, KeeperException&#123; path=&quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 zk.delete(path, -1, new VoidCallback()&#123; @Override public void processResult(int rc, String path, Object ctx) &#123; System.out.println(&quot;rc:&quot;+rc); System.out.println(&quot;path:&quot;+path); System.out.println(&quot;ctx:&quot;+ctx); &#125; &#125;, &quot;delete_async&quot;); System.out.println(&quot;delete async success......&quot;); &#125; &#125;/*------------------ 删除节点 end-------------------------------------*/ /*-------------------- 获取子节点 start --------------------------------*/ //获取子节点(同步) public List&lt;String&gt; getChildrenSync(String path) throws KeeperException, InterruptedException&#123; path = &quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 return zk.getChildren(path, false); &#125; return null; &#125; //获取子节点(同步+stat) public List&lt;String&gt; getChildrenSyncWithStat(String path) throws KeeperException, InterruptedException&#123; path = &quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 Stat stat = new Stat(); List childList = zk.getChildren(path, false, stat);//会将查询到的状态copy到stat中：如DataTree.copyStat(response.getStat(), stat); /*下面是所有的状态信息（stat的封装信息） [zk: localhost:2182(CONNECTED) 65] get /name_async &quot;zshangsna&quot; cZxid = 0x200000070 ctime = Mon Nov 07 22:31:48 CST 2016 mZxid = 0x200000070 mtime = Mon Nov 07 22:31:48 CST 2016 pZxid = 0x200000074 cversion = 2 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 2 */ System.out.println(&quot;stat:&quot;+stat.getNumChildren()+&quot;;;;dataLength:&quot;+stat.getDataLength()); return childList; &#125; return null; &#125; //获取子节点(异步) public void getChildrenASync(String path) throws KeeperException, InterruptedException&#123; path = &quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 zk.getChildren(path, false, new AsyncCallback.Children2Callback()&#123; @Override public void processResult(int rc, String path, Object ctx,List&lt;String&gt; children, Stat stat) &#123; if(children != null)&#123; for(String child : children)&#123; System.out.println(&quot;child-name:&quot;+child); &#125; &#125; &#125;&#125;, &quot;async_getchildren&quot;); &#125; &#125; public void printChildList(String path) throws KeeperException, InterruptedException&#123; // List&lt;String&gt; childList = this.getChildrenSync(path); List&lt;String&gt; childList = this.getChildrenSyncWithStat(path); if(childList != null)&#123; for(String child : childList)&#123; System.out.println(&quot;child-name:&quot;+child); &#125; &#125; &#125;/*--------------- 获取子节点 end-------------------------*/ /*------------------ 获取节点数据 start ----------------*/ //获取节点数据(异步) public void printNodeData(String path) throws KeeperException, InterruptedException&#123; path = &quot;/&quot;+path; if(zk.exists(path, false)!= null)&#123;//判断路径是否存在 zk.getData( path, false, new AsyncCallback.DataCallback()&#123; @Override public void processResult(int rc, String path, Object ctx,byte[] data, Stat stat) &#123; if(data != null)&#123; System.out.println(&quot;path:&quot;+path); System.out.println(&quot;data:&quot;+new String(data)); System.out.println(&quot;stat:&quot;+stat); System.out.println(&quot;ctx:&quot;+ctx); &#125; &#125;&#125;,&quot;ctx_data&quot;); &#125; &#125;/*------------------ 获取节点数据 end--------------------*/ public static void main(String[] args) throws IOException, InterruptedException, KeeperException &#123; WatchExample watchExample = new WatchExample(); String path = &quot;name_async&quot;; //连接 watchExample.connect(&quot;192.168.0.50:2182&quot;); //创建// watchExample.createSync(&quot;name/beijing&quot;,&quot;zhangsan&quot;);// watchExample.createAsync(path,&quot;async&quot;); //删除// watchExample.deleteSync(path);// watchExample.deleteAsync(path); System.out.println(&quot;main..............&quot;); //打印子节点列表// watchExample.printChildList(path);// watchExample.getChildrenASync(path); //打印节点数据 watchExample.printNodeData(path); Thread.sleep(8000);//如果是异步的话，那么就要在子线程结束后，主线程才能推出，不然子线程的结果无法返回，所以这里会sleep watchExample.close(); &#125;&#125;","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper安装配置(单机，伪分布式、集群)","date":"2017-04-16T04:47:25.338Z","path":"2017/04/16/bigdata/zookeeper/zookeeper安装配置(单机，伪分布式、集群)/","text":"1.单机模式搭建1.1.解压123cd /home/oldboy/tools/ tar -zxvf zookeeper-3.4.5.tar.gz mv zookeeper-3.4.5 zookeeper 1.2.修改配置文件12345678910cd zookeeper/conf/ cp zoo_sample.cfg zoo_sample.cfg.bak #首先备份配置文件mv zoo_sample.cfg zoo.cfg #将配置文件改名&apos;修改下列选项&apos;tickTime=2000dataDir=/usr/local/zk/datadataLogDir=/usr/local/zk/dataLog clientPort=2181 1.3.配置环境变量为了今后操作方便，我们需要对Zookeeper的环境变量进行配置，方法如下在/etc/profile文件中加入如下内容：1234&apos;需要Java的环境，所以如果没有jdk，需要配置jdk&apos;ZOOKEEPER=/home/oldboy/tools/zookeeperPATH=$ZOOKEEPER/bin:$PATHexport PATH 1.4.启动/停止服务12345#启动ZooKeeper的Server./bin/zkServer.sh start#关闭ZooKeeper的Server./bin/zkServer.sh stop 2.伪集群模式搭建2.1.复制及修改配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182[root@data-1-1 ~]# cd /home/oldboy/tools/zookeeper[root@data-1-1 zookeeper0]# cp ./conf/zoo.cfg zoo1.cfg[root@data-1-1 zookeeper0]# cp ./conf/zoo.cfg zoo2.cfg[root@data-1-1 zookeeper0]# cp ./conf/zoo.cfg zoo3.cfg&apos;################## zoo1.cfg ###########################&apos;# The number of milliseconds of each ticktickTime=2000 # The number of ticks that the initial# synchronization phase can takeinitLimit=10 # The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5 # the directory where the snapshot is stored.dataDir=/usr/local/zk/data_1 # the port at which the clients will connectclientPort=2181 #the location of the log filedataLogDir=/usr/local/zk/logs_1 server.0=localhost:2287:3387server.1=localhost:2288:3388server.2=localhost:2289:3389&apos;################## zoo2.cfg ###########################&apos;# The number of milliseconds of each ticktickTime=2000 # The number of ticks that the initial# synchronization phase can takeinitLimit=10 # The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5 # the directory where the snapshot is stored.dataDir=/usr/local/zk/data_2 # the port at which the clients will connectclientPort=2182 #the location of the log filedataLogDir=/usr/local/zk/logs_2 server.0=localhost:2287:3387server.1=localhost:2288:3388server.2=localhost:2289:3389&apos;################## zoo3.cfg ###########################&apos;# The number of milliseconds of each ticktickTime=2000 # The number of ticks that the initial# synchronization phase can takeinitLimit=10 # The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5 # the directory where the snapshot is stored.dataDir=/usr/local/zk/data_3 # the port at which the clients will connectclientPort=2183 #the location of the log filedataLogDir=/usr/local/zk/logs_3 server.0=localhost:2287:3387server.1=localhost:2288:3388server.2=localhost:2289:3389&apos;#其实上述配置文件就是：clientPort、dataDir、dataLogDir、server.x 不同&apos; 2.2.创建相应的文件1234567891011121314151617181920//创建dataLogDirmkdir /usr/local/zk/logs_1 -pmkdir /usr/local/zk/logs_2 -pmkdir /usr/local/zk/logs_3 -p//创建dataDirmkdir /usr/local/zk/data_1 -pmkdir /usr/local/zk/data_2 -pmkdir /usr/local/zk/data_3 -p//在dataDir下创建myid/*在/usr/local/zk/data_x目录下创建myid文件，在对应的myid文件中写入数字，server.X和myid： server.X 这个数字就是对应，data/myid中的数字。在3个server的myid文件中分别写入了0，1，2*/echo &quot;0&quot;&gt;/usr/local/zk/data_1/myidecho &quot;1&quot;&gt;/usr/local/zk/data_2/myidecho &quot;2&quot;&gt;/usr/local/zk/data_3/myidcat /usr/local/zk/data_3/myid #检查一下 2.3.启动123456789101112131415161718192021222324252627282930313233343536373839[root@data-1-1 conf]# zkServer.sh start zoo1.cfg JMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo1.cfg #为什么没有指定路径，因为配置了zk的环境变量，这里就可以看到回去找对应的bin和conf目录Starting zookeeper ... STARTED[root@data-1-1 conf]# zkServer.sh start zoo2.cfgJMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo2.cfgStarting zookeeper ... STARTED[root@data-1-1 conf]# zkServer.sh start zoo3.cfgJMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo3.cfgStarting zookeeper ... STARTED//检查，是3个表示启动成功[root@data-1-1 conf]# jps2566 QuorumPeerMain2539 QuorumPeerMain2638 Jps2607 QuorumPeerMain[root@data-1-1 conf]#//检查2：通过命令的方式[root@data-1-1 tools]# zkServer.sh status zoo1.cfgJMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo1.cfgMode: follower[root@data-1-1 tools]# zkServer.sh status zoo2.cfgJMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo2.cfgMode: leader[root@data-1-1 tools]# zkServer.sh status zoo3.cfgJMX enabled by defaultUsing config: /home/oldboy/tools/zookeeper0/bin/../conf/zoo3.cfgMode: follower 3.集群模式搭建3.1.相同的配置文件创建一个配置文件zoo.cfg12345678910111213141516171819202122232425# The number of milliseconds of each ticktickTime=2000 # The number of ticks that the initial# synchronization phase can takeinitLimit=10 # The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5 # the directory where the snapshot is stored.dataDir=/usr/local/zk/data # the port at which the clients will connectclientPort=2183 #the location of the log filedataLogDir=/usr/local/zk/log server.0=hadoop:2288:3388server.1=hadoop0:2288:3388server.2=hadoop1:2288:3388#此时的server.x就相同，因为配置文件在不同的机器上，所以不必担心端口冲突的问题 3.2.创建myid12345在dataDir(/usr/local/zk/data)目录创建myid文件 Server0机器的内容为：0Server1机器的内容为：1Server2机器的内容为：2 3.3.创建dataDir和dataLogDir在3台机器上都执行123456//创建dataLogDirmkdir /usr/local/zk/logs -p //创建dataDirmkdir /usr/local/zk/data -p 3.4.启动在3台机器上都执行1234zkServer.sh start//检查zkServer.sh status 3.5.客户端连接12zkCli.sh -server 192.168.0.50:2182 #带-server连接的是远端zkCli.sh #连接的是本地","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper四字命令","date":"2017-04-16T04:47:25.337Z","path":"2017/04/16/bigdata/zookeeper/zookeeper四字命令/","text":"定义长度为4个英文字母的管理命令,比如stat就是其中一个 使用方式 Telnet Telnet ip port 命令执行 nc echo 命令|nc ip port conf用于输出基本配置信息,也可以查看某些运行时参数123telnet localhost 2181 ,然后执行conf#或者echo conf|nc localhost 2181 cons用于输出当前客户端所有连接的详细信息,包括客户端ip,会话id等123telnet localhost 2181 ,然后执行cons#orecho cons | nc localhost 2181 crst用于重置客户端连接统计信息123telnet localhost 2181 ,然后执行 crst#orecho crst | nc localhost 2181 dump用于输出当前集群的所有会话信息,包括会话id以及临时节点等信息如果dump的是leader节点,则还会有会话的超时时间123telnet localhost 2181 ,然后执行dump#orecho dump | nc localhost 2181 envi用于输出运行时的环境信息123telnet localhost 2181 ,然后执行envi#orecho envi | nc localhost 2181 ruok用于输出当前zk服务器运行时是否正常,仅仅代表2181端口和四字命令流程执行正常,不能完全代表zk运行正常,最有效的命令是stat123telnet localhost 2181 ,然后执行ruok#orecho ruok | nc localhost 2181 stat用于获取服务器端的运行状态:zk版本 打包信息,运行时角色,集群节点等 123telnet localhost 2181 ,然后执行stat#orecho stat | nc localhost 2181 srvr与stat功能类似,但是不输出连接信息123telnet localhost 2181 ,然后执行srvr#orecho srvr | nc localhost 2181 srst重置服务器的统计信息123telnet localhost 2181 ,然后执行srst#orecho srst | nc localhost 2181 wchs用于输出当前服务器上管理的watcher的概要信息,通过zk构造器创建的默认watcher不在此统计范围 123telnet localhost 2181 ,然后执行wchs#orecho wchs | nc localhost 2181 wchc用于输出当前服务器上管理的watcher的详细信息，以会话单位为组，通过zk构造器创建的默认watcher,不在此统计范围 123telnet localhost 2181 ,然后执行wchc#orecho wchc | nc localhost 2181 wchp与wchc类似，但是以节点路径分组，通过zk构造器创建的默认watcher不在此统计范围 123telnet localhost 2181 ,然后执行wchp#orecho wchp | nc localhost 2181 mntr与stat类似，但是比stat更详细，包括请求的延迟情况,服务区内存数据库大小,集群同步等情况 123telnet localhost 2181 ,然后执行mntr#orecho mntr | nc localhost 2181","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper命令行简单的CRUD命令","date":"2017-04-16T04:47:25.335Z","path":"2017/04/16/bigdata/zookeeper/zookeeper命令行简单的CRUD命令/","text":"1.登录客户端在启动Zookeeper服务之后，输入以下命令，连接到Zookeeper服务：1zkCli.sh -server localhost:2181 #连接的是localhost：2181服务 2.查看客户端帮助信息123456789101112131415161718192021222324[zk: localhost:2181(CONNECTED) 0] helpZooKeeper -server host:port cmd args stat path [watch] set path data [version] ls path [watch] delquota [-n|-b] path ls2 path [watch] setAcl path acl setquota -n|-b val path history redo cmdno printwatches on|off delete path [version] sync path listquota path rmr path get path [watch] create [-s] [-e] path data acl addauth scheme auth quit getAcl path close connect host:port[zk: localhost:2181(CONNECTED) 1] 3.简单的CRUD客户端操作命令在敲客户端命令的时候，可以使用tab键来补全 3.1.查看节点123[zk: localhost:2181(CONNECTED) 1] ls /[zookeeper][zk: localhost:2181(CONNECTED) 2] 3.2.创建节点create [-s] [-e] path data acl 创建zookeeper节点 -s或者-e表示创建的是顺序或临时节点，不加默认创建的是持久节点 Path为节点的全路径，没有相对节点的表示方式 data位当前创建节点内存储的数据 acl 用来进行权限控制，缺省情况下不做任何权限控制 123456789101112131415161718192021222324[zk: localhost:2181(CONNECTED) 14] create /test &quot;te_data&quot;Created /test//查看[zk: localhost:2181(CONNECTED) 8] ls /[zookeeper, test][zk: localhost:2181(CONNECTED) 9] #创建顺序节点：会在节点的名字后面添加一系列数字[zk: localhost:2181(CONNECTED) 21] create -s /name &quot;zhangsan&quot;Created /name0000000003[zk: localhost:2181(CONNECTED) 21] create -s /name &quot;zhangsan2&quot; #此时可以重复指定节点名字，因为最终要在节点名后添加序号的，所以名字最终还是唯一的Created /name0000000004[zk: localhost:2181(CONNECTED) 22] ls /[zookeeper, name0000000003, name0000000004][zk: localhost:2181(CONNECTED) 23] #创建临时节点，在会话退出后，节点将会被删除[zk: localhost:2181(CONNECTED) 1] create -e /age 21Created /age[zk: localhost:2181(CONNECTED) 2] ls /[zookeeper, age, name0000000003]#可以退出之后在登录客户端，ls / 查看 3.3.查看节点数据12345678910111213 [zk: localhost:2181(CONNECTED) 15] get /test&quot;te_data&quot;cZxid = 0x10000000bctime = Sun Nov 06 22:53:59 CST 2016mZxid = 0x10000000bmtime = Sun Nov 06 22:53:59 CST 2016pZxid = 0x10000000bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 9numChildren = 0 3.4.修改节点数据12345678910111213141516171819202122232425262728[zk: localhost:2181(CONNECTED) 16] set /test &quot;update_test&quot;cZxid = 0x10000000bctime = Sun Nov 06 22:53:59 CST 2016mZxid = 0x10000000cmtime = Sun Nov 06 22:54:47 CST 2016pZxid = 0x10000000bcversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0//查看[zk: localhost:2181(CONNECTED) 17] get /test&quot;update_test&quot; #修改之后的数据cZxid = 0x10000000bctime = Sun Nov 06 22:53:59 CST 2016mZxid = 0x10000000cmtime = Sun Nov 06 22:54:47 CST 2016pZxid = 0x10000000bcversion = 0dataVersion = 1aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0[zk: localhost:2181(CONNECTED) 18] 3.5.删除节点1234567891011//删除前查看[zk: localhost:2181(CONNECTED) 18] ls /[zookeeper, test]#删除[zk: localhost:2181(CONNECTED) 19] delete /test#删除后查看[zk: localhost:2181(CONNECTED) 20] ls /[zookeeper][zk: localhost:2181(CONNECTED) 21]","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zookeeper会话管理","date":"2017-04-16T04:47:25.334Z","path":"2017/04/16/bigdata/zookeeper/zookeeper会话管理/","text":"什么是会话? 代表客户端与服务器端的一个zk连接 底层通信是通过tcp协议进行连接通信 zookeeper会在服务器端创建一个会话对象来维护这个连接属性 当网络出现断网的抖动现象时,并不代表会话一定断开 会话对象的实现是SessionImpl,包括以下四个属性 sessionID:唯一标识一个会话,具备全局唯一性 Timeout:会话超时时间,创建zookeeper客户端对象时传入,服务器会根据最小会话时间和最大会话时间来明确指定此值具体是什么 TickTime:下次会话超时时间 isClosing:标记一个会话是否已经被关闭,当服务器端检测到右会话失效时,就会把此会话标记为已关闭 会话状态 Connecting connected reconnecting reconnected close 服务器端判断会话超时后,在进行会话清除之前,会把会话的状态置为close sessionTracker服务器端通过此类来管理会话,包括会话的创建,管理和清除 通过三个数据结构从三个维度来管理会话 sessionById属性:用于根据sessionID来查找session sessionWithTimeout:通过sessionID来查找此session的失效时间是什么时候 sessionSets属性:通过某个时间查询都有哪些会话在这个时间点会失效 12345678public class SessionTrackerImpl extends Thread implements SessionTracker &#123; HashMap&lt;Long, SessionImpl&gt; sessionsById = new HashMap&lt;Long, SessionImpl&gt;(); HashMap&lt;Long, SessionSet&gt; sessionSets = new HashMap&lt;Long, SessionSet&gt;(); ConcurrentHashMap&lt;Long, Integer&gt; sessionsWithTimeout;&#125; 分桶策略把所有的会话按照时间维度来进行分类管理,即同一个时间点失效的会话都在一起管理,即上面的属性ConcurrentHashMap sessionsWithTimeout 会话失效时间计算: 约定:把所有的时间按照某个单位时间进行等分(默认是服务器的tickTime配置) 公式:某次超时时间=((currentTime+sessiontimeout)/ ExpirationInterval +1) x ExpirationInterval 举例说明: 由于服务器的tickTime的默认值是2000ms,则ExpirationInterval=2000ms 第一次创建会话时,currenttime=1370907000000 创建会话时,客户端传入的超时时间是15000ms 则,此会话的超时时间为((1370907000000+2000)+1)x2000=1370907016000 当某个会话由于有操作而导致超时时间变化,则会把会话从上一个桶移动到下一个桶中 会话激活 当此会话一直有操作,则会话就不会失效 影响会话失效超时时间的因素 心跳检测,及PING命令 当客户端发现sessionTimeout/3时间范围内还没有任何操作命令产生,就发送一个ping心跳请求 正常业务操作,比如get或者set 每次业务操作或者心跳检测,都会重新计算超时时间,然后在桶之间转移会话 会话超时检测 由于sessionTracker中的一个线程负责检查session是否失效 线程检查周期也是ExpirationInterval的倍数 当某次检查时,如果在此次的分桶(即前面的ExpirationInterval)之前还有会话,就说明这些会话都超时了,因为会话如果有业务操作或者靠心跳,会不断的从小的分桶迁移到大的分桶 举例:系统启动时的时间是100001,此时ExpirationInterval=2000ms,则桶的刻度为100001/2000=50,下一次的检查时间为(100001/2000+1)x2000=102000 会话清理流程 修改会话状态为close 由于清理过程中需要一定时间,为了避免清理期间会话状态发生变化 向所有的集群节点发送会话关闭请求 收集跟被清理的会话相关的临时节点 向集群节点发出删除临时节点的事务请求 集群中的所有节点执行删除临时节点事务 从sessionTracker的列表中移除会话 关闭会话的网络连接,具体类是NIOServerCnxnFactory 会话重连当客户端与服务器端的网络断开后,客户端会不断的重新连接,当连接上后会话的状态时以下两种状态: connected:服务器端会话依然存在 expired:服务器端的会话已经被关闭清除了 注意:网络断开并不代表会话超时 三个会话异常: connection_loss session_expired session_moved connection_loss 网络闪断导致或者是客户端服务器出现问题导致,出现此问题,客户端会重新找地址进行连接,当某个操作过程(比如setData)中出现connection_loss现象,则客户端会收到NoneDisconnected(设置了默认watcher情况下),同时会抛出异常:org.apache.zookeeper.KeeperException$ConnectionLossException,当重新连接上后,客户端会收到事件通知(None-SyncConnected)//在设置了默认watcher时 session_expired 通常发生在connection_loss期间,因为没有网络连接,就不能有操作和心跳进行,会话就会超时,由于重新连接时间较长,导致服务器端关闭了会话,并清除会话,此时会话相关联的watcher等数据都会丢失,watcher失效,出现这种情况,客户端需要重新创建zookeeper对象,并且恢复数据(比如注册watcher),会收到SessionExpiredException session_moved 出现CONNECTION_LOSS时，客户端尝试重新连接下个节点(connectstring)例如:客户端刚开始连接的是s1,由于网络中断,客户端尝试连接s2,连接成功之后,s2延续了会话,即会话从s1迁移到了s2当出现以下业务场景时,服务器端回抛出SessionMovedException异常,由于客户端的连接已经发生了变化(从s1–&gt;s2),所以客户端收不到异常1234567有三台服务器:s1 s2 s3开始时,客户端连接s1,此时客户端发出一个修改数据的请求r1在修改数据的请求到达s1之前,客户端重新连接上了s2服务器,此时出现了会话转译连接s2后,客户端又发起一次数据修改请求r2r1被s1服务器处理,r2被s2处理(比r1被处理要早)这样对于客户端来说,请求被处理两次,并且r2被r1的处理结果覆盖了服务器端通过检查会话的所有者来判断此次会话请求是否合法,不合法就抛出moved异常","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zk详细配置说明","date":"2017-04-16T04:47:25.333Z","path":"2017/04/16/bigdata/zookeeper/zk详细配置说明/","text":"两种配置形式 基于java的系统属性配置:比如:-Djava.library.path zk自身的zoo.cfg文件 属性名称 作用 说明 dataLogDir 配置事务日志文件存储目录 1.不支持系统属性配置2.默认为属性dataDir的值3.在高并发下,有大量的事务日志和快照，会导致磁盘IO瓶颈，因此在高并发下,不建议使用默认配置，最好把dataDir和此属性配置的目录不同的磁盘下，从而提高IO snapCount 两次快照间隔的事务日志条数 1.事务日志条数达到这个数目，就要触发数据快照2.默认值Ѫ 1000003.仅支持系统属性配置方式 preAllocSize 事务日志文件预分配的磁盘空间大小 1.仅支持系统属性配置， zookeeper.preAllocSize2.默认值Ѫ 65535，即64M3.此参数与snapCount有关， snapCount大，就需要多分配 minSessionTimeoutmaxSessionTimeout 会话失效的时间的边界控制(服务器端) 1.不支持系统属性2.默认为ticktime的2倍和20倍3.当客户端传递过来的超时时间不在这两个参数之间，则最小取minSessionTimeout，最大取maxSessionTimeout maxClientCnxns 从socket层限制客户端与单台服务器的并发连接数 1.不支持系统属性,默认值为60,0表示不限制2.以IP地址为粒度进行控制3.只能控制单台机器,不能控制总连接 jute.maxbuffer 配置单个节点最大的数据大小 1.默认是10M，单位是字节，仅支持系统属性方式配置2.Zk上存储的数据不易过多,主要是考虑多节点写入的性能3.需要在服务器端和客户端都配置才能生效 Autopurge.snapRetainCount 自动清理快照和事务日志时需要保留的文件数 1.不支持系统属性配置,系统默认为3,可以不用配置2.最小值为3,避免磁盘损坏后不能回复数据 Autopurge.purgeInterval 自动清理快照和事务的周期 1.不支持系统属性,默认为0,表示不开启自动清理2.与Autopurge.snapRetainCount属性一起配合使用3.配置为负数也表示不清理 fsync.warningthresholdms 事务日志刷新到磁盘的报警阈值 1.支持系统属性,默认值是1000ms2.如果fsync的操作超过此时间就会在日志中打印报警日志 forceSync 日志提交时是否强磁盘 1.默认为true2.仅支持系统属性配置:zookeeper.forceSync,3.如果设置为true,可以提升写入性能,但是会有数据丢失风险 cnxTimeout 选举过程中，服务器之间创建tcp连接的超时时间 1.仅支持系统属性配置:zokeeper.cnxTimeout 2.默认为5000ms 参见:zookeeper配置文件详解","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zk的几个基本的概念介绍","date":"2017-04-16T04:47:25.332Z","path":"2017/04/16/bigdata/zookeeper/zk的几个基本的概念介绍/","text":"1.集群角色（Leader、Follower、Observer） Leader：为客户端提供读写服务 Follower：仅提供读服务，所有写服务都需要转交给Leader角色，另外参与选举 Observer：仅提供读服务，不参与选举，一般是为了增强zk集群的读请求并发能力 2.会话（session） zk的客户端与zk的服务端之间的连接 通过心跳检测保持客户端连接的存活 接收来自服务端的watch事件通知 可以设置超时时间 3.数据节点（znode) 不是机器节点的意思 zk树形结构中的数据节点，用于存储数据 持久节点：一旦创建，除非主动调用删除，否则一直存储在zk上 临时节点：与客户端的会话绑定，一旦客户端会话失效，这个客户端创建的所有临时节点都会被移除 sequential znode（顺序节点）：创建节点时，如果设置属性sequential，则会自动在节点名后面追加一个整形的数字 4.版本 Version：当前znode的版本 Cversion：当前znode的子节点的版本 Aversion：当前znode的ACL（访问控制）版本 5.Watcher 作用于znode的节点上 多种事件通知，数据更新、子节点状态等会触发 6.ACL（访问控制列表） Access Control Lists 类似于Linux的权限控制 CREATE：创建子节点的权限 READ：获取子节点的权限 WRITE：更新节点数据的权限 DELETE：删除子节点的权限 ADMIN：设置acl的权限 CREATE和DELETE是针对子节点的权限控制","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"zk分布式锁的类型","date":"2017-04-16T04:47:25.330Z","path":"2017/04/16/bigdata/zookeeper/zk分布式锁的类型/","text":"排他锁 定义:只能允许一个线程获得,其他线程都需要等待已经获取的线程完成才能再次争抢资源 zk实现: 获得锁:通过构建一个目录,当叶子节点能创建成功,则任务获取到锁,因为一旦一个节点被某个会话创建,其他会话再次创建这个节点时,将会抛出异常,比如目录为: 释放锁:删除节点或者会话失效 共享锁 定义:读锁:如果前面线程使用的是读锁,则后面的线程还可以获取读锁,从而可以继续进行读操作写锁:如果在线程打算获取锁从而进行操作时,无论前面已经有读锁或者是写锁都必须进入等待 zk实现: 获得读锁:利用zk节点的顺序性,对于读操作,节点名称带一个R标识,如果前面存在序列数比自己小,并且都是带R标识,则说明前面加的都是读锁,还可以继续获取读锁,否则,等待锁释放后由机会再抢 获得写锁:只有自己创建的节点序列化最小,才能获得写锁,否则,进入等待,直到有锁资源被释放,然后再判断是否有机会得到锁 释放锁:删除节点或者会话失效","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"watcher简介及简单示例","date":"2017-04-16T04:47:25.329Z","path":"2017/04/16/bigdata/zookeeper/watcher机制/","text":"watcher 问题 集群中有多个机器,当某个通用的配置发生变化后,怎么让所有服务器的配置都统一生效? 当某个集群节点宕机,其他节点怎么知道? zk中引入了watcher机制来实现发布/订阅功能,能够让多个订阅者同时监听某一个主题对象,当这个主题对象自身状态变化时,会通知所有的订阅者 watcher组成 客户端 客户端watcherManager zk服务器 watcher机制 客户端向zk服务器注册watcher的同时,会将watcher对象存储在客户端的watcherManager zk服务器触发watcher事件后,回向客户端发送通知,客户端线程从watcherManager中调取watcher执行 watcher接口 12public class ZLock implements Watcherpublic void process(WatcherEvent event) 事件类型 通知状态:org.apache.zookeeper.Watcher.Event.KeeperState 事件类型:org.apache.zookeeper.Watcher.Event.EventType keeperState EventType 触发条件 说明 SyncConnected None(-1) 客户端与服务器成功建立会话 此时客户端处于连接状态 SyncConnected NodeCreated(1) Watcher监听的对应数据节点被创建 此时客户端处于连接状态 SyncConnected NodeDataChanged(3) 数据节点的数据内容发生变更 此时客户端处于连接状态 SyncConnected NodeChildrenChanged(4) 被监控的数据节点的子节点列表发生变更 此时客户端处于连接状态 Disconnected(0) None(-1) 客户端与zk服务器端口连接 此时客户端与服务器处于断开连接状态 Expired(-12) None(-1) 会话超时 此时客户端会话失效,通常会收到SessionExpiredException异常 AuthFailed(4) None(-1) 1.使用错误的scheme进行授权检查;2.SALA权限检查失败 通常会受到AuthFailedException异常 Unknown(-1) 从3.0版本开始已经废弃 NoSyncConnected NodeDataChanged事件 无论节点数据发生变化还是数据版本发生变化都会触发 即使被更新数据与新数据一样,数据版本都会发生变化123456789101112131415161718192021222324252627[zk: localhost:2181(CONNECTED) 6] get /student&quot;zhangsan&quot;cZxid = 0x3ctime = Thu Feb 23 17:29:37 CST 2017mZxid = 0x3mtime = Thu Feb 23 17:29:37 CST 2017pZxid = 0x3cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 0#重新以相同的内容设置节点[zk: localhost:2181(CONNECTED) 7] set /student &quot;zhangsan&quot;cZxid = 0x3ctime = Thu Feb 23 17:29:37 CST 2017mZxid = 0x4mtime = Thu Feb 23 17:31:12 CST 2017pZxid = 0x3cversion = 0dataVersion = 1 #数据版本发生了变化aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 0 NodeChildrenChanged 新增节点或者删除节点 AuthFailed 重点不是客户端会话没有权限而是授权失败 客户端只能收到相关事件的通知,但是并不能获取对应数据节点的原始数据内容以及变更之后新数据内容,因此,如果业务需要知道变更前的数据或者是变更收的数据,则需要业务保存变更前的数据和调用接口获取新的数据 watcher注册 创建zk客户端对象实例时注册 12345678public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher)public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, boolean canBeReadOnly)public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd)public ZooKeeper(String connectString, int sessionTimeout, Watcher watcher, long sessionId, byte[] sessionPasswd, boolean canBeReadOnly)/*通过上述方式注册的watcher将会作为整个zk会话期间的默认watcher,会被一直保存在客户端ZKWatchManager的defaultWatcher中,如果有其他的设置,则这个watcher会被覆盖*/ 其他注册API 1234567891011121314151617getChildren(String path, Watcher watcher)getChildren(String path, boolean watch) #Boolean watch 表示是否使用上下文中默认的watcher,即创建zk实例时设置的watcher #The watch willbe triggered by a successful operation that deletes the node of the given path or creates/delete a child under the node. getData(String path, bolean watch, Stat stat) #Boolean watch 表示是否使用上下文中默认的watcher,即创建zk实例时设置的watcher #The watch will be triggered by a successful operation that sets data on the node, or deletes the node. getData(String path, Watcher watcher, AsyncCallback.DataCallback cb, Objcet ctx)exists(String path, boolean watch) #Boolean watch 表示是否使用上下文中默认的watcher,即创建zk实例时设置的watcher #The watch will be triggered by a successful operation that creates/delete the node or sets the data on the node.exists(String path, Watcher watcher) watcher设置后,一旦触发一次即会失效,如果需要一直监听,就需要再注册(很重要) 下面用代码来说明:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#Watcherpublic class WatcherExample1 implements Watcher &#123; private ZooKeeper zk = null; @Override public void process(WatchedEvent event) &#123;#watcher事件触发后的处理动作 System.out.println(&quot;watcher=&quot;+this.getClass().getName()); System.out.println(&quot;path=&quot;+event.getPath()); System.out.println(&quot;eventType=&quot;+event.getType().name()); &#125; public ZooKeeper getZk() &#123; return zk; &#125; public void setZk(ZooKeeper zk) &#123; this.zk = zk; &#125;&#125;-----------------------------------#注册watcherpublic class WatcherRegister &#123; private ZooKeeper zk = null; public WatcherRegister(String connectString,Watcher watcher) &#123; try &#123; zk = new ZooKeeper(connectString,10000,watcher); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void testWatcherdisabled(String path) throws KeeperException, InterruptedException&#123; WatcherExample1 we1 = new WatcherExample1(); we1.setZk(zk); //向getData上注册一个watcher zk.getData(path, we1, null); &#125; public static void main(String[] args) &#123; WatcherExample we = new WatcherExample(); //注册watcher WatcherRegister wr = new WatcherRegister(&quot;localhost:2181&quot;,we); try &#123; wr.testWatcherdisabled(&quot;/student&quot;); Thread.sleep(300000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动执行123watcher=com.zk.example.watcher.WatcherExamplepath=nulleventType=None 执行: set /student “zhangsan3” 打印下面的结果123watcher=com.zk.example.watcher.WatcherExample1path=/studenteventType=NodeDataChanged 再次执行: set /student “zhangsan3” 没有打印说明:watcher设置后,一旦触发一次即会失效,如果需要一直监听,就需要再注册 下面是触发一次watcher之后,重新注册的例子12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#watcherpublic class WatcherExample1 implements Watcher &#123; private ZooKeeper zk = null; @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;watcher=&quot;+this.getClass().getName()); System.out.println(&quot;path=&quot;+event.getPath()); System.out.println(&quot;eventType=&quot;+event.getType().name()); try &#123; //在一次触发watcher之后,重新设置watcher WatcherExample1 we1 = new WatcherExample1(); we1.setZk(zk); zk.getData(event.getPath(), we1, null); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; public ZooKeeper getZk() &#123; return zk; &#125; public void setZk(ZooKeeper zk) &#123; this.zk = zk; &#125;&#125;---------------------------------------------# 注册watcherpublic class WatcherRegister &#123; private ZooKeeper zk = null; public WatcherRegister(String connectString,Watcher watcher) &#123; try &#123; zk = new ZooKeeper(connectString,10000,watcher); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; public void testWatcherdisabled(String path) throws KeeperException, InterruptedException&#123; WatcherExample1 we1 = new WatcherExample1(); we1.setZk(zk); zk.getData(path, we1, null); &#125; public static void main(String[] args) &#123; WatcherExample we = new WatcherExample(); WatcherRegister wr = new WatcherRegister(&quot;localhost:2181&quot;,we); try &#123; wr.testWatcherdisabled(&quot;/student&quot;); Thread.sleep(300000); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 启动123watcher=com.zk.example.watcher.WatcherExamplepath=nulleventType=None 修改: set /student “zhangsan3”123watcher=com.zk.example.watcher.WatcherExample1path=/studenteventType=NodeDataChanged 再次修改: set /student “zhangsan3” ( 这里需要说明的是修改此次修改数据的内容还是”zhangsan3”,但是数据的版本是被改变了,所以数据节点还是被改变了 ) 123watcher=com.zk.example.watcher.WatcherExample1path=/studenteventType=NodeDataChanged","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"watcher处理流程源码解析","date":"2017-04-16T04:47:25.328Z","path":"2017/04/16/bigdata/zookeeper/watcher处理流程源码解析/","text":"客户端watcher注册流程 服务器端处理watcher 在服务器端的FinalRequestProcessor类,判断是否需要注册watcher 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class FinalRequestProcessor implements RequestProcessor &#123; ZooKeeperServer zks; public FinalRequestProcessor(ZooKeeperServer zks) &#123; this.zks = zks; &#125; public void processRequest(Request request) &#123; //... ServerCnxn cnxn = request.cnxn; //... try &#123; //... switch (request.type) &#123; case OpCode.create: &#123;//创建节点 lastOp = &quot;CREA&quot;; rsp = new CreateResponse(rc.path); err = Code.get(rc.err); break; &#125; case OpCode.delete: &#123;//删除节点 lastOp = &quot;DELE&quot;; err = Code.get(rc.err); break; &#125; case OpCode.getData: &#123;//getData lastOp = &quot;GETD&quot;; GetDataRequest getDataRequest = new GetDataRequest(); ByteBufferInputStream.byteBuffer2Record(request.request, getDataRequest); DataNode n = zks.getZKDatabase().getNode(getDataRequest.getPath()); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; Long aclL; synchronized(n) &#123; aclL = n.acl; &#125; PrepRequestProcessor.checkACL(zks, zks.getZKDatabase().convertLong(aclL), ZooDefs.Perms.READ, request.authInfo); Stat stat = new Stat(); byte b[] = zks.getZKDatabase().getData(getDataRequest.getPath(), stat, getDataRequest.getWatch() ? cnxn : null);//请求中watcher对象,则传入cnxn对象 rsp = new GetDataResponse(b, stat); break; &#125; &#125;&#125; ServerCnxn类及cnxn对象 zk客户端与服务器之间的tcp连接 实现了watcher接口 总结:既包含了连接信息,有包含了watcher信息 1234567891011121314151617181920212223public abstract class ServerCnxn implements Stats, Watcher &#123; // This is just an arbitrary object to represent requests issued by // (aka owned by) this class final public static Object me = new Object(); protected ArrayList&lt;Id&gt; authInfo = new ArrayList&lt;Id&gt;(); boolean isOldClient = true; abstract int getSessionTimeout(); abstract void close(); public abstract void sendResponse(ReplyHeader h, Record r, String tag) throws IOException; abstract void sendCloseSession(); public abstract void process(WatchedEvent event); //watcher的回调函数 //... &#125; watcherManager zk服务器Watcher的管理者 从两个维度维护watcher watcherTable:从数据节点的粒度来维护 watch2Paths:从watcher的粒度来维护 123456789public class WatchManager &#123; private static final Logger LOG = LoggerFactory.getLogger(WatchManager.class); private final HashMap&lt;String, HashSet&lt;Watcher&gt;&gt; watchTable = new HashMap&lt;String, HashSet&lt;Watcher&gt;&gt;();//通过数据节点的路径找到watcher private final HashMap&lt;Watcher, HashSet&lt;String&gt;&gt; watch2Paths = new HashMap&lt;Watcher, HashSet&lt;String&gt;&gt;();//通过watcher找到数据节点&#125; watcher触发 DataTree类 维护节点目录树的数据结构 在DataTree类中有如下方法 1234567891011121314151617181920212223242526public Stat setData(String path, byte data[], int version, long zxid, long time) throws KeeperException.NoNodeException &#123; Stat s = new Stat(); DataNode n = nodes.get(path); if (n == null) &#123; throw new KeeperException.NoNodeException(); &#125; byte lastdata[] = null; synchronized (n) &#123;//此处是更新数据 lastdata = n.data; n.data = data; n.stat.setMtime(time); n.stat.setMzxid(zxid); n.stat.setVersion(version); n.copyStat(s); &#125; // now update if the path is in a quota subtree. String lastPrefix; if((lastPrefix = getMaxPrefixWithQuota(path)) != null) &#123; this.updateBytes(lastPrefix, (data == null ? 0 : data.length) - (lastdata == null ? 0 : lastdata.length)); &#125; dataWatches.triggerWatch(path, EventType.NodeDataChanged);//触发事件NodeDataChanged return s;&#125; 客户端回调watcher 客户端回调watcher步骤 反序列化,将字节流转换成WatcherEvent对象 处理chrootPath 还原watchedEvent,把WatcherEvent对象转换成WatchedEvent 回调Watcher:把WatchedEvent对象交给EventThread线程处理 EventThread 从客户端的ZKWatchManager去取出Watcher,并放入waitingEvents队列中 12345678910111213141516171819202122232425262728293031323334353637383940class SendThread extends Thread &#123; void readResponse(ByteBuffer incomingBuffer) throws IOException &#123; ByteBufferInputStream bbis = new ByteBufferInputStream(incomingBuffer); BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis); ReplyHeader replyHdr = new ReplyHeader(); replyHdr.deserialize(bbia, &quot;header&quot;); if (replyHdr.getXid() == -1) &#123; // -1 means notification WatcherEvent event = new WatcherEvent(); event.deserialize(bbia, &quot;response&quot;);//反序列化 // convert from a server path to a client path(转换路径) if (chrootPath != null) &#123; String serverPath = event.getPath(); if(serverPath.compareTo(chrootPath)==0) event.setPath(&quot;/&quot;); else if (serverPath.length() &gt; chrootPath.length()) event.setPath(serverPath.substring(chrootPath.length())); else &#123; LOG.warn(&quot;Got server path &quot; + event.getPath() + &quot; which is too short for chroot path &quot; + chrootPath); &#125; &#125; WatchedEvent we = new WatchedEvent(event);//将WatcherEvent封装成WatchedEvent if (LOG.isDebugEnabled()) &#123; LOG.debug(&quot;Got &quot; + we + &quot; for sessionid 0x&quot; + Long.toHexString(sessionId)); &#125; eventThread.queueEvent( we );//添加到事件处理线程 return; &#125; &#125;&#125;","tags":[{"name":"zookeeper","slug":"zookeeper","permalink":"http://yoursite.com/tags/zookeeper/"}]},{"title":"自己实现一个类似storm数据执行的框架","date":"2017-04-16T04:47:25.326Z","path":"2017/04/16/bigdata/storm/自己实现一个类似storm数据执行的框架/","text":"1.storm框架的简单原理1,任务分配 -----&gt;Task总数 -----&gt;可用worker数量 2,通信机制 -----&gt;去zk获取每个组件的任务 -----&gt;启动不同服务 nimbus，手动 java -server xxx.jar main-class superv，手动 java -server xxx.jar main-class worker，supervisor启动------java -server xxx.jar main-class(main(Worker.mk_work(new Worker()))) Task, Worker启动Task--------Jvm---&gt;Task.mk_Task() 3,心跳机制 thread1------tag=true thread2------&gt;tag=true------tag=false thread1------&gt;tag=true------tag=false------tag=true thread2------&gt;tag=true------tag=false------tag=true-----tag=false 4、任务执行(数据流) spout.nextTuple(tuple)----streamGrouping---&gt; incomingQueue--------&gt;bolt1.exeute(tuple)-----streamGrouping----&gt; incomingQueue--------&gt;bolt2.exeute(tuple) [实现数据执行的框架] spout-----线程1 incomingQueue------queue bolt1-----线程2 incomingQueue------queue bolt2-----线程3 需要技术： 线程池-----&gt;Exeutes.newFixPool(3) 队列-------&gt;ArrayBolckingQueue(1000) 伪代码： MyStrom{ main(){ //1、配置一个线程池 //2、向线程池中提交任务 spoutOutPutQueue = new ArrayBolckingQueue(1000) submit(new MySpout(spoutOutPutQueue))------collector.emit(tuple)------spoutOutPutQueue bolt1OutPutQueue = new ArrayBolckingQueue(1000) submit(new MyBolt1(spoutOutPutQueue,bolt1OutPutQueue))------&gt;spoutOutPutQueue----&gt;bolt1.execute(),collector.emit(tuple)------bolt1OutPutQueue submit(new MyBolt1(bolt1OutPutQueue))------&gt;spoutOutPutQueue----&gt;bolt1.execute() } } 2.尝试自己实现一个类似storm数据执行的框架 流程图 Spout 类12345678910111213141516171819//################## MySpout 类##############################class MySpout extends Thread &#123; private MyStorm myStorm; public MySpout(MyStorm myStorm) &#123; this.myStorm = myStorm; &#125; @Override public void run() &#123; //storm框架在循环调用spout的netxTuple方法 while (true) &#123; myStorm.nextTuple(); try &#123; this.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; BoltSplit123456789101112131415161718//################## MyBoltSplit 类##############################class MyBoltSplit extends Thread &#123; private MyStorm myStorm; @Override public void run() &#123; while (true) &#123; try &#123; String sentence = (String) myStorm.getSentenceQueue().take();//从队列中取Tuple myStorm.split(sentence); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; &#125; public MyBoltSplit(MyStorm myStorm) &#123; this.myStorm = myStorm; &#125;&#125; BoltWordCount12345678910111213141516171819//################## MyBoltWordCount 类##############################class MyBoltWordCount extends Thread &#123; private MyStorm myStorm; @Override public void run() &#123; while (true) &#123; try &#123; //从队列中取Tuple String word = (String) myStorm.getWordQueue().take(); myStorm.wordcounter(word); &#125; catch (Exception e) &#123; System.out.println(e); &#125; &#125; &#125; public MyBoltWordCount(MyStorm myStorm) &#123; this.myStorm = myStorm; &#125;&#125; 启动类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768public class MyStorm &#123; private Random random = new Random(); private BlockingQueue sentenceQueue = new ArrayBlockingQueue(50000); private BlockingQueue wordQueue = new ArrayBlockingQueue(50000); // 用来保存最后计算的结果key=单词，value=单词个数 Map&lt;String, Integer&gt; counters = new HashMap&lt;String, Integer&gt;(); //用来发送句子 public void nextTuple() &#123; String[] sentences = new String[]&#123;&quot;the cow jumped over the moon&quot;, &quot;an apple a day keeps the doctor away&quot;, &quot;four score and seven years ago&quot;, &quot;snow white and the seven dwarfs&quot;, &quot;i am at two with nature&quot;&#125;; String sentence = sentences[random.nextInt(sentences.length)]; try &#123; sentenceQueue.put(sentence); System.out.println(&quot;send sentence:&quot; + sentence); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; //用来切割句子 public void split(String sentence) &#123; System.out.println(&quot;resv sentence&quot; + sentence); String[] words = sentence.split(&quot; &quot;); for (String word : words) &#123; word = word.trim(); if (!word.isEmpty()) &#123; word = word.toLowerCase(); //collector.emit() wordQueue.add(word); System.out.println(&quot;split word:&quot; + word); &#125; &#125; &#125; //用来计算单词 public void wordcounter(String word) &#123; if (!counters.containsKey(word)) &#123; counters.put(word, 1); &#125; else &#123; Integer c = counters.get(word) + 1; counters.put(word, c); &#125; System.out.println(&quot;print map:&quot; + counters); &#125; public static void main(String[] args) &#123; //线程池 ExecutorService executorService = Executors.newFixedThreadPool(10); MyStorm myStorm = new MyStorm(); //发射句子到sentenceQuequ executorService.submit(new MySpout(myStorm)); //接受一个句子，并将句子切割 executorService.submit(new MyBoltSplit(myStorm)); //接受一个单词，并进行据算 executorService.submit(new MyBoltWordCount(myStorm)); &#125; public BlockingQueue getSentenceQueue() &#123; return sentenceQueue; &#125; public void setSentenceQueue(BlockingQueue sentenceQueue) &#123; this.sentenceQueue = sentenceQueue; &#125; public BlockingQueue getWordQueue() &#123; return wordQueue; &#125; public void setWordQueue(BlockingQueue wordQueue) &#123; this.wordQueue = wordQueue; &#125;&#125;","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"关于Storm Stream grouping的分组策略","date":"2017-04-16T04:47:25.324Z","path":"2017/04/16/bigdata/storm/关于Storm Stream grouping的分组策略/","text":"目前，Storm Streaming Grouping支持如下几种类型： Shuffle Grouping：随机分组，跨多个Bolt的Task，能够随机使得每个Bolt的Task接收到大致相同数目的Tuple，但是Tuple不重复 Fields Grouping：根据指定的Field进行分组 ，同一个Field的值一定会被发射到同一个Task上 Partial Key Grouping：与Fields grouping 类似，根据指定的Field的一部分进行分组分发，能够很好地实现Load balance，将Tuple发送给下游的Bolt对应的Task，特别是在存在数据倾斜的场景，使用 Partial Key grouping能够更好地提高资源利用率 All Grouping：所有Bolt的Task都接收同一个Tuple（这里有复制的含义） Global Grouping：所有的流都指向一个Bolt的同一个Task（也就是Task ID最小的） None Grouping：不需要关心Stream如何分组，等价于Shuffle grouping Direct Grouping：由Tupe的生产者来决定发送给下游的哪一个Bolt的Task ，这个要在实际开发编写Bolt代码的逻辑中进行精确控制 Local or Shuffle Grouping：如果目标Bolt有1个或多个Task都在同一个Worker进程对应的JVM实例中，则Tuple只发送给这些Task 另外，Storm还提供了用户自定义Streaming Grouping接口，如果上述Streaming Grouping都无法满足实际业务需求，也可以自己实现，只需要实现backtype.storm.grouping.CustomStreamGrouping接口，该接口定义了如下方法： List&lt;Integer&gt; chooseTasks(int taskId, List&lt;Object&gt; values)上面几种Streaming Group的内置实现中，最常用的应该是Shuffle Grouping、Fields Grouping、Direct Grouping这三种，使用其它的也能满足特定的应用需求。","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm集群的进程及日志熟悉","date":"2017-04-16T04:47:25.323Z","path":"2017/04/16/bigdata/storm/storm集群的进程及日志熟悉/","text":"依次启动集群的各种角色:nimbus,supervisor,ui 查看nimbus的日志信息在nimbus的服务器上 cd /export/servers/storm/logstail -100f /export/servers/storm/logs/nimbus.log 查看ui运行日志信息在ui的服务器上，一般和nimbus一个服务器 cd /export/servers/storm/logstail -100f /export/servers/storm/logs/ui.log 查看supervisor运行日志信息在supervisor服务上 cd /export/servers/storm/logstail -100f /export/servers/storm/logs/supervisor.log 查看supervisor上worker运行日志信息在supervisor服务上 cd /export/servers/storm/logstail -100f /export/servers/storm/logs/worker-6702.log","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm集群安装部署","date":"2017-04-16T04:47:25.322Z","path":"2017/04/16/bigdata/storm/storm集群安装部署/","text":"1.环境准备12345678910111213141516171819202122232425&apos;修改hosts文件&apos;[root@hdp-node-01 bin]# vim /etc/hosts192.168.0.11 hdp-node-01 zk01 storm01192.168.0.22 hdp-node-02 zk02 storm02192.168.0.33 hdp-node-03 zk03 storm03[root@hdp-node-01 bin]# &apos;关闭防火墙和selinux&apos;chkconfig iptables off &amp;&amp; setenforce 0 &apos;创建用户&apos;groupadd realtime &amp;&amp; useradd realtime &amp;&amp; usermod -a -G realtime realtime&apos;创建工作目录并赋权&apos;mkdir /export/servers -pchmod 755 -R /export&apos;切换到realtime用户下&apos;su realtime&apos;配置zookeeper的环境，并启动&apos;#这里就不介绍了，参见zookeeper文档 2.规划表 hdp-node-01 (zk01/storm01) zookeeper nimbus - hdp-node-02 (zk01/storm02) zookeeper - supervisor hdp-node-03 (zk01/storm03) zookeeper - supervisor 3.安装部署123456789101112131415161718192021222324252627282930313233343536373839404142434445&apos;解压安装包，并创建软链接&apos;cd /export/servers/tar -zxvf apache-storm-0.9.6.tar.gz ln -s apache-storm-0.9.6 stormrm -f apache-storm-0.9.6.tar.gz &apos;修改配置文件&apos;vim storm.yamlcd storm/conf/cp storm.yaml storm.yaml.bak #修改前先备份#####################################################################################指定storm使用的zk集群storm.zookeeper.servers: - &quot;zk01&quot; - &quot;zk02&quot; - &quot;zk03&quot;#指定storm本地状态保存地址storm.local.dir: &quot;/export/data/storm/workdir&quot;#指定storm集群中的nimbus节点所在的服务器nimbus.host: &quot;storm01&quot;#指定nimbus启动JVM最大可用内存大小nimbus.childopts: &quot;-Xmx1024m&quot;#指定supervisor启动JVM最大可用内存大小supervisor.childopts: &quot;-Xmx1024m&quot;#指定supervisor节点上，每个worker启动JVM最大可用内存大小worker.childopts: &quot;-Xmx768m&quot;#指定ui启动JVM最大可用内存大小，ui服务一般与nimbus同在一个节点上。ui.childopts: &quot;-Xmx768m&quot;#指定supervisor节点上，启动worker时对应的端口号，每个端口对应槽，每个槽位对应一个workersupervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 ##################################################################################### &apos;分发安装包&apos;#在其他机器上创建目录，并将storm拷贝到其他机器mkdir /export/servers/ -pscp -r /export/servers/storm/ hdp-node-02:/export/servers/scp -r /export/servers/storm/ hdp-node-03:/export/servers/ 4.启动123456789101112131415161718192021222324252627282930313233343536&apos;启动zk&apos;zkServer.sh start #启动zk，并查看状态zkServer.sh status&apos;启动集群&apos;#在nimbus.host所属的机器上启动 nimbus服务cd /export/servers/storm/bin/nohup ./storm nimbus &amp;#在nimbus.host所属的机器上启动ui服务cd /export/servers/storm/bin/nohup ./storm ui &amp;#在其它个点击上启动supervisor服务cd /export/servers/storm/bin/nohup ./storm supervisor &amp;&apos;查看所有的机器上的jps&apos;[root@hdp-node-01 bin]# jps1155 nimbus #nimbus1097 QuorumPeerMain #zk1227 core #ui1470 Jps[root@hdp-node-01 bin]# [root@hdp-node-02 bin]# jps2567 QuorumPeerMain #zk2642 supervisor #supervisor3078 Jps[root@hdp-node-02 bin]# [root@hdp-node-03 storm]# jps3342 Jps2631 supervisor #supervisor1106 QuorumPeerMain #zk[root@hdp-node-03 storm]# 5.测试访问nimbus.host:8080，即可看到storm的ui界面nimbus.host所在的主机的IP","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm集群任务提交流程","date":"2017-04-16T04:47:25.321Z","path":"2017/04/16/bigdata/storm/storm集群任务提交流程/","text":"1.启动集群程序员启动集群，如启动nimbus、supervisor、ui等12345//客户端运行storm nimbus 时，会调用storm的Python脚本，该脚本中为每个命令编写一个方法，每个方法都可以生成一条相应的Java命令， 命令格式如下：java -server xxx.ClasName -args #如启动nimbus：./bin/storm nimbus 调用的是下面的方法 nimbus ------&gt; Running: /export/servers/jdk/bin/java -server backtype.storm.daemon.nimbus #启动supervisor：./bin/storm supervisor 调用的是下面的方法 supervisor -----&gt;Running: /export/servers/jdk/bin/java -server backtype.storm.daemon.supervisor 2.客户端提交任务123456#命令格式：storm jar xxx.jar xx驱动类 参数bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount-28 #wordcount 是拓扑的名#上诉命令实际上回执行下面的方法： Running: /export/servers/jdk/bin/java -client -Dstorm.jar=/export/servers/storm/examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount-28# 该命令会执行：storm-starter-topologies-0.9.6.jar 中 storm.starter.WordCountTopology 的main方法，在main方法中会执行以下代码： 执行上述命令会带来下面的操作 1.上传jar包到/storm/workdir/nimbus/inbox目录下，并且改名，改名规则是添加了一个UUID字符串2.调用jar包中指定驱动类的main方法，在main方法中调用了topologyBuilder.createTopology()，这个方法将程序员编写的spout对象和bolt对象进行序列化（查看createTopology()方法源码知）， 生成一个任务目录，里面包含三个文件12345678910111213[root@hdp-node-01 export]# tree /export/data/storm/workdir/nimbus//export/data/storm/workdir/nimbus/|-- inbox| -- stormjar-4588b45e-9868-4d8d-87bb-6534eb9da61d.jar|-- stormdist |-- wordcount-1-1480906166 | |-- stormcode.ser #序列化对象文件( spout对象和bolt对象) | |-- stormconf.ser #配置文件 | |-- stormjar.jar #jar包 (从nimbus/inbox里面挪过来的) |-- wordcount-8-1-1481088186 |-- stormcode.ser |-- stormconf.ser |-- stormjar.jar 3.nimbus接收到任务之后，将其封装成assignment对象，保存在zookeeper的目录上：/storm/assignment ，该目录只会保存正在运行的topology任务12[zk: localhost:2181(CONNECTED) 3] ls /storm/assignments[wordcount-8-1-1481088186, wordcount-1-1480906166] 3.supervisor启动worker，创建task任务3.1.启动worker&emsp;supervisor通过watch机制，感知（watch会监听zk中节点的变化）到nimbus在zookeeper上的任务分配信息，从zookeeper上拉取任务信息，分辨出属于自己的任务123456789101112131415161718#zookeeper上的任务信息 newAssignment=Assignment[ masterCodeDir=C:\\Users\\MAOXIA~1\\AppData\\Local\\Temp\\\\e73862a8-f7e7-41f3-883d-af494618bc9f\\nimbus\\stormdist\\double11-1-1458909887, #代码目录 nodeHost=&#123;61ce10a7-1e78-4c47-9fb3-c21f43a331ba=192.168.1.106&#125;, #主机 taskStartTimeSecs=&#123;1=1458909910, 2=1458909910, 3=1458909910, 4=1458909910, 5=1458909910, 6=1458909910, 7=1458909910, 8=1458909910&#125;, workers=[ #workers ResourceWorkerSlot[ hostname=192.168.1.106, #主机 memSize=0, cpu=0, tasks=[1, 2, 3, 4, 5, 6, 7, 8], #在一个worker中要启动的tasks jvm=&lt;null&gt;, nodeId=61ce10a7-1e78-4c47-9fb3-c21f43a331ba, port=6900 #绑定的端口 ] ], timeStamp=1458909910633,type=Assign ] supervisor 根据自己的任务信息，启动自己的worker，并发配一个端口，如下：1&apos;export/servers/jdk/bin/java&apos; -server -Xmx768m export/data/storm/workdir/supervisor/stormdist/wordcount1-3-1461683066/stormjar.jar &apos;backtype.storm.daemon.worker&apos; &apos;wordcount1-3-1461683066&apos; &apos;abdfsdf-3083-4d55-b51f-e389b066f90b&apos; &apos;6701&apos; &apos;sfsdf39fd3-7d2b-4e40-aabc-1c88c9848d74&apos; 3.2.创建task任务&emsp; worker启动之后，连接zk，拉取任务12345678910111213141516171819202122#根据任务中的配置信息，去启动task任务，如下是一个work的配置信息ResourceWorkerSlot[ hostname=192.168.1.106, #主机 memSize=0, cpu=0, tasks=[1, 2, 3, 4, 5, 6, 7, 8], #在一个worker中要启动的tasks jvm=&lt;null&gt;, nodeId=61ce10a7-1e78-4c47-9fb3-c21f43a331ba, port=6900 #绑定的端口]/*在supervisor所在的机器上，/export/data/storm/workdir/supervisor/stormdist目录下，有当前正在运行的topology的jar包和配置文件、序列化对象文件（都是从nimbus机器上下载下来的），然后supervisor用上面的jar和从zk中拿到的配置信息去启动task任务*/#假设任务信息：任务序号1--------&gt;spout---------type:spout任务序号2---------&gt;bolt----------type:bolt任务序号3----------&gt;acker-------type:boltworker根据任务类型，分别执行spout任务或者bolt任务，worker通过反序列化，得到程序员自己定义的spout和bolt对象，然后就可以调用spout任务或者bolt任务的生命周期方法： spout的生命周期是：open、nextTuple、outputField bolt的生命周期是：prepare、execute（Tuple） 、 outputField 4.总结1、集群如何启动，任务如何执行？1234java -server nimubs，supervisorclient---&gt;createTopology(序列化)---&gt;提交jar到nimbuinbox---&gt;nimbus分配任务(task总数/worker数)---写到zk。 启动worker&lt;------识别自己的任务&lt;----supervisor----&gt;watch----zk 启动Spout/Bolt----TaskInfo&lt;------worker----&gt;zk 5.图解任务提交流程简图 6.图解任务提交流程详细解说 7.Storm组件本地目录树 8.Storm zookeeper目录树","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"Storm编程模型（各组件：dataSource、spout、bolt、Tuple）","date":"2017-04-16T04:47:25.319Z","path":"2017/04/16/bigdata/storm/Storm编程模型（各组件：dataSource、spout、bolt、Tuple）/","text":"一条数据在storm中是如何流动的呢?一张图说明: DataSource：外部数据源 Spout：接受外部数据源的组件，将外部数据源转化成Storm内部的数据，以Tuple为基本的传输单元下发给Bolt Bolt:接受Spout发送的数据，或上游的bolt的发送的数据。根据业务逻辑进行处理。发送给下一个Bolt或者是存储到某种介质上。介质可以是* Redis可以是mysql，或者其他。 Tuple：Storm内部中数据传输的基本单元，里面封装了一个List对象，用来保存数据。 StreamGrouping:数据分组策略 7种：shuffleGrouping(Random函数),Non Grouping(Random函数),FieldGrouping(Hash取模)、Local or ShuffleGrouping 本地或随机，优先本地。","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm相关问题集锦","date":"2017-04-16T04:47:25.318Z","path":"2017/04/16/bigdata/storm/storm相关问题集锦/","text":"下面是关于storm的一些常见的问题: 1.如何保证一条数据在经过kafka和storm之时，消息被完整处理。12345678kakfa，生产和消费 storm，ackfailproducer---&lt;同步/异步（缓冲区）&gt;-------------------&gt;broker-------------------zk-------------&gt; KafkaSpout-------------------------------&gt;Bolt(订单ID,去重) &apos;缓冲区的时间阈值和数量阈值&apos; partition的目录 offset=1w ack fail(重发) Redis Set &apos;消息响应机制：ack (0,1,-1)&apos; 自定义的Spout需要Map或外部存储保存数据 一批一批拉取(1w+num) 时间阈值和数量阈值触发 更新操作 offset 1w+num 正在此时，KafkaSpout失败了，会导致重复消费 2.Kafka和storm组合有没有丢数据可能。 &emsp;由上图知：可能丢数据的地方有： 在buff缓冲区堆积过多的数据，而向partition中发送的过程较慢，那么缓冲区有两种情况： 1.1. 删除缓冲区数据 1.2. buff继续阻塞 3.对重复消费的问题，该如何解决？ &emsp;将已经处理的消息存入redis中，然后将ack中返回的Tuple去redis中去比较，如果存在那么就将该Tuple抛弃，否则继续发往下一个bolt 4.Storm怎么处理重复的tuple？ 因为Storm要保证tuple的可靠处理，当tuple处理失败或者超时的时候，spout会fail并重新发送该tuple，那么就会有tuple重复计算的问题。这个问题是很难解决的，storm也没有提供机制帮助你解决。一些可行的策略：（1）不处理，这也算是种策略。因为实时计算通常并不要求很高的精确度，后续的批处理计算会更正实时计算的误差。（2）使用第三方集中存储来过滤，比如利用mysql,memcached或者redis根据逻辑主键来去重。（3）使用bloom filter做过滤，简单高效。 5.你们有没有想过如果某一个task节点处理的tuple一直失败，消息一直重发会怎么样？ &emsp;我们都知道，spout作为消息的发送源，在没有收到该tuple来至左右bolt的返回信息前，是不会删除的，那么如果消息一直失败，就会导致spout节点存储的tuple数据越来越多，导致内存溢出。 6.有没有想过，如果该tuple的众多子tuple中，某一个子tuple处理failed了，但是另外的子tuple仍然会继续执行，如果子tuple都是执行数据存储操作，那么就算整个消息失败，那些生成的子tuple还是会成功执行而不会回滚的。 7.tuple的追踪并不一定要是从spout结点到最后一个bolt,只要是spout开始，可以在任意层次bolt停止追踪做出应答。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960问题： 1，kafka+storm如何保证消息完整处理。 一条消息产生----Kafka--KafkaSpout-Storm---&gt;Redis 问题1：kafka数据生产消费如何保证消息的完整处理 Producer-batch(缓存机制queue)--重试机制----&gt;ack(-1,1,0)---Broker(partition leader/slave)------&gt;KafkaConsumer(内存中，new offset)----&gt;zk( old offset) Producer发送时缓存数据，需要阈值设定（数量阈值，时间阈值），当数据太多并来不及发送时，会产生老数据是否保留的问题，如何配置文件中配置的是删除掉，数据就丢失了。 KafkaConsumer 消费数据时，由于offset是周期性更新，导致zk上的offset值必然小于Kafkaconsumer内存中的值，当KafkaConsumer挂掉后重启，必然会导致数据重复消费。 如何解决重复消费：1，技术方法(不可取)，2，业务方法（标识数据：找到消息中的唯一标识 &lt;messageTag,isProcess&gt;） 问题2：Storm中如何保证消息的完整处理 ack - fail 自己定义 缓存Map&lt;msgid,messageObj&gt; Spout----&gt;nextTuple----Tuple---Bolt(ack(tuple)) Bolt1-----Tuple1-1 Bolt1-----Tuple1-2 Bolt1-----Tuple1-3 Bolt1-----Tuple1-4 Bolt2(ack(Tuple1-1)) Bolt2(ack(Tuple1-2)) Bolt2(ack(Tuple1-3)) Bolt2(ack(Tuple1-4)) 如果成功（spout.ack(msg)） map.remove(msgid) 如果失败（spout.fail(msg)） messageObj = map.get(msgid) collector.emit(messageObj,msgid) 问题3：如何保证一条消息路过各个组件时保证全局的完整处理 保存每个环节的数据不丢失，自然就全局不丢失。 问题4：在storm环节中自定义的map如何存储，在KafkaConsumer如何处理重复消费的问题？ 1，保存在当前Jvm中，既然出异常导致消息不能完全处理，存放在jvm中的标识数据，缓存Map必然会丢失 2，保存在外部的存储中，redis。 标识数据&lt;Set&gt; 缓存Map&lt;Map&gt; 问题：请问存储redis时，由于网络原因或其他异常导致数据不能成功存储 怎么办？ 重试机制，保存3次，打印log日志，redis存储失败。 2，数据量大如何保证到Kakfa中，storm如何消费，解决延迟。 问题1：请问Kafka如何处理海量数据 -----整体数据：100G 数据从何而来：producer 集群----DefaultPartition ------保存数据&apos;平均分配&apos; 数据保存在哪里： broker 集群 topic ---partition 10 --------------&gt;每个&apos;分片&apos;保存10G 数据保存&apos;如何快&apos;：pageCache 600M/S的磁盘写入速度 sendfile技术 问题2: 请问Storm如何处理海量数据，尽可能快 数据输入： KafkaSpout(consumerGroup,10) 读取外部数据源 数据计算： 数据计算是根据对数据处理的业务复杂度来的，越复杂并发度越大。 如果bolt的并发读要设置成1万个，才能提高处理速度，很显然是不行的。&apos;需要将bolt中的代码逻辑分解出来，形成多个bolt组合&apos;。 bolt1---&gt;bolt2---&gt;bolt3...... 3，Flume监控文件，异常之后重新启动，如何避免重复读取。 异常的范围是 flumeNg的异常 log文件，读取文件，保存记录的行号 问题1：如何保存行号 command.type = exec shell&lt;tail -F xxx.log&gt; 自定义shell脚本，保存消费的行号到工作目录的某个文件里。 当下次启动时，从行号记录文件中拿取上次读到多少行。","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm的组件(nimbus、supervisor、worker、spout、bolt)关系图示","date":"2017-04-16T04:47:25.317Z","path":"2017/04/16/bigdata/storm/storm的组件(nimbus、supervisor、worker、spout、bolt)关系图示/","text":"下面是storm中各组件的关系示意图: 并发度：用户指定的一个任务，可以被多个线程执行，并发度的数量等于线程的数量。一个任务的多个线程，会被运行在多个Worker（JVM）上，有一种类似于平均算法的负载均衡策略。尽可能减少网络IO，和Hadoop中的MapReduce中的本地计算的道理一样。 Nimbus：任务分配 Supervisor：接受任务，并启动worker。worker的数量根据端口号来的。 Worker:执行任务的具体组件（其实就是一个JVM）,可以执行两种类型的任务，Spout任务或者bolt任务。 Task：Task=线程=executor。 一个Task属于一个Spout或者Bolt并发任务。 Zookeeper：保存任务分配的信息、心跳信息、元数据信息。 Worker与topology关系一个worker只属于一个topology,每个worker中运行的task只能属于这个topology。 反之，一个topology包含多个worker，其实就是这个topology运行在多个worker上。一个topology要求的worker数量如果不被满足，集群在任务分配时，根据现有的worker先运行topology。如果当前集群中worker数量为0，那么最新提交的topology将只会被标识active，不会运行，只有当集群有了空闲资源之后，才会被运行。 worker、Executor、task的关系 Task：Spout/Bolt在运行时所表现出来的实体，都称为Task，一个Spout/Bolt在运行时可能对应一个或多个Spout Task/Bolt Task，与实际在编写Topology时进行配置有关。 Worker：运行时Task所在的一级容器，Executor运行于Worker中，一个Worker对应于Supervisor上创建的一个JVM实例 Executor：运行时Task所在的直接容器，在Executor中执行Task的处理逻辑；一个或多个Executor实例可以运行在同一个Worker进程中，一个或多个Task可以运行于同一个Executor中；在Worker进程并行的基础上，Executor可以并行，进而Task也能够基于Executor实现并行计算","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm的wordcount程序现实及执行流程","date":"2017-04-16T04:47:25.316Z","path":"2017/04/16/bigdata/storm/storm的wordcount程序现实及执行流程/","text":"1.代码结构示意图 2.代码实现2.1.主程序WordCountTopologMain1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package cn.itcast.storm;import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.generated.AlreadyAliveException;import backtype.storm.generated.InvalidTopologyException;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.TopologyBuilder;import backtype.storm.tuple.Fields;/** * Created by maoxiangyi on 2016/4/27. */public class WordCountTopologMain &#123; public static void main(String[] args) throws AlreadyAliveException, InvalidTopologyException &#123; //1、准备一个TopologyBuilder TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout(&quot;mySpout&quot;,new MySpout(),2); topologyBuilder.setBolt(&quot;mybolt1&quot;,new MySplitBolt(),2).shuffleGrouping(&quot;mySpout&quot;); /* * 在MySplitBolt类中声明发送的Tuple都是&lt;word,num&gt;的形式， * public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;word&quot;,&quot;num&quot;)); &#125; * 而：topologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).fieldsGrouping(&quot;mybolt1&quot;, new Fields(&quot;word&quot;)); * 中设置由mybolt1发送到mybolt2的Tuple是按照Tuple&lt;word,num&gt;中的word来进行分组的，即：word字段相同的发送到一个task中 * 有如下的Tuple，如： * Tuple1=&lt;&quot;aaa&quot;,1&gt; * Tuple2=&lt;&quot;abc&quot;,2&gt; * Tuple3=&lt;&quot;dbe&quot;,2&gt; * Tuple4=&lt;&quot;aaa&quot;,5&gt; * 那么Tuple1和Tuple4将发送到一个task中作为同组，因为他们的word字段都是&quot;aaa&quot; */ topologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).fieldsGrouping(&quot;mybolt1&quot;, new Fields(&quot;word&quot;)); /** * 这里用fieldsGrouping分组的原因是对相同的单词，将发射到同一个count-bolt线程中，那么统计才会有效， * 而shuffleGrouping（随机）可能将同一个单词发送到不同的count-bolt中，那么没法实现单词的统计 */ //topologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).shuffleGrouping(&quot;mybolt1&quot;); // config.setNumWorkers(2); //2、创建一个configuration，用来指定当前topology 需要的worker的数量 Config config = new Config(); config.setNumWorkers(2); //3、提交任务 -----两种模式 本地模式和集群模式// StormSubmitter.submitTopology(&quot;mywordcount&quot;,config,topologyBuilder.createTopology()); LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology(&quot;mywordcount&quot;,config,topologyBuilder.createTopology()); &#125;&#125; 2.2.MySpout12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.storm;import java.util.Arrays;import java.util.Map;import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichSpout;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;/** * Created by maoxiangyi on 2016/4/27. */public class MySpout extends BaseRichSpout &#123; SpoutOutputCollector collector; //初始化方法 public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; //storm 框架在 while(true) 调用nextTuple方法 public void nextTuple() &#123; collector.emit(new Values(&quot;i am lilei love hanmeimei&quot;)); &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; //声明发往下一个blot的字段标识 declarer.declare(new Fields(&quot;love&quot;));//Fields就是一个数组：new Fields(&quot;love&quot;, &quot;aaa&quot;, &quot;bbb&quot;) /* public Fields(String... fields) &#123; this(Arrays.asList(fields)); &#125; //这里的Values也是一个数组，其元素和Fields中的元素相对应 collector.emit(new Values(&quot;i am lilei love hanmeimei&quot;, &quot;message-aaa&quot;, &quot;message-bbb&quot;)); */ &#125;&#125; 2.3. MySplitBolt（切分）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package cn.itcast.storm;import java.util.ArrayList;import java.util.Map;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;/** * Created by maoxiangyi on 2016/4/27. */public class MySplitBolt extends BaseRichBolt &#123; OutputCollector collector; //初始化方法 public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; // 被storm框架 while(true) 循环调用 传入参数tuple public void execute(Tuple input) &#123; /* public String getString(int i) &#123; return (String) values.get(i);//values是一个List &#125; */ String line = input.getString(0); /* input.getStringByField(&quot;love&quot;);那么只是拿到的Values数组中Fields.index[&quot;love&quot;]下标的内容 public String getStringByField(String field) &#123; return (String) values.get(fieldIndex(field)); &#125; */ String[] arrWords = line.split(&quot; &quot;); for (String word:arrWords)&#123; collector.emit(new Values(word,1));//Values extends ArrayList &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;word&quot;,&quot;num&quot;)); /* 这里声明的字段word和num和上面collector.emit(new Values(word,1));中的对应 这样在下一个bolt中可以使用input.getStringByField(&quot;word&quot;);去get不同的部分 */ &#125;&#125; 2.4.MyCountBolt（统计）12345678910111213141516171819202122232425262728293031323334353637383940package cn.itcast.storm;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.IBasicBolt;import backtype.storm.topology.IRichBolt;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Tuple;import java.util.HashMap;import java.util.Map;/** * Created by maoxiangyi on 2016/4/27. */public class MyCountBolt extends BaseRichBolt &#123; OutputCollector collector; Map&lt;String, Integer&gt; map = new HashMap&lt;String, Integer&gt;(); public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; public void execute(Tuple input) &#123; String word = input.getString(0); Integer num = input.getInteger(1); /* 因为已经知道上一个blot中发送过来的list中第一个字段是String，而list中第二个字段是Internet，所以我们可以直接取 但是也可以使用: input.getStringByField(&quot;word&quot;); input.getIntegerByField(&quot;num&quot;); */ System.out.println(Thread.currentThread().getId() + &quot; word:&quot;+word); if (map.containsKey(word))&#123; Integer count = map.get(word); map.put(word,count + num); &#125;else &#123; map.put(word,num); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; //不輸出 &#125;&#125; 3.打包上传，并在Linux上执行1234#在分布式上执行storm jar myStormApp.jar cn.itcast.storm.WordCountTooplogMain#可以在浏览器上查看UI界面中是否有topology 4.随机分组和字段分组的区别12345678910111213141516171819202122232425/*在主程序中，从mybolt1出来的Tuple是通过字段分组的形式到达mybolt2中的， 这里用fieldsGrouping分组的原因是对相同的单词，按照值的哈希取模，将发射到同一个count-bolt线程中，那么对全局的统计才会有效，而shuffleGrouping（随机）可能将同一个单词发送到不同的count-bolt中，那么没法实现全局单词的统计*/public class WordCountTopologMain &#123; public static void main(String[] args) throws AlreadyAliveException, InvalidTopologyException &#123; //1、准备一个TopologyBuilder TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout(&quot;mySpout&quot;,new MySpout(),2); topologyBuilder.setBolt(&quot;mybolt1&quot;,new MySplitBolt(),2).shuffleGrouping(&quot;mySpout&quot;); topologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).fieldsGrouping(&quot;mybolt1&quot;, new Fields(&quot;word&quot;));//fieldsGrouping:字段分组 //topologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).shuffleGrouping(&quot;mybolt1&quot;);//随机分组 // config.setNumWorkers(2); //2、创建一个configuration，用来指定当前topology 需要的worker的数量 Config config = new Config(); config.setNumWorkers(2); //3、提交任务 -----两种模式 本地模式和集群模式// StormSubmitter.submitTopology(&quot;mywordcount&quot;,config,topologyBuilder.createTopology()); LocalCluster localCluster = new LocalCluster(); localCluster.submitTopology(&quot;mywordcount&quot;,config,topologyBuilder.createTopology()); &#125;&#125; 5.wordcount执行流程图示 说明:12345678910config.setNumWorkers(2) //所以会分配2个workertopologyBuilder.setSpout(&quot;mySpout&quot;,new MySpout(),2); //分配2个spouttopologyBuilder.setBolt(&quot;mybolt1&quot;,new MySplitBolt(),2).shuffleGrouping(&quot;mySpout&quot;); //分配2个splitBolttopologyBuilder.setBolt(&quot;mybolt2&quot;,new MyCountBolt(),4).fieldsGrouping(&quot;mybolt1&quot;, new Fields(&quot;word&quot;)); //分配4个countBolt 由上知:&emsp;&emsp;总的task=(spout+splitBolt+countBolt)=8则&emsp;&emsp;每个worker: 8/2=4 个task 再来看每个task是在worker中如何分配的:在分配spout和bolt中有一个轮询的策略,如:myspout-0分配给worker1,myspout-1分配给worker2,这样轮询 由spout发送到splitBolt的过程是随机的,因为使用的是shuffleGrouping分组,而由splitBolt发送到CountBolt的过程则是按字段分组:splitBolt发送到CountBolt的Tuple是:&emsp;&emsp;collector.emit(new Values(word,1))word会hash取模:&emsp;&emsp;word.hashcode%4 来决定发送到哪一个CountBolt中","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm的ack机制","date":"2017-04-16T04:47:25.315Z","path":"2017/04/16/bigdata/storm/storm的ack机制/","text":"ack实现原理我们知道，Storm保证发出的每条消息都能够得到完全处理，也就是说，对于从Spout发出的每个tuple，该tuple和其产生的所有tuple（整棵tuple树）都被成功处理。如果在应用设定的超时时间之内，这个tuple没有处理成功，则认为这个tuple处理失败了。tuple处理成功还是失败，Storm又是怎么知道的呢？ 原来Storm中有一类叫Acker的task，它会对tuple树进行跟踪，并检测相应的spout tuple是否处理完成了。当一个tuple被创建时，不管是在Spout还是Bolt中创建，它都会被赋予一个tuple-id（随机生成的64位数字），这些tuple-id就是Acker用来跟踪每个spout tuple产生的tuple树的。如果一个spout tuple被完全处理了，它会给创建这个spout tuple的那个task发送一个成功消息，否则发送一个失败消息。 在Spout创建一个新tuple时，会生成一个root-id（也是随机的64位数字），并且这个root-id会传递给这个spout tuple所生成的tuple树中的每个tuple，因此有了这个root-id，我们就可以追踪这棵tuple树了。如果一个tuple被完全处理了，Storm就会调用Spout对应task的ack方法；否则调用Spout对应的fail方法。每个tuple都必须被ack或者fail，因为Storm追踪每个tuple需要占用内存，如果你不ack或fail每一个tuple， 那么最终会导致OOM（OutOfMemory）。 Acker跟踪算法的基本思想是：对于从Spout发射出来的每个spout tuple，Acker都保存了一个ack-val（校验值），初始值为0，每当tuple被创建或被ack，这些对应tuple的tuple-id（随机生成的64位整数）都会在某个时刻和保存的ack-val进行按位异或运算，并用异或运算的结果更新ack-val。如果每个spout tuple对应tuple树中的每个tuple都被成功处理，那最终的ack-val必然为0。为何呢？因为在这个过程中，同一个tuple-id都会被异或两次，而相同值的异或运算结果为0，且异或运算满足结合律，如a^a=0，a^b^a^b=(a^a)^(b^b)=0 如图1所示，Acker为了实现自己的跟踪算法，它会维护这样一个数据结构： {root-id {:spout-task task-id :val ack-val :failed bool-val …}} 其实就是一个Map，从上面这个Map中，我们知道，一个Acker存储了一个root-id到一对值的映射关系。这对值的第一个是创建这个tuple的task-id，当这个tuple处理完成进行ack的时候会用到。第二个是一个随机的64位的数字，即ack-val，ack-val表示整棵tuple树的状态，不管这棵tuple树多大，它只是简单地把这棵树上的相应的tuple-id做按位异或运算。因此即使一个spout tuple生成一棵有成千上万tuple的tuple树，Acker进行跟踪时也不会耗费太多的内存，对于每个spout tuple，Acker所需要的内存量都是恒定的20字节。这也是Storm的主要突破。 Acker跟踪算法需要不断更新ack-val，那ack-val又是怎么更新的呢？其实主要就是如下3个环节：1）Spout创建新tuple的时候会给Acker发送消息。2）Bolt中的tuple被ack的时候给Acker发送消息(程序中指定ack)3）Acker根据接收到的消息做按位异或运算，更新自己的ack-val。 当Spout创建了一个新的tuple时，会发送消息给Acker，消息的格式为[root-id,tmp-ack-val,task-id]，Acker会根据这个tmp-ack-val更新自己维护的Map中的ack-val值。 在Storm中，当一个spout tuple被完全处理后，会调用Spout中的task的ack或者fail方法，而且这个task必须是创建这个tuple的task。也就是说，如果一个Spout中启动了多个task，消息处理成功还是失败，最终都会通知Spout中发出tuple的那个对应的task。但Acker是如何知道每个spout tuple是由哪个task创建的呢？ 从上面Spout给Acker发送的消息格式即可知道，Spout中创建一个新tuple时，它会创建这个tuple的task的task-id告诉Acker，于是当Acker发现一棵tuple树完成处理时，它知道给Spout中的哪个task发送成功消息，或者在处理失败时发送失败消息。 当一个tuple在Bolt中被ack的时候，它也会给Acker发送一个消息，告诉它这棵tuple树发生了什么样的变化。具体来说就是，它告诉Acker，在这棵tuple树中，我这个tuple已经完成了， 但我生成了这些新的tuple，并让Acker去跟踪一下它们。tuple被ack时发送给Acker的消息格式为[root-id,tmp-ack-val]，Acker会根据这个tmp-ack-val更新自己的ack-val值，当检测到ack-val为0时，就表示一个spout tuple被完全处理了。 在Topology中，Acker的个数我们是可以自己设置的。既然Acker可能有多个，那么当一个tuple需要被ack的时候，它怎么知道选择哪个Acker来发送这个消息呢？ Storm使用mod hashing将一个spout tuple的root-id映射到一个Acker，因为同一棵tuple树中的所有tuple都保存了相同的root-id，那么当一个tuple被ack的时候，它自然就知道应该给哪个Acker发送消息了。 下面我们结合一个具体的例子来揭开Acker实现机制的神秘面纱。这个例子的功能很简单，Spout从外界消息队列中获取句子，Bolt1接收从Spout中发送过来的句子并拆分成单词，其它Bolt（如下图中的Bolt2和Bolt3）会对相应的单词个数做统计。 在图2-1中，Spout创建了一个新的tuple，于是它发消息给Acker，消息内容为[66,8,11]，其中66为这个tuple对应的root-id，8为这个tuple的tuple-id，11为创建这个tuple的task的task-id。Acker接收到这条消息后更新自己维护的数据结构，更新后为{66 {11 8}}（见图左下角），即root-id为66，task-id为11，ack-val为8。 在图2-2中，Bolt1将”good idea”这个输入tuple拆分成”good”和”idea”两个输出tuple，处理完后它给Acker发送消息[66,11]。我们知道66是root-id，但这个11是怎么计算出来的呢？在Storm的实现中，首先会将这个输入tuple生成的所有输出tuple的tuple-id进行异或运算，这里两个输出tuple的tuple-id分别为4和7，4 XOR 7 = 3；然后再将这个结果和输入tuple的tuple-id进行异或，输入tuple的tuple-id为8，即3 XOR 8 = 11。因此它发送Acker的消息为[66,11]。Acker接收到这个消息后更新自己的ack-val，8 XOR 11 = 3，更新后Acker维护的数据结构变为{66 {11 3}}。 在图2-3中，Bolt2中不再生成新的tuple，处理完后它给Acker发送消息[66,4]。Acker接收到这个消息后更新自己的ack-val，3 XOR 4 = 7，更新后Acker维护的数据结构变为{66 {11 7}}。 在图2-4中，同样，Bolt3中不再生成新的tuple，处理完后它给Acker发送消息[66,7]。Acker接收到这个消息后更新自己的ack-val，7 XOR 7 = 0，更新后Acker维护的数据结构变为{66 {11 0}}，这个时候Acker发现ack-val变成0了，它就给Spout中对应的task发送一条成功消息，表明对应的spout tuple被完全处理了。 因为tuple-id是随机的64位数字，所以ack-val碰巧变成0（而不是因为所有创建的tuple都处理完成）的概率可以忽略不计。举个例子， 就算每秒发生10000个ack， 那么也需要50 000 000年才可能发生一个错误。并且就算发生了一个错误，也只有在这个tuple处理失败的时候才会造成数据丢失。 关于ack的api的使用 启动类123456789101112131415161718192021222324252627package cn.itcast.storm;import backtype.storm.Config;import backtype.storm.LocalCluster;import backtype.storm.StormSubmitter;import backtype.storm.topology.TopologyBuilder;public class MyAckFailTopology &#123; public static void main(String[] args) throws Exception &#123; TopologyBuilder topologyBuilder = new TopologyBuilder(); topologyBuilder.setSpout(&quot;mySpout&quot;, new MySpout(), 1); topologyBuilder.setBolt(&quot;mybolt1&quot;, new MyBolt1(), 1).shuffleGrouping(&quot;mySpout&quot;); Config conf = new Config(); String name = MyAckFailTopology.class.getSimpleName(); if (args != null &amp;&amp; args.length &gt; 0) &#123;//如果指定了参数，就用集群模式运行 String nimbus = args[0]; conf.put(Config.NIMBUS_HOST, nimbus); conf.setNumWorkers(1); StormSubmitter.submitTopologyWithProgressBar(name, conf, topologyBuilder.createTopology()); &#125; else &#123;//没有指定参数，就用本地模式运行 LocalCluster cluster = new LocalCluster(); cluster.submitTopology(name, conf, topologyBuilder.createTopology()); Thread.sleep(60 * 60 * 1000); cluster.shutdown(); &#125; &#125;&#125; Spout类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package cn.itcast.storm.ackfail;import backtype.storm.spout.SpoutOutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichSpout;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Values;import java.util.HashMap;import java.util.Map;import java.util.Random;import java.util.UUID;/** * Created by maoxiangyi on 2016/4/25. */public class MySpout extends BaseRichSpout &#123; private SpoutOutputCollector collector; private Random rand; //用来存放Tuple private Map&lt;String,Values&gt; buffer = new HashMap&lt;&gt;(); @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;sentence&quot;)); rand = new Random(); &#125; @Override public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) &#123; this.collector = collector; &#125; @Override public void nextTuple() &#123; String[] sentences = new String[]&#123;&quot;the cow jumped over the moon&quot;, &quot;the cow jumped over the moon&quot;, &quot;the cow jumped over the moon&quot;, &quot;the cow jumped over the moon&quot;, &quot;the cow jumped over the moon&quot;&#125;; String sentence = sentences[rand.nextInt(sentences.length)]; String messageId = UUID.randomUUID().toString().replace(&quot;-&quot;, &quot;&quot;); Values tuple = new Values(sentence); collector.emit(tuple, messageId);//在messageId不为null的情况下，如果Tuple被成功处理，会回调ack，如果失败则回调fail；如果messageId为null，那么将不会回调，无论Tuple是否处理成功 //向buff中存入Tuple buffer.put(messageId,tuple); try &#123; Thread.sleep(20000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; /* * 如果Tuple发送成功，那么会调用ack方法 * */ @Override public void ack(Object msgId) &#123; System.out.println(&quot;消息处理成功，id= &quot; + msgId); //从buff中移除Tuple buffer.remove(msgId); &#125; /*如果Tuple发送失败或者是bolt处理失败，那么会调用fail方法*/ @Override public void fail(Object msgId) &#123; System.out.println(&quot;消息处理失败，id= &quot; + msgId); //取出buffer中的Tuple，重新发送 Values tuple = buffer.get(msgId); collector.emit(tuple,msgId); &#125;&#125; Bolt类123456789101112131415161718192021222324252627282930313233343536373839404142package cn.itcast.storm.ackfail;import backtype.storm.task.OutputCollector;import backtype.storm.task.TopologyContext;import backtype.storm.topology.OutputFieldsDeclarer;import backtype.storm.topology.base.BaseRichBolt;import backtype.storm.tuple.Fields;import backtype.storm.tuple.Tuple;import backtype.storm.tuple.Values;import java.util.Map;/** * Created by maoxiangyi on 2016/4/25. */public class MyBolt1 extends BaseRichBolt &#123; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; @Override public void execute(Tuple input) &#123; String sentence = input.getString(0); String[] words = sentence.split(&quot; &quot;); for (String word : words) &#123; word = word.trim(); if (!word.isEmpty()) &#123; word = word.toLowerCase(); collector.emit(input, new Values(word)); &#125; &#125; collector.ack(input);//一个Tuple处理完成，要通知Acker处理Tuple成功 &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields(&quot;word&quot;)); &#125;&#125; 可靠性配置对于ack机制是否使用，我们要考虑业务场景，比如：对大量的点击流日志，我们没有必要开启，因为即使一个Tuple丢失，也不会影响到统计的结果，所以此时不必要care他 有三种方法可以去掉消息的可靠性： 将参数Config.TOPOLOGY_ACKERS设置为0，通过此方法，当Spout发送一个消息的时候，它的ack方法将立刻被调用； Spout发送一个消息时，不指定此消息的messageID。当需要关闭特定消息可靠性的时候，可以使用此方法； 最后，如果你不在意某个消息派生出来的子孙消息的可靠性，则此消息派生出来的子消息在发送时不要做锚定，即在emit方法中不指定输入消息。因为这些子孙消息没有被锚定在任何tuple tree中，因此他们的失败不会引起任何spout重新发送消息。 Storm怎么处理重复的tuple？因为Storm要保证tuple的可靠处理，当tuple处理失败或者超时的时候，spout会fail并重新发送该tuple，那么就会有tuple重复计算的问题。这个问题是很难解决的，storm也没有提供机制帮助你解决。一些可行的策略：（1）不处理，这也算是种策略。因为实时计算通常并不要求很高的精确度，后续的批处理计算会更正实时计算的误差。（2）使用第三方集中存储来过滤，比如利用mysql,memcached或者redis根据逻辑主键来去重。（3）使用bloom filter做过滤，简单高效。 问题一：你们有没有想过如果某一个task节点处理的tuple一直失败，消息一直重发会怎么样？ 我们都知道，spout作为消息的发送源，在没有收到该tuple来至左右bolt的返回信息前，是不会删除的，那么如果消息一直失败，就会导致spout节点存储的tuple数据越来越多，导致内存溢出。 问题二：有没有想过，如果该tuple的众多子tuple中，某一个子tuple处理failed了，但是另外的子tuple仍然会继续执行，如果子tuple都是执行数据存储操作，那么就算整个消息失败，那些生成的子tuple还是会成功执行而不会回滚的。 这个时候storm的原生api是无法支持这种事务性操作，我们可以使用storm提供的高级api-trident来做到（具体如何我不清楚，目前没有研究它，但是我可以它内部一定是根据分布式协议比如两阶段提交协议等）。向这种业务中要保证事务性功能，我们完全可以根据我们自身的业务来做到，比如这里的入库操作，我们先记录该消息是否已经入库的状态，再入库时查询状态来决定是否给予执行。 问题三：tuple的追踪并不一定要是从spout结点到最后一个bolt,只要是spout开始，可以在任意层次bolt停止追踪做出应答。 Acker task 组件来设置一个topology里面的acker的数量，默认值是一，如果你的topoogy里面的tuple比较多的话，那么请把acker的数量设置多一点，效率会更高一点 参考:storm的ack机制Storm的ack机制在项目应用中的坑","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm执行Wordcount案例分析","date":"2017-04-16T04:47:25.313Z","path":"2017/04/16/bigdata/storm/storm执行Wordcount案例分析/","text":"下面是从UI界面上对storm的各个组件的分布情况进行分析 1.提交wordcount任务我们提交了一个storm自带的wordcount程序12bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount#wordcount 是拓扑的名称 2.浏览器观察2.1.topology页面我们可以看到topology,supervisor和nimbus的情况: 2.2.topology详情页 2.3.spout详情页 2.4.bolt中split任务的详情页 2.5.bolt中count任务的详情页 2.6.总结 3.代码分析3.1.wordcount程序的不同版本实现 3.2.wordcount程序中storm对spout、workers、split的设定 其中spout、bolt、worker的数量的设定规则： 根据上游的数据量来设置Spout的并发度 根据业务复杂度和execute方法执行时间来设置Bolt并发度 根据集群的可用资源配置，一般情况下70%的资源使用率","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"Storm常用shell操作命令","date":"2017-04-16T04:47:25.312Z","path":"2017/04/16/bigdata/storm/Storm常用shell操作命令/","text":"有许多简单且有用的命令可以用来管理拓扑，它们可以提交、杀死、禁用、再平衡拓扑12345678910111213141516171819202122#提交任务命令格式：storm jar 【jar路径】 【拓扑包名.拓扑类名】 【拓扑名称】bin/storm jar examples/storm-starter/storm-starter-topologies-0.9.6.jar storm.starter.WordCountTopology wordcount#杀死任务命令格式：storm kill 【拓扑名称】 -w 10（执行kill命令时可以通过-w [等待秒数]指定拓扑停用以后的等待时间）storm kill topology-name -w 10#停用任务命令格式：storm deactivte 【拓扑名称】storm deactivte topology-name#我们能够挂起或停用运行中的拓扑。当停用拓扑时，所有已分发的元组都会得到处理，但是spouts的nextTuple方法不会被调用。销毁一个拓扑，可以使用kill命令。它会以一种安全的方式销毁一个拓扑，首先停用拓扑，在等待拓扑消息的时间段内允许拓扑完成当前的数据流。#启用任务命令格式：storm activate【拓扑名称】storm activate topology-name#重新部署任务命令格式：storm rebalance 【拓扑名称】storm rebalance topology-name#再平衡使你重分配集群任务。这是个很强大的命令。比如，你向一个运行中的集群增加了节点。再平衡命令将会停用拓扑，然后在相应超时时间之后重分配工人，并重启拓扑。","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm内部通信机制2(转)","date":"2017-04-16T04:47:25.310Z","path":"2017/04/16/bigdata/storm/storm内部通信机制2/","text":"转自:Apache Storm内部原理分析 一个Topology提交到Storm集群上运行，具体的处理流程非常微妙，有点复杂。首先，我们通过要点的方式来概要地说明： 每个Executor可能存在一个Incoming Queue和一个Outgoing Queue，这两个队列都是使用的LMAX Disruptor Queue（可以通过相关资料来了解） 两个LMAX Disruptor Queue的上游和下游，都会有相关线程去存储/取出Tuple 每个Executor可能存在一个Send Thread，用来将处理完成生成的新的Tuple放到属于该Executor的Outgoing Queue队列 每个Executor一定存在一个Main Thread，用来处理衔接Spout Task/Bolt Task与前后的Incoming Queue、Outgoing Queue 每个Worker进程内部可能存在一个Receive Thread，用来接收由上游Worker中的Transfer Thread发送过来的Tuple，在一个Worker内部Receive Thread是被多个Executor共享的 每个Worker进程内部可能存在一个Outgoing Queue，用来存放需要跨Worker传输的Tuple（其内部的Transfer Thread会从该队列中读取Tuple进行传输） 每个Worker进程内部可能存在一个Transfer Thread，用来将需要在Worker之间传输的Tuple发送到下游的Worker内 上面，很多地方我使用了“可能”，实际上大部分情况下是这样的，注意了解即可。下面，我们根据Spout Task/Bolt Task运行时分布的不同情况，分别阐述如下： Spout Task在Executor内部运行Spout Task和Bolt Task运行时在Executor中运行有一点不同，如果Spout Task所在的同一个Executor中没有Bolt Task，则该Executor中只有一个Outgoing Queue用来存放将要传输到Bolt Task的队列，因为Spout Task需要从一个给定的数据源连续不断地读入数据而形成流。在一个Executor中，Spout Task及其相关组件的执行流程，如下图所示： 上图所描述的数据流处理流程，如下所示： Spout Task从外部数据源读取消息或事件，在实现的Topology中的Spout中调用nextTuple()方法，并以Tuple对象的格式emit()读取到的数据 Main Thread处理输入的Tuple，然后放入到属于该Executor的Outgoing Queue中 属于该Executor的Send Thread从Outgoing Queue中读取Tuple，并传输到下游的一个或多个Bolt Task去处理 Bolt Task在Executor内部运行前面说过，Bolt Task运行时在Executor中与Spout Task有一点不同，一个Bolt Task所在的Executor中有Incoming Queue和Outgoing Queue这两个队列，Incoming Queue用来存放数据流处理方向上，该Bolt Task上游的组件（可能是一个或多个Spout Task/Bolt Task）发射过来的Tuple数据，Outgoing Queue用来存放将要传输到下游Bolt Task的队列。如果该Bolt Task是数据流处理方向上最后一个组件，而且对应execute()方法没有再进行emit()创建的Tupe数据，那么该Bolt Task就没有Outgoing Queue这个队列。在一个Executor中，一个Bolt Task用来衔接上游（Spout Task/Bolt Task）和下游（Bolt Task）的组件，在该Bolt Task所在的Executor内其相关组件的执行流程，如下图所示： 上图所描述的数据流处理流程，如下所示： Spout Task/Bolt Task将Tupe传输到下游该Bolt Task所在的Executor的Incoming Queue中 Main Thread从该Executor的Incoming Queue中取出Tuple，并将Tupe发送给Bolt Task去处理 Bolt Task执行execute()方法中的逻辑处理该Tuple数据，并生成新的Tuple，然后调用emit()方法将Tuple发送给下一个Bolt Task处理（这里，实际上是Main Thread将新生成的Tuple放入到该Executor的Outgoing Queue中） 属于该Executor的Send Thread从Outgoing Queue中读取Tuple，并传输到下游的一个或多个Bolt Task去处理 同一Worker内2个Spout Task/Bolt Task之间传输tuple在同一个Worker JVM实例内部，可能创建多个Executor实例，那么我们了解一下，一个Tuple是如何在两个Task之间传输的，可能存在4种情况，在同一个Executor中的情况有如下2种： 1个Spout Task和1个Bolt Task在同一个Executor中 2个Bolt Task在同一个Executor中 我们后面会对类似这种情况详细说明，下面给出的是，2个不同的Executor中Task运行的情况，分别如下图所示： 1个Spout Task和1个Bolt Task在不同的2个Executor中 2个Bolt Task在不同的2个Executor中 通过前面了解一个Spout Task和一个Bolt Task运行的过程，对上面两种情况便很好理解，不再累述。 不同Worker内2个Executor之间传输tuple如果是在不同的Worker进程内，也就是在两个隔离的JVM实例上，无论是否在同一个Supervisor节点上，Tuple的传输的逻辑是统一的。这里，以一个Spout Task和一个Bolt Task分别运行在两个Worker进程内部为例，如下图所示： 处理流程和前面的类似，只不过如果两个Worker进程分别在两个Supervisor节点上，这里Transfer Thread传输Tuple走的是网络，而不是本地。 Tuple在Task之间路由过程下面，我们关心每一个Tuple是如何在各个Bolt的各个Task之间传输，如何将一个Tuple路由（Routing）到下游Bolt的多个Task呢？这里，我们应该了解一下，作为在Task之间传输的消息的表示形式，定义TaskMessage类的代码，如下所示：123456789101112131415161718192021222324252627282930313233343536package backtype.storm.messaging;import java.nio.ByteBuffer;public class TaskMessage &#123; private int _task;//编号 private byte[] _message; public TaskMessage(int task, byte[] message) &#123; _task = task; _message = message; &#125; public int task() &#123; return _task; &#125; public byte[] message() &#123; return _message; &#125; public ByteBuffer serialize() &#123; ByteBuffer bb = ByteBuffer.allocate(_message.length + 2); bb.putShort((short) _task); bb.put(_message); return bb; &#125; public void deserialize(ByteBuffer packet) &#123; if (packet == null) return; _task = packet.getShort(); _message = new byte[packet.limit() - 2]; packet.get(_message); &#125;&#125; 可见，每一个Task都给定一个Topology内部唯一的编号，就能够将任意一个Tuple正确地路由到下游需要处理该Tuple的Bolt Task。 假设，存在一个Topology，包含3个Bolt，分别为Bolt1、Bolt2、Bolt3，他们之间的关系也是按照编号的顺序设置的，其中Bolt1有个2个Task，Bolt2有2个Task，Bolt3有2个Task，这里我们只关心Bolt1 Task到Bolt2 Task之间的数据流。具体的路由过程，如下图所示： 上图中，Bolt2的两个Task分布到两个Worker进程之内，所以，当上游的Bolt1的2个Task处理完输入的Tuple并生成新的Tuple后，会有根据Bolt2的Task的编号，分别进行如下处理： Bolt2 Task4分布在第一个Worker进程内，则Bolt1生成的新的Tupe直接由该Executor的Send Thread，放到第一个Worker内部的另一个Executor的Incoming Queue Bolt2 Task5分布在第二个Worker进程内，则Bolt1生成的新的Tupe被Executor的Send Thread放到该Executor所在的第一个Worker的Outgoing Queue中，由第一个Worker的Transfer Thread发送到另一个Worker内（最终路由给Bolt2 Task5去处理） 通过上面处理流程可以看出，每个Executor应该维护Task与所在的Executor之间的关系，这样才能正确地将Tuple传输到目的Bolt Task进行处理。","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"storm内部通信机制1","date":"2017-04-16T04:47:25.309Z","path":"2017/04/16/bigdata/storm/storm内部通信机制1/","text":"1.storm内部各组件之间的关系 一个物理机（Supervisor）上可以启多个Worker JVM 一个Worker JVM可以有多个Executor 一个Executor可以有多个task（SpoutTask、BoltTask） 2.内部通信机制(一个worker内，一个Executor) 对worker来说： 对于worker进程来说，为了管理流入和传出的消息，每个worker进程有一个独立的接收线程，对配置的TCP端口supervisor.slots.ports进行监听一个worker进程运行一个专用的接收线程来负责将外部发送过来的消息移动到对应的executor线程的incoming-queue中 对应Worker接收线程，每个worker存在一个独立的发送线程，它负责从worker的transfer-queue中读取消息，并通过网络发送给其他worker transfer-queue的大小由参数topology.transfer.buffer.size来设置。transfer-queue的每个元素实际上代表一个tuple的集合 每个worker进程控制一个或多个executor线程，用户可在代码中进行配置。其实就是我们在代码中设置的并发度个数 对executor来说： 每个executor有自己的incoming-queue和outgoing-queue，Worker接收线程将收到的消息通过task编号传递给对应的executor(一个或多个)的incoming-queues; executor的incoming-queue的大小用户可以自定义配置 executor的outgoing-queue的大小用户可以自定义配置 每个executor有单独的线程分别来处理spout/bolt的业务逻辑，业务逻辑输出的中间数据会存放在outgoing-queue中，当executor的outgoing-queue中的tuple达到一定的阀值，executor的发送线程将批量获取outgoing-queue中的tuple,并发送到transfer-queue中。 3.内部通信机制(一个worker内，多个Executor)","tags":[{"name":"storm","slug":"storm","permalink":"http://yoursite.com/tags/storm/"}]},{"title":"sqoop的数据导入导出实例","date":"2017-04-16T04:47:25.306Z","path":"2017/04/16/bigdata/sqoop/sqoop的数据导入导出实例/","text":"1.Sqoop的数据导入&emsp;“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） 1.1.语法1$ sqoop import (generic-args) (import-args) 1.2.实例1.2.1.表数据在mysql中有一个库userdb中三个表：emp, emp_add和emp_contact表emp: id name deg salary dept 1201 gopal manager 50,000 TP 1202 manisha Proof reader 50,000 TP 1203 khalil php dev 30,000 AC 1204 prasanth php dev 30,000 AC 1205 kranthi admin 20,000 TP 表emp_add:地址 id hno street city 1201 288A vgiri jublee 1202 108I aoc sec-bad 1203 144Z pgutta hyd 1204 78B old city sec-bad 1205 720X hitec sec-bad 表emp_conn:联系表 id phno email 1201 2356742 gopal@tp.com 1202 1661663 manisha@tp.com 1203 8887776 khalil@ac.com 1204 9988774 prasanth@ac.com 1205 1231231 kranthi@tp.com 1.2.2.导入表表数据到HDFS下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。1234567891011121314151617$bin/sqoop import \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--table emp \\--m 1 #查看$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-000001201, gopal, manager, 50000, TP1202, manisha, preader, 50000, TP1203, kalil, php dev, 30000, AC1204, prasanth, php dev, 30000, AC1205, kranthi, admin, 20000, TP 1.2.3.导入关系表到HIVE其实就是：首先导入到hdfs，然后从hdfs中load到hive中1bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1 1.2.4.导入到HDFS指定目录&emsp;在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。以下是指定目标目录选项的Sqoop导入命令的语法。 1--target-dir &lt;new or exist directory in HDFS&gt; 下面的命令是用来导入emp_add表数据到’/queryresult’目录。123456789101112131415bin/sqoop import \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--target-dir /queryresult \\--table emp --m 1#查看 $HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*1201, 288A, vgiri, jublee1202, 108I, aoc, sec-bad1203, 144Z, pgutta, hyd1204, 78B, oldcity, sec-bad1205, 720C, hitech, sec-bad 1.2.5.导入表数据子集&emsp;我们可以导入表的使用Sqoop导入工具，”where”子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。where子句的语法如下。1--where &lt;condition&gt; 下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad1234567bin/sqoop import \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--where &quot;city =&apos;sec-bad&apos;&quot; \\--target-dir /wherequery \\--table emp_add --m 1 按需导入：query123456789bin/sqoop import \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--target-dir /wherequery2 \\ --query &apos;select id,name,deg from emp WHERE id&gt;1207 and $CONDITIONS&apos; \\ #query和$CONDITIONS是固定的格式 --split-by id \\--fields-terminated-by &apos;\\t&apos; \\--m 1 1.2.6.增量导入&emsp;增量导入是仅导入新添加的表中的行的技术。它需要添加‘incremental’, ‘check-column’, 和 ‘last-value’选项来执行增量导入。下面的语法用于Sqoop导入命令增量选项。123--incremental &lt;mode&gt;--check-column &lt;column name&gt;--last value &lt;last check column value&gt; 假设新添加的数据转换成emp表如下：1206, satish p, grp des, 20000, GR下面的命令用于在EMP表执行增量导入。12345678bin/sqoop import \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--table emp --m 1 \\--incremental append \\ #增量--check-column id \\ #检查ID的值为1208--last-value 1208 2.Sqoop的数据导出&emsp;将数据从HDFS导出到RDBMS数据库，导出前，目标表必须存在于目标数据库中。 默认操作是从将文件中的数据使用INSERT语句插入到表中 更新模式下，是生成UPDATE语句更新表数据 2.1.语法1$ sqoop export (generic-args) (export-args) 2.2.示例 数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下： 1234561201, gopal, manager, 50000, TP1202, manisha, preader, 50000, TP1203, kalil, php dev, 30000, AC1204, prasanth, php dev, 30000, AC1205, kranthi, admin, 20000, TP1206, satish p, grp des, 20000, GR 1、首先需要手动创建mysql中的目标表12345678$ mysqlmysql&gt; USE db;mysql&gt; CREATE TABLE employee ( id INT NOT NULL PRIMARY KEY, name VARCHAR(20), deg VARCHAR(20), salary INT, dept VARCHAR(10)); 2、然后执行导出命令 123456bin/sqoop export \\--connect jdbc:mysql://hdp-node-01:3306/test \\--username root \\--password root \\--table employee \\--export-dir /user/hadoop/emp/ 3、验证表mysql命令行。 12345678910111213mysql&gt;select * from employee;如果给定的数据存储成功，那么可以找到数据在如下的employee表。+------+--------------+-------------+-------------------+--------+| Id | Name | Designation | Salary | Dept |+------+--------------+-------------+-------------------+--------+| 1201 | gopal | manager | 50000 | TP || 1202 | manisha | preader | 50000 | TP || 1203 | kalil | php dev | 30000 | AC || 1204 | prasanth | php dev | 30000 | AC || 1205 | kranthi | admin | 20000 | TP || 1206 | satish p | grp des | 20000 | GR |+------+--------------+-------------+-------------------+--------+ 3.Sqoop作业(job)&emsp;Sqoop作业——将事先定义好的数据导入导出任务按照指定流程运行 3.1.语法12345$ sqoop job (generic-args) (job-args) [-- [subtool-name] (subtool-args)] $ sqoop-job (generic-args) (job-args) [-- [subtool-name] (subtool-args)] 3.2.创建作业(–create) 在这里，我们创建一个名为myjob，这可以从RDBMS表的数据导入到HDFS作业。 12bin/sqoop job --create myimportjob -- import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --m 1#该命令创建了一个从db库的employee表导入到HDFS文件的作业。 3.3.验证作业 (–list)1234567891011121314151617181920‘--list’ 参数是用来验证保存的作业。下面的命令用来验证保存Sqoop作业的列表。$ sqoop job --list它显示了保存作业列表。Available jobs: myimportjob检查作业(--show)‘--show’ 参数用于检查或验证特定的工作，及其详细信息。以下命令和样本输出用来验证一个名为myjob的作业。$ sqoop job --show myjob它显示了工具和它们的选择，这是使用在myjob中作业情况。Job: myjobTool: import Options:----------------------------direct.import = truecodegen.input.delimiters.record = 0hdfs.append.dir = falsedb.table = employee...incremental.last.value = 1206... 3.4.执行作业 (–exec)&emsp;‘–exec’ 选项用于执行保存的作业。下面的命令用于执行保存的作业称为myjob。1234$ sqoop job --exec myjob它会显示下面的输出。10/08/19 13:08:45 INFO tool.CodeGenTool: Beginning code generation...","tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/tags/sqoop/"}]},{"title":"sqoop常见的导入导出参数说明","date":"2017-04-16T04:47:25.305Z","path":"2017/04/16/bigdata/sqoop/sqoop常见的导入导出参数说明/","text":"导入到HDFS1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950sqoop import --connectjdbc:mysql://192.168.81.176/hivemeta2db --username root --password passwd --table sds/*可以通过--m设置并行数据，即map的数据，决定文件的个数。默认目录是/user/$&#123;user.name&#125;/$&#123;tablename&#125;，可以通过--target-dir设置hdfs上的目标目录。*/#如果想要将整个数据库中的表全部导入到hdfs上，可以使用import-all-tables命令sqoop import-all-tables --connect jdbc:mysql://192.168.81.176/hivemeta2db --username root --password passwd#如果想要指定所需的列，使用如下：sqoop import --connect jdbc:mysql://192.168.81.176/hivemeta2db --username root -password passwd --table sds --columns &quot;SD_ID,CD_ID,LOCATION&quot; #指定需要导出的列#导入文本时可以指定分隔符：sqoop import --connect jdbc:mysql://192.168.81.176/hivemeta2db --username root -password passwd --table sds --fields-terminated-by &apos;\\t&apos; --lines-terminated-by &apos;\\n&apos; --optionally-enclosed-by &apos;\\&quot;&apos;#可以指定过滤条件：sqoop import --connect jdbc:mysql://192.168.81.176/hivemeta2db --username root --password passwd --table sds --where &quot;sd_id &gt; 100&quot; 导入到hive 参数 说明 –hive-home Hive的安装目录，可以通过该参数覆盖掉默认的hive目录 –hive-overwrite 覆盖掉在hive表中已经存在的数据 –create-hive-table 默认是false,如果目标表已经存在了，那么创建任务会失败 –hive-table 后面接要创建的hive表 –table 指定关系数据库表名 12345678sqoop create-hive-table --connect jdbc:mysql://192.168.81.176/sqoop --username root --password passwd --table sds --hive-table sds_bak#默认sds_bak是在default数据库的 导出export选项： 12345678910sqoop export--connect jdbc:mysql://192.168.81.176/sqoop--username root--password passwd--table sds--export-dir /user/guojian/sds /*上例中sqoop数据中的sds表需要先把表结构创建出来，否则export操作会直接失败*/ &emsp;将hdfs上的数据导入到关系数据库中 参数 说明 –direct 工具导入mysqlmysqlimport直接使用 –export-dir 需要export的hdfs数据路径 -m,–num-mappers 并行export的map个数n –table 导出到的目标表 –call 调用存储过程 –update-key 指定需要更新的列名，可以将数据库中已经存在的数据进行更新 –update-mode allowinsert (默认）和updateonly更新模式，包括","tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/tags/sqoop/"}]},{"title":"sqoop介绍及安装","date":"2017-04-16T04:47:25.304Z","path":"2017/04/16/bigdata/sqoop/sqoop介绍及安装/","text":"1.概述sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。 导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统； 导出数据：从Hadoop的文件系统中导出数据到关系数据库 2.工作机制&emsp;将导入或导出命令翻译成mapreduce程序来实现，在翻译出的mapreduce中主要是对inputformat和outputformat进行定制 3.sqoop安装123456789101112131415161718192021222324&apos;解压&apos;tar -zxvf sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz &apos;修改配置文件&apos;$ cd $SQOOP_HOME/conf$ mv sqoop-env-template.sh sqoop-env.sh打开sqoop-env.sh并编辑下面几行：export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/export HIVE_HOME=/home/hadoop/apps/hive-1.2.1&apos;加入mysql的jdbc驱动包&apos;cp ~/app/hive/lib/mysql-connector-java-5.1.28.jar $SQOOP_HOME/lib/&apos;验证启动&apos;$ cd $SQOOP_HOME/bin$ sqoop-version预期的输出：15/12/17 14:52:32 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6Sqoop 1.4.6 git commit id 5b34accaca7de251fc91161733f906af2eddbe83Compiled by abe on Fri Aug 1 11:19:26 PDT 2015到这里，整个Sqoop安装工作完成。","tags":[{"name":"sqoop","slug":"sqoop","permalink":"http://yoursite.com/tags/sqoop/"}]},{"title":"第一个wordcount的local运行测试","date":"2017-04-16T04:47:25.301Z","path":"2017/04/16/bigdata/spark内核解密/第一个WordCount的local运行测试/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package org.dt.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCount &#123; def main(args: Array[String]): Unit = &#123; /** * 之所以要这样写,因为在Windows本地执行的时候,会有null指针异常, * 网上说的解决方法:下载winutils.exe,然后将其放在一个目录下,如:C:\\Users\\Administrator\\Desktop\\hadoop\\bin\\winutils.exe * 然后指定bin\\winutils.exe所在的目录,如下 */ System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) /** * 1.创建spark的配置对象SparkConf,设置Spark程序的运行时的配置信息 * 例如:通过setMaster来设置程序要连接的spark集群的Master的URL, * 如果设置为local,则代表spark程序在本地运行 */ val conf = new SparkConf() //创建sparkConf对象 conf.setAppName(&quot;helloSpark&quot;) //设置应用程序的名称,在程序运行的监控界面可以看到名称 conf.setMaster(&quot;local&quot;) //此时程序在本地运行,不需要安装spark集群 /** * 2.创建SparkContext对象 * sparkContext是spark程序所有功能的唯一入口,无论是采用scala,java,python,R等都必须有一个sparkContext实例 * SparkContext的核心作用,初始化spark应用程序运行所需要的核心组件,包括:DAGScheduler,TaskScheduler,SchedulerBankend * 同时还会负责Spark程序往Master注册程序等 * Sparkcontext是整个Spark应用程序中最为至关重要的一个对象 */ val sc = new SparkContext(conf) //创建SparkContext对象,通过传入Sparkconf实例来定制Spark运行的具体参数和配置信息 /** * 3.根据具体的数据来源(HDFS,HBase,Local FS, DB,S3等),通过sparkContext来创建ＲＤＤ * RDD的创建有三种方式: * 根据外部的数据来源(例如HDFS) * 根据scala集合 * 根据其他的RDD操作产生 * 数据会被RDD划分称为一系列的Partitions,分配到每个Partition的数据属于一个Task的处理范畴 */ val rdd = sc.textFile(&quot;C://Users//Administrator//Desktop//test.txt&quot;, 1) /** * 4.对初始的RDD进行Transformation级别的处理,例如:Map,filter等高阶函数的编程 * 进行具体的数据计算 */ val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2) rdd2.coalesce(1).saveAsTextFile(&quot;C://Users//Administrator//Desktop//test_re2223.txt&quot;) sc.stop() &#125;&#125;/*为什么不能直接在IDEA中直接发布spark程序到spark集群中?如果在IDEA中提交程序的话,那么IDEA机器就相当于Driver,此时的IDEA的内存就必须非常的强大Driver要指挥集群中worker的运行,如果IDEA和spark的集群在不同的网络环境中,运行就会出现缓慢,可能出现任务丢失同时也是不安全的,因为外部可以进入集群一般实际情况下:会有专门的会和spark集群在同一个网络环境下的一台机器,远程连上这台机器,在上面做开发和提交程序这台机器的配置和集群的worker的配置基本上是一致的 */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"wordcount的运行原理及源码解读","date":"2017-04-16T04:47:25.300Z","path":"2017/04/16/bigdata/spark内核解密/wordcount的运行原理及源码解读/","text":"下面是分析wordcount过程中产生的RDD及stage 1234567891011121314151617181920package org.dt.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object WordCountOrdered &#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val conf = new SparkConf() //创建sparkConf对象 conf.setAppName(&quot;helloSpark&quot;) //设置应用程序的名称,在程序运行的监控界面可以看到名称 conf.setMaster(&quot;local&quot;) //此时程序在本地运行,不需要安装spark集群 val sc = new SparkContext(conf) //创建SparkContext对象,通过传入Sparkconf实例来定制Spark运行的具体参数和配置信息 val rdd = sc.textFile(&quot;C://Users//Administrator//Desktop//test.txt&quot;, 1) val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2) rdd2.saveAsTextFile(&quot;C://Users//Administrator//Desktop//test_result&quot;) sc.stop() &#125;&#125; 一个很大的文件存在HDFS上,1T吧,然后要对这个文件进行wordcount,现在有四台机器,那么会将数据发送到四台机器,具体每台机器多少数据,看具体的情况","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark高级排序","date":"2017-04-16T04:47:25.299Z","path":"2017/04/16/bigdata/spark内核解密/spark高级排序/","text":"基础排序算法123456789101112131415161718val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\test.txt&quot;)val result = rdd.flatMap(_.split(&quot; &quot;)) .map((_,1)).reduceByKey(_+_) .map(pair=&gt;(pair._2,pair._1)) .sortByKey(false) //默认是升序的,false是降序 .map(pair=&gt;(pair._2,pair._1))result.collect.foreach(println)/*执行结果:(chen,320)(zhangsan,160)(cheng,160)(zlisi,160)(chengyansong,131)(chenyansong,131)(chengyansongchenyansong,29)(,1) */ 二次排序算法所谓二次排序就是排序的时候考虑2个维度(如指定2个排序列,当第一列的相同,比较第二列) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556package org.dt.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by Administrator on 2017/3/24. */class SecondarySortKey(val first:Int, val second: Int ) extends Ordered[SecondarySortKey] with Serializable&#123; def compare(other: SecondarySortKey): Int = &#123; if(this.first - other.first !=0)&#123; this.first - other.first &#125;else&#123; this.second - other.second &#125; &#125;&#125;object SecondarySortKey&#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) val line = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;) /*xx.txt 1 11 2 22 3 33 2 11 1 22 */ val pairWithSortKey = line.map(line=&gt;&#123; val arr = line.split(&quot; &quot;) val first = arr(0).toInt val second = arr(1).toInt (new SecondarySortKey(first,second), line) //指定Tuple的key为SecondarySortKey &#125;).sortByKey(false).map(pair=&gt;pair._2)//sortByKey排序的时候会以SecondarySortKey为key排序 pairWithSortKey.collect.foreach(println) /* 打印结果: 3 33 2 22 2 11 1 22 1 11 */ &#125; //在实际的生成中,我们是封装函数来进行逻辑的组织 def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道 val sc = new SparkContext(conf) sc &#125;&#125; Top N123456789101112131415161718192021222324 val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)/*6633333355222347722*/ val pairs = rdd.map(line=&gt;(line.toInt,line))//生成key-value方便sortByKey进行排序(为了方便排序,我们添加了key),Int已经实现了比较排序的接口,所以我们不用自己写 val rddSortedArr = pairs.sortByKey(false).map(_._1).take(5) rddSortedArr.foreach(println) /* 打印结果: 355 333 234 77 66 */ 分组排序找出不同的类型的每组中的top N 12345678910111213141516171819202122232425val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)val lines=rdd.map&#123; line =&gt; (line.split(&quot; &quot;)(0),line.split(&quot; &quot;)(1).toInt) &#125;//分组val groups=lines.groupByKey()//组内进行排序val groupsSort=groups.map(tu=&gt;&#123; val key=tu._1 val values=tu._2 val sortValues=values.toList.sortWith(_&gt;_).take(4)//取top 4 (key,sortValues)&#125;)//组与组之间进行排序groupsSort.sortBy(tu=&gt;tu._1, false, 1).collect.foreach(value=&gt;&#123; print(value._1) value._2.foreach(v=&gt;print(&quot;\\t&quot;+v)) println()&#125;)/*打印结果:spark 100 99 94 88hadoop 88 56 35 33 */ 排序算法详解RangePartitioner主要是依赖的RDD的数据划分成不同的范围,关键的地方是不同的范围是有序的,即Partition之间是有序的,但是在Partition内部是不保证有序的,和HashPartition相比(会存在数据倾斜的问题),RangePartitioner尽量保证每个Partition中的数据量是均匀的 乘3的目的保证数据量特别小的分区能够抽取到足够的数据,同时保证数据量特别大的分区能够二次采样1234567val (numItems, sketched) = RangePartitioner.sketch(rdd.map(_._1), sampleSizePerPartition) //对Tuple的key进行采样:_._1def sketch[K : ClassTag](rdd: RDD[K],sampleSizePerPartition: Int): (Long, Array[(Int, Long, Array[K])])//返回(Long, Array[(Int, Long, Array[K])]);其中Int为分区的编号,long为分区中总元素有多少个;Array[K]从父RDD中每次分区中采样到的数据(reservoir, l) //L为一个分区中的数据总和 水塘抽样:","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的提交命令在shell中组织","date":"2017-04-16T04:47:25.297Z","path":"2017/04/16/bigdata/spark内核解密/spark的提交的命令写在一个shell中/","text":"1234567891011121314151617/*/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\--class cn.itcast.spark.WordCount \\--master spark://node1.itcast.cn:7077 \\--executor-memory 2G \\--total-executor-cores 4 \\/root/spark-mvn-1.0-SNAPSHOT.jar \\hdfs://node1.itcast.cn:9000/words.txt \\hdfs://node1.itcast.cn:9000/out其中:--class cn.itcast.spark.WordCount \\ //是启动类/root/spark-mvn-1.0-SNAPSHOT.jar \\ //jar包hdfs://node1.itcast.cn:9000/words.txt \\ //输入参数1 ,要读取的文件hdfs://node1.itcast.cn:9000/out //输入参数2 , 内容输出的文件*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的RDD详解","date":"2017-04-16T04:47:25.296Z","path":"2017/04/16/bigdata/spark内核解密/spark的RDD详解/","text":"无论是基于工作集还是数据集都有:位置感知,容错,负载均衡的特点基于数据集的处理:工作方式:从物理存储上加载数据,然后操作数据,然后写入物理存储设备(如:hadoop的mapreduce), 基于数据集的操作不适应的场景:1.不适应大量的迭代2.不适合于交互式查询3.基于数据流的方式不能够复用中间的计算结果(1.对于相同的查询,需要进行重复的计算,2.对于有相同的查询步骤的计算,其相同步骤的结果不能复用)4.spark的RDD是基于工作集 RDD:Resilient Distributed Dataset (弹性分布式数据集)弹性之一:自动的进行内存和磁盘数据存储的切换弹性之二:基于Lineage的高效容错(依赖关系实现)弹性之三:Task如果失败会自动进行特定次数的重试弹性之四:Stage如果失败会自动进行特定次数的重试,而且只会计算失败的分片弹性之五:checkpoint和persist弹性之六:数据调度弹性,DAG Task和资源管理无关弹性之七:数据分片的高度弹性,(当我们的Partition比较小的时候,如果每个Partition都占用一个线程去处理,那么会占用很多的资源,此时将多个Partition合并成一个相对较大的Partition,这样来提高效率;另外一个方面,如果内存不是那么多,但是每个Partition比较的大,即数据的block比较的大,这个时候可能考虑把他变成更小的分片,这样让spark有更多的处理批次,但是不会出现OOM),所以降低并行度和提高并行度是高度弹性的一个表现(重新分片的时候,例如将100万的PartitionsShuffle成10万个Partitions,此时不要使用reparation,因为reparation使用了Shuffle=true,可以使用coalesce,coalesce内部默认Shuffle=false) 因为对RDD的操作是只读的,那么会在stage中产生很多的中间结果,那么怎么办,解决的办法是不让其产生中间结果,即使用lazy的模式,只不过产生了一个操作的标记(因为在源码中可以看到一个RDD产生另一个RDD的操作的过程实际上是将父RDD传给了子RDD,如下)1234567891011val rdd = sc.textFile(&quot;xx.txt&quot;)val rdd2 = rdd.flatMap(_.split(&quot; &quot;))/*从rdd到rdd2的过程中,是flatMap返回的一个新的rdd,而flatMap源码实现是:*/def flatMap[](f:xx)&#123; //... new MapPartitionRDD[T](this,(context,pid,iter)=&gt;iter.flatMap(cleanF))&#125;//可以知道只是将this传给了子rdd,也就是将rdd作为this传给了子rdd2,从而确定了两个rdd的关系链,而在一个stage中是lazy的模式,所以spark并不是立即进行计算,即不会在一个stage中产生中间的结果 综上:spark在每次产生新的RDD的时候,都是将父RDD的作为this传到子RDD中,所以就构成了一个链条,在最后又action的时候才会触发,如下的例子:123456z=2;y=z+4;x=y+3;f(x)=x+2;//现在要计算f(x),那么就会从后向前追溯x的值,在追溯y的值,最后追溯z的值 常规的容错方式:1.数据检查点2.记录数据的更新(每次数据变化的时候就记录一下,存在的问题:复杂;耗性能) spark采用的是数据更新的方式,那么RDD通过记录数据更新的方式为何是高效的呢?1.RDD是不可变的,并且是lazy2.RDD的写操作是粗粒度的,但是RDD的读操作既可以是粗粒度的,也可以是细粒度的 RDD的创建过程创建RDD的几种方式1.使用程序中的集合创建RDD:本地测试2.使用本地文件系统创建RDD:测试大量数据的文件3.使用HDFS创建RDD:生产环境最常用的创建方式4.基于DB创建RDD5.基于NoSql创建RDD,例如HBase6.基于S3创建RDD7.基于数据流创建RDD 集合创建RDD的源码实现过程:12345678910111213141516171819202122232425val sc = new SparkContext(conf)val numbers = 1 to 100val rdd = sc.parallelize(numbers)val sum = rdd.reduce(_+_)//parallelize方法def parallelize[T: ClassTag](seq: Seq[T], numSlices: Int = defaultParallelism): RDD[T] = withScope &#123; //.. new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())&#125;//ParallelCollectionRDD 类中有如下的方法:override def getPartitions: Array[Partition] = &#123; val slices = ParallelCollectionRDD.slice(data, numSlices).toArray slices.indices.map(i =&gt; new ParallelCollectionPartition(id, i, slices(i))).toArray //因为slice返回的是Seq[Seq[T]],所以toArray之后返回的是Array[Seq[T]],即对数据集进行了分片&#125;def slice[T: ClassTag](seq: Seq[T], numSlices: Int): Seq[Seq[T]]//计算每一个分片override def compute(s: Partition, context: TaskContext): Iterator[T] = &#123; new InterruptibleIterator(context, s.asInstanceOf[ParallelCollectionPartition[T]].iterator)&#125; 1234567891011rdd.collect.foreach(pair=&gt;print(pair))collect是将结果收集到Driver,然后变成Array,所以可以使用foreach去遍历def collect(): Array[T] = withScope &#123; val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*) //返回的是一个Array&#125;在实际生产中,不要使用collect,因为数据量将非常的大 RDD执行手动绘图 常见的RDD使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135package org.dt.sparkimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object MakeRDD &#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) //main方法中的调用的每一个功能都必须是模块化的,而每一个调用的模块必须使用函数来封装 //reduceByKeyTransformation(sc) joinTransformation(sc) sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源 &#125; //在实际的生成中,我们是封装函数来进行逻辑的组织 def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道 val sc = new SparkContext(conf) sc &#125; //map def mapTransformation(sc:SparkContext)&#123; val nums = sc.parallelize(1 to 10) //map遍历集合的每一个元素x,对每一个元素都作用指定的函数f(x),将f(x)作为集合新的元素返回 val mapped = nums.map(_*2) mapped.collect.foreach(println) //收集计算结果并通过foreach循环打印 &#125; //filter def filterTransformation(sc:SparkContext)&#123; val nums = sc.parallelize(1 to 10) //遍历集合的每一个元素x,满足f(x)==true,就留下x,则集合中最终留下的都是满足f(x)==true的元素 val filtered = nums.filter(_%2==0) // filter的源码实现: def filter(f: T =&gt; Boolean): RDD[T] filtered.collect.foreach(println) &#125; //flatMap def flatMapTransformation(sc:SparkContext): Unit =&#123; val bigData = Array(&quot;scala cccc&quot;, &quot;java jjjjj&quot;, &quot;spark sss&quot;) val bigDataRDD = sc.parallelize(bigData) //返回ParallelCollectionRDD[String] //遍历集合的每一个元素x,首先返回Array[f(x1)],Array[f(x2)],..然后对所有的Array[f(x)]进行flat,最终返回Array[f(x1),f(x2)...] val flatMaped = bigDataRDD.flatMap(_.split(&quot; &quot;))//对每个元素,split切分的结果是一个Array,所以每一个元素都会产生一个Array,最后要对每个Array进行flat,产生一个大的集合 flatMaped.collect.foreach(println) &#125; //groupByKey def groupByKeyTransformation(sc:SparkContext): Unit =&#123; val data = Array((11,&quot;zhangsan&quot;), (22,&quot;zhangsan22&quot;),(22,&quot;zhangsan2222222&quot;), (33,&quot;zhangsan33&quot;), (44,&quot;zhangsan44&quot;)) val groupByKeyRdd = sc.parallelize(data) //将Tuple的第一个元素作为key,然后返回:RDD[(K, Iterable[V]) ----按照相同的key对value进行分组 val groupByKeyed = groupByKeyRdd.groupByKey() //返回:def groupByKey(): RDD[(K, Iterable[V])] 注意返回的集合中的元素是:(K, Iterable[V]) groupByKeyed.collect.foreach(println) /* 打印结果: (22,CompactBuffer(zhangsan22, zhangsan2222222)) (11,CompactBuffer(zhangsan)) (33,CompactBuffer(zhangsan33)) (44,CompactBuffer(zhangsan44)) */ &#125; //reduceByKey def reduceByKeyTransformation(sc: SparkContext): Unit =&#123; val data = Array((11,&quot;zhangsan&quot;), (22,&quot;zhangsan22&quot;),(22,&quot;zhangsan2222222&quot;), (33,&quot;zhangsan33&quot;), (44,&quot;zhangsan44&quot;)) val reduceByKeyRdd = sc.parallelize(data) //例如集合中的元素是:(Int,String),则reduceByKey之后,集合中的元素还是(Int,String),只不过是对相同key的String进行处理,如下面是将相同key的所有字符串进行concat拼接,返回的还是String val reduceByKeyed = reduceByKeyRdd.reduceByKey(_.concat(&quot;:&quot;).concat(_))//返回:def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)] reduceByKeyed.collect.foreach(println) /* 打印结果: (22,zhangsan22:zhangsan2222222) (11,zhangsan) (33,zhangsan33) (44,zhangsan44) */ &#125; //join:将两个集合中根据key进行内容的连接 def joinTransformation(sc: SparkContext): Unit =&#123; //大数据中最重要的算子 val studentName = Array((1,&quot;zhangsan&quot;), (2,&quot;wangwu&quot;), (3,&quot;lisi&quot;), (3,&quot;lisi333&quot;), (4,&quot;zhaoliu&quot;)) val studentScore = Array((1,11), (2,22), (2,222222),(3,33),(7,77)) val nameRdd = sc.parallelize(studentName) val scoreRdd = sc.parallelize(studentScore) //def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))] val studentNameScore = nameRdd.join(scoreRdd) studentNameScore.collect.foreach(println) /* 打印结果: (1,(zhangsan,11)) (3,(lisi,33)) (3,(lisi333,33)) (2,(wangwu,22)) (2,(wangwu,222222)) */ &#125; //cogroup def cogroupTransformation(sc: SparkContext): Unit =&#123; val studentName = Array((1,&quot;zhangsan&quot;), (1,&quot;zhangsan1111&quot;), (2,&quot;wangwu&quot;), (3,&quot;lisi&quot;), (3,&quot;lisi333&quot;), (4,&quot;zhaoliu44&quot;)) val studentScore = Array((1,11), (2,22), (3,33), (2,2222), (3,333333),(5,5555)) val nameRdd = sc.parallelize(studentName) val scoreRdd = sc.parallelize(studentScore) //def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] //先将两个RDD groupByKey之后再join val studentNameScore = nameRdd.cogroup(scoreRdd) //返回:RDD[(K, (Iterable[V], Iterable[W])) studentNameScore.collect.foreach(println) /* 打印结果: (4,(CompactBuffer(zhaoliu44),CompactBuffer())) &lt;-----------这里有null的存在 (1,(CompactBuffer(zhangsan, zhangsan1111),CompactBuffer(11))) (3,(CompactBuffer(lisi, lisi333),CompactBuffer(33, 333333))) (5,(CompactBuffer(),CompactBuffer(5555))) &lt;-----------这里有null的存在 (2,(CompactBuffer(wangwu),CompactBuffer(22, 2222))) */ &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的job","date":"2017-04-16T04:47:25.295Z","path":"2017/04/16/bigdata/spark内核解密/spark的job/","text":"job在一个Executor中一次性最多能够运行多少并发的Task取决于当前Executor能够使用的Cores数量 如果有88个文件,而每个文件的大小小于128M,那么将会有88个Partition,所以会启动88个task,而由于数据本地性,假如有的机器上有54个文件,那么在该机器上会启动54个task 如果在进行cache时,cache的数据放置在哪台机器上,那么后续的操作会在那台机器上进行,这就是数据本地性 rdd的依赖关系 窄依赖:每个父RDD的Partition最多被子RDD的一个Partition所使用;例如map,filter宽依赖:多个子RDD的Partition会依赖同一个父RDD的Partition;例如groupByKey,reduceByKey 特别说明:对join操作有两种情况,如果说join操作的使用每个Partition仅仅和已知的Partition进行join,这次是join操作就是窄依赖 每个stage里面的Task的数量是由该stage中最后一个RDD的Partition的数量决定的从后往前推理,遇到宽依赖就断开,遇到窄依赖就把当前的RDD加入到该stage中 最后一个stage里面的任务类型是ResultTask,前面其他所有的Stage里面的任务的类型为ShuffleMapTask hadoop中的MapReduce中的Mapper和Reducer在spark中等量的算子是:map和reduceByKey","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核架构","date":"2017-04-16T04:47:25.293Z","path":"2017/04/16/bigdata/spark内核解密/spark内核架构/","text":"Driver部分的代码:Sparkconf+SparkContext (其实就是new SparkContext的过程) 接下来创建RDD (会在Executor中执行的代码,会在worker上的Executor上处理的逻辑) worker管理当前node的资源,并接受Master的指令来分配具体的计算资源Executor(在新的进程中分配) Worker不会向Master汇报自身的资源情况,发送心跳的时候只是发送workID Job:一般每个action会触发一个作业, spark的快不是基于内存, 依赖:有一个情况是range级别的依赖,不会因为数据规模的增大,而改变依赖的个数(即父rdd的个数) stage内部计算逻辑,完全一样,只是计算的数据不同罢了 启动spark:sbin/start-all.sh启动历史命令,这样在程序运行完成之后依然可以看到log:sbin/start-history-server.shjps —-&gt;HistoryServerUI——-&gt;http://master:18080 内核架构图:","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark RDD的持久化,广播,累加器","date":"2017-04-16T04:47:25.292Z","path":"2017/04/16/bigdata/spark内核解密/spark RDD的持久化,广播,累加器/","text":"常见的Action凡是Action级别的操作都会触发:sc.runJob reduce123456789val numbers = sc.parallelize(1 to 100)numbers.reduce(_+_) //会将上一次计算的结果作为下一次的第一个参数def reduce(f: (T, T) =&gt; T): T = withScope &#123; //.... sc.runJob(this, reducePartition, mergeResult) jobResult.getOrElse(throw new UnsupportedOperationException(&quot;empty collection&quot;))&#125; collect 1234567numbers.collect def collect(): Array[T] = withScope &#123; val results = sc.runJob(this, (iter: Iterator[T]) =&gt; iter.toArray) Array.concat(results: _*)&#125; count 1234567val numbers = sc.parallelize(1 to 100)numbers.count /*//Return the number of elements in the RDD.def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum*/ take:取结果的一批元素12345678910111213141516171819202122232425val numbers = sc.parallelize(1 to 10)val arr = numbers.map(_*2).take(5)arr.foreach(println)/*打印结果:246810 */源码: def take(num: Int): Array[T] = withScope &#123; if (num == 0) &#123; new Array[T](0) &#125; else &#123; //...... val res = sc.runJob(this, (it: Iterator[T]) =&gt; it.take(left).toArray, p) res.foreach(buf ++= _.take(num - buf.size)) partsScanned += numPartsToTry &#125; buf.toArray &#125; &#125; countByKey:统计Tuple中key的次数123456789101112131415161718val numbers = sc.parallelize(Seq(1,2,3,5,1,3,5))val rdd = numbers.map((_,1)).countByKey()rdd.foreach(println)/*打印结果:(1,2)(3,2)(5,2)(2,1)*///源码: def countByKey(): Map[K, Long] = self.withScope &#123; self.mapValues(_ =&gt; 1L).reduceByKey(_ + _).collect().toMap &#125;因为因countByKey也是进行了collect,而collect是Action,所以countByKey也是Action saveAsTextFile1在源码中有这样的一句:self.context.runJob(self, writeToFile) RDD持久化1.某步骤计算特别耗时2.计算链条特别长的情况3.checkpoint所在的RDD也一定要持久化数据(checkpoint之前persist或cache)4.Shuffle之后要进行persist(因为Shuffle要进行网络传输,如果失败,数据丢失,那么又要进行网络传输)5.Shuffle之前(框架默认帮我们把数据持久化到磁盘) 123456/** Persist this RDD with the default storage level (`MEMORY_ONLY`).默认是内存中缓存 */def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)/** Persist this RDD with the default storage level (`MEMORY_ONLY`). */def cache(): this.type = persist() 1234567891011121314151617181920212223242526272829303132几种持久化的方式object StorageLevel &#123; //不持久化 val NONE = new StorageLevel(false, false, false, false) //持久化到磁盘2分 val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2) val DISK_ONLY = new StorageLevel(true, false, false, false) //只是缓存到内存 val MEMORY_ONLY = new StorageLevel(false, true, false, true) val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2) //缓存到内存并序列化,这样存储就会小,但是反序列化的时候,耗CPU val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false) val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2) //优先考虑内存,然后再写入到磁盘,这样防止内存溢出 val MEMORY_AND_DISK = new StorageLevel(true, true, false, true) val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2) val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false) val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2) val OFF_HEAP = new StorageLevel(false, false, true, false)&#125;//上述情况可以看到,有些情况下有2分副本,为什么要有两份?因为如果一份内存崩溃掉了,那么另外一份可以立即顶上,虽然2分副本占用了空间,但这就是使用空间(2份副本)换时间 cache:是persist的一种特殊情况 12cache实际上是调用的是下面的persist:即只是缓存在内存中,并且只是缓存一份def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) 执行的效果说明:12345678910111213141516val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\test.txt&quot;)val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).cacherdd2.collect.foreach(println)下面是连续2次执行上述代码所用的时间://第一次17/03/24 14:41:55 INFO DAGScheduler: Job 0 finished: collect at MakeActionRDD.scala:30, took 1.117844 s//第二次17/03/24 14:42:46 INFO DAGScheduler: Job 0 finished: collect at MakeActionRDD.scala:30, took 0.600663 scache之后一定不能立即有其他算子val rdd2 = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).cache.count是没有对计算结果缓存 cache从内存中清除1.使用unpersist,强制从内存中去掉 Spark广播广播的应用场景:1.大变量: 每次task在执行任务的时候都要拷贝数据副本,因为函数式编程,即变量不变val,因为要将变量拷贝一份到task中,一个Executor保存一份,Executor中所有的task只读共享这个大变量广播是由Driver发给当前Application分配的所有Executor内存级别的全局只读变量,Executor中的线程池中共享该全局变量,极大的减少了网络传输,否则的话每个task都要传输一次该变量,并极大的节省了内存,当也隐式的提高了CPU的有效工作 图示广播: 代码: 1234567val number = 10val broadcastNumber = sc.broadcast(number)val dataRdd = sc.parallelize(1 to 100)val dataMap = dataRdd.map(_ * broadcastNumber.value)dataMap.collect.foreach(println) Spark累加器Accumulator:对于Executor只能修改,但不可读,只对Driver可读,在记录集群的状态,尤其是全局唯一的状态的时候很重要,即一个作业中的所有的Executor共享 123456val sum = sc.accumulator(0)//sc.accumulator(0,&quot;test_Acc&quot;) 指定Acc的名称val dataRdd = sc.parallelize(1 to 100)dataRdd.foreach(sum += _)println(sum)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"HA下的spark集群工作原理","date":"2017-04-16T04:47:25.290Z","path":"2017/04/16/bigdata/spark内核解密/HA下的spark集群工作原理/","text":"添加zk来解决master的单点问题到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：Spark集群规划：node1，node2是Master；node3，node4，node5是Worker , 安装配置zk集群，并启动zk集群, 停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置1export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&quot; 在node1节点上修改slaves配置文件内容指定worker节点 在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master 启动集群的时候,我们需要向zk要master1--master spark://node1:7077,node2:7077","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"基于Yarn的两种提交模式","date":"2017-04-16T04:47:25.287Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/基于Yarn的两种提交模式/","text":"spark的三种提交模式1.Standalone模式,基于spark自己的Master-Worker集群2.基于Yarn的yarn-cluster模式3.基于Yarn的yarn-client模式 在我们之前提交的spark应用程序的spark-submit脚本,加上–master参数,设置yarn-cluster,或yarn-client,即可,如果你没有设置,那么就是Standalone模式,同时需要在spark-env.sh中补充配置1export HADOOP_HOME=XXXXX yarn-cluster和yarn-client的提交模式","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"共享变量","date":"2017-04-16T04:47:25.286Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/共享变量/","text":"默认情况下,如果一个算子的函数中使用到了某个外部的变量,那么这个变量的值会被拷贝到每个task中,此时每个task只能操作自己的那份变量副本,如果多个task想要共享某个变量,那么这种方式是做不到的 spark为此提供了两种共享变量,一种是Broadcast Variable(广播变量),另一种是Accumulator(累加变量),Broadcast Variable会将使用到的变量,仅仅为每个节点拷贝一份,更大的作用是优化性能,减少网络传输以及内存消耗,主要用于共享读 Accumulator则可以让多个task共同操作同一份变量,主要可以进行累加操作 广播变量的使用 可以通过sparkContext的broadcast方法来针对某个变量创建广播变量,然后在算子的函数内,使用广播变量时,每个节点只会拷贝一份副本了,每个节点可以使用广播变量的value()方法获取值,记住:广播变量是只读的123456val numberRdd = sc.parallelize(Seq(1,2,3,4))val factor = 2val factorBroadcast = sc.broadcast(factor)val mapRdd = numberRdd.map(_*factorBroadcast.value)mapRdd.foreach(println) spark提供的Accumulator,主要用于多个节点对一个变量进行共享操作,Accumulator只提供了累加的操作,但是却给我们提供了多个task对一个变量并行操作的功能,但是task只能对Accumulator进行累加操作,不能读取他的值,只有Driver程序可以读取Accumulator的值 下面是Accumulator的使用12345val numberRdd = sc.parallelize(Seq(1,2,3,4))val sum = sc.accumulator(0)numberRdd.foreach(sum += _)println(sum)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"wordcount程序原理解析","date":"2017-04-16T04:47:25.284Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/wordcount程序原理解析/","text":"程序如下:1234567891011val conf = new SparkConf().setAppName(&quot;WordCount&quot;)val sc = new SparkContext(conf)val linesRdd = sc.textFile(&quot;/etc/hosts&quot;)val wordsRdd = linesRdd.flatMap(_.split(&quot; &quot;))val pairsRdd = wordRdd.map((_,1))val wordCountRdd = pairsRdd.reduceByKey(_+_)//打印wordCountRdd.foreach(println) 下面的图是通过RDD的产生流程进行解析的 下面的图是通过源码的角度进行解析的","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的排序","date":"2017-04-16T04:47:25.283Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark的排序/","text":"这里使用的是sortByKey对Tuple按照key进行排序 12345678910111213141516171819202122232425val lineRdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)val reducedRdd = lineRdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)//将排序的key转换到Tuple的key列val countWords = reducedRdd.map(count=&gt;(count._2,count._1))val sortedRdd = countWords.sortByKey(false)//重新组织val result = sortedRdd.map(sort=&gt;(sort._2,sort._1))result.foreach(println)/*执行结果:(spark,19)(hadoop,13)(88,6)(100,6)(56,6)(22,4)(33,4)(99,4)(94,2)(94spark,1) */ 二次排序也是使用sortByKey,只不过此时key为我们自定义的Bean作为key来进行排序,而Bean中有比较的方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class SecondarySortKey(val first:Int, val second: Int ) extends Ordered[SecondarySortKey] with Serializable&#123; def compare(other: SecondarySortKey): Int = &#123; if(this.first - other.first !=0)&#123; this.first - other.first &#125;else&#123; this.second - other.second &#125; &#125;&#125;object SecondarySortKey&#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) val line = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;) /*xx.txt 1 11 2 22 3 33 2 11 1 22 */ val pairWithSortKey = line.map(line=&gt;&#123; val arr = line.split(&quot; &quot;) val first = arr(0).toInt val second = arr(1).toInt (new SecondarySortKey(first,second), line) //指定Tuple的key为SecondarySortKey &#125;).sortByKey(false).map(pair=&gt;pair._2)//sortByKey排序的时候会以SecondarySortKey为key排序 pairWithSortKey.collect.foreach(println) /* 打印结果: 3 33 2 22 2 11 1 22 1 11 */ &#125; //在实际的生成中,我们是封装函数来进行逻辑的组织 def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道 val sc = new SparkContext(conf) sc &#125;&#125; 其实对基本类型spark有对基本类型的比较方法的实现,所以不用我们实现比较方法也能实现排序","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的常见的rdd","date":"2017-04-16T04:47:25.281Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark的常见的rdd/","text":"创建RDD 使用程序中的集合创建rdd主要用于进行测试,可以在实际部署到集群运行之前,自己使用集合构造册数数据,来测试后面的spark应用的流程 使用本地文件创建rdd 使用HDFS文件创建rdd主要可以针对HDFS上存储的大数据,进行离线批处理操作 使用程序中的集合创建rdd 12345678910111213141516171819202122232425object MakeRDD &#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) test(sc) sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源 &#125; def test(sc:SparkContext)=&#123; val numberRdd = sc.parallelize(Seq(1,2,3,8,22)) val reducedRdd = numberRdd.reduce(_+_) println(reducedRdd) &#125; //构建SparkContext def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) val sc = new SparkContext(conf) sc &#125;&#125; 使用本地文件创建rdd和HDFS文件创建rdd1.如果是针对本地文件的话,如果是在Windows上本地测试,Windows上有一份文件即可;如果是在spark集群上针对linux本地文件,那么需要将文件拷贝到所有worker节点上2.spark的textFile方法支持针对目录,压缩文件以及通配符进行rdd的创建3.spark默认会为HDFS的每一个block创建一个partition,但是也可以通过textFile()的第二个参数手动设置分区数量,只能比block数量多,不能比block数量少 123456789101112131415161718192021222324252627282930object MakeRDD &#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) test(sc) sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源 &#125; def test(sc:SparkContext)=&#123; val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;) val rddreuslt = rdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_) rddreuslt.foreach(println) &#125; //在实际的生成中,我们是封装函数来进行逻辑的组织 def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) val sc = new SparkContext(conf) sc &#125;&#125;//如果是使用HDFS文件创建rdd,只要把textFile的文件路径修改为HDFS文件路径即可val rdd = sc.textFile(&quot;hdfs://spark1:9000/xx.txt&quot;)//如果是在集群中运行的时候,那么需要将.setMaster(&quot;local&quot;)去掉 Transformation和actionspark支持两种rdd操作:Transformation和action,Transformation操作会针对已有的rdd创建一个新的rdd,而action则主要是对rdd进行最后的操作,比如遍历,reduce,保存到文件等,并可以返回结果给Driver程序 例如map就是一种Transformation操作,他用于将已有的rdd的每个元素传入一个自定义的函数,并获取一个新的元素,然后将所有的新元素组成一个新的rdd;而reduce就是一种action操作,它用于对rdd中的所有元素进行聚合操作,并获取一个最终的结果,然后返回给Driver程序 Transformation的特点就是lazy特性,lazy特性的指的是:如果一个spark应用中只定义了Transformation操作,那么即使你执行该应用,这些操作也不会执行,也就是说,Transformation是不会触发spark程序的执行的,他只是记录了对rdd的所作的操作,但是不会自发的执行,只有当Transformation之后,接着执行一个action操作,那么所有的Transformation才会被执行,spark通过这种lazy特性,来进行底层的spark应用执行的优化,避免产生过多的中间结果 action操作执行,会触发一个spark job的运行,从而触发这个action之前所有的Transformation的执行,这是action的特性 综上:Transformation会产生rdd,而action会产生结果而不是rdd 下图是程序提交的流程上来反映spark的Transformation的lazy特性 案例:统计文件中每行出现的次数123val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)var reducedByKeyRdd = rdd.map((_,1)).reduceByKey(_+_)reducedByKeyRdd.foreach(println) 常用的Transformation介绍 操作 介绍 map 将rdd中的每个元素传入自定义函数,获取一个新的元素,然后用新的元素组成的新的rdd filter 对RDD中的每一个元素进行判断,如果返回true就保留 flatMap 与map类似,大那是对每个元素都可以返回一个或多个新元素,然后对所有的元素flat groupByKey 根据key进行分组,每个key对应一个Iterator reduceByKey 对每个key对应的value进行reduce操作 sortByKey 对每个key对应的value进行排序操作 join 对两个包含对的rdd进行join操作,每个keyjoin上的pair,都会传入自定义函数进行处理 cogroup 同join,但是是每个key对应的Iterable都会传入自定义的函数进行处理 常用的action操作 操作 介绍 reduce 将rdd中的所有元素进行聚合操作,第一个和第二个元素聚合产生的值,和第三个元素聚合,产生的值和第四个元素聚合,一次类推 collect 将rdd中的所有元素获取到本地客户端 count 获取rdd元素总数 take(n) 后rdd前n个元素 saveAsTextFile 将rdd元素保存到文件中,对每个元素调用toString方法 countByKey 对每个key对应的值进行count计数 foreach 遍历rdd中的每一个元素 Transformation实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139#map:将集合中每个元素乘以2val numberRdd = sc.parallelize(1 to 7)val resultRdd = numberRdd.map(_*2)resultRdd.foreach(println)/*打印结果:2468101214 */#filter:过滤出集合中的偶数val numberRdd = sc.parallelize(1 to 7)val resultRdd = numberRdd.filter(_%2==0)resultRdd.foreach(println)/*打印结果:246 */#flatMap:将行拆分为单词val linesRdd = sc.parallelize(Seq(&quot;zhangsna 88&quot;,&quot;lisi 99&quot;))val resultRdd = linesRdd.flatMap(_.split(&quot; &quot;))resultRdd.foreach(println)/*打印结果:zhangsna88lisi99 */#groupByKey:将每个班级的成绩进行分组val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))val resultRdd = linesRdd.groupByKey()//返回: RDD[(K, Iterable[V])]resultRdd.foreach&#123; score=&gt;&#123; print(score._1+&quot; :&quot;) //println(score._2.toList) score._2.foreach(sco=&gt;print(sco.toString + &quot; &quot;)) println &#125;&#125;/*打印结果:cls1 :80 82cls2 :88 98 */#reduceByKey:统计每个班级的总分val linesRdd = sc.parallelize(Seq((&quot;cls1&quot;,80),(&quot;cls2&quot;,88),(&quot;cls1&quot;,82),(&quot;cls2&quot;,98)))val resultRdd = linesRdd.reduceByKey(_+_)resultRdd.foreach(println)/*打印结果:(cls1,162)(cls2,186) */#sortByKey:将学生分数进行排序val linesRdd = sc.parallelize(Seq((&quot;zhangsna&quot;,80),(&quot;lisi&quot;,88),(&quot;wangwu&quot;,82),(&quot;zhaoliu&quot;,98)))//要将key放在tuple2的第一个位置,这就是第一个map的作用,而最后一个map的作用就是调整打印的顺序val resultRdd = linesRdd.map(t=&gt;(t._2,t._1)).sortByKey(false).map(t=&gt;(t._2,t._1))resultRdd.foreach(println)/*打印结果:(zhaoliu,98)(lisi,88)(wangwu,82)(zhangsna,80) */#join:打印每个学生的成绩val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(3,82)))val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(3,&quot;wangwu&quot;)))val joinedRdd = scoreRdd.join(studRdd)/*def join[W](other: RDD[(K, W)]): RDD[(K, (V, W))]返回的是:(K, (V, W)) */joinedRdd.foreach&#123; t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取 val snu = t._1 val (score,name) = t._2 println(name + &quot;:&quot; + score + &quot;:&quot; + snu) &#125;&#125;/*打印结果: zhangsna:80:1 wangwu:82:3 lsii:88:2 */#cogroup val scoreRdd = sc.parallelize(Seq((1,80),(2,88),(1,80),(2,88),(3,82))) val studRdd = sc.parallelize(Seq((1,&quot;zhangsna&quot;),(2,&quot;lsii&quot;),(1,&quot;zhangsna2&quot;),(3,&quot;wangwu&quot;))) val joinedRdd = scoreRdd.cogroup(studRdd) /* def cogroup[W](other: RDD[(K, W)]): RDD[(K, (Iterable[V], Iterable[W]))] 返回的是:(K, (Iterable[V], Iterable[W])) */ joinedRdd.foreach&#123; t=&gt;&#123;//因为返回的是(K, (V, W)),所以用_1,_2去取 val snu = t._1 val (score,name) = t._2 println(snu + &quot;: &quot; + score.toList.toString + &quot; &quot; + name.toList.toString) &#125; &#125; /*打印结果: 1: List(80, 80) List(zhangsna, zhangsna2) 3: List(82) List(wangwu) 2: List(88, 88) List(lsii) */ action实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081#reduce操作 val scoreRdd = sc.parallelize(Seq(1,2,3,4,5)) val result = scoreRdd.reduce(_+_) println(result) /*打印结果: 15 */#collectval scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)//使用collect操作将分布在远程的数据拉取到本地,对大数据量不要这么做,测试可以,因为可能造成本地内存溢出,还可能因为将远程的数据拉倒本地,走网络的话,性能会很差val result = scoreRdd.collectresult.foreach(println)#countval scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)val result = scoreRdd.countprintln(result)#take val scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3) //从远程获取指定数量的数据,返回:Array[T] val result = scoreRdd.take(3) result.foreach(println) /*结果打印: 1 2 3 */#saveAsTextFileval scoreRdd = sc.parallelize(Seq(1,2,3,4,5),3)//从远程获取指定数量的数据,返回:Array[T]val result = scoreRdd.saveAsTextFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\result&quot;)/*结果:在C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\result目录下,有下面的文件:._SUCCESS.crc.part-00000.crc.part-00001.crc.part-00002.crc_SUCCESSpart-00000part-00001part-00002因为在parallelize的指定的分区为3,所以会生成3个part,其中的在part-00000文件中存在的数据:1part-00000文件中存在的数据:2\\n3part-00000文件中存在的数据:4\\n5\\n6*/#countByKey val scoreRdd = sc.parallelize(Seq((&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls1&quot;,&quot;zhangsan&quot;),(&quot;cls3&quot;,&quot;zhangsan3&quot;),(&quot;cls2&quot;,&quot;zhangsan2&quot;)),3) val result = scoreRdd.countByKey() //返回:Map[K, Long] for((k,v)&lt;-result)&#123; println(k+&quot;:&quot;+v.toString) &#125; /*结果打印: cls2:1 cls3:1 cls1:3 */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的基本工作原理","date":"2017-04-16T04:47:25.279Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark的基本工作原理/","text":"下面的这张图表示的是spark的基本的工作原理图(简图) 下面是对RDD的概念解释1.RDD在抽象上来说是一种元素的集合,包含了数据,他是被分区的,分为多个分区,每个分区分布在集群中的不同节点上,从而让RDD中的数据可以被并行操作(分布式数据集)2.RDD的创建:通过HDFS文件或hive表创建;通过应用程序的集合来创建3.RDD的数据默认情况下存放在内存中,但是在内存资源不足时,spark会自动将RDD数据写入磁盘(弹性)4.RDD最重要的特性是:提供了容错性,可以自动从节点失败中恢复过来,即:如果某个节点山的RDD partition因为节点故障,导致数据丢了,那么RDD会自动通过自己的数据来源重新计算该partition,这一切对使用者是透明的","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的topN","date":"2017-04-16T04:47:25.277Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark的topN/","text":"取最大的前3个数字:其实就是在sortByKey之后取take(3)1234567891011121314val lineRdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)val reducedRdd = lineRdd.flatMap(_.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_)val countWords = reducedRdd.map(count=&gt;(count._2,count._1))val sortedRdd = countWords.sortByKey(false)val result = sortedRdd.map(sort=&gt;(sort._2,sort._1))val top3Number = result.take(3)top3Number.foreach(println)/*执行结果:(spark,19)(hadoop,13)(88,6) */ 获取分组之后的组内的topN,实现步骤如下:1.对rdd进行groupByKey,返回的是(K, Iterable[V])2.在遍历1的结果,然后在每组中使用sortWith(排序的规则),对组内数据进行排序,取组内的topN,并返回组内的数据3.对组与组之间进行排序,sortBy可以指定某列进行排序12345678910111213141516171819202122232425val rdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)val lines=rdd.map&#123; line =&gt; (line.split(&quot; &quot;)(0),line.split(&quot; &quot;)(1).toInt) &#125;//分组val groups=lines.groupByKey() //返回:RDD[(K, Iterable[V])]//组内进行排序val groupsSort=groups.map(tu=&gt;&#123; val key=tu._1 val values=tu._2 val sortValues=values.toList.sortWith(_&gt;_).take(4)//取top 4 (key,sortValues)&#125;)//组与组之间进行排序groupsSort.sortBy(tu=&gt;tu._1, false, 1).collect.foreach(value=&gt;&#123; print(value._1) value._2.foreach(v=&gt;print(&quot;\\t&quot;+v)) println()&#125;)/*打印结果:spark 100 99 94 88hadoop 88 56 35 33 */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark架构原理图解","date":"2017-04-16T04:47:25.276Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark架构原理/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化四之对多次使用的RDD进行持久化或checkpoint","date":"2017-04-16T04:47:25.274Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化四之对多次使用的RDD进行持久化或checkpoint/","text":"如果程序中,对某一个RDD,基于他进行了多次Transformation或者action操作,那么就非常有必要对其进行持久化操作,以避免对一个RDD反复进行计算 此外,如果要保证在RDD的持久化数据可能丢失的情况下,还要保证高性能,那么可以对RDD进行checkpoint操作","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十之reduceByKey和groupByKey性能对比","date":"2017-04-16T04:47:25.273Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十之reduceByKey和groupByKey性能对比/","text":"reduceByKey和groupByKey 12val counts = pairs.reduceByKey(_+_)val counts = pairs.groupByKey().map(wordCounts=&gt;(wordCounts._1,wordCounts._2.sum)) 如果能用reduceByKey,那就用reduceByKey,因为他会在map端,先进行本地combine,可以大大减少要传输到reduce端的数据量,减小网络传输的开销 只有在reduceByKey处理不了时,才用groupByKey().map()来替代","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化十一之shuffle性能优化","date":"2017-04-16T04:47:25.271Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化十一之shuffle性能优化/","text":"12345678new SparkConf().set(&quot;spark.shuffle.consolidateFiles&quot;, &quot;true&quot;)spark.shuffle.consolidateFiles:是否开启shuffle block file的合并,默认是falsespark.reducer.maxSizeFlight: reduce task的拉取缓存,默认48Mspark.shuffle.file.buffer: map task的写磁盘缓存,默认32kspark.shuffle.io.maxRetries:拉取失败的最大重试次数,默认3次spark.shuffle.io.retryWait:拉取失败的重试间隔,默认5sspark.shuffle.memoryFraction:用于reduce端聚合的内存比例,默认0.2,超过比例机会溢出到磁盘上","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化六之java虚拟机垃圾回收调优","date":"2017-04-16T04:47:25.270Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化六之java虚拟机垃圾回收调优/","text":"java虚拟机垃圾回收调优的背景 如果在持久化RDD的时候,持久化了大量的数据,那么java虚拟机的垃圾回收就可能成为一个性能瓶颈,因为java虚拟机会定期进行垃圾回收,此时就会追踪所有的java对象,并且在垃圾回收时,找到那些已经不再使用的对象,然后清理旧的对象,来给新的对象腾出内存空间 垃圾回收的性能开销,是跟内存中的对象的数量,成正比的,所以,对于垃圾回收的性能问题,首先要做的就是,使用高性能的数据结构,比如array和String;其次就是在持久化rdd的时候,使用序列化级别,而且用Kyo序列化类库,这样每个partition就只是一个对象—– 一个字节数组 监测垃圾回收 我们可以对垃圾回收进行监测,包括多久进行一次垃圾回收,以及每次垃圾回收耗费的时间,只要在spark-submit脚本中,增加一个配置即可:1--conf &quot;spark.executor.extraJavaOptions=-verbose:gc-X;+PrintGCDetails-XX;+PrintGCTimeStamps&quot; 但是要记住,这里虽然会打印出java虚拟机的垃圾回收的相关信息,但是是输出到了worker上的日志中,而不是driver的日志中 另一种方式:其实也完全可以通过SparKUI(4040端口)来观察每个stage的垃圾回收情况 优化Executor内存比例 对于垃圾回收来说,最重要的就是调节RDD缓存占用的内存空间,与算子执行时创建的对象占用的内存空间的比例,默认情况下,spark使用每个Executor60%的内存空间来缓存RDD,那么在task执行期间创建的对象,只有40%的内存空间来存放 在这种情况下,很有可能因为你的内存空间的不足,task创建的对象过大,那么一旦发现40%内存空间不够用了,就会触发java虚拟机的垃圾回收操作,因为在极端情况下,垃圾回收操作可能会被频繁触发 在上述情况下,如果发现垃圾回收频繁发生,那么就需要对那个比例进行调优,使用:1SparkConf().set(&quot;spark.storage.memoryFaction&quot;, &quot;0.5&quot;) 可以将RDD缓存占用空间的比例降低,从而给更多的空间让task创建的对象进行使用 因此,对于RDD持久化,完全可以使用Kryo序列化,加上降低其Executor内存占比的方式来减少其内存消耗,给task提供更多的内存,从而避免task的执行频繁触发GC 高级垃圾回收调优 java堆空间被划分成了两块空间,一个是年轻代,一个是老年代,年轻代存放的是段时间存活的对象,老年代存放的是长时间存活的对象,年轻代又被划分为三块空间:Eden,Survivor1,Survivor2 首先,Eden区域和Survivor1区域用于存放对象,Survivor2区域备用,创建的对象,首先放入Eden区域和Survivor1区域,如果Eden区域满了,那么就会触发一次minor gc,进行年轻代的垃圾回收,Eden和Survivor1区域中存活的对象,会被移动到Survivor2区域中,然后Survivor1和Survivor2的角色调换,Survivor1变成了备用 如果一个对象,在年轻代中,撑过了多次垃圾回收,都没有被回收掉,那么会被认为是长时间存活的,因此会被移入老年代中,此外,如果将Eden和Survivor1中的存活对象,尝试放入Survivor2中时,发现Survivor2放满了,那么会直接放入老年代中,此时就出现了,短时间存活的对象进入老年代的问题 如果老年代的空间满了,那么就会触发full gc ,进行老年代的垃圾回收操作 spark中,垃圾回收调优的目标就是:只有真正长时间存活的对象,才能进入老年代,短时间存活的对象,只能待在年轻代中,不能因为某个Survivor区域空间不够,在minor gc时,就进入老年代,从而造成短时间存活的对象长期待在老年代中占据了空间,而且full gc时要回收大量的短时间存活的对象,导致full gc速度缓慢 如果发现,在task执行期间,大量full gc发生了,那么说明,年轻代的Survivor区域,给的空间不够大,此时可以执行一些操作来优化垃圾回收行为:1.包括降低spark.storage.memoryFaction的比例,给年轻代更多的空间,来存放短时间存活的对象2.给Eden区域分配更大的空间,使用-xmn即可(在spark.executor.extraJavaOptions中配置,见上面),通常建议给Eden区域,预计大小的4/33.如果使用的是HDFS文件,那么很好估计Eden区域大小,如果每个Executor有4个task,然后每个HDFS压缩块解压后大小是3倍,此外每个HDFS块的大小为128M,那么Eden区域的预计大小就是:43128M,然后通过-Xmn参数,将Eden区域大小设置为43128*4/3 其实,根据经验来看,对于垃圾回收的调优,尽量就是调节Executor内存的比例就可以了,因为jvm的调优是非常负责和敏感的,除非是真的到了万不得已的地步,自己本身对jvm相关的技术很了解,那么此时进行Eden区域的调优是可以的 一些高级的参数:-XX:SurvivorRatio=4 如果值为4,那么就是一个Survivor跟Eden的比例是1:4,也就是说每个Survivor占据的年轻代的比例是1/6,所以你其实也可以尝试调大Survivor区域的大小 -XX:NewRatio=4 调节新生代和老年代的比例","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化八之广播共享数据","date":"2017-04-16T04:47:25.269Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化八之广播共享数据/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化五之使用序列化的持久化级别","date":"2017-04-16T04:47:25.267Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化五之使用序列化的持久化级别/","text":"除了对多次使用的RDD进行持久化操作之外,还可以进一步优化其性能,因为很有可能,RDD的数据是持久化到内存,或者磁盘中的,那么此时,如果内存大小不是特别充足,完全可以使用序列化的持久化级别,比如:MEMORY_ONLY_SER,MEMORY_AND_DISK_SER等,使用RDD.persist(StorageLevel.MEMORY_ONLY_SER)这样的语法即可 这样的话,将数据序列化之后,再持久化,可以大大减小对内存的消耗,此外,数据量小了之后,如果需要写入磁盘,那么磁盘IO性能消耗也比较小 对RDD持久话序列化后,RDD的每个partition的数据,都是序列化为一个巨大的字节数组,这样,对于内存的消耗就小的多了,但是唯一的缺点是:获取RDD数据时,需要对其进行反序列化,会增大性能开销 因为,对于序列化的持久化级别,还可以进一步优化,也就是说,使用Kryo序列化类库,这样可以获得更快的序列化速度,并且占用更小的内存空间,但是要记住,如果RDD的元素(RDD的泛型类型)是自定义的话,那么在Kryo中提前注册自定义类型","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化二之高性能序列化类库","date":"2017-04-16T04:47:25.266Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化二之高性能序列化类库/","text":"因为在分布式中存在着网络传输,那么数据就必须进行序列化,而如果在执行序列化操作的时候很慢,或者序列化后的数据还是很大,那么会让分布式应用程序的性能下降很多,所以,进行spark性能优化的第一步,就是进行序列化的性能优化 spark自身对于序列化的便捷性和性能进行了一个取舍和权衡,默认,spark倾向于序列化的便捷性,使用了java自身提供的序列化机制—基于ObjectInputStream和ObjectOutputStream的序列化机制,因为这种方式是java原生提供的,很方便使用 但是问题是,java序列化的性能并不高,序列化的速度相对较慢,而且序列化以后的数,还是相对来说比较大,还是比较占用内存孔家你的,因此,如果你的spark应用程序对内存很敏感,那么,实际上默认的java序列化的机制并不是最好的选择. spark提供了两种序列化机制,他默认使用了第一种:1.java序列化机制,默认情况下,spark使用java自身的ObjectInputStream和ObjectOutputStream机制进行对象的序列化,只要你的类实现了Serializable接口,那么都是可以序列化的,而且java序列化机制是提供了自定义序列化支持的,只要你实现Externalizable接口即可实现自己的更高性能的序列化算法,java序列化机制的速度比较慢,而且序列化后的数据占用的内存空间比较大 2.Kryo序列化机制:spark也支持使用Kryo类库来进行序列化,Kryo序列化机制比java机制更快,而且序列化后的数据占用的空间更小,通常比java序列化的数据占用的空间要小10倍,Kryo序列化机制下之所以不是默认序列化机制的原因是:有些类型虽然实现了Seriralizable接口,但是他也不一定能够进行序列化,因此,如果你要得到最佳的性能,Kryo还要求你在spark应用中,对所有你需要序列化的的类型都进行注册 如何使用Kryo序列化机制要序列化的类是实现了Serializable接口的1.首先在SparkConf设置一个参数:SparkConf.set(“spark.serializer”,”org.apache.spark.serializer.KryoSerializer”) 使用Kryo时,他要求需要序列化的类,是要预先进行注册的,以获得最佳性能—如果不注册的话,那么Kryo必须时刻保存类型的全限定名,反而 占用不少内存,spark默认是对scala中常用的类型自动注册了Kryo的,都在AllScalaRegistry类中 2.注册:比如自己的算子中,使用了外部的自定义类型的对象,那么还是需要将其进行注册:1234//实际上,下面的写法是错误的,因为counter不是共享的,所以累加器的功能是无法实现的val count = new Counter()val numbers = sc.parallelize(Array(1,2,3,4,5))numbers.foreach(counter.add(_)) 如果要注册自定义的类型,那么就使用如下的模板代码即可:123val conf = new SparkConf().setMaster(...).setAppName(...)conf.registerKryoClasses(Array(classOf[Counter])) //classOf是拿到某个类的类型(相当于java中的Counter.class)val sc = new SparkContext(conf) 优化Kryo类库的使用1.优化缓存大小如果注册的要序列化的自定义的类型,本身特别大,比如包含了超过100个Field,那么就会导致要序列化的对象过大,此时就需要对Kryo本身进行优化,因为Kryo内部的缓存可能不够存放那么大的class对象,此时就需要调用SparkConf.set(),设置spark.kryoserializer.buffer.mb参数的值,将其调大 默认情况下他的值是2,就是说最大能缓存2M的对象,然后进行序列化,可以在必要时将其调大,比如设置为10 2.预先注册自定义类型 虽然不注册自定义类型,Kryo类库也能正常工作,但是那样的话,对于他要序列化的每个对象,都会保存一份他的全限定类名,此时反而会耗费大量内存,因此通常都建议预先注册好要序列化的自定义的类 在什么场景下使用Kryo序列化类库?首先,这里讨论的都是spark的一些普通的场景,一些特殊的场景,比如RDD的持久化,在后面会讲到 那么这里针对的Kryo序列化类库的使用场景,就是算子函数使用到了外部的大数据(类比较大)的情况,比如,我们在外部定义了一个封装了应用所有配置的对象,比如说自定义了一个MyConfiguration对象,里面包含了100m的数据,然后在算子函数里面使用到了这个外部的大对象,此时如果默认情况下,让spark用java序列化机制来序列化这种外部的大对象,就会导致序列化速度缓慢,并且序列化以后的数据还是比较大,比较占用内存空间,因此在这种情况下,比较适合切换到Kryo类库,来对外部的大对象进行操作,一是序列化的速度会变快,二是会减少序列化后的数据占用的空间","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化九之数据本地化","date":"2017-04-16T04:47:25.264Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化九之数据本地化/","text":"数据本地化背景 数据本地化对于spark job性能有着巨大的影响,如果数据以及要计算他的代码是在一起的,那么性能当然非常高,但是,如果数据和计算他的代码是分开的,那么其中之一必须到另外一方的机器上,通常来说,移动代码到其他节点上,会比移动数据到代码所在的节点上去,速度要快,因为代码比较小,spark也正是基于这个数据本地化的原则来构建task调度算法的 数据本地化,指的是:数据离计算他的代码有多近,基于数据距离代码的举例,有几种数据本地化的级别(其实在剖析TaskSchedulerImpl中有讲到,可以回顾下):1.PROCESS_LOCAL:数据和计算他的代码在同一个JVM进程中2.NODE_LOCAL:数据和计算他的代码在一个节点上,但是不在一个进程中,比如在不同的executor进行中,或者是数据在HDFS文件的block中3.NO_PREF:数据从哪里过来,性能都是一样的4.RACK_LOCAL:数据和计算他的代码在一个机架上5.ANY:数据可能子任意地方,比如其他网络环境内,或者其他机架上 数据本地化原理 spark倾向于使用最好的本地化级别来调度task,但是这是不可能的,如果没有任何未处理的数据在空闲的executor上,那么spark就会放低本地化级别,这时有两个选择:第一,等待直到executor上的cpu释放出来,那么就分配task过去;第二,立即在任意一个executor上启动一个task spark默认会等待一会儿,来期望task要处理的数据所在的节点神的executor空想出一个cpu,从而将task分配过去,只要超过了时间,那么spark就会将task分配到其他任意一个空闲的executor上 可以设置一个参数:spark.locality系列参数,来调节spark等待task可以进行数据本地化的时间123456789//通用的spark.locality.wait(3000 毫秒)//在等待node级别的时候,等待的时间spark.locality.wait.node//在等待process级别的时候,等待的时间spark.locality.wait.process//在等待rack级别的时候,等待的时间spark.locality.wait.rack","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化三之优化数据结构","date":"2017-04-16T04:47:25.263Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化三之优化数据结构/","text":"优化数据结构要减少内存的消耗,除了使用高效的序列化类库以外,还有一个很重要的事情,就是优化数据结构,从而避免java语法特性中所导致的额外的内存开销,比如基于指针的java数据结构,以及包装类型 有一个关键的问题,就是优化什么数据结构,其实主要就是优化你的算子函数,内部使用到的局部数据,或者是算子函数外部的数据都可以进行数据结构的优化,优化之后,都会减少其对内存的消耗和占用 如何优化数据结构?1.优先使用数组以及字符串,而不是集合类,也就是说,优先使用array,而不是ArrayList,LinkedList,HashMap等集合比如,有个List list = new ArrayList(),将其替换为int[] arr = new int[] ,这样的话,array比List少了额外信息的存储开销,还能使用原始数据类型(int)来存储数据,比List中使用Integer这种包装类型存储数据要节省内存的多 还比如,通常企业级应用中的做法是,对于HashMap,List这种数据,统一用String拼接成特殊格式的字符串,比如Map= new HashMap(),可以优化为特殊的字符串格式:1id:name,address|id:name,address.... 2.避免使用多层嵌套的对象结构,比如说:1234public class Teacher&#123; private List&lt;Student&gt; students = new ArrayList&lt;Student&gt;()&#125; 这就是不好的例子,因为Teacher类的内部又嵌套了大量的小Student对象 比如说,对于上述例子,也完全可以使用特殊的字符串来进行数据的存储,比如,用json字符串拉存储数据,就是一个很好的选择1&#123;&quot;teacherId&quot;:1,&quot;teacherNameA&quot;:&quot;leo&quot;,students:[&#123;&quot;studentId&quot;:1,&quot;studentName&quot;:&quot;tom&quot;&#125;]&#125; 3.对于有些能够避免的场景,尽量使用int替代String,因为String虽然比ArrayList,HashMap等数据结构高效多了,占用内存少了,但是之前分析过,还有额外的信息的消耗,比如之前用string表示id,那么现在完全可以用数字类型的int,来进行替代 在这里提醒,在spark应用中,id就不要用常用的uuid了,因为无法转成int,就用自增的int类型的id即可","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化七之提高并行度","date":"2017-04-16T04:47:25.261Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化七之提高并行度/","text":"实际上spark集群的资源并不一定会被充分利用到,所以要尽量设置合理的并行度,来充分地利用集群的资源,才能充分提高spark应用程序的性能 spark会自动设置以文件作为输入源的rdd的并行度,依据其大小,比如HDFS,就会给每一个block创建一个partition,也依据这个设置并行度,对于reduceByKey等会发生shuffle的操作,就使用并行度最大的父rdd的并行度即可 可以手动使用textFile(),parallelize()等方法的第二个参数来设置并行度,也可以是使用1spark.default.parallelism 参数来统一设置并行度,spark官方的推荐是,给集群中的每一个cpu core设置2-3个task 比如说,spark-submit设置了executor数量为10个,每个executor要求分配2个core,那么application总共会有20个core,此时可以设置1SparkConf().set(&quot;spark.default.parallelism&quot;, &quot;60&quot;) 即设置集群cpu数量的2-3倍","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark性能优化一之诊断内存消耗","date":"2017-04-16T04:47:25.260Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark性能优化一之诊断内存消耗/","text":"内存都花费到哪里去了?1.每个java对象,都有一个对象头,会占用16个字节,主要包括了一些对象的元信息,比如指向他的类的指针,如果一个对象本身很小,比如就包含了一个int类型的Field,那么他的对象头实际上比对象自己还要大 2.java的string对象,会比他内部的原始数据,要多出40个字节,因为他内部使用char数组来保存内部的字符序列的,并且还得保存诸如数组长度之类的信息,而且因为string使用的是UTF-16编码,所以每个字符会占用2个字节,比如,包含10个字符的string,会占用60个字节 3.java中的集合类型,比如HashMap和LinkedList,内部使用的是链表数据结构,所以对链表中的每一个数据,都使用了Entry对象来包装,Entry对象不光有对象头,还有指向下一个Entry的指针,通常占用8个字节 4.元素类型为袁术数据类型(比如int)的集合,内部通常会使用原始数据类型的包装类型,比如Integer来存储元素 如何判断你的程序消耗了多少内存?1.首先,自己设置RDD的并行度,有两种方式:要不然在parallelize(),textFile()等方法中,传入第二个参数,设置RDD的task/partition的数量;要不然,用sparkConf.set()方法,设置一个参数,spark.default.parallelism,可以统一设置这个Application所有RDD的partition数量 2.其次,在程序汇总将RDD cache到内存中,调用RDD.cache()方法即可3.最后,观察Driver的log,你会发现类似于:”INFO BlockManagerMasterActor:Added rdd_0 in memory on mbk.local:50311(size:717.5KB,free:332.3MB)”的日志信息,这就显示了每个partition占用了多少内存 4.将这个内存信息乘以partition数量,即可得出RDD的内存占用量","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核讲解","date":"2017-04-16T04:47:25.259Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核讲解/","text":"下图是整个spark的执行流程图详解","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码四之wordcount的job触发流程原理剖析与源码分析","date":"2017-04-16T04:47:25.257Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码四之job触发流程原理剖析与源码分析/","text":"wordcount的执行源码查看 1234567891011121314151617181920212223242526272829/*首先调用hadoopFile方法,会创建一个HadoopRDD,其中的元素就是:(key,value) pairkey是HDFS或文本文件的每一行的offset,value是文本行然后对HadoopRDD调用map方法,会剔除ky,只保留value,map方法返回的是MapPartitionsRDDMapPartitionsRDD内部的元素,其实就是一行一行的文本行*/val lines = sc.textFile(&quot;xx.txt&quot;) //最终返回的是MapPartitionsRDDval words = lines.flatMap(_.split(&quot; &quot;))//返回的是MapPartitionsRDDval pair = words.map((_,1))//返回的是MapPartitionsRDD//pair是一个MapPartitionsRDD,但是MapPartitionsRDD是继承了RDD的,但是在MapPartitionsRDD和RDD中都没有reduceByKey方法,为什么呢?其实在RDD中有一个隐式转换:implicit def rddToPairRDDFunctions会将rdd转成PairRDDFunctions,我们观察PairRDDFunctions中就有reduceByKey方法val counts = pairs.reduceByKey(_+_)//返回的是ShuffledRDDcount.foreach(println)============================在执行foreach时代码如下:因为foreach是一个action,所以会触发jobdef foreach(f: T =&gt; Unit): Unit = withScope &#123; val cleanF = sc.clean(f) sc.runJob(this, (iter: Iterator[T]) =&gt; iter.foreach(cleanF))&#125;在sc中runJob方法中如下:dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)// 看是否是一个action的标志是:是否会触发runJob方法","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码十之BlockManager的原理剖析与源码分析","date":"2017-04-16T04:47:25.255Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码十之BlockManager的原理剖析与源码分析/","text":"BlockManager的原理剖析 BlockManager的源码分析 1.BlockManagerMaster2.BlockManagerMasterEndpoint3.BlockManager initialize()获取数据: doGetLocal() doGetRemote()写入数据: doPut() 4.DiskStore getBytes() getValues() 5.MemoryStore putBytes() putIterator() tryToPut()","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码十一之CacheManager原理分析与源码分析","date":"2017-04-16T04:47:25.254Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码十一之CacheManager原理分析与源码分析/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码六之DAGScheduler原理剖析与源码分析","date":"2017-04-16T04:47:25.252Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码六之DAGScheduler原理剖析与源码分析/","text":"在前面的wordcount程序中,我们可以看到触发action后,会调用dagScheduler.runJob运行job,下面我们来看DAGScheduler的源码","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码八之Executor和Task原理剖析与源码分析","date":"2017-04-16T04:47:25.251Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码八之Executor原理剖析与源码分析/","text":"Executor的注册机制在worker上启动Executor的时候,会在worker上启动一个进程:CoarseGrainedExecutorBackend,然后调用其初始化的方法在其初始化的方法中会向driver去注册,在driver接收到注册消息之后,driver返回 启动task机制在spark内核源码七之TaskScheduler原理剖析与源码分析中最后发送的消息是:LaunchTask消息 task原理图示","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码五之stage划分算法原理剖析","date":"2017-04-16T04:47:25.248Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码五之stage划分算法原理剖析/","text":"下面这张图是wordcount的执行过程,我们可以看出stage的划分过程","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码二之Master原理解析","date":"2017-04-16T04:47:25.246Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码二之Master原理解析/","text":"master主备切换机制 master的注册机制 改变状态机制源码分析DriverStateChanged 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 override def receive: PartialFunction[Any, Unit] = &#123; ///..... case DriverStateChanged(driverId, state, exception) =&gt; &#123; state match &#123; //如果Driver的状态是错误,完成,被杀掉,失败,那么就移除Driver case DriverState.ERROR | DriverState.FINISHED | DriverState.KILLED | DriverState.FAILED =&gt; removeDriver(driverId, state, exception) case _ =&gt; throw new Exception(s&quot;Received unexpected state update for driver $driverId: $state&quot;) &#125; &#125; //...&#125;private def removeDriver( driverId: String, finalState: DriverState, exception: Option[Exception]) &#123; //找到driverId对应的Driver drivers.find(d =&gt; d.id == driverId) match &#123; case Some(driver) =&gt; logInfo(s&quot;Removing driver: $driverId&quot;) //将driver从内存缓存中移除 drivers -= driver if (completedDrivers.size &gt;= RETAINED_DRIVERS) &#123; val toRemove = math.max(RETAINED_DRIVERS / 10, 1) completedDrivers.trimStart(toRemove) &#125; //向completedDrivers中加入driver completedDrivers += driver //使用持久化引擎去除driver的持久化信息(如:假设使用的引擎是zookeeper,那么是移除zk上的driver节点) persistenceEngine.removeDriver(driver) //设置driver的state,exception driver.state = finalState driver.exception = exception //将driver所在的Worker中的driver移除 driver.worker.foreach(w =&gt; w.removeDriver(driver)) //同样调用schedule方法(当有资源改变的时候会调用) schedule() case None =&gt; logWarning(s&quot;Asked to remove unknown driver: $driverId&quot;) &#125;&#125; ExecutorStateChanged 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667override def receive: PartialFunction[Any, Unit] = &#123; //.... case ExecutorStateChanged(appId, execId, state, message, exitStatus) =&gt; &#123; //找到Executor对应的App,然后反过来通过app内部的Executor缓存获取Executor信息 val execOption = idToApp.get(appId).flatMap(app =&gt; app.executors.get(execId)) execOption match &#123; //如果找到了Executor case Some(exec) =&gt; &#123; val appInfo = idToApp(appId) val oldState = exec.state //设置Executor的当前状态 exec.state = state if (state == ExecutorState.RUNNING) &#123; assert(oldState == ExecutorState.LAUNCHING, s&quot;executor $execId state transfer from $oldState to RUNNING is illegal&quot;) appInfo.resetRetryCount() &#125; //向driver同步发送ExecutorUpdated消息(在AppClient.receive方法中有ExecutorUpdated对应接收消息) exec.application.driver.send(ExecutorUpdated(execId, state, message, exitStatus)) //判断如果Executor完成了 if (ExecutorState.isFinished(state)) &#123; // Remove this executor from the worker and app logInfo(s&quot;Removing executor $&#123;exec.fullId&#125; because it is $state&quot;) // If an application has already finished, preserve its // state to display its information properly on the UI //从app的缓冲中移除executor if (!appInfo.isFinished) &#123; appInfo.removeExecutor(exec) &#125; //从运行worker的缓存中移除executor exec.worker.removeExecutor(exec) val normalExit = exitStatus == Some(0) // 判断,如果Executor的退出状态是非正常的 if (!normalExit) &#123; //判断Application当前的重试次数,是否达到了最大值 if (appInfo.incrementRetryCount() &lt; ApplicationState.MAX_NUM_RETRY) &#123; //重新进行调度 schedule() &#125; else &#123; //否则,那么就进行removeApplication //也就是说,Executor反复调度都失败的话,那么就认为Application也失败了,所以移除Application val execs = appInfo.executors.values if (!execs.exists(_.state == ExecutorState.RUNNING)) &#123; logError(s&quot;Application $&#123;appInfo.desc.name&#125; with ID $&#123;appInfo.id&#125; failed &quot; + s&quot;$&#123;appInfo.retryCount&#125; times; removing it&quot;) removeApplication(appInfo, ApplicationState.FAILED) &#125; &#125; &#125; &#125; &#125; case None =&gt; logWarning(s&quot;Got status update for unknown executor $appId/$execId&quot;) &#125; &#125; //....&#125; 资源调度机制源码分析(schedule(),两种资源调度算法)在Master类中有schedule方法,我们先来看:在schedule方法中Driver的启动 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677private def schedule(): Unit = &#123; //判断,master不是alive的话,就直接返回 //也就是说,standby master是不会进行application的资源调度的 if (state != RecoveryState.ALIVE) &#123; return &#125; //Random.shuffle对传入的元素集合进行随机的打乱 val shuffledWorkers = Random.shuffle(workers) // Randomization helps balance drivers for (worker &lt;- shuffledWorkers if worker.state == WorkerState.ALIVE) &#123;//过滤出来状态为ALIVE的worker //首先,调度driver //为什么要调度driver,什么情况下会注册Driver,并且会导致Driver被调度 //其实,只有用yarn-cluster模式提交的时候,才会注册Driver //因为yarn-client和Standalone模式都会在本地直接启动Driver,而不会来注册Driver,就更不可能让master调度Driver,其实在worker.endpoint.send(LaunchDriver(driver.id, driver.desc))中就可以看到这点 for (driver &lt;- waitingDrivers) &#123;//Driver的注册的时候会把Driver放入waitingDrivers中(Master.receiveAndReply()中的case RequestSubmitDriver) //如果当前的worker的可用内存比driver要的内存要大,并且当前worker的可用核数比driver要的要大 //那么就调用launchDriver方法启动driver if (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123; launchDriver(worker, driver)//launchDriver见下面的详解:这里是使用endpoint去启动Worker上的Driver //从等待缓存中移除driver waitingDrivers -= driver &#125; &#125; &#125; //这里是Executor的启动 startExecutorsOnWorkers()&#125;===============================//在某一个worker上启动Driverprivate def launchDriver(worker: WorkerInfo, driver: DriverInfo) &#123; logInfo(&quot;Launching driver &quot; + driver.id + &quot; on worker &quot; + worker.id) //将该driver加入到worker的内存的缓存结构中 worker.addDriver(driver) //同时将worker也加入到driver的内存缓存中 driver.worker = Some(worker) //拿到这个worker的endpoint引用,然后向这个worker发送LaunchDriver消息(让worker来启动driver) worker.endpoint.send(LaunchDriver(driver.id, driver.desc)) //将driver的状态设置为RUNNING driver.state = DriverState.RUNNING&#125;//那么在Worker类中有一个方法去接收发送过来的消息,就是receive方法override def receive: PartialFunction[Any, Unit] = synchronized &#123; //... case LaunchDriver(driverId, driverDesc) =&gt; &#123; logInfo(s&quot;Asked to launch driver $driverId&quot;) val driver = new DriverRunner( conf, driverId, workDir, sparkHome, driverDesc.copy(command = Worker.maybeUpdateSSLSettings(driverDesc.command, conf)), self, workerUri, securityMgr) //加入内存缓存中 drivers(driverId) = driver //启动driver driver.start() //将已使用的内存和核数修改 coresUsed += driverDesc.cores memoryUsed += driverDesc.mem &#125; //...&#125;=============================== 在Master类中有schedule方法中同时还要启动Executor 12345678910111213141516private def schedule(): Unit = &#123; if (state != RecoveryState.ALIVE) &#123; return &#125; val shuffledWorkers = Random.shuffle(workers) for (worker &lt;- shuffledWorkers if worker.state == WorkerState.ALIVE) &#123; for (driver &lt;- waitingDrivers) &#123; if (worker.memoryFree &gt;= driver.desc.mem &amp;&amp; worker.coresFree &gt;= driver.desc.cores) &#123; launchDriver(worker, driver) waitingDrivers -= driver &#125; &#125; &#125; //这里是Executor的启动 startExecutorsOnWorkers()&#125; 而startExecutorsOnWorkers的方法如下 12345678910111213141516171819202122private def startExecutorsOnWorkers(): Unit = &#123; // Right now this is a very simple FIFO scheduler. We keep trying to fit in the first app // in the queue, then the second app, etc. for (app &lt;- waitingApps if app.coresLeft &gt; 0) &#123; val coresPerExecutor: Option[Int] = app.desc.coresPerExecutor // 过滤掉那些不能启动该app的worker val usableWorkers = workers.toArray.filter(_.state == WorkerState.ALIVE) //过滤出worker状态为ALIVE .filter(worker =&gt; worker.memoryFree &gt;= app.desc.memoryPerExecutorMB &amp;&amp; //worker的空闲内存大于memoryPerExecutorMB worker.coresFree &gt;= coresPerExecutor.getOrElse(1)) //worker上的空闲CPU大于每个Executor需要的CPU .sortBy(_.coresFree).reverse//按照worker的coresFree倒序排列 //assignedCores = new Array[Int](numUsable)每个worker上分配的core数量 val assignedCores = scheduleExecutorsOnWorkers(app, usableWorkers, spreadOutApps) // Now that we&apos;ve decided how many cores to allocate on each worker, let&apos;s allocate them for (pos &lt;- 0 until usableWorkers.length if assignedCores(pos) &gt; 0) &#123; allocateWorkerResourceToExecutors(app, assignedCores(pos), coresPerExecutor, usableWorkers(pos)) &#125; &#125;&#125; allocateWorkerResourceToExecutors方法如下:在指定的worker上用分配的核数去启动一个或多个Executor123456789101112131415161718192021222324252627282930313233343536373839404142private def allocateWorkerResourceToExecutors( app: ApplicationInfo, assignedCores: Int, coresPerExecutor: Option[Int], worker: WorkerInfo): Unit = &#123; // If the number of cores per executor is specified, we divide the cores assigned // to this worker evenly among the executors with no remainder. // Otherwise, we launch a single executor that grabs all the assignedCores on this worker. //得到的是在一个worker上的executor数量 val numExecutors = coresPerExecutor.map &#123; assignedCores / _ &#125;.getOrElse(1) val coresToAssign = coresPerExecutor.getOrElse(assignedCores) //如果每个executor的core数量没有指定,那么在worker上的executor数量为1,同时将assignedCores都给该executor //如果指定了每个executor的core数量,那么用分配的assignedCores/coresPerExecutor得到executor的数量 for (i &lt;- 1 to numExecutors) &#123;//循环启动worker上的executor val exec = app.addExecutor(worker, coresToAssign)//coresToAssign是每个要启动的executor上的core数量 //启动Executor launchExecutor(worker, exec) app.state = ApplicationState.RUNNING &#125;&#125;/*spread out调度算法的举例我们在spark-shell脚本中,可以指定要多少个executor,每个executor多少个CPU(core),多少内存,那么基于此处的机制实际上,最后executor的实际数量,以及每个executor的CPU,可能与配置是不一样的,因为这里是基于总的CPU进行分配的比如配置中是:3个executor,每个executor要3个CPU,我们的机器有9个worker,每个有1个CPU那么根据上面的算法,会给每个worker分配一个core,然后给每个worker启动一个executor,最后会启动9个executor,每个executor有1个cpu core*//*非spread out调度算法的举例比如总共有10个worker,每个有10个core,此时有app要总共分配20个core,那么其实,只会分配到两个worker上每个worker占满10个core,那么其他的app,就只能分配到下一个worker了比如我们的spark-shell配置里,要10个executor,每个要2个core,那么总共有20个core,但是在非spread out算法下,其实总共只会启动2个executor,每个要有10个core*///综上 launchExecutor方法如下123456789101112private def launchExecutor(worker: WorkerInfo, exec: ExecutorDesc): Unit = &#123; logInfo(&quot;Launching executor &quot; + exec.fullId + &quot; on worker &quot; + worker.id) worker.addExecutor(exec) //拿到worker的Actor引用,然后发送消息去启动Executor,在Worker.receive方法中有对应的case LaunchExecutor去处理 worker.endpoint.send(LaunchExecutor(masterUrl, exec.application.id, exec.id, exec.application.desc, exec.cores, exec.memory)) //还要通知Driver端有Executor添加了,在APPClient.receive方法中有:case ExecutorAdded处理 exec.application.driver.send( ExecutorAdded(exec.id, worker.id, worker.hostPort, exec.cores, exec.memory))&#125; spread out和非spread out的不同 scheduleExecutorsOnWorkers方法源码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950private def scheduleExecutorsOnWorkers( app: ApplicationInfo, usableWorkers: Array[WorkerInfo], spreadOutApps: Boolean): Array[Int] = &#123; val coresPerExecutor = app.desc.coresPerExecutor val minCoresPerExecutor = coresPerExecutor.getOrElse(1) val oneExecutorPerWorker = coresPerExecutor.isEmpty val memoryPerExecutor = app.desc.memoryPerExecutorMB val numUsable = usableWorkers.length val assignedCores = new Array[Int](numUsable) // Number of cores to give to each worker val assignedExecutors = new Array[Int](numUsable) // Number of new executors on each worker var coresToAssign = math.min(app.coresLeft, usableWorkers.map(_.coresFree).sum) /** Return whether the specified worker can launch an executor for this app. */ def canLaunchExecutor(pos: Int): Boolean = &#123;//..... &#125; // Keep launching executors until no more workers can accommodate any // more executors, or if we have reached this application&apos;s limits var freeWorkers = (0 until numUsable).filter(canLaunchExecutor) while (freeWorkers.nonEmpty) &#123; freeWorkers.foreach &#123; pos =&gt; var keepScheduling = true//keepScheduling默认是true,即:采用spread out调度机制,如果我们指定了为false,那么可能的将一个worker上的资源分配完之后,再去其他的worker上分配资源 while (keepScheduling &amp;&amp; canLaunchExecutor(pos)) &#123; coresToAssign -= minCoresPerExecutor assignedCores(pos) += minCoresPerExecutor // If we are launching one executor per worker, then every iteration assigns 1 core // to the executor. Otherwise, every iteration assigns cores to a new executor. if (oneExecutorPerWorker) &#123; assignedExecutors(pos) = 1 &#125; else &#123; assignedExecutors(pos) += 1 &#125; //spread out 调度算法是将app尽可能多的分布到每个worker上,而非spread out是尽可能多的将每个worker上的资源用完了,再去分配其他的worker if (spreadOutApps) &#123; keepScheduling = false &#125; &#125; &#125; freeWorkers = freeWorkers.filter(canLaunchExecutor) &#125; assignedCores&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码九之shuffle原理剖析与源码分析","date":"2017-04-16T04:47:25.245Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码九之shuffle原理剖析与源码分析/","text":"在spark中,什么情况下会发生shuffle?reduceByKey,groupByKey,sortByKey,countByKey,join,cogroup 默认的shuffle操作的原理 vs 优化后的shuffle操作的原理 shuffle相关源码shuffle的写源码: ShuffleMapTask.runTask()是入口,writer默认是HashShuffleWriter shuffle的读源码: 可以从一个rdd开始入手,如ShuffledRDD的compute方法","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码三之Worker原理解析及源码分析","date":"2017-04-16T04:47:25.243Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码三之Worker原理解析及源码分析/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码七之TaskScheduler原理剖析与源码分析","date":"2017-04-16T04:47:25.242Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码七之TaskScheduler原理剖析与源码分析/","text":"在spark内核源码六之DAGScheduler原理剖析与源码分析一文中,最后dagScheduler将stage分成taskSet,使用taskScheduler.submitTasks去提交1taskScheduler.submitTasks(new TaskSet())","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark内核源码一之SparkContext原理解析","date":"2017-04-16T04:47:25.240Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark内核源码一之SparkContext原理解析/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark shell","date":"2017-04-16T04:47:25.239Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark_shell/","text":"启动1spark-shell 使用一个shell脚本去提交程序 12345678910111213141516171819[root@hdp-node-01 install]# cat wordcount.sh /install/spark-1.6.1-bin-hadoop2.6/bin/spark-submit \\--class cn.... \\--num-executors 3 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 3 \\xxxx.jar####上面的代码是本地模式运行,但是如果要提交到集群上需要加上--master/install/spark-1.6.1-bin-hadoop2.6/bin/spark-submit \\--class cn.... \\--master spark://node1:7077 \\--num-executors 3 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 3 \\xxxx.jar","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之部署,升级,监控应用程序","date":"2017-04-16T04:47:25.237Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之部署,审计,监控应用程序/","text":"部署 升级 监控","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之输入DStream的Transformation操作","date":"2017-04-16T04:47:25.235Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream的Transformation操作/","text":"map 对传入的每个元素,返回一个新的元素flatMap 对传入的每个元素,返回一个或多个元素filter 对传入的元素返回true或false,返回的false的元素被过滤掉union 将两个DStream进行合并count 返回元素的个数reduce 对所有的values进行聚合countByValue 对元素按照值进行分组,对每个组进行计数,最后返回格式reduceByKey 对key对应的values进行聚合cogroup 对两个DStream进行连接操作,一个key连接起来的两个RDD的数据,都会以Iterable的形式,","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之输入DStream和Receiver","date":"2017-04-16T04:47:25.234Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream和Receiver/","text":"输入DStream代表了来自数据源的输入数据流,在之前的WordCount例子中,lines就是一个输入DStream(SocketInputDStream),代表了从socket服务接收到的数据流,除了文件数据流之外,所有的输入DStream都会绑定一个Receiver对象,该对象是一个关键的组件,用来从数据源接收数据,并将其存储在spark的内存中,以供后续处理 spark Streaming提供了两种内置的数据源的支持1.基础数据源:StreamingContext API(如:StreamingContext.socketTextStream()方法)中直接提供了对这些数据源的支持,比如:文件,socket,Akka Actor等,2.高级数据源:诸如Kafka,flume,Kinesis,Twitter等书卷,通过第三方工具类提供支持,这些数据源的使用,需要引用其依赖3.自定义数据源:我们可以自己定义数据源,来决定如何接受和存储数据 输入DStream和Receiver详解 要注意的是,如果你想要在实时计算应用中并行接收多条数据流,可以创建多个输入DStream,这样就会创建多个Receiver,从而并行的接收多个数据流,但是要注意的是,一个spark Streaming Application的Executor是一个长时间运行的任务,因此,他会独占分配给spark streaming Application的cpu core,从而只要spark streaming运行起来以后,这个节点上的cpu core,就没法给其他应用使用了 使用本地模式,运行程序时,绝对不能使用local或者是local[1],因为那样的话,只会给执行输入DStream的executor分配一个线程,而spark streaming底层的原理是,至少要有两条线程,一个线程用来分配给Receiver接收数据,一条线程用来处理接收到的数据,因此必须使用local[n],n&gt;=2的模式 如果不设置Master,也就是直接将spark streaming应用提交到集群上运行,那么首先,必须要求集群节点上,有&gt;1个cpu core,其次给spark streaming的每个executor分配的core,必须&gt;1,这样,才能保证分配到executor上运行的的输入DStream,两条线程并行,一条运行Receiver,接收数据,一条处理数据,否则的话,只会接收数据,不会处理数据 因此,在实际工作中,都要给每个executor的cpu core设置超过1个即可","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之输入DStream之基础数据源","date":"2017-04-16T04:47:25.232Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之基础数据源/","text":"基础数据源1.socket:StreamingContext.socketTextStream() 2.HDFS文件StreamingContext.fileStream()基于HDFS的文件实时计算,其实就是监控一个HDFS目录,只要其中有新文件出现,就实时处理,相当于处理实时的文件流12345def fileStream[ K: ClassTag, V: ClassTag, F &lt;: NewInputFormat[K, V]: ClassTag] (directory: String): InputDStream[(K, V)] spark Streaming会监视指定的HDFS目录,并且处理出现在目录中的文件,要注意的是:1.所有放入HDFS目录中的文件,都必须有相同的格式,2.必须使用移动或者重命名的方式3.将文件移入目录,一旦处理之后,文件的内容即使改变了,也不会再处理了4.基于HDFS文件的数据源是没有Receiver的,因此不会占用一个cpu core 123456789101112131415161718192021222324252627// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(1))// 针对HDFS目录创建DStreamval lines = ssc.textFileStream(&quot;hdfs:spark1:9000/wordcount_dir&quot;)/* 其实在textFileStream底层是调用了fileStream def textFileStream(directory: String): DStream[String] = withNamedScope(&quot;text file stream&quot;) &#123; fileStream[LongWritable, Text, TextInputFormat](directory).map(_._2.toString) &#125; */// 执行WordCount逻辑val words = lines.flatMap(_.split(&quot; &quot;))val pairs = words.map((_,1))val wordcount = pairs.reduceByKey(_ + _)// 打印测试wordcount.printssc.start()ssc.awaitTermination()ssc.stop()","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之输入DStream之Kafka基础数据源","date":"2017-04-16T04:47:25.230Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之输入DStream之Kafka基础数据源/","text":"基于Receiver的方式这种方式使用Receiver来获取数据,Receiver是使用Kafka的高层次ConsumerAPI来实现的,receive从kafka中获取的数据都是存储在spark executor的内存中的,然后spark Streaming启动额job会去处理那些数据 然而,在默认的配置下,这种方式可能会因为底层的失败而丢失数据,如果要启用高可靠机制,让数据零丢失,就必须启用spark streaming 的预写日志机制(Write Ahead Log,WAL),该机制会tongue的将接收到的kafka数据写入分布式文件系统(比如HDFS)山的预写日志中,所以即使底层节点出现了失败,也可以使用预写日志中的数据进行恢复 前提:1.maven添加依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.6.3&lt;/version&gt;&lt;/dependency&gt; 2.使用第三方工具类创建输入DStream1KafkaUtils.createStream(StreamingContext,[ZK quorum], [consumer group id], [per-topic number of kakfa partitions to consume]) 注意事项:1.kafka的topic的partition,与spark中的RDD的partition是没有关系的,所以在KafkaUtils.createStream()中,提高partition的数量,只会增加一个Receiver中读取partition的线程的数量,不会增加spark处理数据的并行度2.可以创建多个kafka输入DStream,使用不同的consumer group和topic,来通过多个receiver并行接收数据3.如果基于容错的文件系统,比如HDFS,启用了预写日志机制,接收到的数据都会被复制一份到预写日志中,因此在KafkaUtils.createStream()中,设置的持久化级别是:StorageLevel.MEMORY_AND_DISK_SER_2 123456//创建topicbin/kafka-topic.sh --zookeeper zk01:2181,zk02:2181,zk03:2181 --topic WordCount --replication-factor 1 --partitions 1 --create//创建consumer生产者bin/kafka-console-producer.sh --broker-list 192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092, --topic WordCount 实例代码 123456789101112131415161718192021222324252627282930313233// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(1))// 创建针对Kafka的输入流val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;val topicThreadMap = Map( &quot;WordCount&quot;-&gt;1)// zk是zookeeper的节点地址// DefalutConsumerGroup是kafka的groupId// topicThreadMap是指定去消费哪个topic//Map of (topic_name -&gt; numPartitions) to consume. Each partition is consumed in its own thread// topic名字-&gt;分区数量,每个分区将会启动一个Receiver线程去消费(而一个Receiver需要一个cpu core)val lines = KafkaUtils.createStream(ssc,zk,&quot;DefalutConsumerGroup&quot;,topicThreadMap)// 这里需要注意的是lines中是Tuple(index,line)这样的数据,所以_._2就是一行的的数据val words = lines.flatMap(_._2.split(&quot; &quot;))val pairs = words.map((_,1))val wordcount = pairs.reduceByKey(_ + _)// 打印测试wordcount.printssc.start()ssc.awaitTermination()ssc.stop() 基于Direct的方式这种是不基于Receiver的直接方式,是在spark1.3中引入,从而能够确保更加健壮的机制,替代掉使用Receiver来接收数据后,这种方式会周期性(就是我们指定的batch的时间)的查询Kafka,来获取每个topic+partition的最新的offset,从而定义每个batch的offset的范围(而每个batch会形成一个Rdd),当处理数据的job启动时,就会使用kafka的简单consumer API来获取kafka指定offset范围的数据,这就得到了这个Rdd的数据 这种方式有如下的优点:1.简化并行读取,如果要有多个partition,不需要创建多个输入DStream然后对他们进行union操作,spark会创建跟kafka partition一样多的Rdd partition,并且会并行从kafka中读取数据,所在kafka partition和RDD partition之间,有一个一对一的映射关系2.高性能:如果要保证零数据丢失,在语句Receiver的方式中,需要开启WAL机制,这种方式其实效率低下,因为数据实际上被复制了两份,kafka自己本身就有高可靠的机制,会对数据复制一份,而这里又会复制一份到WAL中,而基于direct的方式,不依赖Receiver,不需要开启WAL机制,只要kafka中作了数的复制,那么就可以通过kafka的副本进行恢复3.一次仅且一次的事务机制基于Receiver的方式,是使用kafka的高阶API来在zookeeper中保存消费过的offset,这是消费kafka数据的传统的方式,这种方式配合着WAL机制可以保证数据零丢失的高可靠性,但是却无法保证数据被处理一次且仅一次,可能会处理两次,因为spark和zookeeper之间可能是不同步的 基于direct的方式,使用kafka的简单API,spark streaming自己会负责追踪消费的offset并保存在checkpoint中,spark自己一定是同步的,因此可以保证数据是消费一次且仅消费一次 createDirectStream()方法参数说明 1234567891011121314151617181920212223242526272829303132333435363738 * * @param ssc StreamingContext object 这里是传入的一个StreamingContext * @param kafkaParams Kafka &lt;a href=&quot;http://kafka.apache.org/documentation.html#configuration&quot;&gt; * configuration parameters&lt;/a&gt;. Requires &quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot; 必须要指定:&quot;metadata.broker.list&quot; or &quot;bootstrap.servers&quot;中的一个 * to be set with Kafka broker(s) (NOT zookeeper servers), specified in * host1:port1,host2:port2 form. //指定的格式 * If not starting from a checkpoint, &quot;auto.offset.reset&quot; may be set to &quot;largest&quot; or &quot;smallest&quot; //如果没有初始化的offset,那么从哪里开始消费(largest从头开始,smallest:从最近开始消费) * to determine where the stream starts (defaults to &quot;largest&quot;) * 如果开始消费的数据不是从checkpoint中开始的,那么使用&quot;auto.offset.reset&quot; 参数设置成&quot;largest&quot; or &quot;smallest&quot;来决定从Stream流的哪里开始消费数据 * @param topics Names of the topics to consume topic名称 * @tparam K type of Kafka message key 消息key的类型 * @tparam V type of Kafka message value 消息value的类型 * @tparam KD type of Kafka message key decoder key的编码格式 * @tparam VD type of Kafka message value decoder value的编码格式 * @return DStream of (Kafka message key, Kafka message value) */ def createDirectStream[ K: ClassTag, V: ClassTag, KD &lt;: Decoder[K]: ClassTag, VD &lt;: Decoder[V]: ClassTag] ( ssc: StreamingContext, kafkaParams: Map[String, String], topics: Set[String] ): InputDStream[(K, V)]=========================================在kafka中对auto.offset.reset参数的解释是:What to do when there is no initial offset in Kafka or if the current offset does not exist any more on the server (e.g. because that data has been deleted):当没有初始化的offset的时候,此时该从哪里读取数据earliest: automatically reset the offset to the earliest offset 设置offset为最开始的处,即从头开始消费latest: automatically reset the offset to the latest offset 从设置offset为最近的offsetnone: throw exception to the consumer if no previous offset is found for the consumer&apos;s groupanything else: throw exception to the consumer. 实例代码123456789101112131415161718192021222324252627282930val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(1))// 创建针对Kafka的输入流val zk = &quot;192.168.0.107:2181,192.168.0.108:2181,192.168.0.109:2181&quot;val kafkaParams = Map( // kafka的broker-list &quot;meta.broker.list&quot;-&gt;&quot;192.168.1.107:9092,192.168.1.108:9092,192.168.1.109:9092&quot;,)val topics = Set( &quot;WordCount&quot;)val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc,kafkaParams,topics)val words = lines.flatMap(_._2.split(&quot; &quot;))val pairs = words.map((_,1))val wordcount = pairs.reduceByKey(_ + _)// 打印测试wordcount.printssc.start()ssc.awaitTermination()ssc.stop()","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之架构原理","date":"2017-04-16T04:47:25.228Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之架构原理/","text":"StreamingContext的初始化与Receiver的启动原理 SparkStreaming之数据接收原理和源码分析 数据处理原理剖析(block与batch关系)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之性能调优","date":"2017-04-16T04:47:25.227Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之性能调优/","text":"数据接收的并行度调优通过网络接收数据时(比如kafka,flume),会将数据反序列化,并存储在spark的内存中,如果数据接收称为系统的瓶颈,那么可以考虑并行化的数据接收,每一个输入DStream都会在某个Worker的Executor上,启动一个Receiver,该Receiver接收一个数据流,因此可以通过创建多个输入DStream,并且配置他们接收数据源不同的分区数据,达到多个数据流的效果,比如说,一个接收两个kafka topic的输入DStream,可以被拆分为两个DStream,每个分别接收一个topic的数据,这样就会创建两个Receiver,从而并行的接收数据,进而提升吞吐量,读个DStream可以使用union算子进行聚合,从而形成一个DStream,然后后续的Transformation算在操作都针对一个聚合后的DStream即可 123456789101112int numStreams = 5List&lt;DStream&gt; kafkaStreams = new ArrayList&lt;DStream&gt;(numStreams)for(int i=0;i&lt;DStream; i++)&#123; kafkaStreams.add(KafkaUtils.createStream(...))&#125;unionedDStream = streamingContext.union(kafkaStreams.get(0),kafkaStreams.get(2)...)unionedDStream.print() 数据接收并行度调优,除了创建更多输入DStream和Receiver以外,还可以考虑调节block interval参数,”spark.streaming.blockInterval”可以设置block interval(默认是200ms),对于大多数Receiver来说,在将接收到的数据保存到Spark的BlockManager之前,都会将数据切分为一个一个的block,而每个batch中的block数量,则决定了该batch对应的RDD的partition的数量,以及针对该RDD执行Transformation操作时,创建的task的数量,每个batch对应的task数量是可以大约估计的:batch interval / (block interval) 例如说:batch interval为2s,block interval为200ms,会创建10个task,如果你认为每个batch的task的数量太少,即低于每台机器的cpu core数量,那么就说明batch的task数量是不够的,因为所有的cpu资源无法完全被利用起来,要为batch增加block的数量,那么就减小block interval,而然,推荐的block interval最小值是50ms,如果低于这个数值,那么大量task的启动时间,可能会变成一个性能开销点 除了上述说的两个提升设局接收并行度的方式,还有一种方法,技术显示的对输入数据流进行重分区,使用inputStream.reparation(num of partitions)即可,这样就可以将接收到的batch,分布到指定的数量的机器上,然后再进行进一步的操作 任务启动调优如果每秒钟启动的task过多,比如每秒启动50个,那么发送这些task到Worker节点上的Executor的性能开销会比较大,而且此时基本就很难达到毫秒级的延迟了,使用下面的操作可以减少这方面的性能开销;1.Task序列化:使用Kryo序列化类库来序列化task,可以减小task的大小,从而减少发送这些task到各个Worker节点上的Executor的时间2.执行模式:在Strandalone模式下,运行spark,可以达到更少的task启动时间 数据处理的并行度调优如果在计算的任何stage中使用并行task的数量没有足够多,那么集群资源时无法被充分利用的,举例说:对于分布式的reduce操作,比如reduceByKey和reduceByKeyAndWindow,默认的并行task的数量是由”spark.default.parallelism”参数决定的,你可以在reduceByKey等操作中,传入第二个参数,手动指定该参数的并行度,也可以调节全局的”spark.default.parallelism”参数 数据序列化的调优数据序列化造成的系统开销可以由序列化的优化来减小,在流式计算的场景下,有两种类型的数据需要序列化:1.输入数据,默认情况下,接收到的输入数据,是存储在Executor的内存中的,使用的持久化级别是StorageLevel.MEMORY_AND_DISK_SER_2,这意味着,数据被序列化为字节从而减少GC开销,并且会复制以进行Executor失败的容错,因此数据首先会存储在内存中,然后在内存不足时会溢写到磁盘上,从而为流式计算来保存所有需要的数据,这里的序列化有明显的性能开销—Receiver必须反序列化从网络接收到的数据,然后再使用spark的序列化格式序列化数据 3.流式计算操作生成的持久化RDD,流式计算操作生成的持久化RDD可能会持久化到内存中,例如:窗口操作默认就会将数据持久化在内存章,因为这些数据后面可能会在多个窗口中被使用,并被处理多次,然而,不像spark core的默认持久化级别,StorageLevel.MEMORY_ONLY,流式计算操作生成的RDD的默认持久化级别是:StorageLevel.MEMORY_ONLY_SER,默认就会减小GC开销 在上述的场景中,使用Kryo序列化类库可以减小cpu和内存的性能开销,使用Kryo时,一定要考虑注册自定义的类,并且禁用对应引用的tracking(spark.Kryo.referenceTracking) 在写特殊的场景下,比如需要为流式应用保持的数据总量并不是很多,也许可以将数据以非序列化的方式进行持久化,从而减少序列化和反序列化的cpu开销,而且又不会有太昂贵的GC开销,那么你可以考虑通过显示的设置持久化级别,来禁止持久化时对数据进行序列化,这样就减少用于序列化和反序列化的cpu性能开销,并且不用承担太多的gc开销 batch interval如果想让一个运行在集群上的spark streaming应用程序可以稳定,他就必须尽可能快的处理接收到的数据,换句话说,batch应该在生成之后,就尽可能的处理掉,对于一个应用来说,可以通过观察spark UI上的batch的处理时间来定,batch处理时间必须小于batch interval时间,不然上一个batch还没有处理成功,那么下一个batch就来了,这样会造成数据堆积 基于流式计算的本质,batch interval对于,在固定集群资源条件下,应用能保持的数据接收速率,会有巨大的影响,例如:在WordCount例子中,对于一个特定的数据接收速率,应用业务可以保证每2秒打印一次单词计数,而不是每500ms,因为batch interval 需要被设置的让与其的数据接收速率可以在生产环境中保持住 为你的应用计算正确的batch大小的比较好的方法,是在一个很保守的batch interval ,比如5-10s,以很慢的数据接收速率进行测试,要检查应用是否跟得上这个数据速率,可以检查每个batch的处理时间的延迟,如果处理时间与batch interval基本吻合,那么应用就是稳定的,否则,如果batch调度的延迟持续增加,那么就意味着无法跟得上这个速率,也就是不稳定的,因此,你要想有一个稳定的配置,可以尝试提升数据处理的速度,或者增加batch interval,记住,由于临时性的数据增长导致的暂时的延迟,可以合理的,只要延迟情况可以在短时间内恢复即可 内存调优Transformation操作会决定你的内存的使用:spark streaming应用需要的集群内UC你资源,是由使用的Transformation操作类型决定的,举例来说,如果想要使用一个窗口长度为10分钟的window操作,那么集群就必须有足够的内存来保存10分钟的数据,如果想要使用updateStateByKey来维护许多key的state,那么你的内存资源就必须足够大,返货来说,如果想要做一个简单的map-filter-sotre操作,那么需要使用的内存就很少 通常来说,通过Receiver接收到的数据,会使用StorageLevel.MEMORY_AND_DISK_SER_2持久化级别来进行存储,因此无法保存在内存中的数据会溢写到磁盘上,而溢写到磁盘上,是会降低应用的性能的,因此,通常是建议为应用提供他需要的足够的内存资源,建议在一个小规模的场景下测试内存的使用量,并进行评估 内存调优的另外一个方面是垃圾回收,对于流式应用来说,如果要获得低延迟的,肯定不想要有因为JVM垃圾回收导致的长时间延迟,有很多参数可以帮助降低内存使用和GC开销:1.DStream的持久化级别:输入数据和某些操作产生的中间RDD,默认持久化时都会序列化为字节,与非序列化的方式相比,这会降低内存和GC开销,使用Kryo序列化机制可以进一步减少内存使用和GC开销,进一步降低内存使用率,可以对数据进行压缩,由”spark.rdd.compress”参数控制(默认false) 2.清理旧数据:默认情况下,所有输入数据和通过DStream Transformation操作生成的持久化的RDD,会自动被清理,spark streaming会决定何时清理这些数据,取决于Transformation操作类型,例如:你在使用窗口长度为10分钟的window操作,spark会保持10分钟以内的数据,时间过了以后会清理旧数据,但是在某些特殊场景下,比如spark sql和spark streaming整合使用时,在异步开启的线程中,使用spark streaming针对batch RDD进行执行查询,那么就㤇让spark 保持更长时间的数据,知道sparksql查询结束,可以使用:streamingContext.remember()方法来实现 3.CMS垃圾回收:使用并行的mark-sweep垃圾回收机制,被推荐使用,用来保持GC开销,虽然并行的GC会降低吞吐量,但是还是建议使用它,来减少batch的处理时间(降低处理过程中的gc开销),如果要使用,那么要在driver端和Executor端都开启,在spark-submit中使用–driver-java-options设置,使用spark.executor.extra.javaOptions参数设置XX:+UseConMarkSweepGC","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之实时wordcount程序开发","date":"2017-04-16T04:47:25.225Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之实时wordcount程序开发/","text":"12345678910111213141516171819202122232425262728293031323334353637// local后面必须跟一个方括号,里面填写一个数字,代表了用几个线程来执行我们的spark streaming程序val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(1))// 首先,创建输入DStream,代表了一个从数据源(kafka,socket)来的持续不断的实时数据流// 这里创建的数据源是socket:参数:监听的主机和端口val lines = ssc.socketTextStream(&quot;localhost&quot;, 9999)// 返回的是一个DStream,表示每隔一秒会有一个RDD,其中封装了这一秒发送过来的数据// RDD的元素类型为String,即一行一行的文本// 开始对接收到的数据,对DStream执行算子操作// 在底层实际上会对DStream中的一个一个的RDD,执行我们应用在DStream上的算子// 产生的新的RDD会作为新DStream中的RDDval words = lines.flatMap(_.split(&quot; &quot;))val pairs = words.map((_,1))val wordCounts = pairs.reduceByKey(_ + _)// 可以看到spark streaming开发程序和spark core很像// 因为DStream是对Rdd的封装,那么DStream操作,就是对Rdd的操作// 休眠,打印(测试用)Thread.sleep(50000)wordCounts.printssc.start()ssc.awaitTermination()/*总结:1.每秒钟发送到指定socket端口中的数据,都会被lines DStream接收到2.lines DStream会把每秒的数据,也就是一行一行的文本,诸如&quot;hello world&quot;, 封装成一个RDD3.然后对每秒钟中对应的RDD执行后续的一系列的算子操作4.最终就得到了每秒钟发送过来的单词统计5.可以将最后计算出的wordcount中的一个一个的RDD,写入外部的缓存,或者持久化DB */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之基本工作原理","date":"2017-04-16T04:47:25.223Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之基本工作原理/","text":"接收实时输入数据流,然后将数据拆分成多个batch,比如每收集1s的数据封装为一个batch,然后将每个batch交给spark的计算引擎进行处理,最后会产生出一个结果数据流,其中的数据,也是由一个一个的batch所组成的 DStream Spark Streaming提供了一种高级的抽象,叫做Dstream(Discretized Stream 离散流),他代表了一个持续不断的数据流,DStream可以通过输入数据源来创建,比如:Kafka,Flume,Kinesis,也可以通过对其他DStream应用高阶函数来创建,比如:map,reduce,join,window DStream的内部,其实是一系列持续不断产生的RDD,DStream中的每个RDD都包含了一个时间段内的数据 对DStream应用的算子,比如map,其实在底层会被翻译为对DStream中每个RDD的操作,比如对一个DStream执行一个map操作,会产生一个新的DStream,但是,在底层,其实其原理为,对输入DStream中每个时间段的RDD,都应用一遍map操作,然后生成新的RDD,即作为新的DStream中的那个时间段的一个RDD,底层的RDD的Transformation操作,其实,还是由spark core的计算引擎来实现的,spark Streaming对spark core进行了一层封装,隐藏了细节,然后对开发人员提供了方便易用的高层次的API","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之介绍","date":"2017-04-16T04:47:25.221Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之介绍/","text":"SparkStreaming其实就是一种spark提供的一种实时计算框架,他的底层组件或者概念,其实还是最核心的RDD,只不过,针对实时计算的特点,在RDD之上,进行了一层封装,叫做Dstream,就像spark sql针对数据查询应用提供了一种基于RDD之上的全新的概念叫DataFrame一样 spark Streaming是spark core API的一种扩展,他可以用于进行大规模,高吞吐,容错的实时数据流的处理,他支持从多种数据源中消费数据,比如:kafka,flume,Twitter,ZeroMQ,或者TCP Socket,并且能够使用类似高阶函数的复杂算法来进行数据处理,比如:map,reduce,join,和window,处理后的数据可以被保存到文件系统,数据库等存储中","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之与缓存与持久化机制","date":"2017-04-16T04:47:25.219Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与缓存与持久化机制/","text":"与RDD类似,spark streaming也可以让开发人员手动控制,将数据流中的数据持久化到内存中,对DStream调用persist()方法,就可以让spark streaming自动将该数据流中的所有产生的RDD,都持久化到内存中 如果要对一个DStream多次执行操作,那么对DStream持久化是非常有用的,因为多次操作,可以共享使用内存中的一份缓存数据,对于基于窗口的操作,比如reduceByKeyAndWindow,以及基于状态的操作,比如updateStateByKey,默认就隐式开启了持久化的机制,即spark streaming默认就会将上述操作产生的数据,缓存到内存中,不需要开发人员手动调用persist()方法 对于通过网络接收数据的输入流,比如:socket,kafka,flume等,默认的持久化级别是将数据复制一份,以便于容错 与RDD不同的是,默认的持久化级别,统一都是要序列化的","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之与Storm的对比分析","date":"2017-04-16T04:47:25.217Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与Storm的对比分析/","text":"sparkStreaming与storm的对比 sparkStreaming与storm的优劣分析 事实上,spark streaming绝对谈不上比Storm优秀,这两个框架在实时计算领域中,都很优秀,只是擅长的细分场景并不相同 spark streaming仅仅在吞吐量上比storm要优秀,而吞吐量这一点,也是历来挺spark streaming的人着重强调的,但是问题是,并不是所有的实时计算场景下,都那么注重吞吐量,因此,通过吞吐量说spark streaming强于storm并不能说服人 事实上,storm在实时延迟度上,比spark streaming就好多了,前者是纯实时的,但是后者是准实时的,而且,storm的事务机制,健壮性,容错性,动态调整并行度等特性,都要比spark streaming更加优秀 spark streaming有一点是storm绝对比不上的,就是:它位于spark生态技术栈中,因此,spark streaming可以和spark core,spark sql无缝整合,也就意味着,我们可以对实时处理出来的中间数据,立即在程序中无缝进行延时批处理,交互式查询等操作,这个特点大大增强了spark streaming的优势和功能 spark streaming与storm的应用场景对于storm来说:1.建议那种需要纯实时,不能忍受1秒以上延时的场景下使用,比如实时金融系统,要求纯实时进行金融交易和分析2.此外,如果对于实时计算的功能中,要求可靠的事务机制,即数据的处理完全精准,一条也不能少,一条也不能多,那么可以考虑storm3.如果还需要针对高峰低峰时间段,动态调整实时计算程序的并行度,以最大限度利用集群资源(通常是小型公司,集群资源紧张的情况),也可以考虑storm4.如果一个大数据应用系统,他就是纯粹的实时计算,不需要在中间执行sql交互式查询,复杂的Transformation算子等,那么用storm是比较好的选择 对于spark streaming来说:1.如果对上述适用于storm的三点,一条都不满足的实时场景,即:不要求纯实时,不要求强大可靠的事务机制,不要求动态调整并行度,那么可以考虑使用spark streaming2.考虑使用spark streaming最主要的一个因素,应该是针对整个项目进行宏观的考虑,可能还会牵扯到高延迟批处理,交互式查询等功能,那么就应该首先spark生态,用spark core开发离线批处理,用spark sql开发交互式查询,用spark streaming开发实时计算,三者可以无缝整合,给系统提供非常高的可扩展性","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之与Spark sql结合使用","date":"2017-04-16T04:47:25.216Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之与Spark sql结合使用/","text":"spark Streaming 最强大的地方在于,可以与spark core, spark sql整合使用的,下面来看看,如何将DStream中的RDD与spark sql结合起来使用 案例:每隔10秒,统计最近60秒的,每个种类的每个商品的点击次数,然后统计出每个种类的top3热门商品 实现:1.每隔10秒,统计最近60秒的,每个种类的每个商品的点击次数:用到的是reduceByKeyAndWindow2.统计出每个种类的top3热门商品:用到的是DStream.foreachRDD然后对每次生成的窗口中的RDD进行sql查询,取top3 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(5))// 输入日志的格式:username product1 catatory// zhangsan iphone mobile_phoneval productClickLogsDStream = ssc.socketTextStream(&quot;spark1&quot;, 9999)// 要统计:每个种类的每个商品的点击次数,所以我们将数据格式转成:(category_product,1)// 然后使用window操作,对窗口中的数据进行reduceByKey// 从而统计出一个窗口中的每个种类的每个商品的点击次数val productClickLogsPairs = productClickLogsDStream.map&#123; line=&gt; val arr = line.split(&quot; &quot;) (arr(2)+&quot;_&quot;+arr(1),1)&#125;// 计算60s内每个种类的每个商品的点击次数统计val categoryProductCountsDStream = productClickLogsPairs.reduceByKeyAndWindow((x:Int,y:Int)=&gt;x+y, Durations.seconds(60),Durations.seconds(10))//val categoryProductCountRowRdd = categoryProductCountsDStream.foreachRDD&#123; categoryProductCountsRdd=&gt; val rowRdd = categoryProductCountsRdd.map&#123; categoryProductCount=&gt; val (category,product) = categoryProductCount._1.split(&quot;_&quot;).toVector val count = categoryProductCount._2 Row(category,product,count) &#125; val structType = StructType(Array( StructField(&quot;category&quot;, StringType, true), StructField(&quot;product&quot;, StringType, true), StructField(&quot;click_count&quot;, IntegerType, true) )) val sqlContext = new HiveContext(rowRdd.sparkContext) val categoryProductCountsDF = sqlContext.createDataFrame(rowRdd,structType) categoryProductCountsDF.registerTempTable(&quot;product_click_log&quot;) // 使用spark sql执行top3热门商品的统计 val top3ProductDF = sqlContext.sql(&quot;&quot; + &quot;select category, product, click_count&quot; + &quot;from (&quot; + &quot; select &quot; + &quot; category,&quot; + &quot; product,&quot; + &quot; click_count,&quot; + &quot; row_number() over (partition by category order by click_count desc) rank&quot; + &quot; from product_click_log&quot; + &quot;) tmp&quot; + &quot;where rank &lt;= 3&quot;) // 将数据保存到redis或者db中,然后在web中对数据进行展示 // 但是这里只是测试,所以用show top3ProductDF.show&#125;ssc.start()ssc.awaitTermination()ssc.stop() spark-submit123456789/usr/local/spark/bin/spark-submit \\--class cn.xxx.Top3HotProduct--num-executor 3 \\--driver-memory 100m \\--execuotr-memory 100m \\--executor-cores 3 \\--files /usr/local/hive/conf/hive-site.xml--driver-class-path /usr/local/hive/lib/mysql-connector-java-5.1.17.jar \\/user/xx/spark-study-test.jar \\","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之window滑动窗口","date":"2017-04-16T04:47:25.214Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之window滑动窗口/","text":"spark streaming提供了滑动窗口操作的支持,从而让我们可以对一个滑动窗口内的数据执行计算操作,每次掉落在窗口内的RDD的数据,会被聚合起来执行计算操作,然后生成的RDD,会作为window DStream的一个RDD,比如下图中,就是对每三秒钟的数据执行一次滑动窗口计算,这3秒内的3个RDD会被聚合起来进行处理,然后过了2秒钟,又会对最近3秒内的数据执行滑动窗口计算,所以每个滑动窗口操作,都必须指定2个参数,窗口长度以及滑动间隔,而且这两个参数值都必须是batch间隔的整数倍 window滑动窗口的操作 Transformation 含义 window 对每个滑动窗口的数据执行自定义的计算 countByWindow 对每个滑动窗口的数据执行count操作 reduceByWindow 对每个滑动窗口的数据执行reduceByKey操作 reduceByKeyAndWindow 对每个滑动窗口的数据执行reduceByKey操作 countByValueAndWindow 对每个滑动窗口的数据执行countByValue操作 案例:热点搜索词滑动统计,每隔5秒钟,统计最近10秒钟的搜索词的搜索频次,并打印出来排名最靠前的3个搜索词以及出现次数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(5))// 搜素日志的格式: username searchWordval searchLog = ssc.socketTextStream(&quot;spark1&quot;, 9999)// (searchWord,1)val searchWordPairDStream = searchLog.map(_.split(&quot; &quot;)(2)).map((_,1))/*第一个参数是:reduceByKey中的需要指定的函数第二个参数是:窗口长度,这里是60秒第三个参数:滑动间隔,这是是10秒也就是说:每隔10秒钟,将最近60秒的数据,作为一个窗口,进行内部的RDD的聚合,统一成一个RDD,然后进行后续计算每隔10秒钟,就会滑动一下,会将之前60秒的RDD(因为一个batch的间隔是5秒,所以之前60秒就有12个RDD)给聚合起来统一执行reduceByKey操作,所以这里的reduceByKeyAndWindow是针对每隔窗口执行计算的,而不是针对某个DStream中的RDD */val searchWordCountsDStream = searchWordPairDStream.reduceByKeyAndWindow((x:Int,y:Int)=&gt;x+y,Durations.seconds(60),Durations.seconds(10))/* @param reduceFunc associative and commutative reduce function @param windowDuration width of the window; must be a multiple of this DStream&apos;s batching interval(必须是batch的整数倍) @param slideDuration sliding interval of the window */// 执行transform,因为一个窗口就是一个60秒钟的数据,会变成一个RDD,// 然后对这一个RDD根据每个搜索词出现的频率进行排序,// 然后获取排名前3的热点搜索词val finalDStream = searchWordCountsDStream.transform&#123; searchWordCountsRdd=&gt; // 执行搜索词和频率的反转,格式为:(count, searchWord) val searchCountsWordRdd = searchWordCountsRdd.map(t=&gt;(t._2, t._1)) val searchCountsWordSortedRdd = searchCountsWordRdd.sortByKey(false) // 格式为:(searchWord, count) val searchWordCountsSortedRdd = searchCountsWordSortedRdd.map(t=&gt;(t._2, t._1)) // 然后take(),获取排名前3的特点搜索词 val top3SearchWordCounts = searchWordCountsSortedRdd.take(3) top3SearchWordCounts.foreach(println) null&#125;// 触发finalDStream.printssc.start()ssc.awaitTermination()ssc.stop()/*其实window函数就是对一个时间段中的数据进行统计,比如:看过去60秒钟内的热点搜索词 */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之updateStateByKey和WordCount全局统计","date":"2017-04-16T04:47:25.213Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之updateStateByKey/","text":"updateStateByKey操作,可以让我们为每个key维护一份state,并持续不断的更新该state1.首先,定义个state,可以是任意的数据类型2.其次,要定义state更新函数—指定一个函数如何使用之前的state和新值来更新state 对于每个batch,spark都会为每个之前已经存在的key去应用一次state更新函数,无论这个key在batch中是否有新的数据,如果state更新函数返回none,那么key对应的state就会被删除 当然,对于每个新初出现的key,也会执行state更新操作 注意,updateStateByKey操作,要求必须开启checkpoint机制 案例:基于缓存的实时WordCount程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(1))// 如果要使用updateStateByKey算子,就必须设置一个checkpoint目录,// 这样便于在内存数据丢失的时候,可以从checkpoint中恢复数据ssc.checkpoint(&quot;hdfs://spark1:9000/wordcount_checkpoint&quot;)val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x))) iter.flatMap &#123; case (x, y, z) =&gt; Some(y.sum + z.getOrElse(0)).map(i =&gt; (x, i)) &#125;&#125;val lines = ssc.socketTextStream(&quot;localhost&quot;,9999)val pairs = lines.flatMap(_.split(&quot; &quot;)).map((_,1))// 在之前的WordCount中,是直接使用pairs.reduceByKey// 得到的是每个时间段的batch对应的RDD,这样计算出来的是那个时间段的单词计数// 但是如果我们想要统计每个单词的全局的计数呢?// 就是说:统计出来从程序启动开始,到现在为止,统计一个单词出现的次数,那么之前的方式就不好实现了// 就必须基于redis缓存,或者mysql来实现累加// 但是我们的updateStateByKey就可以维护一份每个单词的全局的统计次数/*实际上,对于每个单词,每次batch的时候,都会调用这个函数第一个参数values:相当于这个batch中,这个key的新的值,可能有多个比如说:(hello,1) (hello,1),那么传入的是(1,1)第二个参数state:就是指的是这个key之前的状态,其中的泛型的类型是自己指定的 */val func2 = (values:Seq[Int], state:Option[Int])=&gt;&#123; val newValue = state.getOrElse(0)//之前的状态不存在返回0 Option(newValue + values.sum)//将本次新出现的值求和,然后再和state的值相加,就是这个key目前的全局统计&#125;val wordCounts = pairs.updateStateByKey(func2)//其实内部就是调用的是下面的一种方式,只不过使用func2的方法更加的简洁//val wordCounts = pairs.updateStateByKey(updateFunc,new HashPartitioner(ssc.sparkContext.defaultParallelism), true)/*默认的情况:new HashPartitioner(ssc.sparkContext.defaultParallelism) 是指定分区函数,默认就是使用的是HashPartitionertrue:Whether to remember the partitioner object in the generated RDDs. */wordCounts.printssc.start()ssc.awaitTermination()ssc.stop()","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之transform","date":"2017-04-16T04:47:25.211Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之transform/","text":"transform操作,应用在DStream上时,可以用于执行任意的RDD到RDD的转换操作,他可以用于实现,DStream API中所没有的操作,比如说,DStream API中,并没有提供将一个DStream中的每个batch,与一个特定的RDD进行join的操作,但是我们自己就可以使用transform操作来实现该功能 DStream.join(),只能join其他DStream,在DStream每个batch的RDD计算出来之后,回去跟其他DStream的RDD进行join 案例:实时黑名单过滤 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;) // 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1) val ssc = new StreamingContext(conf,Seconds(1)) // 构造模拟数据:黑名单RDD val blackListData = Array( (&quot;tom&quot;, true) ) val blckListRdd = ssc.sparkContext.parallelize(blackListData) // 日志的格式为:date username val adsChickLogDStream = ssc.socketTextStream(&quot;spark1&quot;, 9999) val userAdsClickLogDStream = adsChickLogDStream.map&#123; line=&gt; // (username, line) (line.split(&quot; &quot;)(1), line) &#125; // 执行transform操作,将每个batch的RDD,与黑名单RDD进行join,filter,map等操作 // 实时黑名单过滤 val validAdsClickLogDstream = userAdsClickLogDStream.transform( userAdsClickLogRdd=&gt;&#123; // (leo, (&quot;20150101 leo&quot;, None)) // (tom, (&quot;20150101 tom&quot;, Some(true)) // leftOuterJoin[W](other: RDD[(K, W)]): RDD[(K, (V, Option[W]))] val joinedRdd = userAdsClickLogRdd.leftOuterJoin(blckListRdd) val filterRdd = joinedRdd.filter(// tuple=&gt;&#123;// if(tuple._2._2.getOrElse(false))&#123;// false// &#125;else&#123;// true// &#125;// &#125; !_._2._2.getOrElse(false) ) // 将:&quot;20150101 leo&quot;这一条数据返回 filterRdd.map(_._2._1) &#125; ) // 写入kafka等消息中间件, // 然后再开发一个专门的后台服务,作为广告计费服务,执行实时的广告计费,这里就拿到了有效的广告点击 validAdsClickLogDstream.print ssc.start() ssc.awaitTermination() ssc.stop() /* 其实使用transform,会将DStream中batch的每个RDD与指定的Rdd进行操作 */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之StreamingContext详解","date":"2017-04-16T04:47:25.209Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之StreamingContext详解/","text":"有两种创建StreamingContext 1234567//通过SparkConf直接创建val conf = new SparkConf().setAppName(&quot;Streaming&quot;).setMaster(&quot;local[2]&quot;)val ssc = new StreamingContext(conf,Seconds(1))//或者:通过SparkContext创建val sc = new SparkContext(conf)val ssc = new StreamingContext(sc,Durations.seconds(1)) 一个StreamingContext定义之后,必须做以下几件事情:1.通过创建输入DStream来创建输入数据源2.通过对DStream定义Transformation和output算子操作,来定义实时计算逻辑3.调用StreamingContext的start()方法,来开始实时处理数据4.调用StreamingContext的waitTermination()方法来等待应用程序的终止,可以使用 Ctrl+c手动终止,或者就是让他持续不断的运行进行计算5.也可以通过调用StreamingContext的stop()方法来停止应用程序的运行 需要主要的要点:1.只要一个StreamingContext启动之后,就不能再往其中添加任何计算逻辑了,比如执行了start()方法之后,还给某个DStream执行一个算子,这样是无效的,所以一般start()方法之后就是waitTermination()方法,然后后面就没有任何执行逻辑了2.一个StreamingContext停止之后,是肯定不能够重启的,调用stop()之后,不能再调用start()3.一个JVM同时只能有一个StreamingContext启动,在你的应用程序中,不能创建两个StreamingContext4.调用stop()方法时,会同时停止内部的SparkContext,如果不希望如此,还希望后面继续使用SparkContext创建其他类型的Context,比如SQLContext,那么就使用stop(false)5.一个SparkContext可以创建多个StreamingContext,只要上一个先用stop(false)停止,在创建下一个即可","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之DStream的output操作","date":"2017-04-16T04:47:25.208Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之DStream的output操作/","text":"DStream的output操作 Output Meaning print 打印每个batch中的前10个元素,主要用于测试,或者是不需要执行什么output操作时,用于简单的触发一下job saveAsTextFile(prefix,[suffix]) 将每个batch的数据保存到文件中,每个batch的文件的命名格式为:prefix-TIME_IN_MS[.suffix] saveAsObjectFile 同上,但是将每个batch的数据以序列化对象的方式,保存到SequenceFile中 saveAsHadoopFile 同上,将数据保存到hadoop文件中 foreachRDD 最常用的output操作,遍历DStream中的每个产生的RDD,进行处理,可以将每个RDD中的数据写入外部村粗,比如:文件,数据库,缓存等,通常在其中,是针对RDD执行action操作的,比如foreach DStream中的所有的计算,都是由output操作触发的,比如print(),如果没有任何output操作,那么,压根儿就不会执行定义的计算逻辑 此外,即使你使用了foreachRDD output操作,也必须在里面对RDD执行action操作,才能触发对每一个batch的计算逻辑,否则,光有foreachRDD output操作,在里面没有对RDD执行action操作,也不会触发任何的逻辑 foreachRDD详解通常在foreachRDD中,都会创建一个Connection,比如 JDBC Connection,然后通过Connection将数据写入外部的存储 误区一:在RDD的foreach操作外部,创建Connection这种方式是错误的,因为他会导致Connection对象呗序列化后传输到每个Task中,而这种Connection对象,实际上一般是不支持序列化的,也就无法被传输 代码如:12345678dstream.foreachRDD&#123; rdd=&gt; val connection = createNewConnection() rdd.foreach&#123; record=&gt; connection.send(record) &#125;&#125; 误区二:在RDD的foreach操作内部,创建connection这种方式是可以的,但是效率低下,因为他会导致对于RDD中的每一条数据,都会创建一个connection对象,而通常来说,connection的创建是很消耗性能的 代码如:123456789dstream.foreachRDD&#123; rdd=&gt; rdd.foreach&#123; record=&gt; val connection = createNewConnection() connection.send(record) connection.close &#125;&#125; 合理方式一:使用RDD的foreachPartition操作,并且在该操作内部,创建Connection对象,这样就相当于为RDD的每个Partition创建一个Connection对象,节省了资源 123456789dstream.foreachRDD&#123; rdd=&gt; rdd.foreachPartition&#123; partitionRecords=&gt; val connection = createNewConnection() partitionRecords.foreach(record=&gt;connection.send(record)) connection.close &#125;&#125; 合理方式二:较”合理方式一”更好自己手动封装一个静态连接池,使用RDD的foreachPartition操作,并且在该操作内部,从静态连接池中,通过静态方法,获取到一个连接,使用之后再还回去,这样额话,甚至在多个RDD的partition之间,也可以复用连接了,而且可以让连接池采取懒创建的策略,并且空闲一段时间后,将其释放掉 123456789dstream.foreachRDD&#123; rdd=&gt; rdd.foreachPartition&#123; partitionRecords=&gt; val connection = ConnectionPool.getConnection() partitionRecords.foreach(record=&gt;connection.send(record)) ConnectionPool.returnConnection(connection) &#125;&#125; 案例:统计全局的WordCount计数,并写入到mysql中 建表123456create table wordcount( id integer auto_increment primary key, updated_time timestamp NOT NULL default CURRENT_TIMESTAMP on update CURRENT_TIMESTAMP, word varchar(255), count integer) 代码12345678910111213141516171819202122232425262728293031323334353637383940414243val conf = new SparkConf() .setAppName(&quot;Streaming&quot;) .setMaster(&quot;local[2]&quot;)// 每收集多长时间的数据就划分为一个batch进行处理,这里设置为1秒:Seconds(1)val ssc = new StreamingContext(conf,Seconds(5))val line = ssc.socketTextStream(&quot;spark1&quot;, 9999)val pairDStream = line.flatMap(_.split(&quot; &quot;)).map((_,1))//updateFunc: (Seq[V], Option[S]) =&gt; Option[S]val func = (values:Seq[Int], state:Option[Int])=&gt;&#123; val sum = values.sum + state.getOrElse(0) Option(sum)&#125;// 全局统计val wordCount = pairDStream.updateStateByKey(func)// 每次得到当前所有单词的统计次数之后,将其写入mysql,以便后续的J2EE应用程序进行web展示wordCount.foreachRDD&#123; wordCountRdd=&gt; wordCountRdd.foreachPartition&#123; partitionRecord=&gt; // 获取连接 val conn = ConnectionPool.getConnection() // 遍历partition中的数据,使用一个连接,插入数据到mysql while(partitionRecord.hasNext)&#123; val (word, count ) = partitionRecord.next() val sql = &quot;insert into wordcount(word,count) values(?, ?)&quot; val ps = conn.prepareStatement(sql) ps.setString(1,word) ps.setInt(2,count) ps.executeUpdate() &#125; // 还回去连接 ConnectionPool.returnConnection(conn) &#125;&#125;ssc.start()ssc.awaitTermination()ssc.stop() 连接池代码12345678910111213141516171819202122232425262728293031package org.dt.sparkimport java.sql.&#123;Connection, DriverManager&#125;import scala.collection.mutableobject ConnectionPool &#123; val connectionQueue = new mutable.LinkedList[Connection]() // 加载驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;) // 获取连接,多线程并发访问控制 def getConnection(): Connection =&#123; if(connectionQueue==null)&#123; for(i&lt;-1 to 10)&#123; val conn = DriverManager.getConnection(&quot;jdbc:mysql://spark1:3306/testdb&quot;, &quot;root&quot;, &quot;root&quot;) connectionQueue.push(conn) &#125; &#125; connectionQueue.poll() &#125; def returnConnection(conn: Connection): Unit =&#123; connectionQueue.push(conn) &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkStreaming之checkpoint机制","date":"2017-04-16T04:47:25.206Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkStreaming之checkpoint机制/","text":"每一个spark streaming应用,正常来说,都是要7*24小时运转的,这就是实时计算程序的特点,因为要持续不断的对数据进行计算,因此,读实时计算应用的要求,应该是必须要能够对与应用程序逻辑无关的失败进行容错 如果要实现这个目标,spark streaming程序就必须将足够的信息checkpoint到容错的存储系统上,从而让他能够从失败中进行恢复,有两种数据需要被进行checkpoint 1.元数据checkpoint—将定义了流式计算逻辑的信息,把偶才能到容错的存储系统上,如HDFS,当运行spark streaming应用程序的driver进程所在节点失败时,该信息可以用于进行行恢复,元数据信息包括了1.1.配置信息—创建spark streaming的应用程序的配置信息,比如sparkConf中的信息1.2.DStream的操作信息—定义了spark stream应用程序的计算逻辑的DStream操作信息1.3.未处理的batch信息—那些job正在排队,还没处理的batch信息 2.数据checkpoint—-将实时计算过程中产生的RDD的数据保存到可靠的存储系统中 对于一些存在多个batch的数据进行聚合的,有状态的Transformation操作,这是非常有用的,在这种Transformation操作中,生成的RDD是依赖于之前的batch的RDD的,这会导致随着时间的推移,RDD的依赖链条变得越来越长 要避免由于依赖链条越来越长,导致的一起变得越来越长的失败恢复时间,有状态的Transformation操作执行过程中间产生的RDD,会定期被checkpoint到可靠的存储系统上,比如HDFS,从而削减RDD的依赖链条,进而缩短失败恢复时,RDD的恢复时间 一句话概括,元数据checkpoint主要是为了从driver失败中进行恢复,而RDD的checkpoint主要是为了使用到有状态的Transformation操作时,能够在其生产出的数据丢失时,进行快速恢复 何时启用checkpoint机制?1.使用了有状态的Transformation操作–比如updateStateByKey,或者reduceByKeyAndWindow操作,被使用了,那么checkpoint目录要求是必须提供的,也就是必须开启checkpoint机制,从而进行周期性的RDD checkpoint 2.要保证可以从Driver失败中进行恢复—-元数据checkpoint需要启用,来进行 这种情况的恢复 要注意的是,并不是说,所有的spark streaming应用程序,都要启用checkpoint机制,如果即不强制要求从Driver失败中自动进行恢复,又没有有状态的Transformation操作,那么就不需要启用checkpoint,事实上,这么做反而有助于提升性能 如何启用checkpoint机制?1.对于有状态的Transformation操作,启用checkpoint机制,定期将其产生的RDD数据checkpoint,是比较简单的 可以通过配置一个容错的,可靠的文件系统(比如HDFS)的目录,来启用checkpoint机制,checkpoint数据就会写入该目录,使用StreamingContext的checkpoint()方法即可,然后,你就可以放心使用有状态的Transformation操作了 2.如果为了要从Driver失败中进行恢复,那么启用checkpoint机制,是比较复杂的,需要改写spark streaming应用程序 当应用程序第一次启动的时候,需要创建一个新的StreamingContext,并且调用其start()方法,进行启动,当Driver从失败中恢复过来的时候,需要从checkpoint目录中记录的元数据中,恢复出来一个StreamingContext 代码123456789101112131415161718192021def func2CreateConext():StreamingContext = &#123; val ssc = new StreamingContext(...) val lines = ssc.socketTextStream(...) //我们要写的代码逻辑 ssc.checkpoint(checkpointDir) ssc&#125;val context =StreamingContext.getOrCreate(checkpointDir,func2CreateConext _)context.start()context.awaitTermination()/*在func2CreateConext中的ssc.checkpoint(checkpointDir)中的checkpointDir和val context =StreamingContext.getOrCreate(checkpointDir,func2CreateConext _)的目录名要一致*/ 按照上诉方法,进行spark streaming应用程序的重写后,当第一次运行程序时,如果发现checkpoint目录不存在,那么就使用定义的函数来第一次创建一个StreamingContext,并将其元数据写入checkpoint指定的目录,当从Driver失败中恢复过来的时候,发现checkpoint目录已经存在了,那么会使用该目录中的元数据创建一个StreamingContext 但是上面的重写应用程序的过程,只是实现Driver失败自动恢复的第一步,第二部是:必须确保Driver可以在失败时,会自动被重启 要能够自动从Driver失败中恢复过来,运行spark Streaming应用程序的集群,就必须监控Driver运行的过程,并且在他失败时将它重启,对于spark自身的Standalone模式,需要进行一些配置去supervise driver,在他失败时将其重启 首先,要在spark-submit中,添加–deploy-mode参数,默认其值为client,即在提交应用的机器上启动Driver,但是要能够自动重启Driver,就必须将其值设置为cluster(在集群中的某个节点启动Driver),此外,需要添加–supervise参数(自动重启) 使用了上述第二部提交应用之后,就可以让Driver在失败时自动被重启,并且通过checkpoint目录的元数据恢复StreamingContext","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL的前世今生","date":"2017-04-16T04:47:25.204Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL的前世今生/","text":"spark1.0版本开始,推出了spark sql,其实最早使用的都是Hadoop自己的hive查询引擎,但是后来spark提供了Shark,再后来Shark被淘汰,退出了Spark sql,Shark的性能比hive就要高出一个数量级,而spark sql的性能又比Shark高出一个数量级 最早来说,hive的诞生,主要是因为要让哪些不熟悉java,无法深入进行MapReduce编程的数据分析师,能够使用它们熟悉的关系型数据库的sql模型,来操作HDFS上的数据,因此推出了hive,hive底层基于MapReduce实现了sql功能,能够让数据分析人员,以及数据开发人员,方便的使用hive进行数据仓库的建模和建设,然后使用sql模型针对数据仓库中的数据进行统计和分析,但是hive有个致命的缺陷,就是他的底层是基于MapReduce的,而MapReduce的shuffle又是基于磁盘的,因此导致hive的性能异常低下,经常出现复杂的sql etl,要运行数个小时,甚至数十个小时的情况 后来,spark推出了Shark,shark与hive实际上还是紧密关联的,shark底层的很多东西还是依赖于hive,但是修改了内存管理,物理计划,执行三个模块,底层使用spark的基于内存的计算模型,从而让性能比hive提升了数倍到上百倍 然而,shark还是他的问题梭子,shark底层依赖了hive的语法解析器,查询优化器等组件,因此对于其性能的提升还是造成了制约,所以后来spark团队决定,完全抛弃shark,退出了全新的spark sql项目 spark sql就不只是针对hive中的数据了,而是可以支持其他很多数据源的查询 spark sql的特点:1.支持多种数据源:hive,rdd,Parquet,Json, jdbc等2.多种性能优化技术:in-memory columnar storage, byte-code generation, cost model动态评估等3.组件扩展性:对于sql的语法解析器,分析器以及优化器,用户都可以自己重新开发,并且动态扩展 这样spark sql比Shark来说,性能又有了数倍的提升 spark sql的性能优化技术简介1.内存列存储(in-memory columnar storage)内存列存储意味着,spark sql的数据,不是使用java对象的方式来进行存储,而是使用面向列的内存存储的方式来进行存储,也就是说,每一个作为一个数据存储的单位,从而大大优化了内存使用的效率,采用了内存列存储之后,减少了对内存的消耗,也就避免了gc大量数据的性能开销 2.字节码生成技术(byte-code generation)spark sql在其catalyst模块的expressions中增加了codegen模块,对于sql语句中的计算表达式,比如select num+num from t 这样的sql,就可以使用动态字节码技术来优化其性能 3.scala代码编写的优化对于scala代码编写中,可能会造成较大的性能开销的地方,自己重写,使用更加复杂的方式,来获取更好的性能,比如Option样例类,for循环,map/filter/foreach等高阶函数,以及不可变对象,都改成了用null,while循环等来实现,并且重用了可变的对象","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL数据源之Parquet数据源","date":"2017-04-16T04:47:25.203Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之Parquet数据源/","text":"数据源Parquet的介绍Parquet是面向分析性业务的列式存储格式,由Twitter和Cloudera合作开发,2015年5月从Apache的孵化器里毕业成为Apache顶级项目 列式存储和行式存储相比有哪些优势呢?1.可以跳过不符合条件的数据,制只读取需要的数据,降低IO数据量2.压缩编码可以降低磁盘存储空间,由于同一列的数据类型是一样的,可以使用更高效的压缩编码(例如:Run Length Encoding和Delta Encoding)进一步节约存储空间3.只读取需要的列,支持向量运算,能够获取更好的扫描性能 数据源Parquet的编程方式加载数据下面介绍的是Parquet数据源,使用编程的方式加载Parquet文件中的数据 案例:查询用户数据中的用户名12345678910111213141516171819202122val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 使用format(&quot;parquet&quot;).load()是通用的方式//val userDF = sqlContext.read.format(&quot;parquet&quot;).load(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\users.parquet&quot;)// parquet()是针对parquet文件具体的读取,例如对于json文件,就有sqlContext.read.json();cvs,jdbc等是一样的val userDF = sqlContext.read.parquet(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\users.parquet&quot;)// 将DataFrame注册为临时表,然后使用sql查询需要的数据userDF.registerTempTable(&quot;users&quot;)val userNameDF = sqlContext.sql(&quot;select name from users&quot;)//对查询出来的DataFrame进行Transformation操作,然后打印// 在进行DF到rdd的转换的时候,一行数据转成rdd就是一个Array,所以用()去取数组元素val userName = userNameDF.rdd.map(row=&gt;row(0).toString+&quot;-xxx&quot;)userName.foreach(println)/*打印结果: Alyssa-xxx Ben-xxx */ Parquet数据源的自动分区推断表分区是一种常见的优化方式,比如hive中就提供了表分区的特性,在一个分区表中,不同分区的数据通常存储在不同的目录中,分区列的值通常就包含在了分区目录的目录名中,spark sql中的Parquet数据源支持自动根据目录名推断出分区信息,例如,如果将入口数据存储在分区表中,并且使用性别和国家作为分区列,那么目录结构可能如下所以: 123456789101112tableName |--gender=male |--country=US .... |--country=CN .... |--gender=female |--country=US .... |--country=CN .... 如果将/tableName传入SQLContext.read.Parquet()或者SQLContext.read.load()方法,那么spark sql就会自动根据目录结构,推断出分区信息,是gender和country,即使数据文件中只包含了两列值:name和age,但是spark sql返回的DataFrame,调用printSchema()方法时,会打印四个列的值:name,age,country,gender,这就是自动分区推断你的功能 此外,分区列的数据类型,也是自动被推断出来的,目前,spark sql仅支持自动推断出数字类型和字符串类型,有时,用户也许不希望spark sql自动推断分区列的数据类型,此时只要设置一个配置即可,spark.sql.source.partitionColumnTypeInference.enabled,默认为true,即:自动推断分区列的类型,设置为false,即不糊自动推断类型,禁止自动推断分区列的类型时,所有分区列的类型就统一默认都是String 案例:自动推断用户数据的性别和国家 创建目录并上传Parquet文件1234hadoop fs -mkdir /spark-study/usershadoop fs -mkdir /spark-study/users/gender=malehadoop fs -mkdir /spark-study/users/gender=male/country=UShadoop fs -put users.parquet /spark-study/users/gender=male/country=US/users.parquet 测试1234567val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)val userDF = sqlContext.read.parquet(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\users\\\\gender=male\\\\country=US\\\\users.parquet&quot;)userDF.printSchema()userDF.show Parquet数据源之合并元数据如同ProtocolBuffer,Avro,Thrift一样,Parquet也是支持元数据的合并的,用户可以在一开始就定义一个简单的元数据,然后随着业务需要,逐渐往元数据中添加更多的列,在这种情况下,用户可能会创建多个Parquet文件,有着多个不同的但是却相互兼容的元数据,Parquet数据源支持自动推断出这种情况,并且进行多个Parquet文件的元数据的合并 因为元数据合并是一种相对耗时的操作,而且在大多数情况下不是一种必要的特性,从spark 1.5.0版本开始,默认是关闭Parquet文件的自动合并元数据的特性的,可以通过以下的两种方式开启Parquet数据源的自动合并元数据的特性:1.读取parquet文件时,将数据源的选项,mergeSchema设置为true2.使用SQLContext.setConf()方法,将”spark.sql.parquet.mergeSchema”参数设置为true 案例:合并学生的基本信息和成绩信息的元数据12345678910111213141516171819202122232425262728 val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) import sqlContext.implicits._ // 首先手动创建一个DataFrame,作为学生的基本信息数据,并将其写入到一个Parquet文件中 val studentWithNameAge = Array((&quot;leo&quot;,23),(&quot;jack&quot;,25)) val studentWithNameAgeDF = sc.parallelize(studentWithNameAge).toDF(&quot;name&quot;, &quot;age&quot;) studentWithNameAgeDF.write.save(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\student&quot;, SaveMode.Append) //创建第二个DataFrame,作为学生的成绩信息,并写入一个Parquet文件中 val studentWithNameGrade = Array((&quot;marry&quot;,&quot;A&quot;),(&quot;tom&quot;,&quot;B&quot;)) val studentWithNameGradeDF = sc.parallelize(studentWithNameGrade).toDF(&quot;name&quot;, &quot;grade&quot;) studentWithNameGradeDF.write.save(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\student&quot;, SaveMode.Append) /* 第一个DataFrame的元数据和第二个DataFrame的元数据是不相同的, 第一个包含了name和age两个列,第二个包含了name和grade两个列 所以,这里期望的是,读取出来的表数据,自动合并两个问价你的元数据, 出现3个列:name,age,grade */ val studentDF = sqlContext.read.option(&quot;mergeSchema&quot;, true).parquet(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\student&quot;) studentDF.printSchema() studentDF.show// 注意在spark1.5.0上是可以的,在spark2.1.0上不行,编译就报错","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL数据源之json数据源","date":"2017-04-16T04:47:25.201Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之json数据源/","text":"spark sql可以自动推断json文件的元数据,并且加载其数据,创建一个DataFrame,可以使用SQLContext.read.json()方法,针对一个元素类型为String的RDD,或者是一个json文件 但是要主要的是,这里使用的json文件与传统意义上的json文件是不一样的,每行都必须,也只能包含一个,单独的,自包含的,有效的json对象,不能让一个json对象分散在多行,否则会报错 案例:查询成绩为80分一身的学生的基本信息与成绩信息 1234//json文件&#123;&quot;name&quot;:&quot;Michael&quot;,&quot;score&quot;:88&#125;&#123;&quot;name&quot;:&quot;Andy&quot;, &quot;score&quot;:99&#125;&#123;&quot;name&quot;:&quot;Justin&quot;, &quot;score&quot;:77&#125; 完整的代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 针对json文件,创建DataFrameval studentScoresDF = sqlContext.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\studentscores.json&quot;)// 针对学生成绩的DataFrame,注册临时表,查询分数大于80的学生的姓名数组studentScoresDF.registerTempTable(&quot;student_scores&quot;)val goodStudentScoresDF = sqlContext.sql(&quot;select name,score from student_scores where score &gt;= 80&quot;)val goodStudentNames = goodStudentScoresDF.rdd.map(row=&gt;row(0).toString).collect// 构造学生信息数据val studentInfoJson = Array( &quot;&#123;\\&quot;name\\&quot;:\\&quot;Michael\\&quot;,\\&quot;age\\&quot;:18&#125;&quot;, &quot;&#123;\\&quot;name\\&quot;:\\&quot;Andy\\&quot;,\\&quot;age\\&quot;:17&#125;&quot;, &quot;&#123;\\&quot;name\\&quot;:\\&quot;Justin\\&quot;,\\&quot;age\\&quot;:19&#125;&quot;)val studentInfoJsonRDD = sc.parallelize(studentInfoJson)val studentInfoJsonDF = sqlContext.read.json(studentInfoJsonRDD)// 查询学生成绩大于80分的学生信息studentInfoJsonDF.registerTempTable(&quot;stduent_infos&quot;)// 构造sql语句var sql = &quot;select name, age from stduent_infos where name in (&quot;for(i &lt;- 0 until goodStudentNames.length)&#123; sql += &quot;&apos;&quot;+goodStudentNames(i)+&quot;&apos;&quot; if(i &lt; goodStudentNames.length-1)&#123; sql += &quot;,&quot; &#125;&#125;sql += &quot;)&quot;val goodStudentInfoDF = sqlContext.sql(sql)// 将DataFrame转换为RDD[Tuple], 这样在进行join的时候,可以按照Tuple的key进行joinval goodStudentInfoRdd = goodStudentInfoDF.rdd.map&#123;row =&gt; (row(0).toString, row(1).toString.toInt)&#125;// 将分数大于80分的学生的成绩信息和基本基本信息进行joinval goodStudentsRdd = goodStudentScoresDF.rdd.map(row=&gt;(row(0).toString, row(1).toString.toInt)) .join(goodStudentInfoRdd)// 将RDD转换为DataFrame,并保存成json格式val goodStudents = goodStudentsRdd.map(t=&gt;Row(t._1,t._2._1.toInt,t._2._2.toInt))//将RDD转成RDD[Row]val meta = StructType(Array( StructField(&quot;name&quot;, StringType, true), StructField(&quot;score&quot;, IntegerType, true), StructField(&quot;age&quot;, IntegerType, true)))val goodStudentsDF = sqlContext.createDataFrame(goodStudents,meta)goodStudentsDF.write.format(&quot;json&quot;).save(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\studentscores.json1&quot;) 总结:1.要实现两张表的join2.而join的过程是RDD的join,所以要将两个DataFrame转成RDD(这里涉及到Row)3.join之后是一个RDD,此时要将RDD转成DataFrame,保存成json格式(这里涉及到RDD转DataFrame的时候,要将RDD抓成RDD[Row]","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL数据源之JDBC数据源","date":"2017-04-16T04:47:25.199Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之JDBC数据源/","text":"spark sql支持使用JDBC从关系型数据库(比如Mysql)中读取数据,读取的数据,依然由DataFrame表示,可以很方便的使用spark core提供的各种算子进行处理 实际上使用spark sql处理JDBC中的数据是非常有用的,比如说,你的mysql业务数据库中,有大量的数据,比如1000万,然后,你现在需要编写一个程序,对线上的脏数据进行某种复杂业务逻辑的处理,甚至复杂到可能涉及到要用spark sql反复查询hive中的数据,来进行关联处理 此时,用spark sql来通过JDBC数据源,加载mysql中的数据,然后通过各种算子进行处理,是最好的选择,因为spark是分布式的计算框架,对于1000万数据,肯定是分布式处理的,而如果你自己手工编写一个java程序,那么你只能分批次处理了,首先处理2万条,再处理2万条,可能运行完你的java程序,已经是好久之后的事情了 12345sqlContext.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;,&quot;dbtable&quot;-&gt;&quot;students&quot;)).load() 案例:查询分数大于80分的学生信息 1234567891011create database testdb;use testdb;create table student_infos(name varchar(20), age int);create table student_scores(name varchar(20), score int);insert into student_infos values(&quot;leo&quot;,18),(&quot;marry&quot;,17),(&quot;jack&quot;,19);insert into student_scores values(&quot;leo&quot;,88),(&quot;marry&quot;,77),(&quot;jack&quot;,99);create table good_student_infos(name varchar(20), age int, score int); 完整的代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455val data2Mysql = (iterator: Iterator[(String, Int, Int)])=&gt;&#123; val conn:Connection = null val ps:PreparedStatement = null var sql = &quot;insert into good_student_infos (name,age,scores) values (?,?,?)&quot; try&#123; //Class.forName(&quot;com.mysql.jdbc.Driver&quot;) val conn = DriverManager.getConnection(&quot;jdbc:mysql://spark1:3306/testdb&quot;,&quot;&quot;,&quot;&quot;) iterator.foreach&#123; case (name,age,scores) =&gt; ps.setString(1,name) ps.setInt(2,age) ps.setInt(3,age) ps.executeUpdate() case _ =&gt; &#125; &#125;catch&#123; case e:Exception =&gt; println(&quot;mysql exception&quot;) &#125;finally &#123; if (ps != null) ps.close() if (conn != null) conn.close() &#125;&#125;def RDD2DataFrameByReflection(): Unit =&#123; val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) // 通过jdbc构建DataFrame val studentInfosDF = sqlContext.read.format(&quot;jdbc&quot;) .options(Map( &quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;, &quot;dbtable&quot;-&gt;&quot;student_infos&quot; )).load() val studentScoresDF = sqlContext.read.format(&quot;jdbc&quot;) .options(Map( &quot;url&quot;-&gt;&quot;jdbc:mysql://spark1:3306/testdb&quot;, &quot;dbtable&quot;-&gt;&quot;student_scores&quot; )).load() // 将两个DataFrame转换为rdd,执行join操作 val studentInfosRdd = studentInfosDF.rdd.map(row=&gt;(row(0).toString,row(1).toString.toInt)) val studentScoresRdd = studentScoresDF.rdd.map(row=&gt;(row(0).toString,row(1).toString.toInt)) val studentInfoScoresRdd = studentInfosRdd.join(studentScoresRdd).map(t=&gt;(t._1, t._2._1, t._2._2)) // 过滤学生成绩大于80的学生信息 val goodStudentRdd = studentInfoScoresRdd.filter(_._3 &gt; 80) goodStudentRdd.foreachPartition(data2Mysql(_))&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL数据源之hive数据源","date":"2017-04-16T04:47:25.198Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之hive数据源/","text":"spark sql支持对hive中存储的数据进行读写,操作hive中的数据时,必须创建HiveContext,而不是SQLContext,HiveContext继承自SQLContext,但是增加了在hive元数据库中查找表,以及用HiveQL语法编写SQL的功能,除了sql()方法,HiveContext还提供了hql()方法,从而用hive语法来编译sql 使用HiveContext,可以执行hive的大部分功能,包括创建表,往表里到入数据以及用sql语句查询表中的数据,查询出来的数据是一个Row数组 将hive-site.xml拷贝到spark/conf目录下,将mysql connector拷贝到spark/lib目录下 123456val sqlContext = new HiveContext(sc)sqlContext.sql(&quot;create table if not exists students (name String, age Int)&quot;)sqlContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/students.txt&apos; into table students&quot;)sqlContext.sql(&quot;select name, age from students where age&lt;=18&quot;).collect spark sql还允许将数据保存到hive表中,调用DataFrame的saveAsTable命令,即可将DataFrame中的数据保存到hive表中,与registerTempTable不同,saveAsTable是会将DataFrame中的数据物化到hive表中的,而且还会在hive元数据库中创建表的元数据 默认情况下,saveAsTable会创建一张hive Managed Table,也就是说,数据的位置都是由元数据库中国你的信息控制的,当managed Table被删除时,表中的数据也会一并被物理删除 registerTempTable只是注册一个临时的表,只要按spark Application重启或者停止了,那么表就没了,而saveAsTable创建的是物化的表,无论spark Application重启或停止,表都会一直存在 调用hiveContext.table()方法还可以直接针对hive中的表,创建一个DataFrame 案例:查询分数大于80分的学生的信息123456789101112131415161718192021222324252627282930313233val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val hiveContext = new HiveContext(sc)// 判断是否存在student_info表,如果存在就删除hiveContext.sql(&quot;drop if exists table student_info&quot;)// 创建表hiveContext.sql(&quot;create if not exists table studnet_info (name STRNG, age INT&quot;)// 将学生基本信息数据导入student_info表hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/student_info.txt&apos; into table student_info &quot;)// 同样的步骤:创建表student_score,并加载数据hiveContext.sql(&quot;drop if exists table student_score&quot;)hiveContext.sql(&quot;create if not exists table student_score (name STRNG, score INT&quot;)hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resouces/student_score.txt&apos; into table student_score &quot;)// 执行sql查询,关联2张表,查询成绩大于80分的学生成绩val goodStudentDF = hiveContext.sql(&quot;select info.name,info.age,score.score&quot; + &quot;from student_info info&quot; + &quot;join student_score score on info.name=score.name&quot; + &quot;where score.score&gt;=80&quot;)// 接着将DataFrame中的数据保存到good_student_info表中hiveContext.sql(&quot;drop if exists table good_student_info&quot;)goodStudentDF.saveAsTable(&quot;good_student_info&quot;)// 然后针对good_student_info表直接创建DataFrameval goodStudentRows = hiveContext.table(&quot;good_student_info&quot;).collectfor(goodStudentRow &lt;- goodStudentRows)&#123; println(goodStudentRow)&#125; 测试12345进入到hive的shell中&gt;hive&gt;show tables;&gt;select * from good_student_info;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL数据源之DataFrame的使用","date":"2017-04-16T04:47:25.196Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL数据源之DataFrame的使用/","text":"DataFrame介绍 DataFrame可以理解为是以列的形式组织的,分布式的数据集合,他其实和关系型数据库中的表非常类似,但是底层做了很多的优化,DataFrame可以通过很多来源进行构建,包括:结构胡的数据文件,hive中的表,外部的关系型数据库,以及RDD SQLContext要使用Spark SQL,首先就得创建一个SQLContext对象,或者是他的子类的对象,比如HiveContext对象 123val sc:SparkContext = ...val sqlContext = new SQLContext(sc)import sqlContext.implicits._ HiveContext除了基本你的SQLContext以外,还可以使用它的子类—–HiveContext,HiveContext的功能除了包含SQLContext提供的所有功能之外,还包含了额外的专门针对Hive的一些功能,这些额外的功能包括:使用hiveSQL语法来编写和执行sql;使用hive中的UDF函数;从hive表中读取数据 要使用hiveContext,就必须预先安装好hive,SQLContext支持的数据源,hiveContext也同样支持—–而不只是支持hive,对于spark1.3.x以上的版本,都推荐使用hiveContext,因为其功能更加丰富和完善 spark sql还支持用spark.sql.dialect参数来设置sql的方言,使用SQLContext的setConf()即可进行设置,对于SQLContext,他只支持”sql”一种方言,对于hiveContext,他默认的方言是”hiveql” 创建DataFrame使用SQLContext,可以从RDD,hive表或者其他数据源,来创建一个DataFrame,以下是一个使用json文件创建DataFrame的例子123456789101112val sc:SparkContext = ...val sqlContext = new SQLContext(sc)val df = sqlContext.read.json(&quot;hdfs://spark1:9000/students.json&quot;)df.show()=======students.json===========&#123;&quot;id&quot;:1, &quot;name&quot;:&quot;leo&quot;, &quot;age&quot;:18&#125;&#123;&quot;id&quot;:2, &quot;name&quot;:&quot;jack&quot;, &quot;age&quot;:19&#125;&#123;&quot;id&quot;:3, &quot;name&quot;:&quot;merry&quot;, &quot;age&quot;:17&#125; 下面的代码是在windows本地的完整测试代码123456789101112131415161718192021222324252627282930313233343536object TopNBasic &#123; def main(args: Array[String]): Unit = &#123; System.setProperty(&quot;hadoop.home.dir&quot;, &quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\hadoop\\\\&quot;) val sc = sparkContext(&quot;Transformation Operations&quot;) testMethod(sc) sc.stop()//停止SparkContext,销毁相关的Driver对象,释放资源 &#125; //在实际的生成中,我们是封装函数来进行逻辑的组织 def sparkContext(name:String)=&#123; val conf = new SparkConf().setAppName(name).setMaster(&quot;local&quot;) //创建SparkContext,这是第一个RDD创建的唯一入口,是通往集群的唯一通道 val sc = new SparkContext(conf) sc &#125; def testMethod(sc: SparkContext): Unit =&#123; val sqlContext = new SQLContext(sc) val df = sqlContext.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;) df.show() /* 打印结果: +---+---+-----+ |age| id| name| +---+---+-----+ | 18| 1| leo| | 19| 2| jack| | 17| 3|merry| +---+---+-----+ */ &#125;&#125; 提交到spark集群的shell12345678/usr/local/spark/bin/spark-submit \\--class cn.spark.study.sql.DataFrameCreate \\--num-executors 3 \\--driver-memory 100m \\--executor-cores 3 \\--files /usr/local/hive/conf/hive-site.xml \\--driver-class-path /usr/local/hive/lib/mysql-connctor-java-5.1.17.jar \\/root/spark-test-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\ 其中会用到了hive的conf文件,和mysql的驱动jar包 DataFrame的常用操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778val sqlContext = new SQLContext(sc)val df = sqlContext.read.json(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;)// 打印DataFrame中的所有的数据df.show()// 打印DataFrame中的元数据信息(Schema)df.printSchema// 查询某一列所有的数据df.select(&quot;name&quot;).show// 查询某几列所有的数据,并对列进行计算(将age列加1)df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(1)).show//df.select(df(&quot;name&quot;), df(&quot;age&quot;) + 1).show// 对于某一列的值进行过滤df.filter(df.col(&quot;age&quot;).gt(18)).show//df.filter(df(&quot;age&quot;) &gt; 18).show// 根据某一列进行分组,然后进行聚合df.groupBy(df.col(&quot;age&quot;)).count.show//df.groupBy(&quot;age&quot;).count.show/*打印结果:// 打印DataFrame中的所有的数据df.show() +---+---+-----+ |age| id| name| +---+---+-----+ | 18| 1| leo| | 19| 2| jack| | 17| 3|merry| +---+---+-----+ // 打印DataFrame中的元数据信息(Schema) df.printSchema root |-- age: long (nullable = true) |-- id: long (nullable = true) |-- name: string (nullable = true) // 查询某一列所有的数据df.select(&quot;name&quot;).show +-----+ | name| +-----+ | leo| | jack| |merry| +-----+ // 查询某几列所有的数据,并对列进行计算(将age列加1)df.select(df.col(&quot;name&quot;), df.col(&quot;age&quot;).plus(1)).show +-----+---------+ | name|(age + 1)| +-----+---------+ | leo| 19| | jack| 20| |merry| 18| +-----+---------+ // 对于某一列的值进行过滤df.filter(df.col(&quot;age&quot;).gt(18)).show +---+---+----+ |age| id|name| +---+---+----+ | 19| 2|jack| +---+---+----+ // 根据某一列进行分组,然后进行聚合df.groupBy(df.col(&quot;age&quot;)).count.show +---+-----+ |age|count| +---+-----+ | 19| 1| | 17| 1| | 18| 1| +---+-----+ */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL工作原理流程剖析及性能优化","date":"2017-04-16T04:47:25.194Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL工作原理剖析及性能优化/","text":"SparkSQL工作原理流程 spark sql核心源码spark sql优化的点1.设置shuffle过程中的并行度:spark.sql.shuffle.partitions(SqlContext.setconf())2.在hive数据仓库建设的过程中,合理设置数据类型,比如能设置为int的,就不要设置为BigInt,减少数据类型导致的不必要的内存开销3.编写sql时尽量列出列名,不要写select *4.并行处理查询结果,对于spark sql查询的结果,如果数据量比较大,比如超过1000条,那么就不要一次性collect到Driver里面,使用foreach()算子,并行处理查询结果5.缓存表:对于一条sql语句中可能多次使用到的表,可以对其进行缓存,使用123SQLContext.cacheTable(tableName)//或者DataFrame.cache() spark sql会用内存列存储的格式进行表的缓存,然后spark sql就可以仅仅扫描需要使用的列,并且自动优化压缩,来最小化内存使用和GC开销,SQLContext.uncacheTable(tableName)可以将表从缓存中移除,用SQLContext.setConf(),设置12spark.sql.inMemoryColumnarStorage.batchSize //默认是1000 可以配置列存储的单位6.广播join表:12spark.sql.autoBroadcastJoinThreshold //默认10485760(10M) ,会将10M以内的表自动广播出去 在内存够用的情况下,可以增加其大小,可以让更多的表被自动广播出去,可以将join中的较小的表广播出去,而不用进行网络数据传输了7.钨丝计划:12spark.sql.tungsten.enabled//默认是true 自动管理内存","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL实战之统计每日top3热点搜索词","date":"2017-04-16T04:47:25.193Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL实战之统计每日top3热点搜索词/","text":"数据格式12日期,用户,搜索词,程序,平台,版本2015-10-01 leo water beijing android 1.0 要求:1.筛选出符合查询条件的数据2.统计出每天搜索uv排名前3的搜索词3.按照每天的top3搜索词的搜索uv总次数,倒序排序4.将数据保存到hive表中 实现思路1.针对原始数据(HDFS文件),获取输入的rdd2.使用filter,去针对输入RDD中的数据进行过滤,过滤出符合查询条件的数据2.1.普通的做法:直接在filter算子函数中,使用外部的查询条件(Map),但是这样做的话,会将查询条件Map发送给每个task一个副本2.2.优化后的做法:将查询条件,封装为Broadcast广播变量,在filter算子中使用Broadcast广播变量3.将数据转换为”(日期搜索词, 用户)” ,然后对其进行分组,其次再对分组后的数据在组内进行去重操作,得到每天每个搜索词的uv,最后的数据格式为:(日期搜索词, uv)4.将3中得到的Rdd转成Rdd[Row],将该Rdd转换成DataFrame5.将DataFrame注册成临时表,使用spark sql开窗函数,来统计每天的uv数量排名前3的搜索词,以及他的搜索uv,最后获取的是一个DataFrame6.将DataFrame转换为rdd,继续操作,按照每天日期进行分组,并进行映射,计算出每天的top3搜索词的搜索uv总数,然后将uv总数作为key,将每天的top3搜索词以及搜索次数拼接为一个字符串7.按照每天的top3搜索总uv,进行倒序排列8.整理格式:日期_搜索词_uv9.再次映射为DataFrame,并将数据保存到hive表中 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new HiveContext(sc)import sqlContext.implicits._// 构造一份数据:查询条件(在实际的开发过程中,通过JavaWeb将查询条件入库到mysql中// 然后这里从mysql中提取查询条件val queryParamMap = Map( &quot;city&quot;-&gt;Array(&quot;beijing&quot;,&quot;hubei&quot;), &quot;platform&quot;-&gt;Array(&quot;android&quot;), &quot;version&quot;-&gt;Array(&quot;1.0&quot;,&quot;1.2&quot;))// 将查询条件广播val queryParamMapBroadCast = sc.broadcast(queryParamMap)//val lineRdd = sc.textFile(&quot;hdfs://spark1:9000/keyword.txt&quot;)//val lineRdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\sousuo.txt&quot;)// 使用查询条件,进行filterval filterRdd = lineRdd.filter&#123; line=&gt; val arr = line.split(&quot;\\t&quot;) /* 日期,用户,搜索词,程序,平台,版本 2015-10-01 leo water beijing android 1.0 */ val city, platform, version = (arr(2),arr(3), arr(4)) val map = queryParamMapBroadCast.value val cities = map.get(&quot;city&quot;) if(!cities.toList.contains(city))&#123; false &#125; val platforms = map.get(&quot;platform&quot;) if(!platforms.toList.contains(platform))&#123; false &#125; val versions = map.get(&quot;version&quot;) if(!versions.toList.contains(version))&#123; false &#125; true&#125;// 改变格式:(日期_搜索词,用户)val dateKeywordUserRdd = filterRdd.map&#123; /* 日期,用户,搜索词,程序,平台,版本 2015-10-01 leo water beijing android 1.0 */ line=&gt; val arr = line.split(&quot;\\t&quot;) val (date,user,keyword) = (arr(0),arr(1), arr(2)) // 返回格式:(日期_搜索词,用户) (date+&quot;_&quot;+keyword, user)&#125;// 进行分组,获取每天每个搜索词,有哪些有用(没有对用户去重)val dateKeywordUsesRdd = dateKeywordUserRdd.groupByKey()// 对每天每个搜索词的搜索用户,执行去重,获取其uvval dateKeywordUvsRdd = dateKeywordUsesRdd.map&#123; line=&gt; val dateKeyword = line._1 val iter = line._2.toIterator val distinctUsers = mutable.Set[String]() while(iter.hasNext)&#123; val user = iter.next distinctUsers+=user &#125; // 返回:(日期_搜索词,uv) val uv = distinctUsers.size (dateKeyword, uv)&#125;// 返回:Row(日期,搜索词,uv)val dateKeywordUsesRowRdd = dateKeywordUvsRdd.map(line=&gt;Row(line._1.split(&quot;_&quot;)(0),line._1.split(&quot;_&quot;)(1),line._2.toString.toInt))// 将每天每个搜索词的uv数据转成DataFrameval structFields = StructType(Array( StructField(&quot;date&quot;, StringType, true), StructField(&quot;keyword&quot;, StringType, true), StructField(&quot;uv&quot;, IntegerType, true)))val dateKeywordUvDF = sqlContext.createDataFrame(dateKeywordUsesRowRdd, structFields)// 使用spark sql的开窗函数,统计每天搜索uv排名前3的热点搜索词dateKeywordUvDF.registerTempTable(&quot;daily_keyword_uv&quot;)val dailyTop3KeywordDF = sqlContext.sql(&quot;&quot; + &quot;select date, keyword, uv&quot; + &quot;from (&quot; + &quot; select &quot; + &quot; date,&quot; + &quot; keyworkd,&quot; + &quot; uv,&quot; + &quot; row_number() over (partition by date order by uv desc) rank&quot; + &quot; from daily_keyword_uv&quot; + &quot;) tmp&quot; + &quot;where rank &lt;= 3&quot;)dailyTop3KeywordDF.show","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL函数之开窗函数","date":"2017-04-16T04:47:25.192Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之开窗函数/","text":"分组取topN的案例 商品在某个分类下的topN 代码如下1234567891011121314151617181920212223242526272829303132val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)val hiveContext = new HiveContext(sc)hiveContext.sql(&quot;drop table if not exists sales&quot;)hiveContext.sql(&quot;create table if not exists sales (&quot; + &quot;product STRING&quot; + &quot;category SRING&quot; + &quot;revenue BIGINT&quot; + &quot;)&quot;)hiveContext.sql(&quot;load data local inpath &apos;/usr/local/spark-study/resources/sales.txt&apos; into table sales&quot;)// 使用row_number()开窗函数:给每个分组内的数据,按照其排序的顺序,打上一个分组内的行号,行号从1开始val top3SalesDF = hiveContext.sql(&quot;select product,category,revenue &quot; + &quot;from (&quot; + &quot; select product,category,revenue,&quot; + &quot; row_number() over (partition by category order by revenue desc) rank&quot; + &quot; from sales&quot; + &quot;) tmp_sales&quot; + &quot;where rank &lt;= 3&quot;)/* row_number函数使用说明:1.row_number函数后面跟的是over关键字2.括号中是partition by 表示根据哪个字段进行分组3.order by表示在组内按照指定的字段进行排序4.row_number就能给每个组内的每行一个组内行号5.在子查询的外部,取组内排名前3 */hiveContext.sql(&quot;drop table if not exists top3_sales&quot;)top3SalesDF.saveAsTable(&quot;top3_sales&quot;)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL函数之内置函数","date":"2017-04-16T04:47:25.190Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之内置函数/","text":"在spark1.5.x版本中,增加了一系列内置函数到DataFrame API中,并且实现了code generation的优化,与普通的函数不同,DataFrame的函数并不会执行后立即返回一个值,而是返回一个Column对象,用于在并行作业中进行求值,Column可以用在DataFrame的操作之中,比如select,filter,groupBy等,函数的输入值,也可以是Column 种类 函数 聚合函数 approxCountDistinct,avg,count,countDistinct,first,last,max,mean,min,sum,sumDistinct 集合函数 array_contains,explode,size,sort_array 日期/时间转换函数 日期时间转换unix_timestamp,from_unixtime,to_date,quarter,day,dayofyear,weekofyear,from_utc_timestamp,to_utc_timestamp;(从日期时间中提取字段:year,month,dayofmonth,hour,minute,second;(日期/时间计算)datediff,date_add,date_sub,add_months,last_day,next_day,months_between;(获取当前时间)current_date,current_timestamp,trunc,date_format 混合函数 array,isNaN,isnotnull,isnull,not,when,if,rand 字符串函数 concat,decode,encode,format_number,format_string,get_json_object,length,lpad,ltrim,lower,rpad,upper 窗口函数 cumeDist,denseRank,lag,lead,ntile,percentRank,rank,rowNumber 案例:根据每天的用户访问日志和购买日志,统计每日的uv和销售额 统计每日的UV 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 导入隐式转换import sqlContext.implicits._// 构造用户访问日志数据,创建DataFrameval userAccessLog = Array(//date , userid &quot;2015-10-01,1122&quot;, &quot;2015-10-01,1123&quot;, &quot;2015-10-01,1124&quot;, &quot;2015-10-02,1122&quot;, &quot;2015-10-02,1123&quot;, &quot;2015-10-02,1122&quot;)val userAccessLogRDD = sc.parallelize(userAccessLog)// 将RDD转成DataFrame,需要处理RDD的元素变成Rowval userAccessLogRowRdd = userAccessLogRDD.map&#123; line=&gt; val arr = line.split(&quot;,&quot;) Row(arr(0), arr(1).toInt)&#125;val structType = StructType(Array( StructField(&quot;date&quot;, StringType, true), StructField(&quot;userid&quot;, IntegerType, true)))// 构建DataFrameval userAccessLogRowDF = sqlContext.createDataFrame(userAccessLogRowRdd, structType)// 内置函数的使用// 1.统计uvuserAccessLogRowDF.groupBy(&quot;date&quot;) // 按照date聚合,在每组中对userid进行去重统计 //.agg(&apos;date, functions.countDistinct(&apos;userid)).show /* 打印结果 +----------+----------+----------------------+ | date| date|count(DISTINCT userid)| +----------+----------+----------------------+ |2015-10-02|2015-10-02| 2| |2015-10-01|2015-10-01| 3| +----------+----------+----------------------+ */// 如果我们要取date,userid列 .agg(&apos;date, functions.countDistinct(&apos;userid)).rdd .map&#123;row=&gt; (row(1),row(2))&#125; .collect .foreach(println)/* 打印结果(2015-10-02,2)(2015-10-01,3) *//*聚合函数总结:1.对DataFrame调用groupBy()方法,对某一列进行分组2.调用agg()方法,第一个参数,必须是之前groupBy()方法中出现的字段 agg()方法的第一个参数是用单引号开头的3.第二个参数,传入countDistinct,sum,first等,spark提供的内置函数 内置函数中传入的参数也是用:单引号作为参数的4.所有的统计函数在:org.apache.spark.sql.functions中,所以要指定functions.xx */ 统计销售额1234567891011121314151617181920212223242526272829303132333435363738394041val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 导入隐式转换import sqlContext.implicits._val userSaleLog = Array( &quot;2015-10-01,55.55&quot;, &quot;2015-10-01,66.55&quot;, &quot;2015-10-02,77.55&quot;, &quot;2015-10-02,55.55&quot;, &quot;2015-10-03,55.55&quot;, &quot;2015-10-03,55.55&quot;)val userSaleLogRdd = sc.parallelize(userSaleLog)val userSaleLogRowRdd = userSaleLogRdd.map&#123; line=&gt; val arr = line.split(&quot;,&quot;) Row(arr(0), arr(1).toDouble)&#125;val structType = StructType(Array( StructField(&quot;date&quot;, StringType, true), StructField(&quot;sale_amount&quot;, DoubleType, true)))val userSaleLogDF = sqlContext.createDataFrame(userSaleLogRowRdd, structType)// 每日销售额的统计userSaleLogDF.groupBy(&quot;date&quot;) .agg(&apos;date, functions.sum(&apos;sale_amount)).rdd .map(row=&gt; (row(1),row(2).toString.toDouble)) .collect .foreach(println)/* 打印结果(2015-10-02,133.1)(2015-10-01,122.1)(2015-10-03,111.1) */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL函数之UDF","date":"2017-04-16T04:47:25.188Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDF/","text":"123456789101112131415161718192021222324252627282930val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 构造模拟数据rddval names = Array(&quot;leo&quot;, &quot;marry&quot;, &quot;jack&quot;)val namesRdd = sc.parallelize(names)// 将RDD转成DataFrameval namesRowRdd = namesRdd.map(Row(_))val structType = StructType(Array(StructField(&quot;name&quot;,StringType,true)))val namesDF = sqlContext.createDataFrame(namesRowRdd, structType)// 注册一张表namesDF.registerTempTable(&quot;names&quot;)// 定义和注册自定义函数// 函数名: strLen// 函数体:这里是一个匿名函数:(str:String)=&gt; str.lengthsqlContext.udf.register(&quot;strLen&quot;, (str:String)=&gt; str.length)// 使用自定义函数sqlContext.sql(&quot;select name, strLen(name) from names&quot;) .collect .foreach(println)/* 打印结果[leo,3][marry,5][jack,4] */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL函数之UDAF自定义聚合函数","date":"2017-04-16T04:47:25.187Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL函数之UDAF自定义聚合函数/","text":"UDF(自定义函数),针对的是单行输入,返回一个输出UDAF(自定义聚合函数),针对的是多行输入,进行聚合计算,返回一个输出 下面是一个自定义的分组统计字符串的个数的函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package org.dt.sparkimport org.apache.spark.sql.Rowimport org.apache.spark.sql.types._import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;/** */class StringCount extends UserDefinedAggregateFunction&#123; // inputSchema:指的是输入数据的类型 override def inputSchema: StructType = &#123; StructType(Array(StructField(&quot;str&quot;, StringType, true))) &#125; // bufferSchema:中间进行聚合时,所处理的数据的类型 override def bufferSchema: StructType = &#123; StructType(Array(StructField(&quot;count&quot;, IntegerType, true))) &#125; // dataType:是函数返回值的类型 override def dataType: DataType = &#123; IntegerType &#125; override def deterministic: Boolean = true // 为每个分组的数据执行初始化操作 override def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0 &#125; // 每个分组有新的值进来的时候,如何进行分组对应的聚合值的计算 override def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; buffer(0) = buffer.getAs[Int](0) + 1 &#125; // 由于spark是分布式的,所以一个分组的数据,可能会在不同的节点上进行局部聚合,这个过程就是update // 但是,最后,在各个节点上的聚合值,要进行merge,也就是合并 override def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getAs[Int](0) + buffer2.getAs[Int](0) &#125; // 个分组的聚合值,如何通过中间的缓存聚合值,返回一个最终的聚合值 override def evaluate(buffer: Row): Any = &#123; // 这里是直接将数据返回,没有做任何的处理 buffer.getAs[Int](0) &#125;&#125; 测试自定义聚合函数123456789101112131415161718192021222324252627282930val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val sqlContext = new SQLContext(sc)// 构造模拟数据rdd:val names = Array(&quot;leo&quot;, &quot;marry&quot;, &quot;jack&quot;, &quot;marry&quot;, &quot;jack&quot;, &quot;marry&quot;, &quot;jack&quot;)val namesRdd = sc.parallelize(names)// 将RDD转成DataFrameval namesRowRdd = namesRdd.map(Row(_))val structType = StructType(Array(StructField(&quot;name&quot;,StringType,true)))val namesDF = sqlContext.createDataFrame(namesRowRdd, structType)// 注册一张表namesDF.registerTempTable(&quot;names&quot;)// 定义和注册自定义函数// 函数名: strCount// 函数体: 是一个自定义的函数sqlContext.udf.register(&quot;strCount&quot;, new StringCount)// 使用自定义函数:统计相同名字出现的次数sqlContext.sql(&quot;select name, strCount(name) from names group by name&quot;) .collect .foreach(println)/* 打印结果 [jack,3] [leo,1] [marry,3] */","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL之通用的load和save操作","date":"2017-04-16T04:47:25.186Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之通用的load和save操作/","text":"通用的load和save操作对于spark sql的DataFrame来说,无论是从什么数据源创建出来的DataFrame,都有一些共同的load和save操作,load操作主要用于加载数据,创建出来DataFrame;save操作,主要用于将DataFrame中的数据保存到文件中 1234// users.parquet是使用parquet面向列存储的文件,用文本打开是乱码val df = sqlContext.read.load(&quot;users.parquet&quot;)df.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save() 完整的代码如下1234567891011121314151617181920212223242526 val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) val userDF = sqlContext.read.load(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\users.parquet&quot;) userDF.printSchema userDF.show /* 打印结果: root |-- name: string (nullable = true) |-- favorite_color: string (nullable = true) |-- favorite_numbers: array (nullable = true) | |-- element: integer (containsNull = true) +------+--------------+----------------+ | name|favorite_color|favorite_numbers| +------+--------------+----------------+ |Alyssa| null| [3, 9, 15, 20]| | Ben| red| []| +------+--------------+----------------+ */ userDF.select(&quot;name&quot;, &quot;favorite_color&quot;).write.save(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\nameAndFavoriteColor.parquet&quot;)// 可以看到在桌面生成了一个nameAndFavoriteColor.parquet文件夹 验证上面的保存文件是否保存成功1234567891011121314151617181920212223 val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc)// 使用上面的保存路径 val userDF = sqlContext.read.load(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\nameAndFavoriteColor.parquet&quot;) userDF.printSchema userDF.show/*打印结果:root |-- name: string (nullable = true) |-- favorite_color: string (nullable = true)+------+--------------+| name|favorite_color|+------+--------------+|Alyssa| null|| Ben| red|+------+--------------+所以说明上面的保存是成功的*/ 手动指定数据源的类型可以手动指定用来操作的数据源类型,数据源通常需要使用其全限定名来指定,比如parquet是org.apache.spark.sql.parquet,但是spark sql 内置了一些数据源类型,比如json,parquet,jdbc等等,实际上,通过这个功能,就可以在不同类型的数据源之间进行转换了,比如将json文件中的数据保存到parquet文件中,默认情况下,如果不指定数据源的类型,那么就是parquet 1234567 val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) val userDF = sqlContext.read.format(&quot;json&quot;).load(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\people.json&quot;)//保存为parquet userDF.select(&quot;name&quot;).write.format(&quot;parquet&quot;).save(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\peopleName.parquet&quot;) save modespark sql对于save操作,提供了不同的savemode,主要用来处理,当目标位置,已经有数据时,应该如何处理,而且save操作并不会执行行锁操作,并且不是原子的,因此是有一定风险出现脏数据的 Save Mode 意义 SaveMode.ErrorIfExists(默认) 如果目标位置已经存在数据,那么就抛出一个异常 SaveMode.Append 如果目标位置已经存在数据,那么将数据追加进去 SaveMode.Overwrite 如果目标位置已经存在数据,那么就将已经存在的数据删除,用新数据进行覆盖 SaveMode.Ignore 如果目标位置已经存在数据,那么就忽略,不做任何操作","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL之RDD与DataFrame的转换","date":"2017-04-16T04:47:25.185Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之RDD与DataFrame的转换/","text":"为什么要将RDD转换为DataFrame,因为这样的话,我们就可以直接针对HDFS等任何可以构建为RDD的数据,使用spark sql进行sql查询了,这个功能是无比强大的,想象一下,针对HDFS中的数据,直接就可以使用sql进行查询 spark sql支持两种方式来将RDD转换为DataFrame1.使用反射来推断包含了特定数据类型的RDD的元数据,这种基于反射的方式,代码比较简洁,当你已经知道你的RDD的元数据时,这是一种不错的方式2.通过编程接口来创建DataFrame,你可以在程序运行时动态构建一份元数据,然后将其应用到已经存在的RDD上,这种方式的代码比较冗长,但是如果在编写程序时,还不知道RDD的元数据,只有在程序运行时,才能动态得知元数据,那么只能通过这种动态构建元数据的方式 使用反射的方式推断元数据由于scala具有隐式转换的特点,所以spark sql的scala接口,是支持自动将包含了case class的RDD转换为DataFrame的,case class就定义了元数据,spark sql会通过反射读取传递给case class的参数的名称,然后将其作为列名,spark sql是支持将包含了嵌套数据结构的case class作为元数据的,比如包含了Array等 12345678910111213141516171819202122232425262728293031323334353637383940414243444546//要求Bean是可序列化的case class Student(id:Int, name:String, age:Int) //extends Serializabledef RDD2DataFrameByReflection(): Unit =&#123; val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) // 在scala中使用反射方式,进行RDD到DataFrame的转换,需要手动导入一个隐式转换 import sqlContext.implicits._ // 对一行数据解析成Student对象返回 def parseStudent(str: String): Student =&#123; val fields = str.split(&quot;,&quot;) assert(fields.size == 3) Student(fields(0).trim.toInt, fields(1).toString, fields(2).trim.toInt) &#125; // 因为前面已经导入了隐式的转换,所以这里可以将rdd转成DF val studentDF = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\student.txt&quot;) .map(parseStudent) .toDF() // 将studentDF注册到成一张临时表 studentDF.registerTempTable(&quot;students&quot;) // 操作这张临时表,返回的是一个DF val teenagerDF = sqlContext.sql(&quot;select * from students where age &lt;= 18&quot;) // 将DF转回rdd val teenagerRdd = teenagerDF.rdd //teenagerRdd.foreach(println) /*因为打印的是数组,那么在row中取的时候,是去数组元素 [1,leo,17] [2,marry,17] [3,jack,18] */ teenagerRdd.foreach&#123;row=&gt;println(Student(row(0).toString.toInt, row(1).toString, row(2).toString.toInt))&#125; /* 打印结果: Student(1,leo,17) Student(2,marry,17) Student(3,jack,18) */&#125; 上面的操作可以概括为下面的步骤: 1.将一个RDD通过反射的方式转换成为一个DataFrame(需要隐式转换);2.将DataFrame注册成为一张表;3.从表中查询数据,返回一个新的DataFrame;4.将新的DataFrame转成rdd;5.将RDD写会存储 通过编程接口来创建DataFrame 在编写程序时,还不知道RDD的元数据,只有在程序运行时,才能动态得知元数据,那么只能通过这种动态构建元数据的方式 123456789101112131415161718192021222324252627282930313233343536373839val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val sqlContext = new SQLContext(sc) // 第一步:构造出元素为Row的普通RDD def parseStudent(str: String): Row =&#123; val fields = str.split(&quot;,&quot;) assert(fields.size == 3) Row(fields(0).trim.toInt, fields(1).toString, fields(2).trim.toInt) &#125; val studentRdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\student.txt&quot;) .map(parseStudent) // 第二步:编程方式动态的构造元数据 val structType = StructType(Array( StructField(&quot;id&quot;, IntegerType, true), StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true) )) // 第三步:进行RDD到DataFrame的转换 val studentDF = sqlContext.createDataFrame(studentRdd, structType) // 使用DF studentDF.registerTempTable(&quot;students&quot;) //使用这张表 val teenagerDF = sqlContext.sql(&quot;select * from students where age &lt;=18&quot;) // 将DF转回RDD teenagerDF.rdd.foreach(println) /* 打印结果: [1,leo,17] [2,marry,17] [3,jack,18] */ 以上的操作主要是围绕着下面的方法进行的1createDataFrame(RDD[Row], StructType) :DataFrame = &#123; /* compiled code */ &#125; 1.首先将构造普通的RDD[Row]2.构造StructType,定义元数据3.调用createDataFrame方法返回DataFrame4.将返回的DataFrame注册成为一张表5.对表进行操作,返回新的DataFrame6.将新的DataFrame转回RDD","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"SparkSQL之Hive On Spark","date":"2017-04-16T04:47:25.183Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/SparkSQL之Hive On Spark/","text":"hive是目前大数据领域,事实上的sql标准,底层默认是基于MapReduce实现的,但是由于MapReduce速度实在比较慢,因此,陆续出现了新的sql查询引擎,包括spark sql,hive on Taz,hive on spark等 spark sql与hive on spark是不一样的,spark sql是spark自己研发出来的针对各种数据源,包括hive,json,Parquet,jdbc,rdd等都可以执行查询的,一套基于spark计算引擎的查询引擎,因此它是spark的一个项目,只不过提供了针对hive执行查询的功能而已,适合在一些使用spark技术栈的大数据应用类系统中使用 而hive on spark,是hive的一个项目,它是指,不通过MapReduce作为唯一的查询引擎,而是将spark作为底层的查询引擎,hive on spark,只适用于hive,在可预见的未来,很有可能hive的默认的底层引擎就从MapReduce切换为spark了,适合于原有的hive数据仓库以及数据统计分析替换为spark引擎 hive on spark 环境搭建 1.安装hive参见:”hive安装”一文 2.使用","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark1.4.x新特性和spark1.5.x的新特性","date":"2017-04-16T04:47:25.182Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark1.4.x新特性和spark1.5.x的新特性/","text":"spark1.4.x新特性1.spark core 1.1.提供REST API供外界开发者获取spark内部的各种信息(jobs/stages/tasks/storage info),基于这些API,可以搭建自己的spark监控系统1.2.shuffle阶段,默认将map端写入磁盘的数据进行序列化,优化IO性能1.3.钨丝计划(Project Tungsten),提供了UNSafeShuffleManager,使用缓存友好的排序算法,降低了shuffle的内存使用,提高了排序性能 2.spark streaming2.1.提供了新的spark streaming的UI,能够更好,更清晰的监控spark streaming应用程序的运行状况2.2.支持kafka0.8.2版本 3.spark sql and DataFrame3.1.支持ORCFile3.2.提供了一些window function(窗口函数)3.3.优化了join的性能 spark1.5.x新特性1.DataFrame底层执行的性能优化(钨丝计划的第一阶段)1.1.spark自己来管理内存,而不再依靠JVM管理内存,这样就可以避免JVM GC的性能开销,并且能够控制OOM的问题1.2.java对象直接使用内部的二进制格式化存储和计算,省去了序列化和反序列的性能开销,而且更加节省内存开销1.3.完善了shuffle阶段的UnsafeShuffleManager,增加了不少新功能,优化shuffle性能1.4.默认使用code-gen,使用cache-aware算法,加强了join,aggregation,shuffle,sorting的性能,增强了window function的性能,性能比1.4.x版本提高了数倍 2.DataFrame2.1.实现了新的聚合函数接口,AggregateFunction2,并且提供了7个新的内置聚合函数2.2.实现了100多个新的expression function,例如unix_timestamp等,增强了对NaN的处理2.3.支持连接不通版本的hive metastore2.4.支持Parquet1.7 3.Spark Streaming,更完善的Python支持,非试验的Kafka Direct API等等","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之新闻网站关键指标离线统计","date":"2017-04-16T04:47:25.180Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之新闻网站关键指标离线统计/","text":"案例背景新闻网站1、版块2、新闻页面3、新用户注册4、用户跳出 案例需求分析 每天每个页面的PV：PV是Page View，是指一个页面被所有用户访问次数的总和，页面被访问一次就被记录1次PV每天每个页面的UV：UV是User View，是指一个页面被多少个用户访问了，一个用户访问一次是1次UV，一个用户访问多次还是1次UV新用户注册比率：当天注册用户数 / 当天未注册用户数用户跳出率：IP只浏览了一个页面就离开网站的次数/网站总访问数（PV）版块热度排行榜：根据每个版块每天被访问的次数，做出一个排行榜 网站日志格式date timestamp userid pageid section action 日志字段说明date: 日期，yyyy-MM-dd格式timestamp: 时间戳userid: 用户idpageid: 页面idsection: 新闻版块action: 用户行为，两类，点击页面和注册 模拟数据生成程序模式数据演示 创建hive表和数据导入在hive中创建访问日志表1234567create table news_access ( date string, timestamp bigint, userid bigint, pageid bigint, section string, action string) 将模拟数据导入hive表中1load data local inpath &apos;/usr/local/test/news_access.log&apos; into table news_access; 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254package cn.spark.study.sql.upgrade.news;import java.math.BigDecimal;import java.text.SimpleDateFormat;import java.util.Calendar;import java.util.Date;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;import org.apache.spark.sql.DataFrame;import org.apache.spark.sql.hive.HiveContext;/** * 新闻网站关键指标离线统计Spark作业 * @author Administrator * */public class NewsOfflineStatSpark &#123; public static void main(String[] args) &#123; // 一般来说，在小公司中，可能就是将我们的spark作业使用linux的crontab进行调度 // 将作业jar放在一台安装了spark客户端的机器上，并编写了对应的spark-submit shell脚本 // 在crontab中可以配置，比如说每天凌晨3点执行一次spark-submit shell脚本，提交一次spark作业 // 一般来说，离线的spark作业，每次运行，都是去计算昨天的数据 // 大公司总，可能是使用较为复杂的开源大数据作业调度平台，比如常用的有azkaban、oozie等 // 但是，最大的那几个互联网公司，比如说BAT、美团、京东，作业调度平台，都是自己开发的 // 我们就会将开发好的Spark作业，以及对应的spark-submit shell脚本，配置在调度平台上，几点运行 // 同理，每次运行，都是计算昨天的数据 // 一般来说，每次spark作业计算出来的结果，实际上，大部分情况下，都会写入mysql等存储 // 这样的话，我们可以基于mysql，用java web技术开发一套系统平台，来使用图表的方式展示每次spark计算 // 出来的关键指标 // 比如用折线图，可以反映最近一周的每天的用户跳出率的变化 // 也可以通过页面，给用户提供一个查询表单，可以查询指定的页面的最近一周的pv变化 // date pageid pv // 插入mysql中，后面用户就可以查询指定日期段内的某个page对应的所有pv，然后用折线图来反映变化曲线 // 拿到昨天的日期，去hive表中，针对昨天的数据执行SQL语句 String yesterday = getYesterday(); // 创建SparkConf以及Spark上下文 SparkConf conf = new SparkConf() .setAppName(&quot;NewsOfflineStatSpark&quot;) .setMaster(&quot;local&quot;); JavaSparkContext sc = new JavaSparkContext(conf); HiveContext hiveContext = new HiveContext(sc.sc()); // 开发第一个关键指标：页面pv统计以及排序 calculateDailyPagePv(hiveContext, yesterday); // 开发第二个关键指标：页面uv统计以及排序 calculateDailyPageUv(hiveContext, yesterday); // 开发第三个关键指标：新用户注册比率统计 calculateDailyNewUserRegisterRate(hiveContext, yesterday); // 开发第四个关键指标：用户跳出率统计 calculateDailyUserJumpRate(hiveContext, yesterday); // 开发第五个关键指标：版块热度排行榜 calculateDailySectionPvSort(hiveContext, yesterday); // 关闭Spark上下文 sc.close(); &#125; /** * 获取昨天的字符串类型的日期 * @return 日期 */ private static String getYesterday() &#123; Calendar cal = Calendar.getInstance(); cal.setTime(new Date()); cal.add(Calendar.DAY_OF_YEAR, -1); Date yesterday = cal.getTime(); SimpleDateFormat sdf = new SimpleDateFormat(&quot;yyyy-MM-dd&quot;); return sdf.format(yesterday); &#125; /** * 计算每天每个页面的pv以及排序 * 排序的好处：排序后，插入mysql，java web系统要查询每天pv top10的页面，直接查询mysql表limit 10就可以 * 如果我们这里不排序，那么java web系统就要做排序，反而会影响java web系统的性能，以及用户响应时间 */ private static void calculateDailyPagePv( HiveContext hiveContext, String date) &#123; String sql = &quot;SELECT &quot; + &quot;date,&quot; + &quot;pageid,&quot; + &quot;pv &quot; + &quot;FROM ( &quot; + &quot;SELECT &quot; + &quot;date,&quot; + &quot;pageid,&quot; + &quot;count(*) pv &quot; + &quot;FROM news_access &quot; + &quot;WHERE action=&apos;view&apos; &quot; + &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot; + &quot;GROUP BY date,pageid &quot; + &quot;) t &quot; + &quot;ORDER BY pv DESC &quot;; DataFrame df = hiveContext.sql(sql); // 在这里，我们也可以转换成一个RDD，然后对RDD执行一个foreach算子 // 在foreach算子中，将数据写入mysql中 df.show(); &#125; /** * 计算每天每个页面的uv以及排序 * Spark SQL的count(distinct)语句，有bug，默认会产生严重的数据倾斜 * 只会用一个task，来做去重和汇总计数，性能很差 * @param hiveContext * @param date */ private static void calculateDailyPageUv( HiveContext hiveContext, String date) &#123; String sql = &quot;SELECT &quot; + &quot;date,&quot; + &quot;pageid,&quot; + &quot;uv &quot; + &quot;FROM ( &quot; + &quot;SELECT &quot; + &quot;date,&quot; + &quot;pageid,&quot; + &quot;count(*) uv &quot; + &quot;FROM ( &quot; + &quot;SELECT &quot; + &quot;date,&quot; + &quot;pageid,&quot; + &quot;userid &quot; + &quot;FROM news_access &quot; + &quot;WHERE action=&apos;view&apos; &quot; + &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot; + &quot;GROUP BY date,pageid,userid &quot; + &quot;) t2 &quot; + &quot;GROUP BY date,pageid &quot; + &quot;) t &quot; + &quot;ORDER BY uv DESC &quot;; DataFrame df = hiveContext.sql(sql); df.show(); &#125; /** * 计算每天的新用户注册比例 * @param hiveContext * @param date */ private static void calculateDailyNewUserRegisterRate( HiveContext hiveContext, String date) &#123; // 昨天所有访问行为中，userid为null，新用户的访问总数 String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NULL&quot;; // 昨天的总注册用户数 String sql2 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;register&apos; AND date=&apos;&quot; + date + &quot;&apos; &quot;; // 执行两条SQL，获取结果 Object result1 = hiveContext.sql(sql1).collect()[0].get(0); long number1 = 0L; if(result1 != null) &#123; number1 = Long.valueOf(String.valueOf(result1)); &#125; Object result2 = hiveContext.sql(sql2).collect()[0].get(0); long number2 = 0L; if(result2 != null) &#123; number2 = Long.valueOf(String.valueOf(result2)); &#125; // 计算结果 System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;); System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;); double rate = (double)number2 / (double)number1; System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;); &#125; /** * 计算每天的用户跳出率 * @param hiveContext * @param date */ private static void calculateDailyUserJumpRate( HiveContext hiveContext, String date) &#123; // 计算已注册用户的昨天的总的访问pv String sql1 = &quot;SELECT count(*) FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL &quot;; // 已注册用户的昨天跳出的总数 String sql2 = &quot;SELECT count(*) FROM ( SELECT count(*) cnt FROM news_access WHERE action=&apos;view&apos; AND date=&apos;&quot; + date + &quot;&apos; AND userid IS NOT NULL GROUP BY userid HAVING cnt=1 ) t &quot;; // 执行两条SQL，获取结果 Object result1 = hiveContext.sql(sql1).collect()[0].get(0); long number1 = 0L; if(result1 != null) &#123; number1 = Long.valueOf(String.valueOf(result1)); &#125; Object result2 = hiveContext.sql(sql2).collect()[0].get(0); long number2 = 0L; if(result2 != null) &#123; number2 = Long.valueOf(String.valueOf(result2)); &#125; // 计算结果 System.out.println(&quot;======================&quot; + number1 + &quot;======================&quot;); System.out.println(&quot;======================&quot; + number2 + &quot;======================&quot;); double rate = (double)number2 / (double)number1; System.out.println(&quot;======================&quot; + formatDouble(rate, 2) + &quot;======================&quot;); &#125; /** * 计算每天的版块热度排行榜 * @param hiveContext * @param date */ private static void calculateDailySectionPvSort( HiveContext hiveContext, String date) &#123; String sql = &quot;SELECT &quot; + &quot;date,&quot; + &quot;section,&quot; + &quot;pv &quot; + &quot;FROM ( &quot; + &quot;SELECT &quot; + &quot;date,&quot; + &quot;section,&quot; + &quot;count(*) pv &quot; + &quot;FROM news_access &quot; + &quot;WHERE action=&apos;view&apos; &quot; + &quot;AND date=&apos;&quot; + date + &quot;&apos; &quot; + &quot;GROUP BY date,section &quot; + &quot;) t &quot; + &quot;ORDER BY pv DESC &quot;; DataFrame df = hiveContext.sql(sql); df.show(); &#125; /** * 格式化小数 * @param str 字符串 * @param scale 四舍五入的位数 * @return 格式化小数 */ private static double formatDouble(double num, int scale) &#123; BigDecimal bd = new BigDecimal(num); return bd.setScale(scale, BigDecimal.ROUND_HALF_UP).doubleValue(); &#125; &#125; 调试","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之单独启动master和worker脚本","date":"2017-04-16T04:47:25.179Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之单独启动master和worker脚本/","text":"单独启动master和worker脚本sbin/start-all.sh脚本可以直接启动集群中的master进程和worker进程 这里讲的是单独启动master和worker进程 因为worker进程启动之后,会向master进程去注册,所以需要先启动master进程 为什么有的时候要单独启动master和worker进程?因为可以通过命令行参数为进程配置一些独特的参数,如监听的端口号,web ui 的端口号,使用的cpu和内存等,比如:你可能向单独给某个节点配置不同的cpu和内存资源的使用限制,那么就可以使用脚本单独启动worker进程的时候,通过命令行参数来设置 手动启动master进程需要在某个部署了spark安装包的节点上,使用sbin/start-master.sh启动,master启动之后,启动日志就会打印一行spark://HOST:PORT出来,这就是master的url地址,worker进程就会通过这个地址来连接到master进程,并且进行注册 另外，除了worker进程要使用这个URL以外，我们自己在编写spark代码时，也可以给SparkContext的setMaster()方法，传入这个URL地址然后我们的spark作业，就会使用standalone模式连接master，并提交作业 此外，还可以通过http://MASTER_HOST:8080 URL来访问master集群的监控web ui，那个web ui上，也会显示master的URL地址 手动启动worker进程在部署了spark安装包的前提下,在你希望作为worker node的节点上,使用sbin/start-slave.sh 在当前节点上启动,启动worker的时候需要指定master的url 启动worker进程之后,再访问:http:MASTER_HOST:8080,在集群web ui上,就可以看到新启动的节点,包括该节点的cpu和内存资源 此外,以下参数是可以在手动启动master和worker的时候指定的:-h host, –host host 在哪台机器上启动,默认都是本机-p port, –port port 在机器上启动后,使用哪个端口对外提供服务,master默认是7077,worker默认是随机的–webui-port port web ui端口,master默认的是8080,worker默认的是8081-c cores, –cores cores 仅限于worker,总共能让spark Application使用多少个cpu core,默认是当前机器上的所有的cpu core-m mem, –memory mem 仅限于worker,总共能让spark Application使用多少内存,是100M或者1G这样的格式-d dir, –worker-dir dir 仅限于worker,工作目录,默认是spark home/work目录–properties-file file master和worker加载配置文件的地址,默认是spark_home/conf/spark-defaults.conf 咱们举个例子，比如说小公司里面，物理集群可能就一套，同一台机器上面，可能要部署Storm的supervisor进程，可能还要同时部署Spark的worker进程机器，cpu和内存，既要供storm使用，还要供spark使用这个时候，可能你就需要限制一下worker节点能够使用的cpu和内存的数量 小公司里面，搭建spark集群的机器可能还不太一样，有的机器比如说是有5个g内存，有的机器才1个g内存那你对于1个g内存的机器，是不是得限制一下内存使用量，比如说500m 实例:1、启动master: 日志和web ui，观察master url 2、启动worker: 观察web ui，是否有新加入的worker节点，以及对应的信息 3、单独关闭master和worker,此时的顺序得反过来,先关闭worker,再去关闭master 4、再次单独启动master和worker，给worker限定，就使用500m内存，跟之前看到的worker信息比对一下内存最大使用量","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之主要的几个术语","date":"2017-04-16T04:47:25.177Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之主要的几个术语/","text":"术语 解释 Application spark应用程序,就是用户基于spark API开发的程序,一定是通过一个有main方法的类执行的 Application jar 这个就是把写好的spark工程，打包成一个jar包，其中包括了所有的第三方jar依赖包，比如java中，就用maven+assembly插件打包最方便 Driver 在使用spark-submit提交应用的时候,会指定一个主类,这个类有一个main方法,driver进程就是在运行程序中的main方法的进程,这就是driver cluster manager 集群管理器,就是为每个spark application，在集群中调度和分配资源的组件，比如Spark Standalone、YARN、Mesos等 deploy mode 部署模式，无论是基于哪种集群管理器(Standalone、YARN、Mesos)，spark作业部署或者运行模式，都分为两种，client和cluster，client模式下driver运行在提交spark作业的机器上,即执行应用的main类(主要用于测试)；cluster模式下，driver运行在spark集群中的某一个节点上 Worker Node 集群中的工作节点，能够运行executor进程，运行作业代码的节点 Executor 集群管理器为application分配的进程，运行在worker节点上，负责执行作业的任务，并将数据保存在内存或磁盘中，每个application都有自己的executor Job 每个spark application，根据你执行了多少次action操作，就会有多少个job Stage 根据是否有shuffle,每个job都会划分为多个stage（阶段），每个stage都会有对应的一批task，分配到executor上去执行 Task driver发送到executor上执行的计算单元，每个task负责在一个阶段（stage），处理一小片数据，计算出对应的结果","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之yarn模式","date":"2017-04-16T04:47:25.175Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之yarn模式/","text":"yarn-client模式原理 yarn-cluster模式原理 yarn-client模式提交spark作业yarn运行spark作业的前提如果想要让spark作业可以运行在yarn上面,那么首先就必须在spark-env.sh文件中,配置HADOOP_CONF_DIR或者YARN_CONF_DIR属性,值为hadoop的配置文件的目录,即:HADOOP_HOME/etc/hadoop,其中包含了hadoop和yarn所有的配置文件,比如:hdfs-site.xml,yarn-site.xml等,spark需要这些配置来读写HDFS,以及连接到yarn ResourceManager上,这个目录中包含的配置文件都会被分发到yarn集群中去的 vim spark/conf/spark-env.sh123456export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/*在/usr/local/hadoop/etc/hadoop目录下有:yarn-site.xml(其中可以找到ResourceManager所在的机器)还有一些其他的配置文件*/ 跟spark standalone模式不同,通常不需要使用–master指定master URL因为spark会从hadoop的配置文件中去读ResourceManager的配置,这样就知道了ResourceManager所在的机器(master),所以不需要我们指定,但是我们需要指定deploy mode,如下示例:12345678910/export/servers/spark/bin/spark-submit \\--class cn.spark.study.core.WordCount \\--master yarn-cluster#--master yarn-client--num-executors 1 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 1 \\--queue hadoop队列/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\ –queue,在不同的部门,或者是不同的大数据项目,共用一个yarn集群,运行spark作业,推荐一定要用–queue,指定不同的hadoop队列,做项目或者部门之间的队列隔离 与Standalone模式类似,yarn-client模式通常建议在测试的时候使用,方便你直接在提交作业的机器上查看日志,但是作业实际部署到生产环境中进行运行的时候,还是使用yarn-cluster模式 yarn模式下需要观察的点:1.日志命令行日志web ui日志 2.web ui的地址不再是spark://192.168.0.108:8080这种URL了,因为那是Standalone模式下的监控web ui,在yarn模式下,要看yarn的web ui: http://192.168.0.108:8088/ 这是yarn的URL地址 3.进程driver是什么进程 AppLicationMaster是什么进程 executor进程 yarn模式下的日志查看在yarn模式下,spark作业运行相关的executor和ApplicationMaster都是运行在yarn的container中的,一个作业运行完了以后,yarn有两种方式来处理spark作业打印出的日志 1.聚合日志方式(推荐,比较常用)这种格式将散落在集群中各个机器上的日志,最后都给聚合起来,让我们可以统一查看,如果yarn的日志聚合的选项打开了,即:yarn.log-aggregation-enable(yarn-site.xml文件中配置), container的日志会拷贝到HDFS上去,并从机器中删除 然后我们使用yarn logs -applicationId 命令来查看日志(app Id在yarn的web ui上看:resourceManager_host:8088) yarn logs命令,会打印出application对应的所有container的日志出来,当然,因为日志是在HDFS上的,我们自然可以通过HDFS的命令行来直接从HDFS中查看日志,日志在HDFS中的目录,可以通过查看yarn.nodemanager.remote-app-log-dir和yarn.nodemanager.remote-app-log-dir-suffix属性来获知 2.web ui日志也可以通过spark web ui来查看executor的输出日志但是此时需要启动History Server,需要让spark history server和mapreduce history server运行着;并且在yarn-site.xml文件中,配置yarn.log.server.url属性spark history server web ui中的log url,会将你重新定向到mapreduce history server上去查看日志 3.分散查看(通常不推荐)如果没有打开聚合日志选项,那么日志默认就是散落在各个机器上的本次磁盘目录中的,在YARN_APP_LOGS_DIR目录下,根据hadoop版本的不同,通常在/tmp/logs目录下,或者在$HADOOP_HOME/logs/userlogs目录下,如果你要查看某个container的日志,那么就得登录到那台机器上去,然后到指定的目录下如,找到那个日志文件,然后才能查看 yarn模式相关的参数123456789101112131415161718192021222324252627yarn模式运行spark作业所有属性详解属性名称 默认值 含义spark.yarn.am.memory 512m client模式下，YARN Application Master使用的内存总量spark.yarn.am.cores 1 client模式下，Application Master使用的cpu数量spark.driver.cores 1 cluster模式下，driver使用的cpu core数量，driver与Application Master运行在一个进程中，所以也控制了Application Master的cpu数量spark.yarn.am.waitTime 100s cluster模式下，Application Master要等待SparkContext初始化的时长; client模式下，application master等待driver来连接它的时长spark.yarn.submit.file.replication hdfs副本数 作业写到hdfs上的文件的副本数量，比如工程jar，依赖jar，配置文件等，最小一定是1spark.yarn.preserve.staging.files false 如果设置为true，那么在作业运行完之后，会避免工程jar等文件被删除掉spark.yarn.scheduler.heartbeat.interval-ms 3000 application master向resourcemanager发送心跳的间隔，单位msspark.yarn.scheduler.initial-allocation.interval 200ms application master在有pending住的container分配需求时，立即向resourcemanager发送心跳的间隔spark.yarn.max.executor.failures executor数量*2，最小3 整个作业判定为失败之前，executor最大的失败次数spark.yarn.historyServer.address 无 spark history server的地址spark.yarn.dist.archives 无 每个executor都要获取并放入工作目录的archivespark.yarn.dist.files 无 每个executor都要放入的工作目录的文件spark.executor.instances 2 默认的executor数量spark.yarn.executor.memoryOverhead executor内存10% 每个executor的堆外内存大小，用来存放诸如常量字符串等东西spark.yarn.driver.memoryOverhead driver内存7% 同上spark.yarn.am.memoryOverhead AM内存7% 同上spark.yarn.am.port 随机 application master端口spark.yarn.jar 无 spark jar文件的位置spark.yarn.access.namenodes 无 spark作业能访问的hdfs namenode地址spark.yarn.containerLauncherMaxThreads 25 application master能用来启动executor container的最大线程数量spark.yarn.am.extraJavaOptions 无 application master的jvm参数spark.yarn.am.extraLibraryPath 无 application master的额外库路径spark.yarn.maxAppAttempts 提交spark作业最大的尝试次数spark.yarn.submit.waitAppCompletion true cluster模式下，client是否等到作业运行完再退出 以上这些参数可以在spark-submit中配置,使用–conf配置 spark-submit详解spark-submit可以通过一个统一的接口,将spark应用程序提交到所有spark支持的集群管理器上(Standalone(mater),Yarn(ResourceManager)等),所以我们并不需要为每种集群管理器都做特殊的配置 –master1.如果不设置,那么就是local模式2.如果设置spark://开头的URL,那么就是Standalone模式,会提交到指定的URL的Mater进程上去3.如果设置yarn-client/yarn-cluster,那么就是yarn模式,会读取hadoop配置文件,然后连接ResourceManager","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之worker节点配置以及spark-env.sh参数详解","date":"2017-04-16T04:47:25.174Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之worker节点配置以及spark-env.sh参数详解/","text":"worker节点配置场景:如果在已有的spark集群中,你想要加入一台新的worker节点 如果你想将某台机器部署成Standalone集群架构中的worker节点,那么就必须在该机器上部署spark安装包,并修改配置文件如下:1234567891011121314#修改配置文件cd spark/conf/mv spark-env.sh.template spark-env.shvim spark-env.sh //添加export JAVA_HOME=/home/hadoop/app/jdk1.7.0_80export SPARK_MASTER_IP=hdp-node-01 //配置master的机器export SPARK_MASTER_PORT=7077#######################################################mv slaves.template slaves vim slaves //添加worker的节点hdp-node-01hdp-node-02#######################################################// 注意要配置多个机器之间的ssh免密码登录 spark-env.sh参数详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556SPARK_MASTER_IP #指定master进程所在的机器的ip地址SPARK_MASTER_PORT #指定master监听的端口号（默认是7077）SPARK_MASTER_WEBUI_PORT #指定master web ui的端口号（默认是8080）#其实使用spark-env.sh配置的参数和我们手动启动master时指定的参数是一样的,sbin/start-master.sh --port 7078，类似这种方式，貌似可以指定一样的配置属性我明确告诉大家，这个作用的确是一模一样的#你可以在spark-evn.sh中就去配置好,但是有时呢，可能你会遇到需要临时更改配置，并启动master或worker进程的情况#此时就比较适合，用sbin/start-master.sh这种脚本的命令行参数，来设置这种配置属性#但是通常来说呢，还是推荐在部署的时候，通过spark-env.sh来设定脚本命令行参数通常用于临时的情况SPARK_MASTER_OPTS #设置master的额外参数，使用&quot;-Dx=y&quot;设置各个参数(x对应的是参数名,y对应的是参数的值)比如说export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=1&quot;参数名 默认值 含义spark.deploy.retainedApplications 200 在spark web ui上最多显示多少个application的信息spark.deploy.retainedDrivers 200 在spark web ui上最多显示多少个driver的信息spark.deploy.spreadOut true 资源调度策略，spreadOut会尽量将application的executor进程分布在更多worker上，适合基于hdfs文件计算的情况，提升数据本地化概率；非spreadOut会尽量将executor分配到一个worker上，适合计算密集型的作业spark.deploy.defaultCores 无限大 每个spark作业最多在standalone集群中使用多少个cpu core，默认是无限大，有多少用多少spark.deploy.timeout 60 单位秒，一个worker多少时间没有响应之后，master认为worker挂掉了------------------------------------------------------------SPARK_LOCAL_DIRS spark的工作目录，包括了shuffle map输出文件，以及持久化到磁盘的RDD等SPARK_WORKER_PORT worker节点的端口号，默认是随机的SPARK_WORKER_WEBUI_PORT worker节点的web ui端口号，默认是8081SPARK_WORKER_CORES worker节点上，允许spark作业使用的最大cpu数量，默认是机器上所有的cpu coreSPARK_WORKER_MEMORY worker节点上，允许spark作业使用的最大内存量，格式为1000m，2g等，默认最小是1g内存就是说，有些master和worker的配置，可以在spark-env.sh中部署时即配置，但是也可以在start-slave.sh脚本启动进程时命令行参数设置但是命令行参数的优先级比较高，会覆盖掉spark-env.sh中的配置比如说，上一讲我们的实验，worker的内存默认是1g，但是我们通过--memory 500m，是可以覆盖掉这个属性的SPARK_WORKER_INSTANCES 当前机器上的worker进程数量，默认是1，可以设置成多个，但是这时一定要设置SPARK_WORKER_CORES，限制每个worker的cpu数量SPARK_WORKER_DIR spark作业的工作目录，包括了作业的日志等，默认是spark_home/workSPARK_WORKER_OPTS worker的额外参数，使用&quot;-Dx=y&quot;设置各个参数参数名 默认值 含义spark.worker.cleanup.enabled false 是否启动自动清理worker工作目录，默认是falsespark.worker.cleanup.interval 1800 单位秒，自动清理的时间间隔，默认是30分钟spark.worker.cleanup.appDataTtl 7 * 24 * 3600 默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天-----------------------------------------------------------------SPARK_DAEMON_MEMORY 分配给master和worker进程自己本身的内存，默认是1gSPARK_DAEMON_JAVA_OPTS 设置master和worker自己的jvm参数，使用&quot;-Dx=y&quot;设置各个参数SPARK_PUBLISC_DNS master和worker的公共dns域名，默认是没有的这里提示一下，大家可以观察一下，咱们的内存使用情况在没有启动spark集群之前，我们的内存使用是1个多g，启动了spark集群之后，就一下子耗费到2个多g每次又执行一个作业时，可能会耗费到3个多g左右所以大家就明白了，为什么之前用分布式的集群，每个worker节点才1个g内存，根本是没有办法使用standalone模式和yarn模式运行作业的 下面是给大家列出spark所有的启动和关闭shell脚本1234567sbin/start-all.sh 根据配置，在集群中各个节点上，启动一个master进程和多个worker进程sbin/stop-all.sh 在集群中停止所有master和worker进程sbin/start-master.sh 在本地启动一个master进程sbin/stop-master.sh 关闭master进程sbin/start-slaves.sh 根据conf/slaves文件中配置的worker节点，启动所有的worker进程sbin/stop-slaves.sh 关闭所有worker进程sbin/start-slave.sh 在本地启动一个worker进程","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之Thrift JDBC_ODBC server","date":"2017-04-16T04:47:25.173Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Thrift JDBC_ODBC server/","text":"Spark SQL的Thrift JDBC/ODBC server是基于Hive 0.13的HiveServer2实现的。这个服务启动之后，最主要的功能就是可以让我们通过Java JDBC来以编程的方式调用Spark SQL。此外，在启动该服务之后，可以通过Spark或Hive 0.13自带的beeline工具来进行测试。 启动thriftserver服务要启动JDBC/ODBC server，主要执行Spark的sbin目录下的start-thriftserver.sh命令即可 start-thriftserver.sh命令可以接收所有spark-submit命令可以接收的参数，额外增加的一个参数是–hiveconf，可以用于指定一些Hive的配置属性。可以通过执行./sbin/start-thriftserver.sh –help来查看所有可用参数的列表。默认情况下，启动的服务会在localhost:10000地址上监听请求。 可以使用两种方式来改变服务监听的地址12345678910111213第一种：指定环境变量export HIVE_SERVER2_THRIFT_PORT=&lt;listening-port&gt;export HIVE_SERVER2_THRIFT_BIND_HOST=&lt;listening-host&gt;./sbin/start-thriftserver.sh \\ --master &lt;master-uri&gt; \\ ... 第二种：使用命令的参数./sbin/start-thriftserver.sh \\ --hiveconf hive.server2.thrift.port=&lt;listening-port&gt; \\ --hiveconf hive.server2.thrift.bind.host=&lt;listening-host&gt; \\ --master &lt;master-uri&gt; ... hdfs dfs -chmod 777 /tmp/hive-root ./sbin/start-thriftserver.sh \\–jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar 这两种方式的区别就在于，第一种是针对整个机器上每次启动服务都生效的; 第二种仅仅针对本次启动生效 beeline工具来测试接着就可以通过Spark或Hive的beeline工具来测试Thrift JDBC/ODBC server在Spark的bin目录中，执行beeline命令（当然，我们也可以使用Hive自带的beeline工具）：1./bin/beeline 进入beeline命令行之后，连接到JDBC/ODBC server上去：1beeline&gt; !connect jdbc:hive2://localhost:10000 beeline通常会要求你输入一个用户名和密码。在非安全模式下，我们只要输入本机的用户名（比如root），以及一个空的密码即可。对于安全模式，需要根据beeline的文档来进行认证。 JDBC/ODBC服务访问Spark SQL除此之外，大家要注意的是，如果我们想要直接通过JDBC/ODBC服务访问Spark SQL，并直接对Hive执行SQL语句，那么就需要将Hive的hive-site.xml配置文件放在Spark的conf目录下。 Thrift JDBC/ODBC server也支持通过HTTP传输协议发送thrift RPC消息。使用以下方式的配置可以启动HTTP模式：123456789101112131415161718192021命令参数./sbin/start-thriftserver.sh \\ --hive.server2.transport.mode=http \\ --hive.server2.thrift.http.port=10001 \\ --hive.server2.http.endpoint=cliservice \\ --master &lt;master-uri&gt; ... ------------------------./sbin/start-thriftserver.sh \\ --jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar \\ --hiveconf hive.server2.transport.mode=http \\ --hiveconf hive.server2.thrift.http.port=10001 \\ --hiveconf hive.server2.http.endpoint=cliservice beeline连接服务时指定参数beeline&gt; !connect jdbc:hive2://localhost:10001/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice//默认访问的是hive的default库------------------------ 最重要的，当然是通过Java JDBC的方式，来访问Thrift JDBC/ODBC server，调用Spark SQL，并直接查询Hive中的数据 需要添加的依赖123456789101112131415&lt;dependency&gt; &lt;groupId&gt;org.apache.hive&lt;/groupId&gt; &lt;artifactId&gt;hive-jdbc&lt;/artifactId&gt; &lt;version&gt;0.13.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpclient&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.httpcomponents&lt;/groupId&gt; &lt;artifactId&gt;httpcore&lt;/artifactId&gt; &lt;version&gt;4.4.1&lt;/version&gt;&lt;/dependency&gt; 编码实现1234567891011121314151617181920212223242526272829303132333435import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class ThriftJDBCServerTest &#123; public static void main(String[] args) &#123; String sql = &quot;select name from users where id=?&quot;; Connection conn = null; PreparedStatement pstmt = null; ResultSet rs = null; try &#123; Class.forName(&quot;org.apache.hive.jdbc.HiveDriver&quot;); //使用的是HiveDriver驱动 conn = DriverManager.getConnection(&quot;jdbc:hive2://192.168.0.103:10001/default?hive.server2.transport.mode=http;hive.server2.thrift.http.path=cliservice&quot;, &quot;root&quot;, &quot;&quot;);//default是hive的中的默认的库,我们可以指定我们的库 pstmt = conn.prepareStatement(sql); pstmt.setInt(1, 1); rs = pstmt.executeQuery(); while(rs.next()) &#123; String name = rs.getString(1); System.out.println(name); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之standalone集群架构","date":"2017-04-16T04:47:25.171Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之standalone集群架构/","text":"spark core之standalone集群架构","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之Standalone模式","date":"2017-04-16T04:47:25.170Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之Standalone模式/","text":"Standalone client模式提交作业通常情况来说,部署在测试机器上去,进行测试运行spark作业的时候,都是使用的是client模式,client模式下,提交作业以后,driver在本地启动,可以实时看到详细的日志信息,方便你追踪和排查错误 client的三种模式:1.硬编码:SparkConf.setMaster(“spark://IP:PORT”)2.spark-submit提交的时候设置一下:–master spark://IP:PORT3.spark-shell去启动的时候可以指定:–master spark://IP:PORT 上面三种写法,使用第二种,是最合适的 在Standalone模式下中,在spark-submit提交脚本中,用–master指定Master的URL的,使用Standalone client模式或者是cluster模式,是要在spark-submit中使用–deploy-mode client/cluster来设置,但是如果不设置,默认的–deploy-mode为client模式 使用spark-submit脚本来提交application时，application jar是会自动被分发到所有worker节点上去的,对于你的application依赖的额外jar包，可以通过spark-submit脚本中的–jars标识，来指定，可以使用逗号分隔多个jar,比如说，你写spark-sql的时候，有的时候，在作业中，要往mysql中写数据，此时可能会出现找不到mysql驱动jar包的情况,此时，就需要你手动在spark-submit脚本中，使用–jars，加入一些依赖的jar包 提交的脚本 123456789/export/servers/spark/bin/spark-submit \\--master spark://hdp-node-01:7077 \\--deploy-mode client \\--class cn.spark.study.core.WordCount \\--num-executors 1 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 1 \\/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\ 提交Standalone client模式的作业1.–master 和 –deploy-mode 来提交作业2.在web UI查看,可以看到completed applications一栏中,有刚刚提交的作业,对比一下UI上的ApplicationID和driver机器上打印的日志的ApplicationID3.使用jps查看进程,看Standalone client模式提交作业的时候,当前机器上会有哪些进程(会看到一个sparksubmit相当于driver进程,还是一个进程是CoarseGrainedExecutorBackend进程,这个进程就是在worker机器上的executor进程) Standalone cluster模式提交作业Standalone cluster模式,通常用于,spark作业部署到生产环境中去使用,Standalone client模式下,在spark-submit脚本执行的机器上,会启动driver进程,然后去进行整个作业的调度,通常来说,你的spark-submit脚本能够执行的机器,也就是,作为一个开发人员能够登录的机器,通常不会直接是spark集群部署的机器,因为不是随便谁都能够登录到spark集群中某个机器上去执行一个脚本,这是没有安全性可言的,用client模式,你的机器可能与spark集群部署的机器,都不在一个机房,或者是举例很远,那么此时远距离的频繁的网络通信会影响整个作业的执行性能,所以在生产环境中是使用的Standalone cluster模式,因为在这种模式下,会由master在集群中的一个节点上来启动driver,然后driver会进行频繁的作业调度,此时driver和集群是在一起的,这样性能是比较高的 此外,在Standalone cluster模式下,还支持监控你的driver进程,并且在driver进程挂掉的时候,自动重启该进程,要使用这个功能,在spark-submit脚本中,使用–supervise参数即可,这个参数其实在spark streaming中作为HA高可用来的,配置driver的高可用 如果想要杀掉反复挂掉的driver进程,使用以下即可:123bin/spark-class org.apache.spark.deploy.Client kill &lt;master url&gt; &lt;driver ID&gt;#如果要查看driver id，通过http://&lt;maser url&gt;:8080即可查看到 提交的脚本 123456789/export/servers/spark/bin/spark-submit \\--master spark://hdp-node-01:7077 \\--deploy-mode cluster \\--class cn.spark.study.core.WordCount \\--num-executors 1 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 1 \\/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\ Standalone模式下的多作业资源调度Standalone集群对于同时提交上来的多个作业(Application),仅仅支持FIFO调度策略,也就是先入先出 默认情况下,集群对多作业同时执行的支持是不好的,没有办法同时执行多个作业,因为先提交上来的每一个作业都会尝试使用集群中的所有可用的cpu资源(spreadOut),此时相当于只能支持Application串行一个一个运行了,因此如果我们希望能够支持多作业同时运行,那么就需要调整一些资源参数了 我们需要调整的资源参数是:spark.cores.max,来限制每个作业能够使用的最大的cpu core数量,这样先提交上来的作业不会使用所有的cpu资源,后面提交上来的作业就可以获取到资源了,这样就可以同时运行多个Application 比如说,如果集群一共有20个节点，每个节点是8核，160 cpu core,那么，如果你不限制每个作业获取的最大cpu资源大小，而且在你spark-submit的时候，或者说，你就设置了num-executors，total-cores，160 此时，你的作业是会使用所有的cpu core资源的,所以，如果我们可以通过设置全局的一个参数，让每个作业最多只能获取到一部分cpu core资源,那么，后面提交上来的作业，就也可以获取到一部分资源,standalone集群，才可以支持同时执行多个作业 使用SparkConf或spark-submit中的–conf标识，设置参数即可 123SparkConf conf = new SparkConf() .set(&quot;spark.cores.max&quot;, &quot;10&quot;) 通常不建议使用SparkConf，硬编码，来设置一些属性，不够灵活,建议使用spark-submit来设置属性1--conf spark.cores.max=10 此外，还可以直接通过spark-env.sh配置每个application默认能使用的最大cpu数量来进行限制，默认是无限大，此时就不需要每个application都自己手动设置了在spark-env.sh中配置spark.deploy.defaultCores即可1export SPARK_MASTER_OPTS=&quot;-Dspark.deploy.defaultCores=10&quot; Standalone模式下的作业监控与日志记录spark standalone模式，提供了一个web界面来让我们监控集群，并监控所有的作业的运行,web界面上,提供了master和worker的相关信息,默认的话,我们的web界面运行在master机器上的8080端口 spark web ui1.哪些作业在跑2.哪些作业跑完了,花了多长时间,使用了多少资源3.哪些作业跑失败了 Application web ui1.可以看到job,stage,task的详细运行信息2.shuffle read,shuffle write,gc,运行时间,每个task分配的数据量3.定位很多性能问题、troubleshooting等等，如task数据分布不允许，那么就是数据倾斜4.哪个stage运行的时间最慢，通过之前讲解的stage划分算法，去你的代码里定位到，那个stage对应的是哪一块儿代码，你的那段代码为什么会运行太慢,使用优化策略去优化性能 但是有个问题，作业运行完了以后，我们就看不到了,此时跟history server有关，需要我们开启 日志记录1、系统级别的，spark自己的日志记录2、我们在程序里面，用log4j，或者System.out.println打印出来的日志 spark web ui中可以看到1、看每个application在每个executor上的日志2、stdout，可以显示我们用System.out.println打印出来的日志，stderr，可以显示我们用System.err.println打印出来的日志 此外，我们自己在spark作业代码中，打出来的日志，比如用System.out.println()等，是打到每个作业在每个节点的工作目录中去的,默认是SPARK_HOME/work目录下,这个目录下，每个作业都有两个文件，一个是stdout，一个是stderr，分别代表了标准输出流和异常输出流","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之spark算子的闭包原理详解","date":"2017-04-16T04:47:25.169Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子的闭包原理详解/","text":"通常来说，这个问题跟在RDD的算子中操作作用域外部的变量有关,所谓RDD算子中，操作作用域外部的变量，指的是，类似下面的语句: var a = 0; rdd.foreach(i -&gt; a += i),此时，对rdd执行的foreach算子的作用域，其实仅仅是它的内部代码，但是这里却操作了作用域外部的a变量,根据不同的编程语言的语法，这种功能是可以做到的，而这种现象就叫做闭包 闭包简单来说，就是操作的不属于一个作用域范围的变量 如果使用local模式运行spark作业，那么实际只有一个jvm进程在执行这个作业,此时，你所有的RDD算子的代码执行以及它们操作的外部变量，都是在一个进程的内存中，这个进程就是driver进程,此时是没有任何问题的 但是在作业提交到集群执行的模式下（无论是client或cluster模式，作业都是在集群中运行的）,为了分布式并行执行你的作业，spark会将你的RDD算子操作，分散成多个task，放到集群中的多个节点上的executor进程中去执行,每个task执行的是相同的代码，但是却是处理不同的数据 在提交作业的task到集群去执行之前，spark会先在driver端处理闭包,spark中的闭包，特指那些，不在算子的作用域内部，但是在作用域外部却被算子处理和操作了的变量而算子代码的执行也需要这些变量才能顺利执行,此时，这些闭包变量会被序列化成多个副本，然后每个副本都发送到各个executor进程中，供那个executor进程运行的task执行代码时使用 注意:从spark的官网描述在local模式下,是可以看到的,但是我们真正试验的时候,在local模式下,并不是如官网描述的那样,如下代码: val sparkConf = new SparkConf().setAppName(&quot;dataFrame&quot;).setMaster(&quot;local&quot;) val sc = new SparkContext(sparkConf) val numberRdd = sc.parallelize(Seq(1,2,3,4,5)) var sum = 0 numberRdd.foreach{ i=&gt;sum+=i println(&quot;--------foreach内部:------------&quot;+sum+&quot;.................&quot;) } println(&quot;---------driver:-----------&quot;+sum+&quot;.................&quot;) /*执行结果: --------foreach内部:------------1................. --------foreach内部:------------3................. --------foreach内部:------------6................. --------foreach内部:------------10................. --------foreach内部:------------15................. ---------driver:-----------0................. */ 对于上面说的闭包变量处理机制 对于local模式，没有任何特别的影响，毕竟都在一个jvm进程中，变量发送到executor，也不过就是进程中的一个线程而已,但是对于集群运行模式来说，每个executor进程，都会得到一个闭包变量的副本，这个时候，就会出问题 因此闭包变量发送到executor进程中之后，就变成了一个一个独立的变量副本了，这就是最关键的一点,此时在executor进程中，执行task和算子代码时，访问的闭包变量，也仅仅只是当前executor进程中的一个变量副本而已了,此时虽然在driver进程中，也有一个变量副本，但是却完全跟各个executor进程中的变量副本不是一个东西,此时，各个executor进程对于自己内存中的变量副本进行操作，即使改变了变量副本的值，但是对于driver端的程序，是完全感知不到的driver端的变量没有被进行任何操作 因此综上所述，在你使用集群模式运行作业的时候，切忌不要在算子内部，对作用域外面的闭包变量进行改变其值的操作,因为那没有任何意义，算子仅仅会在executor进程中，改变变量副本的值对于driver端的变量没有任何影响，我们也获取不到executor端的变量副本的值,如果希望在集群模式下，对某个driver端的变量，进行分布式并行地全局性的修改,可以使用Spark提供的Accumulator，全局累加器","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之spark算子汇总","date":"2017-04-16T04:47:25.167Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark算子汇总/","text":"mapPartitions123456789101112131415161718192021222324252627val sparkConf = new SparkConf() .setAppName(&quot;Rdd&quot;) .setMaster(&quot;local&quot;)val sc = new SparkContext(sparkConf)val studentNames = Array(&quot;张三&quot;, &quot;李四&quot;, &quot;王五&quot;)val studentNamesRdd = sc.parallelize(studentNames, 2)//mapPartitions类似于map// 不同之处在于map算子一次处理一个partition中的一条数据,// mapPartitions算子,一次处理一个partition中的所有的数据// 推荐使用场景// 如果你的Rdd的数据量不是特别大,那么建议采用mapPartitions算子代替map算子,这样可以加快处理速度// 但是如果你的rdd的数据量特别大,比如10条数据,不建议使用mapPartitions,因为可能会内存溢出studentNamesRdd.mapPartitions&#123; ite=&gt; var arr = mutable.ArrayBuffer&lt;String&gt;() while(ite.hasNext)&#123; val studentName = ite.next val studentScore = studentScoresMap.get(studentName) &#125; null&#125; mapPartitionsWithIndex12345678map是对每个元素操作, mapPartitions是对其中的每个partition操作------------------------------------------------------------mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码val func = (index: Int, iter: Iterator[(Int)]) =&gt; &#123; //index是分区的索引,Iterator是一个分区中的数据,可以迭代 iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func).collect groupByKey算子原理 reduceByKey123456789101112#Examplesc.textFile(args(0)).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).#源码 def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123; combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner) &#125;/*在reduceByKey的内部是调用的combineByKeyWithClassTag, 由上面的源码知道combineByKeyWithClassTag的第一个参数是对value原样输出,第二个,第三个参数是调用reduceByKey中指定的函数第二个参数的功能:是在partition内进行操作第三个参数的功能是对各个partition的所有的结果进行操作*/ reduceByKey算子原理 distinct算子原理 cogroup算子原理 intersection1234val rdd6 = sc.parallelize(List(5,6,4,7))val rdd7 = sc.parallelize(List(1,2,3,4))#intersection求交集val rdd9 = rdd6.intersection(rdd7) 123//对rdd中的数据进行去重distinct([numTasks])) //Return a new dataset that contains the distinct elements of the source dataset. intersection算子原理 join算子原理 sortByKey原理 aggregate12345678910111213141516171819202122232425262728293031323334353637383940414243aggregate(参数1)(参数2,参数3) //参数1是初始化的值,参数2是一个函数,对每一个partition的数据聚合,参数3 是对所有partition的结果进行聚合###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]###0 + (0+1+2+3+4/*局部求和*/ + 0+5+6+7+8+9/*局部求和*/)rdd1.aggregate(0)(_+_, _+_/*全局求和*/)rdd1.aggregate(0)(math.max(_, _), _ + _) //传给第一个参数的是:itera ,所以使用math.max(_, _) 去迭代###5和1比, 得5再和234比得5 --&gt; 5和6789比,得9 --&gt; 5 + (5+9)rdd1.aggregate(5)(math.max(_, _), _ + _) add1.reduce(math.max(_,_))//第一个下划线是上一次求max的值,第二个下划线是循环add1中的一个元素 aggregate:先进行局部的(partition)操作(循环迭代所有的局部元素),然后进行全局的操作(循环迭代所有的分区) val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;rdd2.aggregate(&quot;&quot;)(_ + _, _ + _) //abcdefrdd2.aggregate(&quot;=&quot;)(_ + _, _ + _) //==abc=def :局部求和为 =abc 和 =def 最后整体求和:==abc=def val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)//24 或者是42 因为是并行的任务,所以可能先返回2,也有可能先返回4 val rdd4 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;&quot;),2)rdd4.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)//返回10 或者01/*因为有初始值(空字符串)的存在,所以如下过程:分区0: math.min(&quot;&quot;.length,&quot;12&quot;.length) ==&gt;0.toString &quot;0&quot; math.min(&quot;0&quot;.length,&quot;23&quot;.length) ==&gt;1.toString &quot;1&quot; 最终结果===&gt;1 分区1: math.min(&quot;&quot;.length,&quot;345&quot;.length) ==&gt;0.toString &quot;0&quot; math.min(&quot;0&quot;.length,&quot;&quot;.length) ==&gt;0.toString &quot;0&quot; 最终结果===&gt;0 分区全局聚合:&quot;&quot;+&quot;1&quot;+&quot;0&quot; 或者 &quot;&quot;+&quot;0&quot;+&quot;1&quot;*/ val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) //结果: &quot;11&quot; sample123456789101112131415161718192021222324val rand = studentNamesRdd.sample(false,0.1,9)/* /** * Return a sampled subset of this RDD. * * @param withReplacement can elements be sampled multiple times (replaced when sampled out) * @param fraction expected size of the sample as a fraction of this RDD&apos;s size * without replacement: probability that each element is chosen; fraction must be [0, 1] * with replacement: expected number of times each element is chosen; fraction must be &gt;= 0 * @param seed seed for the random number generator */ def sample( withReplacement: Boolean, fraction: Double, seed: Long = Utils.random.nextLong): RDD[T] = withScope &#123; require(fraction &gt;= 0.0, &quot;Negative fraction value: &quot; + fraction) if (withReplacement) &#123; new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed) &#125; else &#123; new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed) &#125; &#125;*/ union12345#union求并集，注意类型要一致val rdd6 = sc.parallelize(List(5,6,4,7))val rdd7 = sc.parallelize(List(1,2,3,4))val rdd8 = rdd6.union(rdd7)rdd8.distinct.sortBy(x=&gt;x).collect union算子原理 aggregateByKey12345678910111213141516171819202122232425262728293031323334353637 val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2) def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator &#125; pairRDD.mapPartitionsWithIndex(func2).foreach(println) //查看分区的结果 pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).foreach(println) //在局部可以将key相同的放在一起迭代,math.max(_, _) 就是取一个分区中key相同的元素中的最大的值 pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).foreach(println)//会在每个分区中有一个初始化的值:100/*第一个参数:每个key的初始值第二个参数:如何进行shuffle map-side的本地聚合第三个参数:如何进行shuffle reduce-side的全局聚合*//*//这里是分区信息[partID:0, val: (cat,2)][partID:0, val: (cat,5)][partID:0, val: (mouse,4)][partID:1, val: (cat,12)][partID:1, val: (dog,12)][partID:1, val: (mouse,2)]//这里是aggregate的结果(dog,12)(cat,17)(mouse,6)(dog,100)(cat,200)(mouse,200)*/ cartesian笛卡尔积123456789101112131415#cartesian笛卡尔积val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))rdd1.cartesian(rdd2).foreach(println)/*(tom,tom)(tom,kitty)(tom,shuke)(jerry,tom)(jerry,kitty)(jerry,shuke)*/ cartesian算子原理 coalesce1234567891011121314151617181920212223242526272829303132333435363738394041/*coalesce算子,功能:将RDD的partition缩减将一定量的数据缩减到更少的partition中去使用场景,配合filter算子使用使用filter算子过滤掉很多数据以后,比如30%的数据,出现很多partition中的数据不均匀的情况此时建议使用coalesce算子,压缩rdd的partition的数量,从而让各个partition中的数据更加紧促*/ val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 3) def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator &#125; pairRDD.mapPartitionsWithIndex(func2).foreach(println) //查看分区的结果 pairRDD.coalesce(2).mapPartitionsWithIndex(func2).foreach(println)/*------------第一次查看分区的结果-----------------------[partID:0, val: (cat,2)][partID:0, val: (cat,5)][partID:1, val: (mouse,4)][partID:1, val: (cat,12)][partID:2, val: (dog,12)][partID:2, val: (mouse,2)]------------第二次查看分区的结果-----------------------[partID:0, val: (cat,2)][partID:0, val: (cat,5)][partID:1, val: (mouse,4)][partID:1, val: (cat,12)][partID:1, val: (dog,12)][partID:1, val: (mouse,2)]*/ coalesce算子原理 repartition1234567891011121314151617181920212223242526272829303132333435363738/*repartition可以将RDD的partition增多或者减少而coalesce仅仅能将rdd的partition减少reparation的使用场景使用spark sql从hive中查询数据时,spark sql会根据hive对应的HDFS文件的block数量来决定加载出来的数据rdd有多少个partition,这里的partition数量,是我们根本无法设置的,有些时候产生的partition数量少了,此时就可以在spark sql加载hive数据到rdd中以后,立即使用reparation算在,将rdd的partition数量变多*/ val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 1) def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator &#125; pairRDD.mapPartitionsWithIndex(func2).foreach(println) //查看分区的结果 pairRDD.repartition(3).mapPartitionsWithIndex(func2).foreach(println)/*------------第一次查看分区的结果-----------------------[partID:0, val: (cat,2)][partID:0, val: (cat,5)][partID:0, val: (mouse,4)][partID:0, val: (cat,12)][partID:0, val: (dog,12)][partID:0, val: (mouse,2)]------------第二次查看分区的结果-----------------------[partID:0, val: (mouse,4)][partID:0, val: (mouse,2)][partID:1, val: (cat,2)][partID:1, val: (cat,12)][partID:2, val: (cat,5)][partID:2, val: (dog,12)]*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之spark-submit说明","date":"2017-04-16T04:47:25.166Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之spark-submit说明/","text":"将我们的spark工程打包好之后，就可以使用spark-submit脚本提交工程中的spark应用了spark-submit脚本会设置好spark的classpath环境变量（用于类加载）和相关的依赖，而且还可以支持多种不同的集群管理器和不同的部署模式 spark-submit脚本参数说明一般会将执行spark-submit脚本的命令，放置在一个自定义的shell脚本里面，所以说这是比较灵活的一种做法,推荐使用 wordcount.sh1234567/usr/local/spark/bin/spark-submit \\--class org.leo.spark.study.WordCount \\--master spark://192.168.0.101:7077 \\--deploy-mode client \\--conf &lt;key&gt;=&lt;value&gt; \\/usr/local/spark-study/spark-study.jar \\$&#123;1&#125; 以下是上面的spark-submit参数说明 –class: spark应用程序对应的主类，也就是spark应用运行的主入口，通常是一个包含了main方法的java类或scala类，需要包含全限定包名，比如org.leo.spark.study.WordCount–master: spark集群管理器的master URL，standalone模式下，就是ip地址+端口号，比如spark://192.168.0.101:7077，standalone默认端口号就是7077–deploy-mode: 部署模式，决定了将driver进程在worker节点上启动，还是在当前本地机器上启动；默认是client模式，就是在当前本地机器上启动driver进程，如果是cluster，那么就会在worker上启动–conf: 配置所有spark支持的配置属性，使用key=value的格式；如果value中包含了空格，那么需要将key=value包裹的双引号中application-jar: 打包好的spark工程jar包，在当前机器上的全路径名application-arguments: 传递给主类的main方法的参数; 在shell中用${1}这种格式获取传递给shell的参数；然后在比如java中，可以通过main方法的args[0]等参数获取 spark-submit给main类传递参数下面是伪代码: 1234567891011121314151617181920main(String[] args)&#123; val conf = new SparkConf().setAppName(&quot;WordCount&quot;) val sc = new SparkContext(conf) val file = _ if(args!=null &amp;&amp; args.length&gt;0)&#123; println(&quot;=====接收到了参数:&quot;+args(0)) file = args(0) &#125;else&#123; file=&quot;hdfs://hadoop-node1:9000/text/hello.txt&quot; &#125; val rdd = sc.textFile(file) //.... sc.close &#125; spark-submit提交脚本 wordcount.sh 1234567/usr/local/spark/bin/spark-submit \\--class org.leo.spark.study.WordCount \\--master spark://192.168.0.101:7077 \\--deploy-mode client \\--conf &lt;key&gt;=&lt;value&gt; \\/usr/local/spark-study/spark-study.jar \\$&#123;1&#125; 执行脚本1wordcount.sh hdfs://hadoop-node1:9000/test/hello.txt spark-submit多个示例,及常用参数详解1234567# 使用local本地模式，以及8个线程运行# --class 指定要执行的main类# --master 指定集群模式，local，本地模式，local[8]，进程中用几个线程来模拟集群的执行./bin/spark-submit \\ --class org.leo.spark.study.WordCount \\ --master local[8] \\ /usr/local/spark-study.jar \\ 123456789# 使用standalone client模式运行# executor-memory，指定每个executor的内存量，这里每个executor内存是2G# total-executor-cores，指定所有executor的总cpu core数量，这里所有executor的总cpu core数量是100个./bin/spark-submit \\ --class org.leo.spark.study.WordCount \\ --master spark://192.168.0.101:7077 \\ --executor-memory 2G \\ --total-executor-cores 100 \\ /usr/local/spark-study.jar \\ 12345678910# 使用standalone cluster模式运行# supervise参数，指定了spark监控driver节点，如果driver挂掉，自动重启driver./bin/spark-submit \\ --class org.leo.spark.study.WordCount \\ --master spark://192.168.0.101:7077 \\ --deploy-mode cluster \\ --supervise \\ --executor-memory 2G \\ --total-executor-cores 100 \\ /usr/local/spark-study.jar \\ 12345678# 使用yarn-cluster模式运行# num-executors，指定总共使用多少个executor运行spark应用./bin/spark-submit \\ --class org.leo.spark.study.WordCount \\ --master yarn-cluster \\ --executor-memory 20G \\ --num-executors 50 \\ /usr/local/spark-study.jar \\ 12345678910111213141516171819202122232425# 使用standalone client模式，运行一个python应用./bin/spark-submit \\ --master spark://192.168.0.101:7077 \\ /usr/local/python-spark-wordcount.py \\--classapplication jar--master--num-executors--executor-cores --total-executor-cores--executor-memory--driver-memory --supervise在实际生产环境中的配置如下:./bin/spark-submit \\ --class org.leo.spark.study.WordCount \\ --master yarn-cluster \\ --num-executors 100 \\ --executor-cores 2 \\ --executor-memory 6G \\ --driver-memory 1G \\ /usr/local/spark-study.jar \\ sparkConf,spark-submit以及spark-defaultconf优先级默认的配置属性 spark-submit脚本会自动加载conf/spark-defaults.conf文件中的配置属性，并传递给我们的spark应用程序加载默认的配置属性，一大好处就在于，我们不需要在spark-submit脚本中设置所有的属性比如说，默认属性中有一个spark.master属性，所以我们的spark-submit脚本中，就不一定要显式地设置–master，默认就是local123456789SparkConf.getOrElse(&quot;spark.master&quot;, &quot;local&quot;)spark配置的优先级如下: SparkConf、spark-submit、spark-defaults.confspark.default.parallelismSparkConf.set(&quot;spark.default.parallelism&quot;, &quot;100&quot;)spark-submit: --conf spark.default.parallelism=50spark-defaults.conf: spark.default.parallelism 10 如果想要了解更多关于配置属性的信息，可以在spark-submit脚本中，使用–verbose，打印详细的调试信息 使用spark-submit设置属性 虽然说SparkConf设置属性的优先级是最高的，但是有的时候咱们可能不希望在代码中硬编码一些配置属性，否则每次修改了参数以后,还得去代码里修改，然后得重新打包应用程序，再部署到生产机器上去，非常得麻烦 对于上述的情况，我们可以在代码中仅仅创建一个空的SparkConf对象，比如: val sc = new SparkContext(new SparkConf()) 然后可以在spark-submit脚本中，配置各种属性的值，比如 ./bin/spark-submit \\ –name “My app” \\ –master local[4] \\ –conf spark.shuffle.spill=false \\ –conf “spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps” \\ myApp.jar 这里的spark.shuffle.spill属性，我们本来如果是在代码中，SparkConf.set(“spark.shuffle.spill”, “false”)来配置的此时在spark-submit中配置了，不需要更改代码，就可以更改属性，非常得方便，尤其是对于spark程序的调优，格外方便，因为调优说白了，就是不断地调整各种各样的参数，然后反复跑反复试的过程 spark的属性配置方式 spark-shell和spark-submit两个工具，都支持两种加载配置的方式一种是基于命令行参数，比如上面的–master，spark-submit可以通过–conf参数，接收所有spark属性另一种是从conf/spark-defaults.conf文件中加载，其中每一行都包括了一个key和value比如spark.executor.memory 4g 所有在SparkConf、spark-submit和spark-defaults.conf中配置的属性，在运行的时候，都会被综合使用直接通过SparkConf设置的属性，优先级是最高的，会覆盖其余两种方式设置的属性其次是spark-submit脚本中通过–conf设置的属性最后是spark-defaults.conf中设置的属性 通常来说，如果你要对所有的spark作业都生效的配置，放在spark-defaults.conf文件中，只要将spark-defaults.conf.template拷贝成那个文，然后在其中编辑即可然后呢，对于某个spark作业比较特殊的配置，推荐放在spark-submit脚本中，用–conf配置，比较灵活SparkConf配置属性，有什么用呢？也有用，在eclipse中用local模式执行运行的时候，那你就只能在SparkConf中设置属性了 这里还有一种特例，就是说，在新的spark版本中，可能会将一些属性的名称改变，那些旧的属性名称就变成过期的了此时旧的属性名称还是会被接受的，但是新的属性名称会覆盖掉旧的属性名称，并且优先级是比旧属性名称更高的 举例来说shuffle reduce read操作的内存缓冲块儿spark 1.3.0: spark.reducer.maxMbInFlightspark 1.5.0: spark.reducer.maxSizeInFlight spark-submit配置第三方依赖使用spark-submit脚本提交spark application时，application jar，还有我们使用–jars命令绑定的其他jar，都会自动被发送到集群上去–jarspark支持以下几种URL来指定关联的其他jar12345file: 是由driver的http文件服务提供支持的，所有的executor都会通过driver的HTTP服务来拉取文件hdfs:，http:，https:，ftp:，这种文件，就是直接根据URI，从指定的地方去拉取，比如hdfs、或者http链接、或者ftp服务器local: 这种格式的文件必须在每个worker节点上都要存在，所以不需要通过网络io去拉取文件，这对于特别大的文件或者jar包特别适用，可以提升作业的执行性能--jars，比如，mysql驱动包，或者是其他的一些包 文件清理 文件和jar都会被拷贝到每个executor的工作目录中，这就会占用很大一片磁盘空间，因此需要在之后清理掉这些文件,在yarn上运行spark作业时，依赖文件的清理都是自动进行的,适用standalone模式，需要配置spark.worker.cleanup.appDataTtl属性，来开启自动清理依赖文件和jar包,在spark-env.sh中如下配置: 123456SPARK_WORKER_OPTS worker的额外参数，使用&quot;-Dx=y&quot;设置各个参数参数名 默认值 含义spark.worker.cleanup.enabled false 是否启动自动清理worker工作目录，默认是falsespark.worker.cleanup.interval 1800 单位秒，自动清理的时间间隔，默认是30分钟spark.worker.cleanup.appDataTtl 7 * 24 * 3600 默认将一个spark作业的文件在worker工作目录保留多少时间，默认是7天 –file 用户还可以通过在spark-submit中，使用–packages，绑定一些maven的依赖包,此外，还可以通过–repositories来绑定过一些额外的仓库,但是说实话，这两种情况还的确不太常见 –files，比如，最典型的就是hive-site.xml配置文件","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之shuffle","date":"2017-04-16T04:47:25.165Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之shuffle/","text":"shuffle原理 shuffle操作过程中进行数据排序对每个分区中的数据进行排序,那么可以使用下面3种方式1.使用mapPartitions算子处理每个partition,对每个partition中的数据进行排序(推荐)2.使用reparationAndSortWithinPartitions,对Rdd进行重新分区,在重分区的过程中同时就进行分区内数据的排序3.使用sortByKey对数据进行全局的排序 1234567//下面是wordcount的部分代码val rdd2 = rdd1.reduceBykey(_+_)rdd2.mapPartitions&#123; ite=&gt; ite.toList.sortBy(_._2).toIterator//在分区内进行排序&#125; 触发shuffle操作的算子1.reparation类的操作:比如:repartition,repartitionAndSortWithinPartitions,coalesce等2.ByKey类的操作,比如:reduceByKey,groupByKey,sortByKey等3.join类的操作,比如:join,cogroup等 shuffle操作对性能消耗的原理讲解shuffle操作是spark中唯一最消耗性能的地方,因此也就成立最需要进行性能调优的地方,最需要解决线上报错的地方,也是唯一可能出现数据倾斜的地方,因为shuffle过程中,会产生大量的磁盘IO,数据序列化和反序列化,网络IO等 为了实时shuffle操作,spark中才有了stage的概念,在发生shuffle操作的算子中,进行stage的拆分,shuffle操作的前半部分,是上一个stage来进行的,也称之为map task,shuffle操作的后半部分是下一个stage来进行的,也称之为reduce task,其中map task负责数据的组织,也就是将同一个key对应的value都写入同一个下游的task对应的分区文件汇总,其中reduce task负责数据的聚合,也就是将上一个stage的task所在的节点上,将属于自己的各个分区文件,都拉取过来聚合,这种类型,是参考和模拟了MapReduce的shuffle过程来的 map task会将数据先保存在本地内存中,如果内存不够时,就溢写到磁盘文件中去,reduce task会读取各个节点上属于自己的分区磁盘文件,到自己节点的内存中,并进行聚合 shuffle操作会消耗大量的内存,因为无论是网络传输数据之前,还是之后,都会使用大量的内存中的数据结构来进行聚合操作,比如reduceByKey和aggregateByKey操作,会在map side使用内存中的数据结构进行预先聚合,其他的ByKey类的操作,都是在reduce side,使用内存数据结构进行聚合,在聚合的过程中,如果内存不够,只能溢写到磁盘文件中去,此时就会发生大量的磁盘IO,降低性能 此外,shuffle过程中,还会产生大量的中间文件,也就是map side写入的大量分区文件,比如spark1.3版本中,这些中间文件会一直保留着,直到Rdd不再被使用,而且被垃圾回收掉了,才会去清理中间文件,但是这种情况下,如果我们的应用程序中,一直保留着对RDD的引用,导致很长时间以后才会进行RDD的垃圾回收操作,保存中间文件的目录,由spark.local.dir属性指定 shuffle操作所有的相关参数以及性能调优我们可以通过对一系列的参数进行调优，来优化shuffle的性能 1234567891011121314151617181920spark 1.5.2版本属性名称 默认值 属性说明spark.reducer.maxSizeInFlight 48m reduce task的buffer缓冲，代表了每个reduce task每次能够拉取的map side数据最大大小，如果内存充足，可以考虑加大大小，从而减少网络传输次数，提升性能spark.shuffle.blockTransferService netty shuffle过程中，传输数据的方式，两种选项，netty或nio，spark 1.2开始，默认就是netty，比较简单而且性能较高，spark 1.5开始nio就是过期的了，而且spark 1.6中去除掉了spark.shuffle.compress true 是否对map side输出的文件进行压缩，默认是启用压缩的，压缩器是由spark.io.compression.codec属性指定的，默认是snappy压缩器，该压缩器强调的是压缩速度，而不是压缩率spark.shuffle.consolidateFiles false 默认为false，如果设置为true，那么就会合并map side输出文件，对于reduce task数量特别的情况下，可以极大减少磁盘IO开销，提升性能spark.shuffle.file.buffer 32k map side task的内存buffer大小，写数据到磁盘文件之前，会先保存在缓冲中，如果内存充足，可以适当加大大小，从而减少map side磁盘IO次数，提升性能spark.shuffle.io.maxRetries 3 网络传输数据过程中(reduce side拉取数据的过程)，如果出现了网络IO异常，重试拉取数据的次数，默认是3次，对于耗时的shuffle操作，建议加大次数，以避免full gc或者网络不通常导致的数据拉取失败，进而导致task lost，增加shuffle操作的稳定性spark.shuffle.io.retryWait 5s 每次重试拉取数据的等待间隔，默认是5s，建议加大时长，理由同上，保证shuffle操作的稳定性spark.shuffle.io.numConnectionsPerPeer 1 机器之间的可以重用的网络连接，主要用于在大型集群中减小网络连接的建立开销，如果一个集群的机器并不多，可以考虑增加这个值spark.shuffle.io.preferDirectBufs true 启用堆外内存，可以避免shuffle过程的频繁gc，如果堆外内存非常紧张，则可以考虑关闭这个选项spark.shuffle.manager sort ShuffleManager，Spark 1.5以后，有三种可选的，hash、sort和tungsten-sort，sort-based ShuffleManager会更高效实用内存，并且避免产生大量的map side磁盘文件，从Spark 1.2开始就是默认的选项，tungsten-sort与sort类似，但是内存性能更高(如果map side不需要排序,那么可以使用hash,然后配合上面讲解的一个参数在map side进行本地聚合操作也是可以的)spark.shuffle.memoryFraction 0.2 (reduce side的内存大小)如果spark.shuffle.spill属性为true，那么该选项生效，代表了executor内存中，用于进行shuffle reduce side聚合的内存比例，默认是20%，如果内存充足，建议调高这个比例，给reduce聚合更多内存，避免内存不足频繁读写磁盘spark.shuffle.service.enabled false 启用外部shuffle服务，这个服务会安全地保存shuffle过程中，executor写的磁盘文件，因此executor即使挂掉也不要紧，必须配合spark.dynamicAllocation.enabled属性设置为true，才能生效，而且外部shuffle服务必须进行安装和启动，才能启用这个属性spark.shuffle.service.port 7337 外部shuffle服务的端口号，具体解释同上spark.shuffle.sort.bypassMergeThreshold 200 对于sort-based ShuffleManager，如果没有进行map side聚合，而且reduce task数量少于这个值，那么就不会进行排序，如果你使用sort ShuffleManager，而且不需要排序，那么可以考虑将这个值加大，直到比你指定的所有task数量都大，以避免进行额外的sort，从而提升性能spark.shuffle.spill true 当reduce side的聚合内存使用量超过了spark.shuffle.memoryFraction指定的比例时，就进行磁盘的溢写操作spark.shuffle.spill.compress true 同上，进行磁盘溢写时，是否进行文件压缩，使用spark.io.compression.codec属性指定的压缩器，默认是snappy，速度优先","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之local模式提交spark的作业","date":"2017-04-16T04:47:25.163Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之local模式提交spark的作业/","text":"local模式下,没有所谓的master+worker这种概念local模式相当于启动一个本地进程,然后在一个进程内,模拟spark集群中作业的运行,一个spark作业,就对应了进程的一个或者多个executor线程 在实际工作中,local模式,主要用于测试,最常见的就是我们的开发环境中,比如IDEA中,通常在local模式下,我们都会手工生成一份测试数据配合测试使用 local模式的提交脚本如下:12345678[root@hdp-node-01 spark]# cat word_count.sh /export/servers/spark/bin/spark-submit \\--class cn.spark.study.core.WordCount \\--num-executors 1 \\--driver-memory 100m \\--executor-memory 100m \\--executor-cores 1 \\/usr/xx/spark-study-java-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark core之CLI命令行使用","date":"2017-04-16T04:47:25.162Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/spark core之CLI命令行使用/","text":"Spark SQL CLI是一个很方便的工具，可以用来在本地模式下运行Hive的元数据服务，并且通过命令行执行针对Hive的SQL查询。但是我们要注意的是，Spark SQL CLI是不能与Thrift JDBC server进行通信的。 如果要启动Spark SQL CLI，只要执行Spark的bin目录下的spark-sql命令即可1./bin/spark-sql --jars /usr/local/hive/lib/mysql-connector-java-5.1.17.jar 这里同样要注意的是，必须将我们的hive-site.xml文件放在Spark的conf目录下。 我们也可以通过执行./bin/spark-sql –help命令，来获取该命令的所有帮助选项。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"rdd的持久化","date":"2017-04-16T04:47:25.159Z","path":"2017/04/16/bigdata/spark从入门到精通_笔记/rdd的持久化/","text":"为什么要有rdd的持久化? 不进行持久化的情况 有持久化的情况 RDD持久化的原理spark非常重要的一个功能特性就是可以将RDD持久化到内存中,当对RDD执行持久化操作时,每个节点都会将自己操作的RDD持久化到内存中,并且在之后对该RDD额反复使用中,直接使用内存缓存的partition,这样的话,对于针对一个RDD反复执行多个操作的场景,就只要对RDD计算一次即可,后面直接使用该RDD,而不需要反复计算多次该RDD 要持久化一个RDD,只要调用其cache()或者persist()方法即可,在该RDD第一次被计算出来时,就会直接缓存在每个节点中,而且spark的持久化机制还是自动容错的,如果持久化的RDD的任何partition丢失了,那么spark会自动通过其源RDD,使用Transformation操作重新计算该partition cache()和persist()的区别在于,cache()是persist()的一种简化方式,cache()的底层就是调用的persist()的无参版本,同时就是调用persist(MEMORY_ONLY),将数据持久化到内存中,如果需要从内存中清除缓存,那么可以使用unpersist()方法 spark自己在也会在shuffle操作时,进行数据的持久化,比如写入磁盘,主要是为了在节点失败时,避免要重复计算整个过程 RDD持久化实例12345678910111213141516 val linesRdd = sc.textFile(&quot;C:\\\\Users\\\\Administrator\\\\Desktop\\\\xx.txt&quot;).cache val beginTime = System.currentTimeMillis() val count = linesRdd.count() val endTime = System.currentTimeMillis() println(&quot;cost: &quot; + count +&quot;:time&quot; + (endTime-beginTime)) val beginTime2 = System.currentTimeMillis() val count2 = linesRdd.count() val endTime2 = System.currentTimeMillis() println(&quot;cost2: &quot; + count2 +&quot;:time&quot; + (endTime2-beginTime2))/*可以连续两次执行上面的程序,然后观察结果,可以看出有cache和注释掉.cache的时间差是很明显的*/ cache或者persist的使用规则1.必须在Transformation或者textFile等创建了一个RDD之后,直接连续调用cache或persist才可以2.如果你先创建了一个rdd,然后单独另起一行执行cache或者persist方法,是没有用的,而且会报错(大量文件会丢失) 如何选择RDD的持久化策略?spark提供的多种持久化级别,主要是为了在CPU和内存消耗之间进行取舍,下面是一些通用的持久化级别的选择建议:1.优先选择MEMORY_ONLY,如果可以缓存所有的数据的话,那么就使用这种策略,因为纯内存速度最快,而且没有序列化,不需要消耗CPU进行反序列化操作2.如果MEMORY_ONLY策略,无法存储的下所有的数据,那么使用MEMORY_ONLY_SER,将数据进行序列徐进行存储,纯内存操作还是非常快,只是消耗CPU进行反序列化3.如果需要进行快速的失效恢复的话,那么就选择带后缀为_2的策略,进行数据的备份,这样在失败时,就不需要重新计算了4.能不使用DISK相关的策略,就不使用,有的时候,从磁盘读取数据,还不如重新计算,除非计算你的数据集的方法开销很大，或者他们会过滤大量的数据，就不要溢写到磁盘中。不然,重新进计算一个分区会和从磁盘中读取的速度一样慢。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"远程dug","date":"2017-04-16T04:47:25.155Z","path":"2017/04/16/bigdata/spark/远程dug/","text":"1.master使用远程debug1234567891011121314151617#步骤1在Master端的spark-env.sh文件中添加如下参数export SPARK_MASTER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10000&quot;#步骤2start-master.sh执行完这个脚本jps4828 -- main class information unavailable4860 Jps #步骤3通过一个IDE 建立一个remote application172.16.0.11 10000在本地的代码打断点debug按钮开始调试 2.worker使用远程debug12345678910111213141516#步骤1在Worker所在的配置文件中添加一个环境变量export SPARK_WORKER_OPTS=&quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10001&quot; #步骤2start-slave.sh spark://node-1.itcast.cn:7077执行jps命令2891 -- main class information unavailable2923 Jps #步骤3用一个IDE工具连接 建立一个remote application172.16.0.12 10001在本地的代码打断点debug按钮开始调试 3.远程dug自己的一个工程123456789101112#步骤1#任务提交#--driver-java-options就是配置远程dugbin/spark-submit --class cn.itcast.spark.WC --master spark://node-1.itcast.cn:7077 --driver-java-options &quot;-Xdebug -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=10002&quot; /root/bigdata-2.0.jar hdfs://node-1.itcast.cn:9000/words.txt hdfs://node-1.itcast.cn:9000/wordsout #步骤2用一个IDE工具连接 建立一个remote application172.16.0.13 10002在本地的代码打断点debug按钮开始调试","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"网站访问次数","date":"2017-04-16T04:47:25.154Z","path":"2017/04/16/bigdata/spark/网站访问次数/","text":"1.数据格式123456 /*数据格式:其中java.itcast.cn表示Java学院,net.itcast.cn表示net学院,php.itcast.cn表示php学院...20160321102002 http://java.itcast.cn/java/course/javaee.shtml20160321102002 http://net.itcast.cn/net/teacher.shtml20160321102002 http://php.itcast.cn/php/teacher.shtml*/ 2需求&emsp;取出各个学院点击前三的网页 3.代码实现112345678910111213141516171819202122232425262728293031323334353637383940414243package cn.itcast.spark.day2import java.net.URLimport org.apache.spark.&#123;SparkConf, SparkContext&#125;object UrlCount &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;UrlCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //rdd1将数据切分，元组中放的是（URL， 1） val rdd1 = sc.textFile(&quot;D:\\\\itcast-大数据\\\\day29\\\\itcast.log&quot;).map(line =&gt; &#123; val f = line.split(&quot;\\t&quot;) //(http://php.itcast.cn/php/teacher.shtml, 1) (f(1), 1) &#125;) //(http://php.itcast.cn/php/teacher.shtml, 10) val rdd2 = rdd1.reduceByKey(_+_) val rdd3 = rdd2.map(t =&gt; &#123;//(http://php.itcast.cn/php/teacher.shtml, 10) val url = t._1 val host = new URL(url).getHost //(php.itcast.cn, http://php.itcast.cn/php/teacher.shtml, 10) (host, url, t._2) &#125;) //(php.itcast.cn, List((php.itcast.cn, http://php.itcast.cn/php/teacher.shtml, 10), (php.itcast.cn, http://php.itcast.cn/php/course.shtml, 30))) val rdd4 = rdd3.groupBy(_._1) val rdd5 = rdd4.mapValues(it =&gt; &#123;// (php.itcast.cn, http://php.itcast.cn/php/course.shtml, 30) it.toList.sortBy(_._3).reverse.take(3) &#125;) println(rdd2.collect().toBuffer) sc.stop() &#125;&#125; 4.代码实现21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package cn.itcast.spark.day2import java.net.URLimport org.apache.spark.&#123;SparkConf, SparkContext&#125;/**将每个学院单独拿出来形成一个单独的rdd,这样当内存不够的时候,可以将数据写入到磁盘,不至于将内存撑爆 */object AdvUrlCount &#123; def main(args: Array[String]) &#123; //从数据库中加载规则 val arr = Array(&quot;java.itcast.cn&quot;, &quot;php.itcast.cn&quot;, &quot;net.itcast.cn&quot;) val conf = new SparkConf().setAppName(&quot;AdvUrlCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //rdd1将数据切分，元组中放的是（URL， 1） val rdd1 = sc.textFile(&quot;c://itcast.log&quot;).map(line =&gt; &#123; val f = line.split(&quot;\\t&quot;) (f(1), 1) &#125;) val rdd2 = rdd1.reduceByKey(_ + _) val rdd3 = rdd2.map(t =&gt; &#123; val url = t._1 val host = new URL(url).getHost (host, url, t._2) &#125;) //println(rdd3.collect().toBuffer)// val rddjava = rdd3.filter(_._1 == &quot;java.itcast.cn&quot;)// val sortdjava = rddjava.sortBy(_._3, false).take(3)// val rddphp = rdd3.filter(_._1 == &quot;php.itcast.cn&quot;) for (ins &lt;- arr) &#123; val rdd = rdd3.filter(_._1 == ins)//形成单独的rdd val result= rdd.sortBy(_._3, false).take(3) //通过JDBC向数据库中存储数据 //id，学院，URL，次数， 访问日期 println(result.toBuffer) &#125; //println(sortdjava.toBuffer) sc.stop() &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"编写WordCount程序并提交到集群","date":"2017-04-16T04:47:25.153Z","path":"2017/04/16/bigdata/spark/编写WordCount程序并提交到集群/","text":"&emsp;spark shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。 1.创建一个项目 2.选择Maven项目，然后点击next 3.填写maven的GAV，然后点击next 4.填写项目名称，然后点击finish 5.创建好maven项目后，点击Enable Auto-Import 6.配置Maven的pom.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.itcast.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-mvn&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;1.7&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;1.7&lt;/maven.compiler.target&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;scala.version&gt;2.10.6&lt;/scala.version&gt; &lt;scala.compat.version&gt;2.10&lt;/scala.compat.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.10&lt;/artifactId&gt; &lt;version&gt;1.5.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;args&gt; &lt;arg&gt;-make:transitive&lt;/arg&gt; &lt;arg&gt;-dependencyfile&lt;/arg&gt; &lt;arg&gt;$&#123;project.build.directory&#125;/.scala_dependencies&lt;/arg&gt; &lt;/args&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt; &lt;version&gt;2.18.1&lt;/version&gt; &lt;configuration&gt; &lt;useFile&gt;false&lt;/useFile&gt; &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt; &lt;includes&gt; &lt;include&gt;**/*Test.*&lt;/include&gt; &lt;include&gt;**/*Suite.*&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;version&gt;2.3&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;transformers&gt; &lt;transformer implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt; &lt;mainClass&gt;cn.itcast.spark.WordCount&lt;/mainClass&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 7.将src/main/java和src/test/java分别修改成src/main/scala和src/test/scala，与pom.xml中的配置保持一致 8.新建一个scala class，类型为Object 9.编写spark程序12345678910111213141516package cn.itcast.sparkimport org.apache.spark.&#123;SparkContext, SparkConf&#125;object WordCount &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName(&quot;WC&quot;) //创建SparkContext，该对象是提交spark App的入口 val sc = new SparkContext(conf) //使用sc创建RDD并执行相应的transformation和action sc.textFile(args(0)).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_, 1).sortBy(_._2, false).saveAsTextFile(args(1)) //停止sc，结束该任务 sc.stop() &#125;&#125; 10.使用Maven打包 11.选择编译成功的jar包，并将该jar上传到Spark集群中的某个节点上 12.首先启动hdfs和Spark集群1234启动hdfs/usr/local/hadoop-2.6.1/sbin/start-dfs.sh启动spark/usr/local/spark-1.5.2-bin-hadoop2.6/sbin/start-all.sh 13.使用spark-submit命令提交Spark应用（注意参数的顺序）12345678910111213141516/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\--class cn.itcast.spark.WordCount \\--master spark://node1.itcast.cn:7077 \\--executor-memory 2G \\--total-executor-cores 4 \\/root/spark-mvn-1.0-SNAPSHOT.jar \\hdfs://node1.itcast.cn:9000/words.txt \\hdfs://node1.itcast.cn:9000/out /*--class cn.itcast.spark.WordCount \\ //是启动类/root/spark-mvn-1.0-SNAPSHOT.jar \\ //jar包hdfs://node1.itcast.cn:9000/words.txt \\ //输入参数1 ,要读取的文件hdfs://node1.itcast.cn:9000/out //输入参数2 , 内容输出的文件*/ 14.查看程序执行结果12345hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-00000(hello,6)(tom,3)(kitty,2)(jerry,1)","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"窗口函数","date":"2017-04-16T04:47:25.152Z","path":"2017/04/16/bigdata/spark/窗口函数/","text":"12345678910111213141516171819202122232425262728package cn.itcast.spark.day5 import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Milliseconds, Seconds, StreamingContext&#125; object WindowOpts &#123; def main(args: Array[String]) &#123; LoggerLevels.setStreamingLogLevels() val conf = new SparkConf().setAppName(&quot;WindowOpts&quot;).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf, Milliseconds(5000))//毫秒 val lines = ssc.socketTextStream(&quot;172.16.0.11&quot;, 9999) val pairs = lines.flatMap(_.split(&quot; &quot;)).map((_, 1)) val windowedWordCounts = pairs.reduceByKeyAndWindow((a:Int,b:Int) =&gt; (a + b), Seconds(15), Seconds(10))//窗口长度15秒,滑动的时间间隔10秒 ,这两个时间必须是5000ms的倍数 //Map((hello, 5), (jerry, 2), (kitty, 3)) windowedWordCounts.print()// val a = windowedWordCounts.map(_._2).reduce(_+_)// a.foreachRDD(rdd =&gt; &#123;// println(rdd.take(0))// &#125;)// a.print()// //windowedWordCounts.map(t =&gt; (t._1, t._2.toDouble / a.toD))// windowedWordCounts.print()// //result.print() ssc.start() ssc.awaitTermination() &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"一张图说明spark","date":"2017-04-16T04:47:25.151Z","path":"2017/04/16/bigdata/spark/一张图说明spark/","text":"","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的wordcount的执行流程图解","date":"2017-04-16T04:47:25.150Z","path":"2017/04/16/bigdata/spark/wordcount的执行流程图解/","text":"代码12345678910111213141516171819202122232425262728package cn.itcast.spark.day1import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by root on 2016/5/14. */object WordCount &#123; def main(args: Array[String]) &#123; //非常重要，是通向Spark集群的入口 val conf = new SparkConf().setAppName(&quot;WC&quot;) .setJars(Array(&quot;C:\\\\HelloSpark\\\\target\\\\hello-spark-1.0.jar&quot;)) .setMaster(&quot;spark://node-1.itcast.cn:7077&quot;) val sc = new SparkContext(conf) //textFile会产生两个RDD：HadoopRDD -&gt; MapPartitinsRDD sc.textFile(args(0)).cache() // 产生一个RDD ：MapPartitinsRDD .flatMap(_.split(&quot; &quot;)) //产生一个RDD MapPartitionsRDD .map((_, 1)) //产生一个RDD ShuffledRDD .reduceByKey(_+_) //产生一个RDD: mapPartitions .saveAsTextFile(args(1)) sc.stop() &#125;&#125; 流程图","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark集群安装","date":"2017-04-16T04:47:25.148Z","path":"2017/04/16/bigdata/spark/spark集群安装/","text":"1.安装12345678910111213141516171819202122232425262728293031323334cd /export/servers/#上传安装文件,解压tar -zxvf spark-1.6.1-bin-hadoop2.6.tgz ln -s spark-1.6.1-bin-hadoop2.6/ spark //创建软链接//删除源文件,节省空间rm -rf spark-1.6.1-bin-hadoop2.6.tgz #修改配置文件cd spark/conf/mv spark-env.sh.template spark-env.shvim spark-env.sh //添加export JAVA_HOME=/home/hadoop/app/jdk1.7.0_80export SPARK_MASTER_IP=hdp-node-01 //配置master的机器export SPARK_MASTER_PORT=7077####################################################### mv slaves.template slaves vim slaves //添加worker的节点 hdp-node-01hdp-node-02 ########################################################拷贝到其他节点scp -r /export/servers/spark/ hdp-node-02:/export/servers/#hdp-node-01 在启动/export/servers/spark/sbin/start-all.sh 注意: 因为在mater所在的机器上要去启动其他的机器,所以在master所在的机器上要配置到其他机器的ssh , 这里省略了 需要关闭防火墙 #2.测试1234567891011#在mater上[root@hdp-node-01 conf]# jps1270 Jps1159 Worker1093 Master#在worker上[root@hdp-node-02 servers]# jps1073 Worker1107 Jps 3.浏览器访问 4.添加zk来解决master的单点问题到此为止，Spark集群安装完毕，但是有一个很大的问题，那就是Master节点存在单点故障，要解决此问题，就要借助zookeeper，并且启动至少两个Master节点来实现高可靠，配置方式比较简单：Spark集群规划：node1，node2是Master；node3，node4，node5是Worker , 安装配置zk集群，并启动zk集群, 停止spark所有服务，修改配置文件spark-env.sh，在该配置文件中删掉SPARK_MASTER_IP并添加如下配置1export SPARK_DAEMON_JAVA_OPTS=&quot;-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=zk1,zk2,zk3 -Dspark.deploy.zookeeper.dir=/spark&quot; 在node1节点上修改slaves配置文件内容指定worker节点 在node1上执行sbin/start-all.sh脚本，然后在node2上执行sbin/start-master.sh启动第二个Master","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark计算用户在小区停留时间最长的两个小区","date":"2017-04-16T04:47:25.147Z","path":"2017/04/16/bigdata/spark/spark计算用户在小区停留时间最长的两个小区/","text":"统计一个用户经常出现的2个位置 1.数据格式 手机访问基站的数据格式 12345678918611132889,20160327081100,CC0710CC94ECC657A8561DE549D940E0,118688888888,20160327081200,CC0710CC94ECC657A8561DE549D940E0,118688888888,20160327081900,CC0710CC94ECC657A8561DE549D940E0,018611132889,20160327082000,CC0710CC94ECC657A8561DE549D940E0,018688888888,20160327171000,CC0710CC94ECC657A8561DE549D940E0,118688888888,20160327171600,CC0710CC94ECC657A8561DE549D940E0,018611132889,20160327180500,CC0710CC94ECC657A8561DE549D940E0,118611132889,20160327181500,CC0710CC94ECC657A8561DE549D940E0,0/*手机号, 时间, 基站code, (1是进入基站,0是出基站)*/ 基站信息:local_info.txt12349F36407EAD0629FC166F14DDE7970F68,116.304864,40.050645,6CC0710CC94ECC657A8561DE549D940E0,116.303955,40.041935,616030401EAFB68F1E3CDF819735E1C66,116.296302,40.032296,6/*基站code, 经度, 纬度*/ 2.实现方式一12345678910111213141516171819202122232425262728293031323334353637383940414243444546package cn.itcast.spark.day2 import org.apache.spark.&#123;SparkConf, SparkContext&#125; /** * 根据日志统计出每个用户在站点所呆时间最长的前2个的信息 * 1, 先根据&quot;手机号_站点&quot;为唯一标识, 算一次进站出站的时间, 返回(手机号_站点, 时间间隔) * 2, 以&quot;手机号_站点&quot;为key, 统计每个站点的时间总和, (&quot;手机号_站点&quot;, 时间总和) * 3, (&quot;手机号_站点&quot;, 时间总和) --&gt; (手机号, 站点, 时间总和) * 4, (手机号, 站点, 时间总和) --&gt; groupBy().mapValues(以时间排序,取出前2个) --&gt; (手机-&gt;((m,s,t)(m,s,t))) */object UserLocation &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;ForeachDemo&quot;).setMaster(&quot;local[2]&quot;) //local为本地运行 val sc = new SparkContext(conf) //sc.textFile(&quot;c://bs_log&quot;).map(_.split(&quot;,&quot;)).map(x =&gt; (x(0), x(1), x(2), x(3))) val mbt = sc.textFile(&quot;c://bs_log&quot;).map( line =&gt; &#123; val fields = line.split(&quot;,&quot;) val eventType = fields(3) val time = fields(1) val timeLong = if(eventType == &quot;1&quot;) -time.toLong else time.toLong (fields(0) + &quot;_&quot; + fields(2), timeLong) &#125;) //println(mbt.collect().toBuffer) //(18611132889_9F36407EAD0629FC166F14DDE7970F68,54000) val rdd1 = mbt.groupBy(_._1).mapValues(_.foldLeft(0L)(_ + _._2)) //(18611132889,9F36407EAD0629FC166F14DDE7970F68,54000) val rdd2 = rdd1.map( t =&gt; &#123; val mobile_bs = t._1 val mobile = mobile_bs.split(&quot;_&quot;)(0) val lac = mobile_bs.split(&quot;_&quot;)(1) val time = t._2 (mobile, lac, time) &#125;) val rdd3 = rdd2.groupBy(_._1) //ArrayBuffer((18688888888,List((18688888888,16030401EAFB68F1E3CDF819735E1C66,87600), (18688888888,9F36407EAD0629FC166F14DDE7970F68,51200))), (18611132889,List((18611132889,16030401EAFB68F1E3CDF819735E1C66,97500), (18611132889,9F36407EAD0629FC166F14DDE7970F68,54000)))) val rdd4 = rdd3.mapValues(it =&gt; &#123;//it是一个itera //List((18688888888,16030401EAFB68F1E3CDF819735E1C66,87600), (18688888888,9F36407EAD0629FC166F14DDE7970F68,51200))) it.toList.sortBy(_._3).reverse.take(2)//对时间差进行排序,然后取top2 &#125;) println(rdd4.collect().toBuffer) sc.stop() &#125;&#125; 3.实现方式二1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package cn.itcast.spark.day2import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * ((fields(0),fields(2)), timeLong) --&gt;reduceByKey(_+_).map --&gt; (lac, (mobile, time)) * --&gt;rdd1.join(rdd2).map--&gt;(mobile, lac, time, x, y) * --&gt; groupBy().mapValues(以时间排序,取出前2个) --&gt; (手机-&gt;((m,s,t)(m,s,t))) * Created by root on 2016/5/16. */object AdvUserLocation &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;AdvUserLocation&quot;).setMaster(&quot;local[2]&quot;) //local为本地运行 val sc = new SparkContext(conf) val rdd0 = sc.textFile(&quot;c://bs_log&quot;).map( line =&gt; &#123; val fields = line.split(&quot;,&quot;) val eventType = fields(3) val time = fields(1) val mobile = fields(0) val location_area_code = fields(2) val timeLong = if(eventType == &quot;1&quot;) -time.toLong else time.toLong //将(mobile, location_area_code) 作为key ((mobile, location_area_code), timeLong) &#125;) val rdd1 = rdd0.reduceByKey(_+_).map(t =&gt; &#123; val mobile = t._1._1 val lac = t._1._2 //location_area_code val time = t._2 (lac, (mobile, time)) &#125;) val rdd2 = sc.textFile(&quot;c://lac_info.txt&quot;).map(line =&gt; &#123;//基站信息表 val f = line.split(&quot;,&quot;) //(基站ID， （经度，纬度）) (f(0), (f(1), f(2))) &#125;) //rdd1.join(rdd2)--&gt;(CC0710CC94ECC657A8561DE549D940E0,((18688888888,1300),(116.303955,40.041935))) val rdd3 = rdd1.join(rdd2).map(t =&gt; &#123; val lac = t._1 val mobile = t._2._1._1 val time = t._2._1._2 val x = t._2._2._1 val y = t._2._2._2 (mobile, lac, time, x, y) &#125;) //rdd4分组后的 val rdd4 = rdd3.groupBy(_._1)// (mobile, List((mobile, lac, time, x, y), (mobile, lac, time, x, y)) ) val rdd5 = rdd4.mapValues(it =&gt; &#123; it.toList.sortBy(_._3).reverse.take(2) &#125;) println(rdd1.join(rdd2).collect().toBuffer) // println(rdd5.collect().toBuffer) rdd5.saveAsTextFile(&quot;c://out&quot;) sc.stop() &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的自定义排序","date":"2017-04-16T04:47:25.146Z","path":"2017/04/16/bigdata/spark/spark的自定义排序/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657package cn.itcast.spark.day3import org.apache.spark.&#123;SparkConf, SparkContext&#125;object OrderContext &#123; implicit val girlOrdering = new Ordering[Girl] &#123; override def compare(x: Girl, y: Girl): Int = &#123; if(x.faceValue &gt; y.faceValue) 1 else if (x.faceValue == y.faceValue) &#123; if(x.age &gt; y.age) -1 else 1 &#125; else -1 &#125; &#125;&#125;/** * Created by root on 2016/5/18. *///sort =&gt;规则 先按faveValue，比较年龄//name,faveValue,ageobject CustomSort &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;CustomSort&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) val rdd1 = sc.parallelize(List((&quot;yuihatano&quot;, 90, 28, 1), (&quot;angelababy&quot;, 90, 27, 2),(&quot;JuJingYi&quot;, 95, 22, 3))) import OrderContext._ val rdd2 = rdd1.sortBy(x =&gt; Girl(x._2, x._3), false)//false是降序 println(rdd2.collect().toBuffer) sc.stop() &#125;&#125;/** * 第一种方式case class Girl(val faceValue: Int, val age: Int) extends Ordered[Girl] with Serializable &#123;//Serializable 是因为要走网络,所以需要实现序列化 //比较规则 override def compare(that: Girl): Int = &#123; if(this.faceValue == that.faceValue) &#123; that.age - this.age &#125; else &#123; this.faceValue -that.faceValue &#125; &#125;&#125; *//** * 第二种，通过隐式转换完成排序 */case class Girl(faceValue: Int, age: Int) extends Serializable","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的自定义分区partition","date":"2017-04-16T04:47:25.145Z","path":"2017/04/16/bigdata/spark/spark的自定义分区partition/","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869package cn.itcast.spark.day3import java.net.URLimport org.apache.spark.&#123;HashPartitioner, Partitioner, SparkConf, SparkContext&#125;import scala.collection.mutable/** * Created by root on 2016/5/18. */object UrlCountPartition &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;UrlCountPartition&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //rdd1将数据切分，元组中放的是（URL， 1） val rdd1 = sc.textFile(&quot;c://itcast.log&quot;).map(line =&gt; &#123; val f = line.split(&quot;\\t&quot;) (f(1), 1) &#125;) val rdd2 = rdd1.reduceByKey(_ + _) val rdd3 = rdd2.map(t =&gt; &#123; val url = t._1 val host = new URL(url).getHost (host, (url, t._2)) &#125;) val ints = rdd3.map(_._1).distinct().collect() val hostParitioner = new HostParitioner(ints)// val rdd4 = rdd3.partitionBy(new HashPartitioner(ints.length)) val rdd4 = rdd3.partitionBy(hostParitioner).mapPartitions(it =&gt; &#123;//partitionBy通过什么进行分区,mapPartitions遍历每一个分区 it.toList.sortBy(_._2._2).reverse.take(2).iterator //在每一个分区中进行排序,取top2 &#125;) rdd4.saveAsTextFile(&quot;c://out4&quot;) //println(rdd4.collect().toBuffer) sc.stop() &#125;&#125;/** * 决定了数据到哪个分区里面 * @param ins */class HostParitioner(ins: Array[String]) extends Partitioner &#123; val parMap = new mutable.HashMap[String, Int]() var count = 0 for(i &lt;- ins)&#123; parMap += (i -&gt; count) count += 1 &#125; override def numPartitions: Int = ins.length override def getPartition(key: Any): Int = &#123; parMap.getOrElse(key.toString, 0) &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的算子","date":"2017-04-16T04:47:25.143Z","path":"2017/04/16/bigdata/spark/spark的算子/","text":"1.什么是RDD&emsp;RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。 2.RDD的属性 一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。 一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。 RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。 一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。 一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 3.创建RDD创建RDD的两种方式:1.由一个已经存在的Scala集合创建(通过Scala集合或数组以并行化的方式创建RDD)1val rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) 2.由外部存储系统的数据集创建，包括本地的文件系统，还有所有Hadoop支持的数据集，比如HDFS、Cassandra、HBase等1val rdd2 = sc.textFile(&quot;hdfs://node1.itcast.cn:9000/words.txt&quot;) 4.RDD编程APIspark的算子分为两类 Transformation (转换) Action (动作) 4.1.Transformation&emsp;RDD中的所有转换都是延迟加载的，也就是说，它们并不会直接计算结果。相反的，它们只是记住这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给Driver的动作时，这些转换才会真正运行。这种设计让Spark更加有效率地运行。 常用的Transformation： 转换 含义 map(func) 返回一个新的RDD，该RDD由每一个输入元素经过func函数转换后组成 filter(func) 返回一个新的RDD，该RDD由经过func函数计算后返回值为true的输入元素组成 flatMap(func) 类似于map，但是每一个输入元素可以被映射为0或多个输出元素（所以func应该返回一个序列，而不是单一元素） mapPartitions(func) 类似于map，但独立地在RDD的每一个分片上运行，因此在类型为T的RDD上运行时，func的函数类型必须是Iterator[T] =&gt; Iterator[U] mapPartitionsWithIndex(func) 类似于mapPartitions，但func带有一个整数参数表示分片的索引值，因此在类型为T的RDD上运行时，func的函数类型必须是(Int, Interator[T]) =&gt; Iterator[U] sample(withReplacement, fraction, seed) 根据fraction指定的比例对数据进行采样，可以选择是否使用随机数进行替换，seed用于指定随机数生成器种子 union(otherDataset) 对源RDD和参数RDD求并集后返回一个新的RDD intersection(otherDataset) 对源RDD和参数RDD求交集后返回一个新的RDD distinct([numTasks])) 对源RDD进行去重后返回一个新的RDD groupByKey([numTasks]) 在一个(K,V)的RDD上调用，返回一个(K, Iterator[V])的RDD reduceByKey(func, [numTasks]) 在一个(K,V)的RDD上调用，返回一个(K,V)的RDD，使用指定的reduce函数，将相同key的值聚合到一起，与groupByKey类似，reduce任务的个数可以通过第二个可选的参数来设置 aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) sortByKey([ascending], [numTasks]) 在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD sortBy(func,[ascending], [numTasks]) 与sortByKey类似，但是更灵活 join(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个相同key对应的所有元素对在一起的(K,(V,W))的RDD cogroup(otherDataset, [numTasks]) 在类型为(K,V)和(K,W)的RDD上调用，返回一个(K,(Iterable,Iterable))类型的RDD cartesian(otherDataset) 笛卡尔积 pipe(command, [envVars]) coalesce(numPartitions) repartition(numPartitions) repartitionAndSortWithinPartitions(partitioner) 4.2.Action 动作 含义 reduce(func) 通过func函数聚集RDD中的所有元素，这个功能必须是课交换且可并联的 collect() 在驱动程序中，以数组的形式返回数据集的所有元素 count() 返回RDD的元素个数 first() 返回RDD的第一个元素（类似于take(1)） take(n) 返回一个由数据集的前n个元素组成的数组 takeSample(withReplacement,num, [seed]) 返回一个数组，该数组由从数据集中随机采样的num个元素组成，可以选择是否用随机数替换不足的部分，seed用于指定随机数生成器种子 takeOrdered(n, [ordering]) saveAsTextFile(path) 将数据集的元素以textfile的形式保存到HDFS文件系统或者其他支持的文件系统，对于每个元素，Spark将会调用toString方法，将它装换为文件中的文本 saveAsSequenceFile(path) 将数据集中的元素以Hadoop sequencefile的格式保存到指定的目录下，可以使HDFS或者其他Hadoop支持的文件系统。 saveAsObjectFile(path) countByKey() 针对(K,V)类型的RDD，返回一个(K,Int)的map，表示每一个key对应的元素个数。 foreach(func) 在数据集的每一个元素上，运行函数func进行更新。 4.3.练习123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125#常用Transformation(即转换，延迟加载)#通过并行化scala集合创建RDDval rdd1 = sc.parallelize(Array(1,2,3,4,5,6,7,8)) #查看该rdd的分区数量rdd1.partitions.lengthval rdd1 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10),5)//可以手动指定分区的大小 val rdd1 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10))val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=&gt;x,true)val rdd3 = rdd2.filter(_&gt;10) //取出大于10的数据 //转成字符串之后,排序就是字典顺序val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=&gt;x+&quot;&quot;,true)val rdd2 = sc.parallelize(List(5,6,4,7,3,8,2,9,1,10)).map(_*2).sortBy(x=&gt;x.toString,true) val rdd4 = sc.parallelize(Array(&quot;a b c&quot;, &quot;d e f&quot;, &quot;h i j&quot;))rdd4.flatMap(_.split(&apos; &apos;)).collect val rdd5 = sc.parallelize(List(List(&quot;a b c&quot;, &quot;a b b&quot;),List(&quot;e f g&quot;, &quot;a f g&quot;), List(&quot;h i j&quot;, &quot;a a b&quot;))) List(&quot;a b c&quot;, &quot;a b b&quot;) =List(&quot;a&quot;,&quot;b&quot;,)) rdd5.flatMap(_.flatMap(_.split(&quot; &quot;))).collect #union求并集，注意类型要一致val rdd6 = sc.parallelize(List(5,6,4,7))val rdd7 = sc.parallelize(List(1,2,3,4))val rdd8 = rdd6.union(rdd7)rdd8.distinct.sortBy(x=&gt;x).collect #intersection求交集val rdd9 = rdd6.intersection(rdd7) #joinval rdd1 = sc.parallelize(List((&quot;tom&quot;, 1), (&quot;jerry&quot;, 2), (&quot;kitty&quot;, 3)))val rdd2 = sc.parallelize(List((&quot;jerry&quot;, 9), (&quot;tom&quot;, 8), (&quot;shuke&quot;, 7))) val rdd3 = rdd1.join(rdd2)val rdd3 = rdd1.leftOuterJoin(rdd2)val rdd3 = rdd1.rightOuterJoin(rdd2) #groupByKeyval rdd3 = rdd1 union rdd2rdd3.groupByKeyrdd3.groupByKey.map(x=&gt;(x._1,x._2.sum))rdd3.groupByKey.mapValues(_.sum).collect #WordCount, 第二个效率低sc.textFile(&quot;/root/words.txt&quot;).flatMap(x=&gt;x.split(&quot; &quot;)).map((_,1)).reduceByKey(_+_).sortBy(_._2,false).collectsc.textFile(&quot;/root/words.txt&quot;).flatMap(x=&gt;x.split(&quot; &quot;)).map((_,1)).groupByKey.map(t=&gt;(t._1, t._2.sum)).collect #cogroupval rdd1 = sc.parallelize(List((&quot;tom&quot;, 1), (&quot;tom&quot;, 2), (&quot;jerry&quot;, 3), (&quot;kitty&quot;, 2)))val rdd2 = sc.parallelize(List((&quot;jerry&quot;, 2), (&quot;tom&quot;, 1), (&quot;shuke&quot;, 2)))val rdd3 = rdd1.cogroup(rdd2)val rdd4 = rdd3.map(t=&gt;(t._1, t._2._1.sum + t._2._2.sum)) #cartesian笛卡尔积val rdd1 = sc.parallelize(List(&quot;tom&quot;, &quot;jerry&quot;))val rdd2 = sc.parallelize(List(&quot;tom&quot;, &quot;kitty&quot;, &quot;shuke&quot;))val rdd3 = rdd1.cartesian(rdd2) ################################################################################################### #spark actionval rdd1 = sc.parallelize(List(1,2,3,4,5), 2) #collectrdd1.collect #reduceval rdd2 = rdd1.reduce(_+_) #countrdd1.count #toprdd1.top(2) #takerdd1.take(2) #first(similer to take(1))rdd1.first #takeOrderedrdd1.takeOrdered(3) # map(func) Return a new distributed dataset formed by passing each element of the source through a function func.filter(func) Return a new dataset formed by selecting those elements of the source on which func returns true.flatMap(func) Similar to map, but each input item can be mapped to 0 or more output items (so func should return a Seq rather than a single item).mapPartitions(func) Similar to map, but runs separately on each partition (block) of the RDD, so func must be of type Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; when running on an RDD of type T.mapPartitionsWithIndex(func) Similar to mapPartitions, but also provides func with an integer value representing the index of the partition, so func must be of type (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; when running on an RDD of type T.sample(withReplacement, fraction, seed) Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.union(otherDataset) Return a new dataset that contains the union of the elements in the source dataset and the argument.intersection(otherDataset) Return a new RDD that contains the intersection of elements in the source dataset and the argument.distinct([numTasks])) Return a new dataset that contains the distinct elements of the source dataset.groupByKey([numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable&lt;V&gt;) pairs.reduceByKey(func, [numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) =&gt; V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.aggregateByKey(zeroValue)(seqOp, combOp, [numTasks]) When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral &quot;zero&quot; value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.sortByKey([ascending], [numTasks]) When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.join(otherDataset, [numTasks]) When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.cogroup(otherDataset, [numTasks]) When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples. This operation is also called groupWith.cartesian(otherDataset) When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).pipe(command, [envVars]) Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process&apos;s stdin and lines output to its stdout are returned as an RDD of strings.coalesce(numPartitions) Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.repartition(numPartitions) Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.repartitionAndSortWithinPartitions(partitioner) Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery. (K,(Iterable&lt;V&gt;,Iterable&lt;W&gt;)) 5.相关网站推荐比spark官网的例子多: http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark的rdd算子讲解2","date":"2017-04-16T04:47:25.142Z","path":"2017/04/16/bigdata/spark/spark的rdd算子讲解2/","text":"reduceByKey123456789101112#Examplesc.textFile(args(0)).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_).#源码 def reduceByKey(partitioner: Partitioner, func: (V, V) =&gt; V): RDD[(K, V)] = self.withScope &#123; combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner) &#125;/*在reduceByKey的内部是调用的combineByKeyWithClassTag, 由上面的源码知道combineByKeyWithClassTag的第一个参数是对value原样输出,第二个,第三个参数是调用reduceByKey中指定的函数第二个参数的功能:是在partition内进行操作第三个参数的功能是对各个partition的所有的结果进行操作*/ collect123456789101112131415161718192021222324252627282930package spark.examples.rddapiimport org.apache.spark.&#123;SparkContext, SparkConf&#125;object CollectTest_07 &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;CoGroupTest_05&quot;) val sc = new SparkContext(conf); val z1 = sc.parallelize(List((3, &quot;A&quot;), (6, &quot;B1&quot;), (7, &quot;Z1&quot;), (9, &quot;E&quot;), (7, &quot;F&quot;), (9, &quot;Y&quot;), (77, &quot;Z&quot;), (31, &quot;X&quot;)), 3) /** * Return an array that contains all of the elements in this RDD. */ //这是一个行动算子 z1.collect().foreach(println) /** * Return an RDD that contains all matching values by applying `f`. */ // def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = &#123; // filter(f.isDefinedAt).map(f) // &#125;// val f = &#123;// case x: (Int, String) =&gt; x// &#125;// val z2 = z1.collect(f)// println(z2) &#125;&#125; mapPartitionsWithIndex12345678910map是对每个元素操作, mapPartitions是对其中的每个partition操作 --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------mapPartitionsWithIndex : 把每个partition中的分区号和对应的值拿出来, 看源码val func = (index: Int, iter: Iterator[(Int)]) =&gt; &#123; //index是分区的索引,Iterator是一个分区中的数据,可以迭代 iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func).collect aggregate12345678910111213141516171819202122232425262728293031323334353637383940414243444546aggregate(参数1)(参数2,参数3) //参数1是初始化的值,参数2是一个函数,对每一个partition的数据聚合,参数3 是对所有partition的结果进行聚合 def func1(index: Int, iter: Iterator[(Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9), 2)rdd1.mapPartitionsWithIndex(func1).collect###是action操作, 第一个参数是初始值, 二:是2个函数[每个函数都是2个参数(第一个参数:先对个个分区进行合并, 第二个:对个个分区合并后的结果再进行合并), 输出一个参数]###0 + (0+1+2+3+4/*局部求和*/ + 0+5+6+7+8+9/*局部求和*/)rdd1.aggregate(0)(_+_, _+_/*全局求和*/)rdd1.aggregate(0)(math.max(_, _), _ + _) //传给第一个参数的是:itera ,所以使用math.max(_, _) 去迭代###5和1比, 得5再和234比得5 --&gt; 5和6789比,得9 --&gt; 5 + (5+9)rdd1.aggregate(5)(math.max(_, _), _ + _) add1.reduce(math.max(_,_))//第一个下划线是上一次求max的值,第二个下划线是循环add1中的一个元素 aggregate:先进行局部的(partition)操作(循环迭代所有的局部元素),然后进行全局的操作(循环迭代所有的分区) val rdd2 = sc.parallelize(List(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;,&quot;e&quot;,&quot;f&quot;),2)def func2(index: Int, iter: Iterator[(String)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;rdd2.aggregate(&quot;&quot;)(_ + _, _ + _) //abcdefrdd2.aggregate(&quot;=&quot;)(_ + _, _ + _) //==abc=def :局部求和为 =abc 和 =def 最后整体求和:==abc=def val rdd3 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;4567&quot;),2)rdd3.aggregate(&quot;&quot;)((x,y) =&gt; math.max(x.length, y.length).toString, (x,y) =&gt; x + y)//24 或者是42 因为是并行的任务,所以可能先返回2,也有可能先返回4 val rdd4 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;345&quot;,&quot;&quot;),2)rdd4.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y)//返回10 或者01/*因为有初始值(空字符串)的存在,所以如下过程:分区0: math.min(&quot;&quot;.length,&quot;12&quot;.length) ==&gt;0.toString &quot;0&quot; math.min(&quot;0&quot;.length,&quot;23&quot;.length) ==&gt;1.toString &quot;1&quot; 最终结果===&gt;1 分区1: math.min(&quot;&quot;.length,&quot;345&quot;.length) ==&gt;0.toString &quot;0&quot; math.min(&quot;0&quot;.length,&quot;&quot;.length) ==&gt;0.toString &quot;0&quot; 最终结果===&gt;0 分区全局聚合:&quot;&quot;+&quot;1&quot;+&quot;0&quot; 或者 &quot;&quot;+&quot;0&quot;+&quot;1&quot;*/ val rdd5 = sc.parallelize(List(&quot;12&quot;,&quot;23&quot;,&quot;&quot;,&quot;345&quot;),2)rdd5.aggregate(&quot;&quot;)((x,y) =&gt; math.min(x.length, y.length).toString, (x,y) =&gt; x + y) //结果: &quot;11&quot; aggregateByKey12345678910111213//参见浏览器 val pairRDD = sc.parallelize(List( (&quot;cat&quot;,2), (&quot;cat&quot;, 5), (&quot;mouse&quot;, 4),(&quot;cat&quot;, 12), (&quot;dog&quot;, 12), (&quot;mouse&quot;, 2)), 2)def func2(index: Int, iter: Iterator[(String, Int)]) : Iterator[String] = &#123; iter.toList.map(x =&gt; &quot;[partID:&quot; + index + &quot;, val: &quot; + x + &quot;]&quot;).iterator&#125;pairRDD.mapPartitionsWithIndex(func2).collect //查看分区的结果pairRDD.aggregateByKey(0)(math.max(_, _), _ + _).collect //在局部可以将key相同的放在一起迭代,math.max(_, _) 就是取一个分区中key相同的元素中的最大的值pairRDD.aggregateByKey(100)(math.max(_, _), _ + _).collect coalesce 合并, repartition1234567重新分区之后,要走网络coalesce 合并, repartitionval rdd1 = sc.parallelize(1 to 10, 10)val rdd2 = rdd1.coalesce(2, false) //false表示不进行shufflerdd2.partitions.length combineByKey123456789101112131415161718 combineByKey : 和reduceByKey是相同的效果###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)--&gt;(hello(1,1),good(1))--&gt;x就相当于hello的第一个1, good中的1val rdd1 = sc.textFile(&quot;hdfs://master:9000/wordcount/input/&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1))val rdd2 = rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)rdd1.collectrdd2.collect ###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10val rdd3 = rdd1.combineByKey(x =&gt; x + 10, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n) //x =&gt; x + 10 只是将分区的第一个元素作为x,然后加10 ,将x+10作为局部计算的初始值rdd3.collect val rdd4 = sc.parallelize(List(&quot;dog&quot;,&quot;cat&quot;,&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;turkey&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;), 3)val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)val rdd6 = rdd5.zip(rdd4) //List((1,dog),(1,cat),(2,gnu),(2,salmon),(2,rabbit),(1,turkey),(2,wolf),(2,bear),(2,bee))val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) =&gt; x :+ y, (m: List[String], n: List[String]) =&gt; m ++ n) //(1,list(&quot;dog&quot;,&quot;cat&quot;,&quot;turkey&quot;) ) , (2, list(&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;)) ) collectAsMap1234#可以将list转成MapcollectAsMap : Map(b -&gt; 2, a -&gt; 1)val rdd = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;, 2)))rdd.collectAsMap countByKey12345val rdd1 = sc.parallelize(List((&quot;a&quot;, 1), (&quot;b&quot;, 2), (&quot;b&quot;, 2), (&quot;c&quot;, 2), (&quot;c&quot;, 1)))rdd1.countByKey // Map(a-&gt;1, b-&gt;2, c-&gt;2)rdd1.countByValue //Map((c,2)-&gt;1, (a,1)-&gt;1, (b,2)-&gt;2, (c,1)-&gt;1) //将元组当做一个value filterByRange1234val rdd1 = sc.parallelize(List((&quot;e&quot;, 5), (&quot;c&quot;, 3), (&quot;d&quot;, 4), (&quot;c&quot;, 2), (&quot;a&quot;, 1), (b,6)))val rdd2 = rdd1.filterByRange(&quot;b&quot;, &quot;d&quot;) //取指定key范围内的元组rdd2.collect flatMapValues12345 flatMapValues : Array((a,1), (a,2), (b,3), (b,4))val rdd3 = sc.parallelize(List((&quot;a&quot;, &quot;1 2&quot;), (&quot;b&quot;, &quot;3 4&quot;)))val rdd4 = rdd3.flatMapValues(_.split(&quot; &quot;))rdd4.collect foldByKey1234567 val rdd1 = sc.parallelize(List(&quot;dog&quot;, &quot;wolf&quot;, &quot;cat&quot;, &quot;bear&quot;), 2)val rdd2 = rdd1.map(x =&gt; (x.length, x))val rdd3 = rdd2.foldByKey(&quot;&quot;)(_+_) val rdd = sc.textFile(&quot;hdfs://node-1.itcast.cn:9000/wc&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1))rdd.foldByKey(0)(_+_) foreachPartition123456789val rdd1 = sc.parallelize(List(1, 2, 3, 4, 5, 6, 7, 8, 9), 3)rdd1.foreachPartition(x =&gt; println(x.reduce(_ + _)))//map会遍历rdd中的每一个元素,然后返回生成一个新的rdd//foreach 会遍历rdd中的每一个元素,但是不会生成新的rdd//foreachPartition 会对一个partition进行操作例子;想要存储rdd中的每一个元素,那么使用map和foreach在遍历每一个元素的时候都去哪一个数据库连接池中的连接,而使用foreachPartition将只是针对每一个分区,拿一个数据库连接,然后使用这一个连接,遍历一个分区中的所有的元素,存入数据库 keyBy1234keyBy : 以传入的参数做keyval rdd1 = sc.parallelize(List(&quot;dog&quot;, &quot;salmon&quot;, &quot;salmon&quot;, &quot;rat&quot;, &quot;elephant&quot;), 3)val rdd2 = rdd1.keyBy(_.length) //遍历每一个元素,并为每一个元素指定keyrdd2.collect keys values12345val rdd1 = sc.parallelize(List(&quot;dog&quot;, &quot;tiger&quot;, &quot;lion&quot;, &quot;cat&quot;, &quot;panther&quot;, &quot;eagle&quot;), 2)val rdd2 = rdd1.map(x =&gt; (x.length, x))rdd2.keys.collectrdd2.values.collect checkpoint12345678sc.setCheckpointDir(&quot;hdfs://node-1.itcast.cn:9000/ck&quot;)val rdd = sc.textFile(&quot;hdfs://node-1.itcast.cn:9000/wc&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1)).reduceByKey(_+_)rdd.checkpointrdd.isCheckpointedrdd.countrdd.isCheckpointedrdd.getCheckpointFile","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark源码编译","date":"2017-04-16T04:47:25.141Z","path":"2017/04/16/bigdata/spark/spark源码编译/","text":"123456789101112131415161718#解压源码包tar -zxvf spark-1.6.1.tgz -C /usr/local/src/cd /usr/local/src/spark-1.6.1/ #设置内存2Gexport MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot; #编译前安装一些压缩解压缩工具yum install -y snappy snappy-devel bzip2 bzip2-devel lzo lzo-devel lzop openssl openssl-devel #需要使用maven,所以要安装maven #仅仅是为了编译源码, 编译后可以导入idea中mvn clean package -Phadoop-2.6 -Dhadoop.version=2.6.4 -Phive -Phive-thriftserver -Pyarn -DskipTests #编译后并打包, 打包后可以丢到生产环境了./make-distribution.sh --tgz -Phadoop-2.6 -Dhadoop.version=2.6.4 -Phive -Phive-thriftserver -Pyarn -DskipTests","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"sparkSQL","date":"2017-04-16T04:47:25.140Z","path":"2017/04/16/bigdata/spark/sparkSQL/","text":"1.什么是Spark SQL&emsp;Spark SQL是Spark用来处理结构化数据的一个模块，它提供了一个编程抽象叫做DataFrame并且作为分布式SQL查询引擎的作用。 2.为什么要学习Spark SQL&emsp;我们已经学习了Hive，它是将Hive SQL转换成MapReduce然后提交到集群上执行，大大简化了编写MapReduce的程序的复杂性，由于MapReduce这种计算模型执行效率比较慢。所有Spark SQL的应运而生，它是将Spark SQL转换成RDD，然后提交到集群执行，执行效率非常快！ 3.DataFrames&emsp;与RDD类似，DataFrame也是一个分布式数据容器。然而DataFrame更像传统数据库的二维表格，除了数据以外，还记录数据的结构信息，即schema。同时，与Hive类似，DataFrame也支持嵌套数据类型（struct、array和map）。从API易用性的角度上 看，DataFrame API提供的是一套高层的关系操作，比函数式的RDD API要更加友好，门槛更低。由于与R和Pandas的DataFrame类似，Spark DataFrame很好地继承了传统单机数据分析的开发体验。 4.创建DataFrames&emsp;在Spark SQL中SQLContext是创建DataFrames和执行SQL的入口，在spark-1.5.2中已经内置了一个sqlContext 1234567891011121314151617#在本地创建一个文件，有三列，分别是id、name、age，用空格分隔，然后上传到hdfs上hdfs dfs -put person.txt /#在spark shell执行下面命令，读取数据，将每一行的数据使用列分隔符分割val lineRDD = sc.textFile(&quot;hdfs://node1.itcast.cn:9000/person.txt&quot;).map(_.split(&quot; &quot;))#定义case class（相当于表的schema）case class Person(id:Int, name:String, age:Int)#将RDD和case class关联val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt))#将RDD转换成DataFrameval personDF = personRDD.toDF#对DataFrame进行处理personDF.show 5.DataFrame常用操作5.1.DSL风格语法123456789101112131415161718192021//查看DataFrame中的内容personDF.show //查看DataFrame部分列中的内容personDF.select(personDF.col(&quot;name&quot;)).showpersonDF.select(col(&quot;name&quot;), col(&quot;age&quot;)).showpersonDF.select(&quot;name&quot;).show //打印DataFrame的Schema信息personDF.printSchema //查询所有的name和age，并将age+1personDF.select(col(&quot;id&quot;), col(&quot;name&quot;), col(&quot;age&quot;) + 1).showpersonDF.select(personDF(&quot;id&quot;), personDF(&quot;name&quot;), personDF(&quot;age&quot;) + 1).show//过滤age大于等于18的personDF.filter(col(&quot;age&quot;) &gt;= 18).show//按年龄进行分组并统计相同年龄的人数personDF.groupBy(&quot;age&quot;).count().show() 5.2.SQL风格语法12345678//如果想使用SQL风格的语法，需要将DataFrame注册成表personDF.registerTempTable(&quot;t_person&quot;) //查询年龄最大的前两名sqlContext.sql(&quot;select * from t_person order by age desc limit 2&quot;).show//显示表的Schema信息sqlContext.sql(&quot;desc t_person&quot;).show 6.以编程方式执行Spark SQL查询6.1.通过反射推断Schema&emsp;创建一个object为cn.itcast.spark.sql.InferringSchema12345678910111213141516171819202122232425262728293031323334353637package cn.itcast.spark.sql import org.apache.spark.&#123;SparkConf, SparkContext&#125;import org.apache.spark.sql.SQLContext object InferringSchema &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName(&quot;SQL-1&quot;) //SQLContext要依赖SparkContext val sc = new SparkContext(conf) //创建SQLContext val sqlContext = new SQLContext(sc) //从指定的地址创建RDD val lineRDD = sc.textFile(args(0)).map(_.split(&quot; &quot;)) //创建case class //将RDD和case class关联 val personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) //导入隐式转换，如果不导入无法将RDD转换成DataFrame //将RDD转换成DataFrame import sqlContext.implicits._ val personDF = personRDD.toDF //注册表 personDF.registerTempTable(&quot;t_person&quot;) //传入SQL val df = sqlContext.sql(&quot;select * from t_person order by age desc limit 2&quot;) //将结果以JSON的方式存储到指定位置 df.write.json(args(1)) //停止Spark Context sc.stop() &#125;&#125;//case class一定要放到外面case class Person(id: Int, name: String, age: Int) 将程序打成jar包，上传到spark集群，提交Spark任务123456789/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\--class cn.itcast.spark.sql.InferringSchema \\--master spark://node1.itcast.cn:7077 \\/root/spark-mvn-1.0-SNAPSHOT.jar \\hdfs://node1.itcast.cn:9000/person.txt \\hdfs://node1.itcast.cn:9000/out //查看运行结果hdfs dfs -cat hdfs://node1.itcast.cn:9000/out/part-r-* 6.2.通过StructType直接指定Schema&emsp;创建一个object为cn.itcast.spark.sql.SpecifyingSchema 1234567891011121314151617181920212223242526272829303132333435363738394041package cn.itcast.spark.sql import org.apache.spark.sql.&#123;Row, SQLContext&#125;import org.apache.spark.sql.types._import org.apache.spark.&#123;SparkContext, SparkConf&#125; /** * Created by ZX on 2015/12/11. */object SpecifyingSchema &#123; def main(args: Array[String]) &#123; //创建SparkConf()并设置App名称 val conf = new SparkConf().setAppName(&quot;SQL-2&quot;) //SQLContext要依赖SparkContext val sc = new SparkContext(conf) //创建SQLContext val sqlContext = new SQLContext(sc) //从指定的地址创建RDD val personRDD = sc.textFile(args(0)).map(_.split(&quot; &quot;)) //通过StructType直接指定每个字段的schema val schema = StructType( List( StructField(&quot;id&quot;, IntegerType, true),//true允许为null StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true) ) ) //将RDD映射到rowRDD val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt, p(1).trim, p(2).toInt)) //将schema信息应用到rowRDD上 val personDataFrame = sqlContext.createDataFrame(rowRDD, schema) //注册表 personDataFrame.registerTempTable(&quot;t_person&quot;) //执行SQL val df = sqlContext.sql(&quot;select * from t_person order by age desc limit 4&quot;) //将结果以JSON的方式存储到指定位置 df.write.json(args(1))//以json的格式写入到指定的路径 //停止Spark Context sc.stop() &#125;&#125; 将程序打成jar包，上传到spark集群，提交Spark任务123456789/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\--class cn.itcast.spark.sql.InferringSchema \\--master spark://node1.itcast.cn:7077 \\/root/spark-mvn-1.0-SNAPSHOT.jar \\hdfs://node1.itcast.cn:9000/person.txt \\hdfs://node1.itcast.cn:9000/out1 //查看结果hdfs dfs -cat hdfs://node1.itcast.cn:9000/out1/part-r-* 7.数据源7.1.JDBC&emsp;Spark SQL可以通过JDBC从关系型数据库中读取数据的方式创建DataFrame，通过对DataFrame一系列的计算后，还可以将数据再写回关系型数据库中。 7.1.1.从MySQL中加载数据（Spark Shell方式）1234567891011//1.启动Spark Shell，必须指定mysql连接驱动jar包/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-shell \\--master spark://node1.itcast.cn:7077 \\--jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \\--driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar //2.从mysql中加载数据val jdbcDF = sqlContext.read.format(&quot;jdbc&quot;).options(Map(&quot;url&quot; -&gt; &quot;jdbc:mysql://192.168.10.1:3306/bigdata&quot;, &quot;driver&quot; -&gt; &quot;com.mysql.jdbc.Driver&quot;, &quot;dbtable&quot; -&gt; &quot;person&quot;, &quot;user&quot; -&gt; &quot;root&quot;, &quot;password&quot; -&gt; &quot;123456&quot;)).load() //3.执行查询jdbcDF.show() 7.1.2.将数据写入到MySQL中（打jar包方式） 1.编写Spark SQL程序123456789101112131415161718192021222324252627282930313233343536package cn.itcast.spark.sql import java.util.Propertiesimport org.apache.spark.sql.&#123;SQLContext, Row&#125;import org.apache.spark.sql.types.&#123;StringType, IntegerType, StructField, StructType&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125; object JdbcRDD &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;MySQL-Demo&quot;) val sc = new SparkContext(conf) val sqlContext = new SQLContext(sc) //通过并行化创建RDD val personRDD = sc.parallelize(Array(&quot;1 tom 5&quot;, &quot;2 jerry 3&quot;, &quot;3 kitty 6&quot;)).map(_.split(&quot; &quot;)) //通过StructType直接指定每个字段的schema val schema = StructType( List( StructField(&quot;id&quot;, IntegerType, true), StructField(&quot;name&quot;, StringType, true), StructField(&quot;age&quot;, IntegerType, true) ) ) //将RDD映射到rowRDD val rowRDD = personRDD.map(p =&gt; Row(p(0).toInt, p(1).trim, p(2).toInt)) //将schema信息应用到rowRDD上 val personDataFrame = sqlContext.createDataFrame(rowRDD, schema) //创建Properties存储数据库相关属性 val prop = new Properties() prop.put(&quot;user&quot;, &quot;root&quot;) prop.put(&quot;password&quot;, &quot;123456&quot;) //将数据追加到数据库 personDataFrame.write.mode(&quot;append&quot;).jdbc(&quot;jdbc:mysql://192.168.10.1:3306/bigdata&quot;, &quot;bigdata.person&quot;, prop) //停止SparkContext sc.stop() &#125;&#125; 2.用maven将程序打包 3.将Jar包提交到spark集群123456/usr/local/spark-1.5.2-bin-hadoop2.6/bin/spark-submit \\--class cn.itcast.spark.sql.JdbcRDD \\--master spark://node1.itcast.cn:7077 \\--jars /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \\--driver-class-path /usr/local/spark-1.5.2-bin-hadoop2.6/mysql-connector-java-5.1.35-bin.jar \\/root/spark-mvn-1.0-SNAPSHOT.jar 7.2.HDFS123456789101112131415161718192021222324252627282930313233343536//1.读取数据，将每一行的数据使用列分隔符分割val lineRDD = sc.textFile(&quot;hdfs://node1.itcast.cn:9000/person.txt&quot;, 1).map(_.split(&quot; &quot;)) //2.定义case class（相当于表的schema）case class Person(id:Int, name:String, age:Int) //3.导入隐式转换,在当前版本中可以不用导入import sqlContext.implicits._ //4.将lineRDD转换成personRDDval personRDD = lineRDD.map(x =&gt; Person(x(0).toInt, x(1), x(2).toInt)) //5.将personRDD转换成DataFrameval personDF = personRDD.toDF //6.对personDF进行处理 #(SQL风格语法)personDF.registerTempTable(&quot;t_person&quot;)sqlContext.sql(&quot;select * from t_person order by age desc limit 2&quot;).showsqlContext.sql(&quot;desc t_person&quot;).showval result = sqlContext.sql(&quot;select * from t_person order by age desc&quot;) //7.保存结果result.save(&quot;hdfs://hadoop.itcast.cn:9000/sql/res1&quot;)result.save(&quot;hdfs://hadoop.itcast.cn:9000/sql/res2&quot;, &quot;json&quot;) #以JSON文件格式覆写HDFS上的JSON文件import org.apache.spark.sql.SaveMode._result.save(&quot;hdfs://hadoop.itcast.cn:9000/sql/res2&quot;, &quot;json&quot; , Overwrite) 8.重新加载以前的处理结果（可选）sqlContext.load(&quot;hdfs://hadoop.itcast.cn:9000/sql/res1&quot;)sqlContext.load(&quot;hdfs://hadoop.itcast.cn:9000/sql/res2&quot;, &quot;json&quot;)//指定加载的格式","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark streaming整合kafka的两种方式","date":"2017-04-16T04:47:25.138Z","path":"2017/04/16/bigdata/spark/spark streaming整合kafka的两种方式/","text":"方式一有receiver,会记录log日志 启动 123456789101112131415161718#首先启动zk#启动kafkabin/kafka-server-start.sh config/server.properties#创建topicbin/kafka-topics.sh --create --zookeeper 192.168.80.10:2181 --replication-factor 1 --partitions 1 --topic wordcount#查看主题bin/kafka-topics.sh --list --zookeeper 192.168.80.10:2181#启动spark-streaming应用程序:KafkaWordCount 如下bin/spark-submit --class cn.itcast.spark.streaming.KafkaWordCount /root/streaming-1.0.jar 192.168.80.10:2181 group1 wordcount 1 bin/spark-submit --class cn.itcast.spark.UrlCount --master spark://node1.itcast.cn:7077 --executor-memory 1G --total-executor-cores 2 /root/SparkDemo-1.0.jar node1.itcast.cn:2181,node2.itcast.cn:2181,node3.itcast.cn:2181 group1 weblog 2 KafkaWordCount 代码1234567891011121314151617181920212223242526272829303132package cn.itcast.spark.day5 import org.apache.spark.storage.StorageLevelimport org.apache.spark.&#123;HashPartitioner, SparkConf&#125;import org.apache.spark.streaming.kafka.KafkaUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125; object KafkaWordCount &#123; val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x))) iter.flatMap &#123; case (x, y, z) =&gt; Some(y.sum + z.getOrElse(0)).map(i =&gt; (x, i)) &#125; &#125; def main(args: Array[String]) &#123; LoggerLevels.setStreamingLogLevels() val Array(zkQuorum, group, topics, numThreads) = args val sparkConf = new SparkConf().setAppName(&quot;KafkaWordCount&quot;).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(sparkConf, Seconds(5)) ssc.checkpoint(&quot;c://ck2&quot;) //&quot;alog-2016-04-16,alog-2016-04-17,alog-2016-04-18&quot; //&quot;Array((alog-2016-04-16, 2), (alog-2016-04-17, 2), (alog-2016-04-18, 2))&quot; val topicMap = topics.split(&quot;,&quot;).map((_, numThreads.toInt)).toMap val data = KafkaUtils.createStream(ssc, zkQuorum, group, topicMap, StorageLevel.MEMORY_AND_DISK_SER) //kafka发送下来的是(key,value),我们只是需要的是value val words = data.map(_._2).flatMap(_.split(&quot; &quot;)) val wordCounts = words.map((_, 1)).updateStateByKey(updateFunc, new HashPartitioner(ssc.sparkContext.defaultParallelism), true) ssc.start() ssc.awaitTermination() &#125;&#125; 启动一个生产者发送消息123 #启动一个生产者发送消息bin/kafka-console-producer.sh --broker-list 192.168.80.10:9092 --topic wordcount 方式二直连方式,一个kafka的partition对应DStream里Rdd的一个分区,要自己来管理偏移量 方式二的DirectKafkaWordCount 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.itcast.spark.day5 import kafka.serializer.StringDecoderimport org.apache.log4j.&#123;Level, Logger&#125;import org.apache.spark.SparkConfimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.kafka.&#123;KafkaManager, KafkaUtils&#125;import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125; object DirectKafkaWordCount &#123; /* def dealLine(line: String): String = &#123; val list = line.split(&apos;,&apos;).toList // val list = AnalysisUtil.dealString(line, &apos;,&apos;, &apos;&quot;&apos;)// 把dealString函数当做split即可 list.get(0).substring(0, 10) + &quot;-&quot; + list.get(26) &#125;*/ def processRdd(rdd: RDD[(String, String)]): Unit = &#123; val lines = rdd.map(_._2) val words = lines.map(_.split(&quot; &quot;)) val wordCounts = words.map(x =&gt; (x, 1L)).reduceByKey(_ + _) wordCounts.foreach(println) &#125; def main(args: Array[String]) &#123; if (args.length &lt; 3) &#123; System.err.println( s&quot;&quot;&quot; |Usage: DirectKafkaWordCount &lt;brokers&gt; &lt;topics&gt; &lt;groupid&gt; | &lt;brokers&gt; is a list of one or more Kafka brokers | &lt;topics&gt; is a list of one or more kafka topics to consume from | &lt;groupid&gt; is a consume group | &quot;&quot;&quot;.stripMargin) System.exit(1) &#125; Logger.getLogger(&quot;org&quot;).setLevel(Level.WARN) val Array(brokers, topics, groupId) = args // Create context with 2 second batch interval val sparkConf = new SparkConf().setAppName(&quot;DirectKafkaWordCount&quot;) sparkConf.setMaster(&quot;local[*]&quot;) sparkConf.set(&quot;spark.streaming.kafka.maxRatePerPartition&quot;, &quot;5&quot;) sparkConf.set(&quot;spark.serializer&quot;, &quot;org.apache.spark.serializer.KryoSerializer&quot;) val ssc = new StreamingContext(sparkConf, Seconds(2)) // Create direct kafka stream with brokers and topics val topicsSet = topics.split(&quot;,&quot;).toSet val kafkaParams = Map[String, String]( &quot;metadata.broker.list&quot; -&gt; brokers,//直接连broker &quot;group.id&quot; -&gt; groupId, &quot;auto.offset.reset&quot; -&gt; &quot;smallest&quot; ) val km = new KafkaManager(kafkaParams) val messages = km.createDirectStream[String, String, StringDecoder, StringDecoder]( ssc, kafkaParams, topicsSet) messages.foreachRDD(rdd =&gt; &#123; if (!rdd.isEmpty()) &#123; // 先处理消息 processRdd(rdd) // 再更新offsets km.updateZKOffsets(rdd) &#125; &#125;) ssc.start() ssc.awaitTermination() &#125;&#125; org.apache.spark.streaming.kafka.KafkaManager 类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125package org.apache.spark.streaming.kafka import kafka.common.TopicAndPartitionimport kafka.message.MessageAndMetadataimport kafka.serializer.Decoderimport org.apache.spark.SparkExceptionimport org.apache.spark.rdd.RDDimport org.apache.spark.streaming.StreamingContextimport org.apache.spark.streaming.dstream.InputDStreamimport org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset import scala.reflect.ClassTag /** * 自己管理offset */class KafkaManager(val kafkaParams: Map[String, String]) extends Serializable &#123; //因为new KafkaCluster 是 private[spark] 所以取包名为:org.apache.spark.streaming.kafka private val kc = new KafkaCluster(kafkaParams) /** * 创建数据流 */ def createDirectStream[K: ClassTag, V: ClassTag, KD &lt;: Decoder[K]: ClassTag, VD &lt;: Decoder[V]: ClassTag]( ssc: StreamingContext, kafkaParams: Map[String, String], topics: Set[String]): InputDStream[(K, V)] = &#123; val groupId = kafkaParams.get(&quot;group.id&quot;).get // 在zookeeper上读取offsets前先根据实际情况更新offsets setOrUpdateOffsets(topics, groupId) //从zookeeper上读取offset开始消费message val messages = &#123; val partitionsE = kc.getPartitions(topics) if (partitionsE.isLeft) throw new SparkException(s&quot;get kafka partition failed: $&#123;partitionsE.left.get&#125;&quot;) val partitions = partitionsE.right.get val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions) if (consumerOffsetsE.isLeft) throw new SparkException(s&quot;get kafka consumer offsets failed: $&#123;consumerOffsetsE.left.get&#125;&quot;) val consumerOffsets = consumerOffsetsE.right.get KafkaUtils.createDirectStream[K, V, KD, VD, (K, V)]( ssc, kafkaParams, consumerOffsets, (mmd: MessageAndMetadata[K, V]) =&gt; (mmd.key, mmd.message)) &#125; messages &#125; /** * 创建数据流前，根据实际消费情况更新消费offsets * @param topics * @param groupId */ private def setOrUpdateOffsets(topics: Set[String], groupId: String): Unit = &#123; topics.foreach(topic =&gt; &#123; var hasConsumed = true val partitionsE = kc.getPartitions(Set(topic)) if (partitionsE.isLeft) throw new SparkException(s&quot;get kafka partition failed: $&#123;partitionsE.left.get&#125;&quot;) val partitions = partitionsE.right.get val consumerOffsetsE = kc.getConsumerOffsets(groupId, partitions) if (consumerOffsetsE.isLeft) hasConsumed = false if (hasConsumed) &#123;// 消费过 /** * 如果streaming程序执行的时候出现kafka.common.OffsetOutOfRangeException， * 说明zk上保存的offsets已经过时了，即kafka的定时清理策略已经将包含该offsets的文件删除。 * 针对这种情况，只要判断一下zk上的consumerOffsets和earliestLeaderOffsets的大小， * 如果consumerOffsets比earliestLeaderOffsets还小的话，说明consumerOffsets已过时, * 这时把consumerOffsets更新为earliestLeaderOffsets */ val earliestLeaderOffsetsE = kc.getEarliestLeaderOffsets(partitions) if (earliestLeaderOffsetsE.isLeft) throw new SparkException(s&quot;get earliest leader offsets failed: $&#123;earliestLeaderOffsetsE.left.get&#125;&quot;) val earliestLeaderOffsets = earliestLeaderOffsetsE.right.get val consumerOffsets = consumerOffsetsE.right.get // 可能只是存在部分分区consumerOffsets过时，所以只更新过时分区的consumerOffsets为earliestLeaderOffsets var offsets: Map[TopicAndPartition, Long] = Map() consumerOffsets.foreach(&#123; case(tp, n) =&gt; val earliestLeaderOffset = earliestLeaderOffsets(tp).offset if (n &lt; earliestLeaderOffset) &#123; println(&quot;consumer group:&quot; + groupId + &quot;,topic:&quot; + tp.topic + &quot;,partition:&quot; + tp.partition + &quot; offsets已经过时，更新为&quot; + earliestLeaderOffset) offsets += (tp -&gt; earliestLeaderOffset) &#125; &#125;) if (!offsets.isEmpty) &#123; kc.setConsumerOffsets(groupId, offsets)//针对某一个topic的组来设置offset &#125; &#125; else &#123;// 没有消费过 val reset = kafkaParams.get(&quot;auto.offset.reset&quot;).map(_.toLowerCase) var leaderOffsets: Map[TopicAndPartition, LeaderOffset] = null if (reset == Some(&quot;smallest&quot;)) &#123; val leaderOffsetsE = kc.getEarliestLeaderOffsets(partitions) if (leaderOffsetsE.isLeft) throw new SparkException(s&quot;get earliest leader offsets failed: $&#123;leaderOffsetsE.left.get&#125;&quot;) leaderOffsets = leaderOffsetsE.right.get &#125; else &#123; val leaderOffsetsE = kc.getLatestLeaderOffsets(partitions) if (leaderOffsetsE.isLeft) throw new SparkException(s&quot;get latest leader offsets failed: $&#123;leaderOffsetsE.left.get&#125;&quot;) leaderOffsets = leaderOffsetsE.right.get &#125; val offsets = leaderOffsets.map &#123; case (tp, offset) =&gt; (tp, offset.offset) &#125; kc.setConsumerOffsets(groupId, offsets) &#125; &#125;) &#125; /** * 更新zookeeper上的消费offsets * @param rdd */ def updateZKOffsets(rdd: RDD[(String, String)]) : Unit = &#123; val groupId = kafkaParams.get(&quot;group.id&quot;).get val offsetsList = rdd.asInstanceOf[HasOffsetRanges].offsetRanges for (offsets &lt;- offsetsList) &#123; val topicAndPartition = TopicAndPartition(offsets.topic, offsets.partition) val o = kc.setConsumerOffsets(groupId, Map((topicAndPartition, offsets.untilOffset))) if (o.isLeft) &#123; println(s&quot;Error updating the offset to Kafka cluster: $&#123;o.left.get&#125;&quot;) &#125; &#125; &#125;&#125; 参考: http://blog.csdn.net/ligt0610/article/details/47311771","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark streaming数据累加小例子","date":"2017-04-16T04:47:25.137Z","path":"2017/04/16/bigdata/spark/spark streaming数据累加小例子/","text":"1.结构图使用nc命令向spark streaming 发送数据 2.spark streaming 接收程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package cn.itcast.spark.day5 import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.&#123;HashPartitioner, SparkConf, SparkContext&#125; /** * Created by root on 2016/5/21. */object StateFulWordCount &#123; /** * String : 单词 hello * Seq[Int] ：单词在当前批次出现的次数 * Option[Int] ： 以前的结果 * */ //分好组的数据 val updateFunc = (iter: Iterator[(String, Seq[Int], Option[Int])]) =&gt; &#123; //iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x))) //iter.map&#123;case(x,y,z)=&gt;Some(y.sum + z.getOrElse(0)).map(m=&gt;(x, m))&#125; //iter.map(t =&gt; (t._1, t._2.sum + t._3.getOrElse(0))) iter.map&#123; //这是一个模式匹配 case(word, current_count, history_count) =&gt; (word, current_count.sum + history_count.getOrElse(0)) &#125; &#125; def main(args: Array[String]) &#123; //设置日志的级别 LoggerLevels.setStreamingLogLevels() //StreamingContext val conf = new SparkConf().setAppName(&quot;StateFulWordCount&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //updateStateByKey必须设置setCheckpointDir sc.setCheckpointDir(&quot;c://ck&quot;)//实际场景是使用HDFS val ssc = new StreamingContext(sc, Seconds(5)) val ds = ssc.socketTextStream(&quot;172.16.0.11&quot;, 8888) //DStream是一个特殊的RDD //hello tom hello jerry val result = ds.flatMap(_.split(&quot; &quot;)).map((_, 1)).updateStateByKey(updateFunc, new HashPartitioner(sc.defaultParallelism), true) result.print() ssc.start() ssc.awaitTermination() &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"spark streaming 整合flume","date":"2017-04-16T04:47:25.136Z","path":"2017/04/16/bigdata/spark/spark streaming 整合flume/","text":"官方文档 1.方式一:flume直接和spark streaming 结合如果数据量不是很大,那么直接将数据通过flume采集到spark streaming中 1.1.flume向spark streaming 中push spark streaming 代码12345678910111213141516171819202122232425package cn.itcast.spark.day5 import org.apache.spark.SparkConfimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125; object FlumePushWordCount &#123; def main(args: Array[String]) &#123; val host = args(0) val port = args(1).toInt LoggerLevels.setStreamingLogLevels() val conf = new SparkConf().setAppName(&quot;FlumeWordCount&quot;)//.setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf, Seconds(5)) //推送方式: flume向spark发送数据:这里只能够指定一个主机和端口,那么在集群环境下,这种方式肯定是不可取的 val flumeStream = FlumeUtils.createStream(ssc, host, port)//指定flume向spark streaming发送数据的ip和port //flume中的数据通过event.getBody()才能拿到真正的内容 val words = flumeStream.flatMap(x =&gt; new String(x.event.getBody().array()).split(&quot; &quot;)).map((_, 1)) val results = words.reduceByKey(_ + _) results.print() ssc.start() ssc.awaitTermination() &#125;&#125; flume配置文件 flume-push.conf123456789101112131415161718192021222324# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # sourcea1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/data/flumea1.sources.r1.fileHeader = true # Describe the sinka1.sinks.k1.type = avro#这是接收方a1.sinks.k1.hostname = 192.168.31.172a1.sinks.k1.port = 8888 # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动1234#首先启动spark-streaming应用程序bin/spark-submit --class cn.itcast.spark.streaming.FlumeWordCount /root/streaming-1.0.jar#再启动flmuebin/flume-ng agent -n a1 -c conf/ -f conf/flume-push.conf -Dflume.root.logger=WARN,console 1.2.spark streaming 从flume中拉取数据(poll) spark streaming 代码123456789101112131415161718192021222324package cn.itcast.spark.day5 import java.net.InetSocketAddress import org.apache.spark.SparkConfimport org.apache.spark.storage.StorageLevelimport org.apache.spark.streaming.flume.FlumeUtilsimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125; object FlumePollWordCount &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;FlumePollWordCount&quot;).setMaster(&quot;local[2]&quot;) val ssc = new StreamingContext(conf, Seconds(5)) //从多个flume中拉取数据(flume的地址) val address = Seq(new InetSocketAddress(&quot;172.16.0.11&quot;, 8888)) val flumeStream = FlumeUtils.createPollingStream(ssc, address, StorageLevel.MEMORY_AND_DISK) //flume中的数据通过event.getBody()才能拿到真正的内容 val words = flumeStream.flatMap(x =&gt; new String(x.event.getBody().array()).split(&quot; &quot;)).map((_,1)) val results = words.reduceByKey(_+_) results.print() ssc.start() ssc.awaitTermination() &#125;&#125; flume配置文件 flume-poll.conf1234567891011121314151617181920212223# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # sourcea1.sources.r1.type = spooldira1.sources.r1.spoolDir = /export/data/flumea1.sources.r1.fileHeader = true # Describe the sinka1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSinka1.sinks.k1.hostname = mastera1.sinks.k1.port = 8888 # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动12345#首先将下载好的spark-streaming-flume-sink_2.10-1.6.1.jar和scala-library-2.10.5.jar还有commons-lang3-3.3.2.jar三个包放入到flume的lib目录下#启动flumebin/flume-ng agent -n a1 -c conf/ -f conf/flume-poll.conf -Dflume.root.logger=WARN,console#再启动spark-streaming应用程序bin/spark-submit --class cn.itcast.spark.streaming.FlumePollWordCount /root/streaming-1.0.jar","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark Shell","date":"2017-04-16T04:47:25.135Z","path":"2017/04/16/bigdata/spark/Spark Shell/","text":"1.启动shell&emsp;spark-shell是Spark自带的交互式Shell程序，方便用户进行交互式编程，用户可以在该命令行下用scala编写spark程序 1234567891011121314151617181920212223/export/servers/spark/bin/spark-shell \\--master spark://hdp-node-01:7077 \\--executor-memory 1g \\--total-executor-cores 2/*参数说明：--master spark://node1.itcast.cn:7077 指定Master的地址--executor-memory 2g 指定每个worker可用内存为2G--total-executor-cores 2 指定整个集群使用的cup核数为2个注意：如果启动spark shell时没有指定master地址，但是也可以正常启动spark shell和执行spark shell中的程序，其实是启动了spark的local模式，该模式仅在本机启动一个进程，没有与集群建立联系。 Spark Shell中已经默认将SparkContext类初始化为对象sc。用户代码如果需要用到，则直接应用sc即可exit 退出shell*/ 2.在spark shell中编写WordCount程序1.首先启动hdfs2.向hdfs上传一个文件到 words.txt1234567[root@hdp-node-02 export]# cat words.txttom jerrychenyansong zhangsantom jerrywo shi whochenyansong 上传文件1hdfs dfs -put words.txt /wordcount/ 3.在spark shell中用scala语言编写spark程序1scala&gt; sc.textFile(&quot;hdfs://hdp-node-01:9000/wordcount/words.txt&quot;).flatMap(_.split(&quot; &quot;)).map((_ ,1)).reduceByKey(_+_).saveAsTextFile(&quot;hdfs://hdp-node-01:9000/wordcount/out&quot;) 4.使用hdfs命令查看结果1234567891011121314[root@hdp-node-02 export]# hdfs dfs -ls /wordcount/out3Found 3 items-rw-r--r-- 3 root supergroup 0 2016-12-17 21:13 /wordcount/out3/_SUCCESS-rw-r--r-- 3 root supergroup 54 2016-12-17 21:13 /wordcount/out3/part-00000-rw-r--r-- 3 root supergroup 16 2016-12-17 21:13 /wordcount/out3/part-00001[root@hdp-node-02 export]# hdfs dfs -cat /wordcount/out3/p*(zhangsan,1)(shi,1)(tom,2)(wo,1)(who,1)(jerry,2)(chenyansong,2) 说明： sc是SparkContext对象，该对象时提交spark程序的入口 textFile(hdfs://node1.itcast.cn:9000/words.txt)是hdfs中读取数据 flatMap(_.split(“ “))先map在压平 map((_,1))将单词和1构成元组 reduceByKey(+)按照key进行reduce，并将value累加 saveAsTextFile(“hdfs://node1.itcast.cn:9000/out”)将结果写入到hdfs中","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"Spark On Hive","date":"2017-04-16T04:47:25.134Z","path":"2017/04/16/bigdata/spark/Spark On Hive/","text":"1.编译spark源码Spark On Hive，通过spark sql模块访问和使用Hive，默认Spark预编译(pre-built)版不包含hive相关依赖，并不支持此功能，因此需要对spark源码进行重新编译，并进行相关的配置，具体操作步骤参见: spark源码编译.md 2.安装hive123CREATE USER &apos;hive&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos;;GRANT all privileges ON hive.* TO &apos;hive&apos;@&apos;%&apos;;flush privileges; 3.将配置好的hive-site.xml放入$SPARK-HOME/conf目录下 hive-site.xml文件:其实就是写:连接的数据库/jdbc驱动/用户名/密码12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;!-- Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to You under the Apache License, Version 2.0 (the &quot;License&quot;); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.--&gt;&lt;configuration&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://172.16.0.1:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;123456&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; 4.写测试sql12345678910111213141516171819202122232425262728293031323334#方式一:启动spark-shell时指定mysql连接驱动位置bin/spark-shell \\--master spark://node1.itcast.cn:7077 \\--executor-memory 1g \\--total-executor-cores 2 \\--driver-class-path /usr/local/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.35-bin.jarsqlContext.sql(&quot;select * from spark.person limit 2&quot;)#方式二:API 的方式org.apache.spark.sql.hive.HiveContextimport org.apache.spark.sql.hive.HiveContextval hiveContext = new HiveContext(sc)hiveContext.sql(&quot;select * from spark.person&quot;)#方式三bin/spark-sql \\--master spark://node1.itcast.cn:7077 \\--executor-memory 1g \\--total-executor-cores 2 \\--driver-class-path /usr/local/apache-hive-0.13.1-bin/lib/mysql-connector-java-5.1.35-bin.jar#直接写sql就可以了/*因为连接mysql的时候需要指定驱动类,所以不管是在spark-shell还是在spark-sql中的时候,都要指定mysql驱动类,两种方式:1.将驱动类放在spark的lib下2.手动指定驱动类的位置:--driver-class-path (上面 的方式都是手动指定的驱动的位置)*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"rdd缓存","date":"2017-04-16T04:47:25.133Z","path":"2017/04/16/bigdata/spark/rdd缓存/","text":"1.Spark RDD缓存源码分析我们知道,spark相比Hadoop最大的一个优势就是可以将数据cache到内存,以供后面的计算使用,我们可以通过rdd.persist()或rdd.cache()来缓存RDD中的数据cache()其实就是调用的persist()实现的,persist()支持下面的几种存储级别:123456789101112val NONE = new StorageLevel(false, false, false, false)val DISK_ONLY = new StorageLevel(true, false, false, false)val DISK_ONLY_2 = new StorageLevel(true, false, false, false, 2)val MEMORY_ONLY = new StorageLevel(false, true, false, true)val MEMORY_ONLY_2 = new StorageLevel(false, true, false, true, 2)val MEMORY_ONLY_SER = new StorageLevel(false, true, false, false)val MEMORY_ONLY_SER_2 = new StorageLevel(false, true, false, false, 2)val MEMORY_AND_DISK = new StorageLevel(true, true, false, true)val MEMORY_AND_DISK_2 = new StorageLevel(true, true, false, true, 2)val MEMORY_AND_DISK_SER = new StorageLevel(true, true, false, false)val MEMORY_AND_DISK_SER_2 = new StorageLevel(true, true, false, false, 2)val OFF_HEAP = new StorageLevel(false, false, true, false) &emsp;而cache()最终调用的是persist(StorageLevel.MEMORY_ONLY)，也就是默认的缓存级别。我们可以根据自己的需要去设置不同的缓存级别，这里各种缓存级别的含义我就不介绍了，可以参见官方文档说明。通过调用rdd.persist()来缓存RDD中的数据，其最终调用的都是下面的代码： 1234567891011121314private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = &#123; // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE &amp;&amp; newLevel != storageLevel &amp;&amp; !allowOverride) &#123; throw new UnsupportedOperationException( &quot;Cannot change storage level of an RDD after it was already assigned a level&quot;) &#125; // If this is the first time this RDD is marked for persisting, register it if (storageLevel == StorageLevel.NONE) &#123; sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) &#125; storageLevel = newLevel this&#125; &emsp; 这段代码的最主要作用其实就是将storageLevel设置为persist()函数传进来的存储级别，而且一旦设置好RDD的存储级别之后就不能再对相同RDD设置别的存储级别，否则将会出现异常。设置好存储级别在之后除非触发了action操作，否则不会真正地执行缓存操作。当我们触发了action，它会调用sc.runJob方法来真正的计算，而这个方法最终会调用org.apache.spark.scheduler.Task#run，而这个方法最后会调用ResultTask或者ShuffleMapTask的runTask方法，runTask方法最后会调用org.apache.spark.rdd.RDD#iterator方法，iterator的代码如下：1234567final def iterator(split: Partition, context: TaskContext): Iterator[T] = &#123; if (storageLevel != StorageLevel.NONE) &#123; SparkEnv.get.cacheManager.getOrCompute(this, split, context, storageLevel) &#125; else &#123; computeOrReadCheckpoint(split, context) &#125;&#125; &emsp;如果当前RDD设置了存储级别（也就是通过上面的rdd.persist()设置的），那么会从cacheManager中判断是否有缓存数据。如果有，则直接获取，如果没有则计算。getOrCompute的代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def getOrCompute[T]( rdd: RDD[T], partition: Partition, context: TaskContext, storageLevel: StorageLevel): Iterator[T] = &#123; val key = RDDBlockId(rdd.id, partition.index) logDebug(s&quot;Looking for partition $key&quot;) blockManager.get(key) match &#123; case Some(blockResult) =&gt; // Partition is already materialized, so just return its values val existingMetrics = context.taskMetrics .getInputMetricsForReadMethod(blockResult.readMethod) existingMetrics.incBytesRead(blockResult.bytes) val iter = blockResult.data.asInstanceOf[Iterator[T]] new InterruptibleIterator[T](context, iter) &#123; override def next(): T = &#123; existingMetrics.incRecordsRead(1) delegate.next() &#125; &#125; case None =&gt; // Acquire a lock for loading this partition // If another thread already holds the lock, wait for it to finish return its results val storedValues = acquireLockForPartition[T](key) if (storedValues.isDefined) &#123; return new InterruptibleIterator[T](context, storedValues.get) &#125; // Otherwise, we have to load the partition ourselves try &#123; logInfo(s&quot;Partition $key not found, computing it&quot;) val computedValues = rdd.computeOrReadCheckpoint(partition, context) // If the task is running locally, do not persist the result if (context.isRunningLocally) &#123; return computedValues &#125; // Otherwise, cache the values and keep track of any updates in block statuses val updatedBlocks = new ArrayBuffer[(BlockId, BlockStatus)] val cachedValues = putInBlockManager(key, computedValues, storageLevel, updatedBlocks) val metrics = context.taskMetrics val lastUpdatedBlocks = metrics.updatedBlocks.getOrElse(Seq[(BlockId, BlockStatus)]()) metrics.updatedBlocks = Some(lastUpdatedBlocks ++ updatedBlocks.toSeq) new InterruptibleIterator(context, cachedValues) &#125; finally &#123; loading.synchronized &#123; loading.remove(key) loading.notifyAll() &#125; &#125; &#125;&#125; &emsp;首先通过RDD的ID和当前计算的分区ID构成一个key，并向blockManager中查找是否存在相关的block信息。如果能够获取得到，说明当前分区已经被缓存了；否者需要重新计算。如果重新计算，我们需要获取到相关的锁，因为可能有多个线程对请求同一分区的数据。如果获取到相关的锁，则会调用rdd.computeOrReadCheckpoint(partition, context)计算当前分区的数据，并放计算完的数据放到BlockManager中，如果有相关的线程等待该分区的计算，那么在计算完数据之后还得通知它们（loading.notifyAll()）。 &emsp;如果获取锁失败，则说明已经有其他线程在计算该分区中的数据了，那么我们就得等（loading.wait()），获取锁的代码如下：123456789101112131415161718192021222324252627282930private def acquireLockForPartition[T](id: RDDBlockId): Option[Iterator[T]] = &#123; loading.synchronized &#123; if (!loading.contains(id)) &#123; // If the partition is free, acquire its lock to compute its value loading.add(id) None &#125; else &#123; // Otherwise, wait for another thread to finish and return its result logInfo(s&quot;Another thread is loading $id, waiting for it to finish...&quot;) while (loading.contains(id)) &#123; try &#123; loading.wait() &#125; catch &#123; case e: Exception =&gt; logWarning(s&quot;Exception while waiting for another thread to load $id&quot;, e) &#125; &#125; logInfo(s&quot;Finished waiting for $id&quot;) val values = blockManager.get(id) if (!values.isDefined) &#123; /* The block is not guaranteed to exist even after the other thread has finished. * For instance, the block could be evicted after it was put, but before our get. * In this case, we still need to load the partition ourselves. */ logInfo(s&quot;Whoever was loading $id failed; we&apos;ll try it ourselves&quot;) loading.add(id) &#125; values.map(_.data.asInstanceOf[Iterator[T]]) &#125; &#125;&#125; &emsp;等待的线程（也就是没有获取到锁的线程）是通过获取到锁的线程调用loading.notifyAll()唤醒的，唤醒之后之后调用new InterruptibleIteratorT获取已经缓存的数据。以后后续RDD需要这个RDD的数据我们就可以直接在缓存中获取了，而不需要再计算了。后面我会对checkpoint相关代码进行分析。 2.示例代码1234567891011121314151617181920212223242526272829303132333435363738394041424344 def main(args: Array[String]) &#123; val conf = new SparkConf().setAppName(&quot;UrlCountPartition&quot;).setMaster(&quot;local[2]&quot;) val sc = new SparkContext(conf) //rdd1将数据切分，元组中放的是（URL， 1） val rdd1 = sc.textFile(&quot;c://itcast.log&quot;).map(line =&gt; &#123; val f = line.split(&quot;\\t&quot;) (f(1), 1) &#125;) val rdd2 = rdd1.reduceByKey(_ + _) &apos;步骤1&apos; val rdd3 = rdd2.map(t =&gt; &#123; val url = t._1 val host = new URL(url).getHost (host, (url, t._2)) &#125;).cache()//cache会将数据缓存到内存当中，cache是一个Transformation，lazy &apos;步骤2&apos; val ints = rdd3.map(_._1).distinct().collect() val hostParitioner = new HostParitioner(ints) &apos;步骤3&apos; val rdd4 = rdd3.partitionBy(hostParitioner).mapPartitions(it =&gt; &#123; it.toList.sortBy(_._2._2).reverse.take(2).iterator &#125;) rdd4.saveAsTextFile(&quot;c://out4&quot;) //println(rdd4.collect().toBuffer) sc.stop() &#125;/*因为transaction是延迟加载,transaction只是保存了所有动作的执行轨迹,并没有真正的执行,所以只有当有action的动作的时候,才会有真正的执行的动作,在步骤1的最后进行了cache()标记,在进行步骤2(action)的时候会将步骤1中的结果进行缓存,所以在进行步骤3操作的时候,会利用步骤1的缓存的结果直接进行计算,但是如果步骤1没有进行cache操作,那么在进行步骤3的时候,会重新计算前面所有的结果*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"RDD的依赖关系以及DAG","date":"2017-04-16T04:47:25.132Z","path":"2017/04/16/bigdata/spark/RDD的依赖关系以及DAG/","text":"RDD的依赖关系 DAG&emsp;DAG(Directed Acyclic Graph)叫做有向无环图，原始的RDD通过一系列的转换就就形成了DAG，根据RDD之间的依赖关系的不同将DAG划分成不同的Stage，对于窄依赖，partition的转换处理在Stage中完成计算。对于宽依赖，由于有Shuffle的存在，只能在parent RDD处理完成后，才能开始接下来的计算，因此宽依赖是划分Stage的依据。","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"RDD-API之groupBy","date":"2017-04-16T04:47:25.130Z","path":"2017/04/16/bigdata/spark/RDD-API之groupBy/","text":"12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273package spark.examples.rddapiimport org.apache.spark.&#123;Partitioner, SparkContext, SparkConf&#125;object GroupByTest_06 &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;CoGroupTest_05&quot;) val sc = new SparkContext(conf); val z1 = sc.parallelize(List((3, &quot;A&quot;), (6, &quot;B1&quot;), (7, &quot;Z1&quot;), (9, &quot;E&quot;), (7, &quot;F&quot;), (9, &quot;Y&quot;), (77, &quot;Z&quot;), (31, &quot;X&quot;)), 3) /** * Return an RDD of grouped items. Each group consists of a key and a sequence of elements * mapping to that key. The ordering of elements within each group is not guaranteed, and * may even differ each time the resulting RDD is evaluated. * * Note: This operation may be very expensive. If you are grouping in order to perform an * aggregation (such as a sum or average) over each key, using [[PairRDDFunctions.aggregateByKey]] * or [[PairRDDFunctions.reduceByKey]] will provide much better performance. */ // def groupBy[K](f: T =&gt; K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = groupBy[K](f, defaultPartitioner(this)) //根据指定的函数进行分组,分组得到的集合的元素类型是(K,V),K是分组函数的返回值，V是组内元素列表 val r = z1.groupBy(x =&gt; if (x._1 % 2 == 0) &quot;even&quot; else &quot;odd&quot;) r.collect().foreach(println) //结果： /* (even,CompactBuffer((6,B1))) (odd,CompactBuffer((3,A), (7,Z1), (9,E), (7,F), (9,Y), (77,Z), (31,X))) */ //Partitioner是HashPartitioner val r2 = z1.groupBy(_._1 % 2) r2.collect().foreach(println) //结果： /* (0,CompactBuffer((6,B1))) (1,CompactBuffer((3,A), (7,Z1), (9,E), (7,F), (9,Y), (77,Z), (31,X))) */ class MyPartitioner extends Partitioner &#123; override def numPartitions = 3 def getPartition(key: Any): Int = &#123; key match &#123; case null =&gt; 0 case key: Int =&gt; key % numPartitions case _ =&gt; key.hashCode % numPartitions &#125; &#125; override def equals(other: Any): Boolean = &#123; other match &#123; case h: MyPartitioner =&gt; true case _ =&gt; false &#125; &#125; &#125; println(&quot;=======================GroupBy with Partitioner====================&quot;) //分组的同时进行分区；分区的key是分组函数的计算结果？ val r3 = z1.groupBy((x:(Int, String)) =&gt; x._1, new MyPartitioner()) r3.collect().foreach(println) /* //6,3,9一个分区，7,31一个分区，77一个分区 (6,CompactBuffer((6,B1))) (3,CompactBuffer((3,A))) (9,CompactBuffer((9,E), (9,Y))) (7,CompactBuffer((7,Z1), (7,F))) (31,CompactBuffer((31,X))) (77,CompactBuffer((77,Z))) */ &#125;&#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"RDD-API之combineByKey","date":"2017-04-16T04:47:25.129Z","path":"2017/04/16/bigdata/spark/RDD-API之combineByKey/","text":"1.combineByKey函数的运行机制&emsp;RDD提供了很多针对元素类型为(K,V)的API，这些API封装在PairRDDFunctions类中，通过Scala隐式转换使用。这些API实现上是借助于combineByKey实现的。combineByKey函数本身也是RDD开放给Spark开发人员使用的API之一 首先看一下combineByKey的方法说明： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Generic function to combine the elements for each key using a custom set of aggregation * functions. Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a &quot;combined type&quot; C * Note that V and C can be different -- for example, one might group an RDD of type * (Int, Int) into an RDD of type (Int, Seq[Int]). Users provide three functions: * * - `createCombiner`, which turns a V into a C (e.g., creates a one-element list) * - `mergeValue`, to merge a V into a C (e.g., adds it to the end of a list) * - `mergeCombiners`, to combine two C&apos;s into a single one. * * In addition, users can control the partitioning of the output RDD, and whether to perform * map-side aggregation (if a mapper can produce multiple items with the same key). */ def combineByKey[C](createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)] = &#123; require(mergeCombiners != null, &quot;mergeCombiners must be defined&quot;) // required as of Spark 0.9.0 if (keyClass.isArray) &#123; if (mapSideCombine) &#123; throw new SparkException(&quot;Cannot use map-side combining with array keys.&quot;) &#125; if (partitioner.isInstanceOf[HashPartitioner]) &#123; throw new SparkException(&quot;Default partitioner cannot partition array keys.&quot;) &#125; &#125; val aggregator = new Aggregator[K, V, C]( self.context.clean(createCombiner), self.context.clean(mergeValue), self.context.clean(mergeCombiners)) if (self.partitioner == Some(partitioner)) &#123; self.mapPartitions(iter =&gt; &#123; val context = TaskContext.get() new InterruptibleIterator(context, aggregator.combineValuesByKey(iter, context)) &#125;, preservesPartitioning = true) &#125; else &#123; new ShuffledRDD[K, V, C](self, partitioner) .setSerializer(serializer) .setAggregator(aggregator) .setMapSideCombine(mapSideCombine) &#125; &#125;/* 主要看: createCombiner: V =&gt; C, mergeValue: (C, V) =&gt; C, mergeCombiners: (C, C) =&gt; C,combineByKey的功能是对RDD中的数据集按照Key进行聚合(想象下Hadoop MapReduce的Combiner，用于Map端做Reduce)。聚合的逻辑是通过自定义函数提供给combineByKey。从上面的源代码中可以看到，combineByKey是把(K,V)类型的RDD转换为(K,C)类型的RDD，C和V可以不一样。combineByKey函数需要三个重要的函数作为参数createCombiner：在遍历RDD的数据集合过程中，对于遍历到的(k,v)，如果combineByKey第一次遇到值为k的Key（类型K），那么将对这个(k,v)调用combineCombiner函数，它的作用是将v转换为c(类型是C，聚合对象的类型，c作为局和对象的初始值)mergeValue：在遍历RDD的数据集合过程中，对于遍历到的(k,v)，如果combineByKey不是第一次(或者第二次，第三次...)遇到值为k的Key（类型K），那么将对这个(k,v)调用mergeValue函数，它的作用是将v累加到聚合对象（类型C）中，mergeValue的类型是(C,V)=&gt;C,参数中的C遍历到此处的聚合对象，然后对v进行聚合得到新的聚合对象值mergeCombiners：因为combineByKey是在分布式环境下执行，RDD的每个分区单独进行combineByKey操作，最后需要对各个分区的结果进行最后的聚合，它的函数类型是(C,C)=&gt;C，每个参数是分区聚合得到的聚合对象。 */ combineByKey的流程是： 假设一组具有相同 K 的 records 正在一个个流向 combineByKey()，createCombiner 将第一个 record 的value 初始化为 c （比如，c = value），然后从第二个 record 开始，来一个 record 就使用 mergeValue(c,record.value) 来更新 c，比如想要对这些 records 的所有 values 做 sum，那么使用 c = c + record.value。等到records 全部被 mergeValue()，得到结果 c。假设还有一组 records（key 与前面那组的 key 均相同）一个个到来，combineByKey() 使用前面的方法不断计算得到 c’。现在如果要求这两组 records 总的 combineByKey() 后的结果，那么可以使用 final c = mergeCombiners(c, c’) 来计算。 举例123456789101112131415161718 combineByKey : 和reduceByKey是相同的效果###第一个参数x:原封不动取出来, 第二个参数:是函数, 局部运算, 第三个:是函数, 对局部运算后的结果再做运算###每个分区中每个key中value中的第一个值, (hello,1)(hello,1)(good,1)--&gt;(hello(1,1),good(1))--&gt;x就相当于hello的第一个1, good中的1val rdd1 = sc.textFile(&quot;hdfs://master:9000/wordcount/input/&quot;).flatMap(_.split(&quot; &quot;)).map((_, 1))val rdd2 = rdd1.combineByKey(x =&gt; x, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n)rdd1.collectrdd2.collect ###当input下有3个文件时(有3个block块, 不是有3个文件就有3个block, ), 每个会多加3个10val rdd3 = rdd1.combineByKey(x =&gt; x + 10, (a: Int, b: Int) =&gt; a + b, (m: Int, n: Int) =&gt; m + n) //x =&gt; x + 10 只是将分区的第一个元素作为x,然后加10 ,将x+10作为局部计算的初始值rdd3.collect val rdd4 = sc.parallelize(List(&quot;dog&quot;,&quot;cat&quot;,&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;turkey&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;), 3)val rdd5 = sc.parallelize(List(1,1,2,2,2,1,2,2,2), 3)val rdd6 = rdd5.zip(rdd4) //List((1,dog),(1,cat),(2,gnu),(2,salmon),(2,rabbit),(1,turkey),(2,wolf),(2,bear),(2,bee))val rdd7 = rdd6.combineByKey(List(_), (x: List[String], y: String) =&gt; x :+ y, (m: List[String], n: List[String]) =&gt; m ++ n) //(1,list(&quot;dog&quot;,&quot;cat&quot;,&quot;turkey&quot;) ) , (2, list(&quot;gnu&quot;,&quot;salmon&quot;,&quot;rabbit&quot;,&quot;wolf&quot;,&quot;bear&quot;,&quot;bee&quot;)) )","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"RDD-API之aggregateByKey.md","date":"2017-04-16T04:47:25.128Z","path":"2017/04/16/bigdata/spark/RDD-API之aggregateByKey/","text":"1. aggregateByKey的原理1234567891011121314151617181920212223242526272829303132 /** * Aggregate the values of each key, using given combine functions and a neutral &quot;zero value&quot;. * This function can return a different result type, U, than the type of the values in this RDD, * V. Thus, we need one operation for merging a V into a U and one operation for merging two U&apos;s, * as in scala.TraversableOnce. The former operation is used for merging values within a * partition, and the latter is used for merging values between partitions. To avoid memory * allocation, both of these functions are allowed to modify and return their first argument * instead of creating a new U. */ def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =&gt; U, combOp: (U, U) =&gt; U): RDD[(K, U)] = &#123; // Serialize the zero value to a byte array so that we can get a new clone of it on each key val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue) val zeroArray = new Array[Byte](zeroBuffer.limit) zeroBuffer.get(zeroArray) lazy val cachedSerializer = SparkEnv.get.serializer.newInstance() val createZero = () =&gt; cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray)) combineByKey[U]((v: V) =&gt; seqOp(createZero(), v), seqOp, combOp, partitioner) &#125;/*从aggregateByKey的源代码中，可以看出a.aggregateByKey把类型为(K,V)的RDD转换为类型为(K,U)的RDD，V和U的类型可以不一样，这一点跟combineByKey是一样的，即返回的二元组的值类型可以不一样b.aggregateByKey内部是通过调用combineByKey实现的，combineByKey的createCombiner函数逻辑由zeroValue这个变量实现，zeroValue作为聚合的初始值，通常对于加法聚合则为0，乘法聚合则为1，集合操作则为空集合c.seqOp在combineByKey中的功能是mergeValues，(U,V)=&gt;Ud.combOp在combineByKey中的功能是mergeCombiners*/ 2.aggregateByKey举例12345678910111213141516171819202122232425#求均值val rdd = sc.textFile(&quot;气象数据&quot;) val rdd2 = rdd.map(x=&gt;x.split(&quot; &quot;)).map(x =&gt; (x(0).substring(&quot;从年月日中提取年月&quot;),x(1).toInt)) val zeroValue = (0,0) val seqOp= (u:(Int, Int), v:Int) =&gt; &#123; (u._1 + v, u._2 + 1) &#125; val compOp= (c1:(Int,Int),c2:(Int,Int))=&gt;&#123; (u1._1 + u2._1, u1._2 + u2._2) &#125; val vdd3 = vdd2.aggregateByKey( zeroValue , seqOp, compOp) rdd3.foreach(x=&gt;println(x._1 + &quot;: average tempreture is &quot; + x._2._1/x._2._2) /*从求均值的实现来看，aggregate通过提供零值的方式，避免了combineByKey中的createCombiner步骤(createCombiner本质工作就是遇到第一个key时进行初始化操作，这个初始化不是提供零值，而是对第一个(k,v)进行转换得到c的初始值））*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"RDD-API(转)","date":"2017-04-16T04:47:25.127Z","path":"2017/04/16/bigdata/spark/RDD-API(转)/","text":"aggregate12345678910111213141516171819202122232425262728293031323334353637383940414243package spark.examples.rddapi import org.apache.spark.&#123;SparkConf, SparkContext&#125; //测试RDD的aggregate方法 object AggregateTest &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AggregateTest_00&quot;) val sc = new SparkContext(conf); val z1 = sc.parallelize(List(1, 3, 5, 7, 7, 5, 3, 3, 79), 2) /** * Aggregate the elements of each partition, and then the results for all the partitions, using * given combine functions and a neutral &quot;zero value&quot;. This function can return a different result * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U * and one operation for merging two U&apos;s, as in scala.TraversableOnce. Both of these functions are * allowed to modify and return their first argument instead of creating a new U to avoid memory * allocation. */ // def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =&gt; U, combOp: (U, U) =&gt; U): U //T是RDD中的元素类型，U是aggregate方法自定义的泛型参数，aggregate返回U(而不一定是T) //两个分区取最大值，然后相加 //math.max(_, _)表示针对每个partition实施的操作, _ + _表示combiner val r1 = z1.aggregate(0)(math.max(_, _), _ + _) println(r1) //86 //RDD元素类型字符串，aggregate的返回类型同样为String val z2 = sc.parallelize(List(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;), 2) val r2 = z2.aggregate(&quot;xx&quot;)(_ + _, _ + _) println(r2) //连接操作，结果xxxxabcxxdef，每个分区计算时，加上xx，最后两个分区计算时，继续把xx加上 //_ + _的道理也是(x,y) =&gt; x + y //(x,y)=&gt;math.max是做两两比较吗？ val z3 = sc.parallelize(List(&quot;12&quot;, &quot;23&quot;, &quot;345&quot;, &quot;4567&quot;), 2) val r3 = z3.aggregate(&quot;&quot;)((x, y) =&gt; math.max(x.length, y.length).toString, (x, y) =&gt; x + y) println(r3) ///结果24，表示两个分区的字符串长度最长的长度转成String后，做拼接 //结果11 val r4 = sc.parallelize(List(&quot;12&quot;, &quot;23&quot;, &quot;345&quot;, &quot;4567&quot;), 2).aggregate(&quot;&quot;)((x, y) =&gt; math.min(x.length, y.length).toString, (x, y) =&gt; x + y) println(r4) &#125; &#125; cartesian12345678910111213141516171819202122232425262728package spark.examples.rddapi import org.apache.spark.rdd.&#123;CartesianRDD, RDD&#125; import org.apache.spark.&#123;SparkContext, SparkConf&#125; object CartesianTest_01 &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AggregateTest_00&quot;) val sc = new SparkContext(conf); val z1 = sc.parallelize(List(2, 3, 4, 5, 6), 2) val z2 = sc.parallelize(List(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;, &quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;), 3) /** * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of * elements (a, b) where a is in `this` and b is in `other`. */ //def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = new CartesianRDD(sc, this, other) //z1 和 z2集合的元素类型可以不同，并且cartesian是个转换算子， //调用z.collect触发作业 val z = z1.cartesian(z2) println(&quot;Number of partitions: &quot; + z.partitions.length) //6 var count = 0 z.collect().foreach(x =&gt; &#123;println(x._1 + &quot;,&quot; + x._2); count = count + 1&#125;) // println(&quot;count =&quot; + count) //50 Repartition123456789101112131415161718192021222324package spark.examples.rddapi import org.apache.spark.&#123;SparkContext, SparkConf&#125; object RepartitionTest_04 &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;RepartitionTest_04&quot;) val sc = new SparkContext(conf); val z1 = sc.parallelize(List(3, 9, 18, 22, 11, 9, 8), 3) //z1.coalesce(5, true)的效果一样，开启shuffle /** * Return a new RDD that has exactly numPartitions partitions. * * Can increase or decrease the level of parallelism in this RDD. Internally, this uses * a shuffle to redistribute data. * * If you are decreasing the number of partitions in this RDD, consider using `coalesce`, * which can avoid performing a shuffle. */ val r1 = z1.repartition(5) r1.collect().foreach(println) &#125; &#125; coalesce123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package spark.examples.rddapi import org.apache.spark.&#123;SparkContext, SparkConf&#125; //coalesce：合并 object CoalesceTest_03 &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;CoalesceTest_03&quot;) val sc = new SparkContext(conf); val z = sc.parallelize(List(3, 9, 18, 22, 11, 9, 8), 3) /** * Return a new RDD that is reduced into `numPartitions` partitions. * * This results in a narrow dependency, e.g. if you go from 1000 partitions * to 100 partitions, there will not be a shuffle, instead each of the 100 * new partitions will claim 10 of the current partitions. * * However, if you&apos;re doing a drastic coalesce, e.g. to numPartitions = 1, * this may result in your computation taking place on fewer nodes than * you like (e.g. one node in the case of numPartitions = 1). To avoid this, * you can pass shuffle = true. This will add a shuffle step, but means the * current upstream partitions will be executed in parallel (per whatever * the current partitioning is). * * Note: With shuffle = true, you can actually coalesce to a larger number * of partitions. This is useful if you have a small number of partitions, * say 100, potentially with a few partitions being abnormally large. Calling * coalesce(1000, shuffle = true) will result in 1000 partitions with the * data distributed using a hash partitioner. */ //shuffle默认为false //将分区数由3变成2，大变小使用narrow dependency val zz = z.coalesce(2, false) println(&quot;Partitions length: &quot; + zz.partitions.length) //2 println(zz.collect()) //结果是[I@100498c？ zz.collect().foreach(println) //将分区数由3变成6，少变多必须使用shuffle=true //在单机上没有发现有问题 //在cluster环境下，为了保证新的分区分布到不同的节点，应该使用shuffle为true //也就是说，少变多也可以使用shuffle为false，但是达不到分区数据进行重新分布的目的 val z2 = z.coalesce(6, false) z2.collect().foreach(println) //分区扩大，同时设置shuffle为true val z3 = z.coalesce(6, true) z3.collect().foreach(println) &#125; &#125;","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"IP查找并插入数据到mysql","date":"2017-04-16T04:47:25.126Z","path":"2017/04/16/bigdata/spark/IP查找并插入数据到mysql/","text":"1.需求&emsp;根据网关日志,查询用户的地址, 并在此基础上统计所有的地址的用户数量 2.数据格式 用户上网的网关日志1234#下面是一条网关日志20090121000132095572000|125.213.100.123|show.51.com|/shoplist.php?phpfile=shoplist2.php&amp;style=1&amp;sex=137|Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Mozilla/4.0(Compatible Mozilla/4.0(Compatible-EmbeddedWB 14.59 http://bsalsa.com/ EmbeddedWB- 14.59 from: http://bsalsa.com/ )|http://show.51.com/main.php|#125.213.100.123就是出网的IP,根据该IP就可以知道对应的归属地 IP和所属地映射文件123456#(起始IP, 结束IP, 起始IP的十进制, 结束IP的十进制, 所属地, ....)1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.0753021.0.8.0|1.0.15.255|16779264|16781311|亚洲|中国|广东|广州||电信|440100|China|CN|113.280637|23.1251781.0.32.0|1.0.63.255|16785408|16793599|亚洲|中国|广东|广州||电信|440100|China|CN|113.280637|23.1251781.1.0.0|1.1.0.255|16842752|16843007|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.0753021.1.2.0|1.1.7.255|16843264|16844799|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302 3.代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109package cn.itcast.spark.day3import java.sql.&#123;Connection, Date, DriverManager, PreparedStatement&#125;import org.apache.spark.&#123;SparkConf, SparkContext&#125;/** * Created by root on 2016/5/18. */object IPLocation &#123; val data2MySQL = (iterator: Iterator[(String, Int)]) =&gt; &#123;//遍历每个分区中的iteration var conn: Connection = null var ps : PreparedStatement = null val sql = &quot;INSERT INTO location_info (location, counts, accesse_date) VALUES (?, ?, ?)&quot; try &#123; conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/bigdata&quot;, &quot;root&quot;, &quot;123456&quot;)//拿到一个连接 iterator.foreach(line =&gt; &#123;//遍历每个分区中的元素 ps = conn.prepareStatement(sql) ps.setString(1, line._1) ps.setInt(2, line._2) ps.setDate(3, new Date(System.currentTimeMillis())) ps.executeUpdate()//保存数据 &#125;) &#125; catch &#123; case e: Exception =&gt; println(&quot;Mysql Exception&quot;) &#125; finally &#123; if (ps != null) ps.close() if (conn != null) conn.close() &#125; &#125; /** * 将IP(127.0.0.1)转成long */ def ip2Long(ip: String): Long = &#123; val fragments = ip.split(&quot;[.]&quot;) var ipNum = 0L for (i &lt;- 0 until fragments.length)&#123; ipNum = fragments(i).toLong | ipNum &lt;&lt; 8L &#125; ipNum &#125; /* * 二分法查找 * */ def binarySearch(lines: Array[(String, String, String)], ip: Long) : Int = &#123; var low = 0 var high = lines.length - 1 while (low &lt;= high) &#123; val middle = (low + high) / 2 //lines(middle) 为(start_num_ip, end_num_ip, province) if ((ip &gt;= lines(middle)._1.toLong) &amp;&amp; (ip &lt;= lines(middle)._2.toLong)) return middle if (ip &lt; lines(middle)._1.toLong) high = middle - 1 else &#123; low = middle + 1 &#125; &#125; -1 &#125; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local[2]&quot;).setAppName(&quot;IpLocation&quot;) val sc = new SparkContext(conf) //所有的IP映射表 // 1.0.1.0|1.0.3.255|16777472|16778239|亚洲|中国|福建|福州||电信|350100|China|CN|119.306239|26.075302 val ipRulesRdd = sc.textFile(&quot;c://ip.txt&quot;).map(line =&gt;&#123; val fields = line.split(&quot;\\\\|&quot;)//需要转义,因为|是正则的关键字 val start_num = fields(2) //起始IP(long类型) val end_num = fields(3)//结束IP(long类型) val province = fields(6)//IP所属的省份 (start_num, end_num, province) &#125;) //全部的ip映射规则 val ipRulesArrary = ipRulesRdd.collect()//将iteration转成listbuffer //广播规则(将ip映射规则数据 发送到所有的worker) val ipRulesBroadcast = sc.broadcast(ipRulesArrary) //加载要处理的数据 val ipsRDD = sc.textFile(&quot;c://access_log&quot;).map(line =&gt; &#123; //数据格式: 20090121000132095572000|125.213.100.123|show.51.com|/shoplist.php?phpfile=shoplist2.php&amp;style=1&amp;sex=137|Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; Mozilla/4.0(Compatible Mozilla/4.0(Compatible-EmbeddedWB 14.59 http://bsalsa.com/ EmbeddedWB- 14.59 from: http://bsalsa.com/ )|http://show.51.com/main.php| val fields = line.split(&quot;\\\\|&quot;) fields(1)//取IP字段: 125.213.100.123 &#125;) val result = ipsRDD.map(ip =&gt; &#123; val ipNum = ip2Long(ip) //返回一个数组的下标 val index = binarySearch(ipRulesBroadcast.value, ipNum) //ipRulesBroadcast.value 是一个Array val info = ipRulesBroadcast.value(index)//取数组中的某一个下标的元素 //(ip的起始Num， ip的结束Num，省份名) info &#125;).map(t =&gt; (t._3, 1)).reduceByKey(_+_)//对相同的省份进行统计计数 //向MySQL写入数据 result.foreachPartition(data2MySQL(_))//每个分区拿到一个数据库连接 //println(result.collect().toBuffer) sc.stop() &#125;&#125; 需要注意的一些坑按照Java程序员使用JDBC的习惯，首先通过Class.forName(“com.mysql.jdbc.Driver “)注册MySQL的JDBC驱动，但是在Scala中却不需要这么做，这么做还出错，包ClassNotFoundExeception（但是com.mysql.jdbc.Driver明明在classpath上）","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"checkpoint","date":"2017-04-16T04:47:25.125Z","path":"2017/04/16/bigdata/spark/checkpoint/","text":"1.为什么要有checkpoint?因为在spark进行计算的时候,会有很多的中间结果,但是一旦中间某一步失败,那么又要重新从头开始计算,但是如果我们将中间的某一个计算的结果checkpoint下来,那么下次计算的时候,直接从checkpoint的点拿数据,那么将会大大提高计算的速度 2.checkpoint的源码说明123456789101112131415161718192021/** * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint * directory set with `SparkContext#setCheckpointDir` and all references to its parent * RDDs will be removed. This function must be called before any job has been * executed on this RDD. It is strongly recommended that this RDD is persisted in * memory, otherwise saving it on a file will require recomputation.直到一个action被调用,那么checkpoint才会被执行如果做了checkpoint,那么checkpoint之前的所有的lineage(rdd之间的依赖关系)将被移除强烈建议将checkpoint的rdd保存到内存中(cache),不然在进行checkpoint的时候,又要重新进行计算 */def checkpoint(): Unit = RDDCheckpointData.synchronized &#123; // NOTE: we use a global lock here due to complexities downstream with ensuring // children RDD partitions point to the correct parent partitions. In the future // we should revisit this consideration. if (context.checkpointDir.isEmpty) &#123; throw new SparkException(&quot;Checkpoint directory has not been set in the SparkContext&quot;) &#125; else if (checkpointData.isEmpty) &#123; checkpointData = Some(new ReliableRDDCheckpointData(this)) &#125;&#125; 实例代码&emsp;在进行checkpoint的时候,会进行重新的计算,然后将checkpoint的结果放到磁盘,但是如果我们在checkpoint之前就进行一次cache,那么checkpoint的时候需要的计算结果就直接从内存中拿到,然后在将数据保存到磁盘123456789101112131415package spark.examples.rddapi import org.apache.spark.&#123;SparkContext, SparkConf&#125; object CheckpointTest &#123; def main(args: Array[String]) &#123; val conf = new SparkConf().setMaster(&quot;local&quot;).setAppName(&quot;AggregateTest_00&quot;) val sc = new SparkContext(conf); val z = sc.parallelize(List(3, 6, 7, 9, 11)).cache() //将数据cache,然后在checkpoint的时候,直接从cache中的拿数据,不用从头开始计算 sc.setCheckpointDir(&quot;file:///d:/checkpoint&quot;) //这里指定的是本地文件系统,不建议使用,因为如果本地机器宕机了,其他机器将无法拿到数据进行恢复,实际的生产过程中使用的是hdfs z.checkpoint() println(&quot;length: &quot; + z.collect().length) //rdd存入目录 println(&quot;count: &quot; + z.count()) //5 &#125; &#125; checkpoint的文件保存路径123456789101112131415d:\\checkpoint&gt;tree /f 文件夹 PATH 列表 卷序列号为 EA23-0890 D:. └─9b0ca0d9-f7fb-46bb-84dc-097d95b9e7b8 └─rdd-0 .part-00000.crc part-00000 /*1. 运行过程中发现，checkpoint目录会自动创建，无需预创建2.程序运行结束后，checkpoint目录并没有删除，上面这些属于checkpoint目录下的目录和文件也没有删除，再次运行会产生新的目录*/","tags":[{"name":"spark","slug":"spark","permalink":"http://yoursite.com/tags/spark/"}]},{"title":"logstash安装及启动","date":"2017-04-16T04:47:25.122Z","path":"2017/04/16/bigdata/logstash/logstash安装及启动/","text":"架构 安装1234567891011121314#Download and unzip Losgstash下载地址: https://www.elastic.co/downloads/logstash#config file Create a file named &quot;logstash-simple.conf&quot; and save it in the same directory as Logstash.input &#123; stdin &#123; &#125; &#125;output &#123; elasticsearch &#123; hosts =&gt; [&quot;localhost:9200&quot;] &#125; stdout &#123; codec =&gt; rubydebug &#125;&#125;#运行bin/logstash -f logstash-simple.conf 配置文件的其他的例子 https://www.elastic.co/guide/en/logstash/current/config-examples.html 官方文档:https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html 中文网站:http://kibana.logstash.es/content/","tags":[{"name":"logstash","slug":"logstash","permalink":"http://yoursite.com/tags/logstash/"}]},{"title":"logstash几种配置示例","date":"2017-04-16T04:47:25.121Z","path":"2017/04/16/bigdata/logstash/logstash几种配置示例/","text":"下面是logstash的几种配置示例,更多的配置看官网 logstash采集数据到kafka的配置文件123456789101112131415161718input &#123; file &#123; path =&gt; &quot;/var/nginx_logs/*.log&quot; #采集路径 discover_interval =&gt; 5 #5s采集一次 start_position =&gt; &quot;beginning&quot; #刚开始从头开始采集 &#125;&#125; output &#123; kafka &#123; topic_id =&gt; &quot;accesslog&quot; #kafka的topic codec =&gt; plain &#123; format =&gt; &quot;%&#123;message&#125;&quot; charset =&gt; &quot;UTF-8&quot; &#125; bootstrap_servers =&gt; &quot;172.16.0.11:9092,172.16.0.12:9092,172.16.0.13:9092&quot; #指定kafka的broker 的地址 &#125;&#125; 采集数据从kafka到elasticsearch123456789101112131415161718192021222324252627282930313233343536373839input &#123; kafka &#123; type =&gt; &quot;level-one&quot; auto_offset_reset =&gt; &quot;smallest&quot;//从最小偏移读 codec =&gt; plain &#123; //文本 charset =&gt; &quot;GB2312&quot; &#125; group_id =&gt; &quot;es&quot;//groupID topic_id =&gt; &quot;itcast&quot; //topic zk_connect =&gt; &quot;172.16.0.11:2181,172.16.0.12:2181,172.16.0.13:2181&quot; //zk &#125;&#125;filter &#123; mutate &#123; split =&gt; &#123; &quot;message&quot; =&gt; &quot; &quot; &#125;//字段分割符,如果是\\t(制表符),那么需要手动输入一个制表符(所以此处你看到的空格其实是制表符) add_field =&gt; &#123; &quot;event_type&quot; =&gt; &quot;%&#123;message[3]&#125;&quot; &quot;current_map&quot; =&gt; &quot;%&#123;message[4]&#125;&quot; &quot;current_X&quot; =&gt; &quot;%&#123;message[5]&#125;&quot; &quot;current_y&quot; =&gt; &quot;%&#123;message[6]&#125;&quot; &quot;user&quot; =&gt; &quot;%&#123;message[7]&#125;&quot; &quot;item&quot; =&gt; &quot;%&#123;message[8]&#125;&quot; &quot;item_id&quot; =&gt; &quot;%&#123;message[9]&#125;&quot; &quot;current_time&quot; =&gt; &quot;%&#123;message[12]&#125;&quot; &#125; remove_field =&gt; [ &quot;message&quot; ] &#125; &#125;output &#123; elasticsearch &#123; index =&gt; &quot;level-one-%&#123;+YYYY.MM.dd&#125;&quot; codec =&gt; plain &#123; charset =&gt; &quot;GB2312&quot; &#125; hosts =&gt; [&quot;172.16.0.14:9200&quot;, &quot;172.16.0.15:9200&quot;, &quot;172.16.0.16:9200&quot;] &#125; &#125;","tags":[{"name":"logstash","slug":"logstash","permalink":"http://yoursite.com/tags/logstash/"}]},{"title":"kafka集群安装及常用shell命令","date":"2017-04-16T04:47:25.119Z","path":"2017/04/16/bigdata/kafka/kafka集群安装及常用shell命令/","text":"1.集群安装1234567891011121314151617181920212223242526#解压，创建软链接cd /home/hadoop/app/tar -zxvf kafka_2.11-0.9.0.1.tgzln -s kafka_2.11-0.9.0.1 kafkarm -rf kafka_2.11-0.9.0.1.tgz #删除安装文件，节省空间#修改配置文件cd kafka/config/vim server.propertiesbroker.id=1 #每个机器上不同，如（broker.id=2， broker.id=3）log.dirs=/export/servers/log/kafka #是kafka的日志文件目录，存放的是消息数据，以主题命名的分区zookeeper.connect=zk03:2181,zk02:2181,zk01:2181##################################################分发到其他机器mkdir /export/servers/log/kafka -pscp -rp ./kafka hdp-node-02:/home/hadoop/app/scp -rp ./kafka hdp-node-03:/home/hadoop/app/#启动#依次在各节点上启动kafkabin/kafka-server-start.sh config/server.properties 2.Kafka常用操作命令 查看当前服务器中的所有topicbin/kafka-topics.sh –list –zookeeper zk01:2181 创建topic./kafka-topics.sh –create –zookeeper zk01:2181 –replication-factor 1 –partitions 3 –topic first 删除topicsh bin/kafka-topics.sh –delete –zookeeper zk01:2181 –topic test需要server.properties中设置delete.topic.enable=true否则只是标记删除或者直接重启。 通过shell命令发送消息kafka-console-producer.sh –broker-list kafka01:9092 –topic itheima 通过shell消费消息sh bin/kafka-console-consumer.sh –zookeeper zk01:2181 –from-beginning –topic test1写–from-beginning会显示历史消息，如果只想显示最新的可以不写 查看消费位置sh kafka-run-class.sh kafka.tools.ConsumerOffsetChecker –zookeeper zk01:2181 –group testGroup 查看某个Topic的详情sh kafka-topics.sh –topic test –describe –zookeeper zk01:2181 停止服务./kafka-server-stop.sh 3.报错和解决12345678910[2015-06-16 11:24:13,015] ERROR Failed to send requests for topics mykafka with correlation ids in [0,8] (kafka.producer.async.DefaultEventHandler) [2015-06-16 11:24:13,015] ERROR Error in handling batch of 1 events (kafka.producer.async.ProducerSendThread) kafka.common.FailedToSendMessageException: Failed to send messages after 3 tries. at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:90) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) at scala.collection.immutable.Stream.foreach(Stream.scala:594) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45) 这个错误是config/server.properties的host.name写的不对，可能是前面的“#”没有去掉或是写的主机名称，改成服务器ip地址就可以了，如果改成localhost单机模式不会有问题，但分布式的时候会报下面错误。1234567891011121314151617181920212223242526272829303132333435363738394041[2015-06-16 14:20:59,519] WARN Fetching topic metadata with correlation id 9 for topics [Set(mykafka)] from broker [id:0,host:192.168.10.114,port:9092] failed (kafka.client.ClientUtils$) java.nio.channels.ClosedChannelException at kafka.network.BlockingChannel.send(BlockingChannel.scala:100) at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73) at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:72) at kafka.producer.SyncProducer.send(SyncProducer.scala:113) at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:58) at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82) at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67) at kafka.utils.Utils$.swallow(Utils.scala:172) at kafka.utils.Logging$class.swallowError(Logging.scala:106) at kafka.utils.Utils$.swallowError(Utils.scala:45) at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) at scala.collection.immutable.Stream.foreach(Stream.scala:594) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45) [2015-06-16 14:20:59,520] ERROR fetching topic metadata for topics [Set(mykafka)] from broker [ArrayBuffer(id:0,host:192.168.10.114,port:9092)] failed (kafka.utils.Utils$) kafka.common.KafkaException: fetching topic metadata for topics [Set(mykafka)] from broker [ArrayBuffer(id:0,host:192.168.10.114,port:9092)] failed at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:72) at kafka.producer.BrokerPartitionInfo.updateInfo(BrokerPartitionInfo.scala:82) at kafka.producer.async.DefaultEventHandler$$anonfun$handle$1.apply$mcV$sp(DefaultEventHandler.scala:67) at kafka.utils.Utils$.swallow(Utils.scala:172) at kafka.utils.Logging$class.swallowError(Logging.scala:106) at kafka.utils.Utils$.swallowError(Utils.scala:45) at kafka.producer.async.DefaultEventHandler.handle(DefaultEventHandler.scala:67) at kafka.producer.async.ProducerSendThread.tryToHandle(ProducerSendThread.scala:105) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:88) at kafka.producer.async.ProducerSendThread$$anonfun$processEvents$3.apply(ProducerSendThread.scala:68) at scala.collection.immutable.Stream.foreach(Stream.scala:594) at kafka.producer.async.ProducerSendThread.processEvents(ProducerSendThread.scala:67) at kafka.producer.async.ProducerSendThread.run(ProducerSendThread.scala:45) Caused by: java.nio.channels.ClosedChannelException at kafka.network.BlockingChannel.send(BlockingChannel.scala:100) at kafka.producer.SyncProducer.liftedTree1$1(SyncProducer.scala:73) at kafka.producer.SyncProducer.kafka$producer$SyncProducer$$doSend(SyncProducer.scala:72) at kafka.producer.SyncProducer.send(SyncProducer.scala:113) at kafka.client.ClientUtils$.fetchTopicMetadata(ClientUtils.scala:58) ... 12 more","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"Kafka配置文件说明图示","date":"2017-04-16T04:47:25.117Z","path":"2017/04/16/bigdata/kafka/Kafka配置文件说明图示/","text":"","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"kafka简介","date":"2017-04-16T04:47:25.116Z","path":"2017/04/16/bigdata/kafka/kafka简介以及原理详解/","text":"Kafka是什么在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算。如: KAFKA + STORM +REDIS Apache Kafka是一个开源消息系统，由Scala写成。是由Apache软件基金会开发的一个开源消息系统项目。 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer,此外kafka集群有多个kafka实例组成，每个实例(server)成为broker。 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性 JMS是什么JMS的基础JMS是什么：JMS是Java提供的一套技术规范JMS干什么用：用来异构系统 集成通信，缓解系统瓶颈，提高系统的伸缩性增强系统用户体验，使得系统模块化和组件化变得可行并更加灵活通过什么方式：生产消费者模式（生产者、服务器、消费者） jdk，kafka，activemq…… JMS消息传输模型 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。 发布/订阅模式（一对多，数据生产后，推送给所有订阅者）发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即时当前订阅者不可用，处于离线状态。 queue.put（object） 数据生产queue.take(object) 数据消费 JMS核心组件 Destination：消息发送的目的地，也就是前面说的Queue和Topic。 Message:从字面上就可以看出是被发送的消息 StreamMessage：Java 数据流消息，用标准流操作来顺序的填充和读取。 MapMessage：一个Map类型的消息；名称为 string 类型，而值为 Java 的基本类型。 TextMessage：普通字符串消息，包含一个String。 ObjectMessage：对象消息，包含一个可序列化的Java 对象 BytesMessage：二进制数组消息，包含一个byte[]。 XMLMessage: 一个XML类型的消息。最常用的是TextMessage和ObjectMessage。 Producer： 消息的生产者，要发送一个消息，必须通过这个生产者来发送 MessageConsumer： 与生产者相对应，这是消息的消费者或接收者，通过它来接收一个消息 为什么需要消息队列消息系统的核心作用就是三点：解耦，异步和并行 以用户注册的案列来说明消息系统的作用: 用户注册的一般流程问题：随着后端流程越来越多，每步流程都需要额外的耗费很多时间，从而会导致用户更长的等待延迟。 用户注册的并行执行 问题：系统并行的发起了4个请求，4个请求中，如果某一个环节执行1分钟，其他环节再快，用户也需要等待1分钟。如果其中一个环节异常之后，整个服务挂掉了。 用户注册的最终一致1、保证主流程的正常执行、执行成功之后，发送MQ消息出去。2、需要这个destination的其他系统通过消费数据再执行，最终一致。 Kafka核心组件 Topic ：消息根据Topic进行归类 Producer：发送消息者 Consumer：消息接受者 broker：每个kafka实例(server) Zookeeper：依赖集群保存meta信息","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"kafka的配置之roker(server)、producter、consumer三个不同的配置详解","date":"2017-04-16T04:47:25.115Z","path":"2017/04/16/bigdata/kafka/kafka的配置之roker(server)、producter、consumer三个不同的配置详解/","text":"kafka的配置分为 broker、producter、consumer三个不同的配置,分别对应: config/server.properties config/producer.properties config/consumer.properties 1.BROKER(server)的全局配置最为核心的三个配置 broker.id、log.dir、zookeeper.connect 。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212#------------------------------------------- 系统相关 -------------------------------------------##每一个broker在集群中的唯一标示，要求是正数。在改变IP地址，不改变broker.id的话不会影响consumersbroker.id = 1 ##kafka数据的存放地址，多个地址的话用逗号分割 /tmp/kafka-logs-1，/tmp/kafka-logs-2log.dirs = /tmp/kafka-logs ##提供给客户端响应的端口port = 6667 ##消息体的最大大小，单位是字节message.max.bytes = 1000000 ## broker 处理消息的最大线程数，一般情况下不需要去修改num.network.threads = 3 ## broker处理磁盘IO 的线程数，数值应该大于你的硬盘数num.io.threads = 8 ## 一些后台任务处理的线程数，例如过期消息文件的删除等，一般情况下不需要去做修改background.threads = 4 ## 等待IO线程处理的请求队列最大数，若是等待IO的请求超过这个数值，那么会停止接受外部消息，算是一种自我保护机制queued.max.requests = 500 ##broker的主机地址，若是设置了，那么会绑定到这个地址上，若是没有，会绑定到所有的接口上，并将其中之一发送到ZK，一般不设置host.name ## 打广告的地址，若是设置的话，会提供给producers, consumers,其他broker连接，具体如何使用还未深究advertised.host.name ## 广告地址端口，必须不同于port中的设置advertised.port ## socket的发送缓冲区，socket的调优参数SO_SNDBUFFsocket.send.buffer.bytes = 100 * 1024 ## socket的接受缓冲区，socket的调优参数SO_RCVBUFFsocket.receive.buffer.bytes = 100 * 1024 ## socket请求的最大数值，防止serverOOM，message.max.bytes必然要小于socket.request.max.bytes，会被topic创建时的指定参数覆盖socket.request.max.bytes = 100 * 1024 * 1024 ------------------------------------------- LOG 相关 -------------------------------------------## topic的分区是以一堆segment文件存储的，这个控制每个segment的大小，会被topic创建时的指定参数覆盖log.segment.bytes = 1024 * 1024 * 1024 ## 这个参数会在日志segment没有达到log.segment.bytes设置的大小，也会强制新建一个segment 会被 topic创建时的指定参数覆盖log.roll.hours = 24*7 ## 日志清理策略选择有：delete和compact 主要针对过期数据的处理，或是日志文件达到限制的额度，会被 topic创建时的指定参数覆盖log.cleanup.policy = delete ## 数据存储的最大时间超过这个时间会根据log.cleanup.policy设置的策略处理数据，也就是消费端能够多久去消费数据## log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖log.retention.minutes=7 days ## topic每个分区的最大文件大小，一个topic的大小限制 = 分区数*log.retention.bytes 。-1 没有大小限制## log.retention.bytes和log.retention.minutes任意一个达到要求，都会执行删除，会被topic创建时的指定参数覆盖log.retention.bytes=-1 ## 文件大小检查的周期时间，是否处罚 log.cleanup.policy中设置的策略log.retention.check.interval.ms=5 minutes ## 是否开启日志压缩log.cleaner.enable=false ## 日志压缩运行的线程数log.cleaner.threads =1 ## 日志压缩时候处理的最大大小log.cleaner.io.max.bytes.per.second=None ## 日志压缩去重时候的缓存空间，在空间允许的情况下，越大越好log.cleaner.dedupe.buffer.size=500*1024*1024 ## 日志清理时候用到的IO块大小一般不需要修改log.cleaner.io.buffer.size=512*1024 ## 日志清理中hash表的扩大因子一般不需要修改log.cleaner.io.buffer.load.factor = 0.9 ## 检查是否处罚日志清理的间隔log.cleaner.backoff.ms =15000 ## 日志清理的频率控制，越大意味着更高效的清理，同时会存在一些空间上的浪费，会被topic创建时的指定参数覆盖log.cleaner.min.cleanable.ratio=0.5 ## 对于压缩的日志保留的最长时间，也是客户端消费消息的最长时间，同log.retention.minutes的区别在于一个控制未压缩数据，一个控制压缩后的数据。会被topic创建时的指定参数覆盖log.cleaner.delete.retention.ms = 1 day ## 对于segment日志的索引文件大小限制，会被topic创建时的指定参数覆盖log.index.size.max.bytes = 10 * 1024 * 1024 ## 当执行一个fetch操作后，需要一定的空间来扫描最近的offset大小，设置越大，代表扫描速度越快，但是也更好内存，一般情况下不需要搭理这个参数log.index.interval.bytes = 4096 ## log文件&quot;sync&quot;到磁盘之前累积的消息条数## 因为磁盘IO操作是一个慢操作,但又是一个&quot;数据可靠性&quot;的必要手段## 所以此参数的设置,需要在&quot;数据可靠性&quot;与&quot;性能&quot;之间做必要的权衡.## 如果此值过大,将会导致每次&quot;fsync&quot;的时间较长(IO阻塞)## 如果此值过小,将会导致&quot;fsync&quot;的次数较多,这也意味着整体的client请求有一定的延迟.## 物理server故障,将会导致没有fsync的消息丢失.log.flush.interval.messages=None ## 检查是否需要固化到硬盘的时间间隔log.flush.scheduler.interval.ms = 3000 ## 仅仅通过interval来控制消息的磁盘写入时机,是不足的.## 此参数用于控制&quot;fsync&quot;的时间间隔,如果消息量始终没有达到阀值,但是离上一次磁盘同步的时间间隔## 达到阀值,也将触发.log.flush.interval.ms = None ## 文件在索引中清除后保留的时间一般不需要去修改log.delete.delay.ms = 60000 ## 控制上次固化硬盘的时间点，以便于数据恢复一般不需要去修改log.flush.offset.checkpoint.interval.ms =60000 ------------------------------------------- TOPIC 相关 -------------------------------------------## 是否允许自动创建topic ，若是false，就需要通过命令创建topicauto.create.topics.enable =true ## 一个topic ，默认分区的replication个数，不得大于集群中broker的个数default.replication.factor =1 ## 每个topic的分区个数，若是在topic创建时候没有指定的话会被topic创建时的指定参数覆盖num.partitions = 1 实例 --replication-factor 3 --partitions 1 --topic replicated-topic ：名称replicated-topic有一个分区，分区被复制到三个broker上。 ------------------------------------------- 复制(Leader、replicas) 相关 -------------------------------------------## partition leader与replicas之间通讯时,socket的超时时间controller.socket.timeout.ms = 30000 ## partition leader与replicas数据同步时,消息的队列尺寸controller.message.queue.size=10 ## replicas响应partition leader的最长等待时间，若是超过这个时间，就将replicas列入ISR(in-sync replicas)，并认为它是死的，不会再加入管理中replica.lag.time.max.ms = 10000 ## 如果follower落后与leader太多,将会认为此follower[或者说partition relicas]已经失效## 通常,在follower与leader通讯时,因为网络延迟或者链接断开,总会导致replicas中消息同步滞后## 如果消息之后太多,leader将认为此follower网络延迟较大或者消息吞吐能力有限,将会把此replicas迁移## 到其他follower中.## 在broker数量较少,或者网络不足的环境中,建议提高此值.replica.lag.max.messages = 4000 ##follower与leader之间的socket超时时间replica.socket.timeout.ms= 30 * 1000 ## leader复制时候的socket缓存大小replica.socket.receive.buffer.bytes=64 * 1024 ## replicas每次获取数据的最大大小replica.fetch.max.bytes = 1024 * 1024 ## replicas同leader之间通信的最大等待时间，失败了会重试replica.fetch.wait.max.ms = 500 ## fetch的最小数据尺寸,如果leader中尚未同步的数据不足此值,将会阻塞,直到满足条件replica.fetch.min.bytes =1 ## leader 进行复制的线程数，增大这个数值会增加follower的IOnum.replica.fetchers=1 ## 每个replica检查是否将最高水位进行固化的频率replica.high.watermark.checkpoint.interval.ms = 5000 ## 是否允许控制器关闭broker ,若是设置为true,会关闭所有在这个broker上的leader，并转移到其他brokercontrolled.shutdown.enable = false ## 控制器关闭的尝试次数controlled.shutdown.max.retries = 3 ## 每次关闭尝试的时间间隔controlled.shutdown.retry.backoff.ms = 5000 ## 是否自动平衡broker之间的分配策略auto.leader.rebalance.enable = false ## leader的不平衡比例，若是超过这个数值，会对分区进行重新的平衡leader.imbalance.per.broker.percentage = 10 ## 检查leader是否不平衡的时间间隔leader.imbalance.check.interval.seconds = 300 ## 客户端保留offset信息的最大空间大小offset.metadata.max.bytes ------------------------------------------- ZooKeeper 相关 -------------------------------------------##zookeeper集群的地址，可以是多个，多个之间用逗号分割 hostname1:port1,hostname2:port2,hostname3:port3zookeeper.connect = localhost:2181 ## ZooKeeper的最大超时时间，就是心跳的间隔，若是没有反映，那么认为已经死了，不易过大zookeeper.session.timeout.ms=6000 ## ZooKeeper的连接超时时间zookeeper.connection.timeout.ms = 6000 ## ZooKeeper集群中leader和follower之间的同步实际那zookeeper.sync.time.ms = 2000配置的修改其中一部分配置是可以被每个topic自身的配置所代替，例如新增配置bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic my-topic --partitions 1 --replication-factor 1 --config max.message.bytes=64000 --config flush.messages=1 修改配置bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --config max.message.bytes=128000 删除配置：bin/kafka-topics.sh --zookeeper localhost:2181 --alter --topic my-topic --deleteConfig max.message.bytes 2.CONSUMER 配置最为核心的配置是group.id、zookeeper.connect12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364## Consumer归属的组ID，broker是根据group.id来判断是队列模式还是发布订阅模式，非常重要 group.id ## 消费者的ID，若是没有设置的话，会自增 consumer.id ## 一个用于跟踪调查的ID ，最好同group.id相同 client.id = group id value ## 对于zookeeper集群的指定，可以是多个 hostname1:port1,hostname2:port2,hostname3:port3 必须和broker使用同样的zk配置 zookeeper.connect=localhost:2182 ## zookeeper的心跳超时时间，查过这个时间就认为是dead消费者 zookeeper.session.timeout.ms = 6000 ## zookeeper的等待连接时间 zookeeper.connection.timeout.ms = 6000 ##指定多久消费者更新offset到zookeeper中，注意offset更新时基于time而不是每次获得消息就立即更新zookeeper##一旦在更新zookeeper发生异常并重启，将可能拿到已经拿到过的消息 zookeeper.sync.time.ms = 2000 ## 当zookeeper中没有初始的offset时候的处理方式。smallest ：重置为最小值 largest:重置为最大值 anything else：抛出异常 auto.offset.reset = largest ## socket的超时时间，实际的超时时间是：max.fetch.wait + socket.timeout.ms. socket.timeout.ms= 30 * 1000 ## socket的接受缓存空间大小 socket.receive.buffer.bytes=64 * 1024 ## 是否在消费消息后将offset同步到zookeeper，当Consumer失败后就能从zookeeper获取最新的offset auto.commit.enable = true ## 自动提交的时间间隔 auto.commit.interval.ms = 60 * 1000##从每个分区获取的消息大小限制（也就是每个chunk的大小） fetch.message.max.bytes = 1024 * 1024 ## 用来处理消费消息的块，每个块可以等同于fetch.message.max.bytes中数值 queued.max.message.chunks = 10 ## 当有新的consumer加入到group时,将会reblance,此后将会有partitions的消费端迁移到新## 的consumer上,如果一个consumer获得了某个partition的消费权限,那么它将会向zk注册## &quot;Partition Owner registry&quot;节点信息,但是有可能此时旧的consumer尚没有释放此节点,## 此值用于控制,注册节点的重试次数. rebalance.max.retries = 4 ## 每次再平衡的时间间隔 rebalance.backoff.ms = 2000 ## 每次重新选举leader的时间 refresh.leader.backoff.ms ## server发送到消费端的最小数据，若是不满足这个数值则会等待，知道满足数值要求 fetch.min.bytes = 1 ## 若是不满足最小大小(fetch.min.bytes)的话，等待消费端请求的最长等待时间 fetch.wait.max.ms = 100 ## 指定时间内没有消息到达就抛出异常，一般不需要改 consumer.timeout.ms = -1 3.PRODUCER 的配置比较核心的配置：metadata.broker.list、request.required.acks、producer.type、serializer.class123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566## 消费者获取消息元信息(topics, partitions and replicas)的地址,配置格式是：host1:port1,host2:port2，也可以在外面设置一个vip metadata.broker.list ##消息的确认模式（生产者发送消息到partition，partition是否确认（ack）） ## 0：不保证消息的到达确认，只管发送，低延迟但是会出现消息的丢失，在某个server失败的情况下，有点像TCP ## 1：发送消息，并会等待leader 收到消息之后发送ack ## -1：发送消息，当所有的follower都同步消息成功后发送ack（follower是partition的副本） request.required.acks = 0 ## 消息发送的最长等待时间 request.timeout.ms = 10000 ## socket的缓存大小 send.buffer.bytes=100*1024 ## key的序列化方式，若是没有设置，同serializer.class key.serializer.class ## 分区的策略，默认是取模，表示通过key.hash%num(partition)到对应的分区，可以指定我们自定义的class来分区 partitioner.class=kafka.producer.DefaultPartitioner ## 消息的压缩模式，默认是none，可以有gzip和snappy compression.codec = none ## 可以针对默写特定的topic进行压缩 compressed.topics=null ## 消息发送失败后的重试次数 message.send.max.retries = 3 ## 每次失败后的间隔时间 retry.backoff.ms = 100 ## 生产者定时更新topic元信息的时间间隔，若是设置为0，那么会在每个消息发送后都去更新数据#producer刷新topic meta的时间间隔，producer需要知道partition leader的位置，以及当前topic的情况#因此producer需要一个机制来获取最新的metadata，当producer遇到特定错误时，将会立即刷新#（比如topic失效，partition丢失，leader失效等），此外也可以通过此参数来配置额外的刷新机制，默认是600000 topic.metadata.refresh.interval.ms = 600 * 1000 ## 用户随意指定，但是不能重复，主要用于跟踪记录消息 client.id=&quot;&quot; ------------------------------------------- 消息模式相关 ------------------------------------------- ## 生产者的类型 async:异步执行消息的发送(数据缓存在buff中，达到一定的时间或者是数据量时才发送)； sync：同步执行消息的发送（数据产生就发送） producer.type=sync ## 异步模式下，那么就会在设置的时间缓存消息，并一次性发送 queue.buffering.max.ms = 5000 ## 异步的模式下最长等待的消息数 queue.buffering.max.messages = 10000 #当消息在produce端沉积的条数达到&quot;queue.buffering.max.messages&quot;后#阻塞一定时间后，队列仍然没有发送出任何消息#此时produce可以继续阻塞或者将消息抛弃，此timeout值用于控制“阻塞”的时间#-1：无阻塞超时限制，消息不会被抛弃#0：立即清空队列，消息被抛弃 queue.enqueue.timeout.ms = -1 ## 异步模式下，每次发送的最大消息数，前提是触发了queue.buffering.max.messages或是queue.buffering.max.ms的限制 batch.num.messages=200 ## 消息体的系列化处理类，转化为字节流进行传输 serializer.class = kafka.serializer.DefaultEncoder更多的配置可见 kafka.producer.ProducerConfig","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"Kafka生产者消费者API举例2","date":"2017-04-16T04:47:25.113Z","path":"2017/04/16/bigdata/kafka/Kafka生产者消费者API举例2/","text":"下面是kafka生产者,消费者和自定义分区的代码示例 1.生产者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package cn.itcast.storm.kafka.simple;import kafka.javaapi.producer.Producer;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;import java.util.Properties;import java.util.UUID;/** * 这是一个简单的Kafka producer代码 * 包含两个功能: * 1、数据发送 * 2、数据按照自定义的partition策略进行发送 * * * KafkaSpout的类 */public class KafkaProducerSimple &#123; public static void main(String[] args) &#123; /** * 1、指定当前kafka producer生产的数据的目的地 * 创建topic可以输入以下命令，在kafka集群的任一节点进行创建。 * bin/kafka-topics.sh --create --zookeeper zk01:2181 --replication-factor 1 --partitions 1 --topic test */ String TOPIC = &quot;orderMq&quot;; /** * 2、读取配置文件 */ Properties props = new Properties(); /* * key.serializer.class默认为serializer.class */ props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); /* * kafka broker对应的主机，格式为host1:port1,host2:port2，这样生产者知道向哪里生产 */ props.put(&quot;metadata.broker.list&quot;, &quot;kafka01:9092,kafka02:9092,kafka03:9092&quot;); /* * request.required.acks,设置发送数据是否需要服务端的反馈,有三个值0,1,-1 * 0，意味着producer永远不会等待一个来自broker的ack，这就是0.7版本的行为。 * 这个选项提供了最低的延迟，但是持久化的保证是最弱的，当server挂掉的时候会丢失一些数据。 * 1，意味着在leader replica已经接收到数据后，producer会得到一个ack。 * 这个选项提供了更好的持久性，因为在server确认请求成功处理后，client才会返回。 * 如果刚写到leader上，还没来得及复制leader就挂了，那么消息才可能会丢失。 * -1，意味着在所有的ISR都接收到数据后，producer才得到一个ack。 * 这个选项提供了最好的持久性，只要还有一个replica存活，那么数据就不会丢失 */ props.put(&quot;request.required.acks&quot;, &quot;1&quot;); /* * 可选配置，如果不配置，则使用默认的partitioner partitioner.class * 默认值：kafka.producer.DefaultPartitioner * 用来把消息分到各个partition中，默认行为是对key进行hash。 */ props.put(&quot;partitioner.class&quot;, &quot;cn.itcast.storm.kafka.MyLogPartitioner&quot;);// props.put(&quot;partitioner.class&quot;, &quot;kafka.producer.DefaultPartitioner&quot;); /** * 3、通过配置文件，创建生产者 */ Producer&lt;String, String&gt; producer = new Producer&lt;String, String&gt;(new ProducerConfig(props)); /** * 4、通过for循环生产数据 */ for (int messageNo = 1; messageNo &lt; 100000; messageNo++) &#123; String messageStr = new String(messageNo + &quot; 用来配合自定义的MyLogPartitioner进行数据分发&quot;); /** * 5、调用producer的send方法发送数据 * 注意：这里需要指定 partitionKey，用来配合自定义的MyLogPartitioner进行数据分发，自定义partition拿这个partitionKey来hash取模，参见下面的 “自定义分区类” */ producer.send(new KeyedMessage&lt;String, String&gt;(TOPIC, messageNo + &quot;&quot;, &quot;appid&quot; + UUID.randomUUID() + &quot;itcast&quot;)); &#125; &#125;&#125; 2.消费者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 package cn.itcast.storm.kafka.simple;import kafka.consumer.Consumer;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.javaapi.consumer.ConsumerConnector;import kafka.message.MessageAndMetadata;import java.util.HashMap;import java.util.List;import java.util.Map;import java.util.Properties;import java.util.concurrent.ExecutorService;import java.util.concurrent.Executors;public class KafkaConsumerSimple implements Runnable &#123; public String title; public KafkaStream&lt;byte[], byte[]&gt; stream; public KafkaConsumerSimple(String title, KafkaStream&lt;byte[], byte[]&gt; stream) &#123; this.title = title; this.stream = stream; &#125; @Override public void run() &#123; System.out.println(&quot;开始运行 &quot; + title); ConsumerIterator&lt;byte[], byte[]&gt; it = stream.iterator(); /** * 不停地从stream读取新到来的消息，在等待新的消息时，hasNext()会阻塞 * 如果调用 `ConsumerConnector#shutdown`，那么`hasNext`会返回false * */ while (it.hasNext()) &#123; MessageAndMetadata&lt;byte[], byte[]&gt; data = it.next(); String topic = data.topic(); int partition = data.partition(); long offset = data.offset(); String msg = new String(data.message()); System.out.println(String.format( &quot;Consumer: [%s], Topic: [%s], PartitionId: [%d], Offset: [%d], msg: [%s]&quot;, title, topic, partition, offset, msg)); &#125; System.out.println(String.format(&quot;Consumer: [%s] exiting ...&quot;, title)); &#125; public static void main(String[] args) throws Exception&#123; Properties props = new Properties(); //组名 props.put(&quot;group.id&quot;, &quot;dashujujiagoushi&quot;); props.put(&quot;zookeeper.connect&quot;, &quot;zk01:2181,zk02:2181,zk03:2181&quot;); props.put(&quot;auto.offset.reset&quot;, &quot;largest&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); props.put(&quot;partition.assignment.strategy&quot;, &quot;roundrobin&quot;); ConsumerConfig config = new ConsumerConfig(props); String topic1 = &quot;orderMq&quot;; String topic2 = &quot;paymentMq&quot;; //只要ConsumerConnector还在的话，consumer会一直等待新消息，不会自己退出 ConsumerConnector consumerConn = Consumer.createJavaConsumerConnector(config); //定义一个map Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;&gt;(); topicCountMap.put(topic1, 3);//指定在topic上创建多少个KafkaStream，而每个KafkaStream对应一个partition //Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; 中String是topic， List&lt;KafkaStream&lt;byte[], byte[]&gt;是对应的流 Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; topicStreamsMap = consumerConn.createMessageStreams(topicCountMap); //取出 名字为 `orderMq`的topic 对应的分区 List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt; streams = topicStreamsMap.get(topic1); //创建一个容量为4的线程池 ExecutorService executor = Executors.newFixedThreadPool(3); //创建consumer threads for (int i = 0; i &lt; streams.size(); i++) executor.execute(new KafkaConsumerSimple(&quot;消费者&quot; + (i + 1), streams.get(i)));//streams.get(i)表示每个分区对应的流 &#125;&#125; 3.自定义分区类12345678910111213141516171819package cn.itcast.storm.kafka;import kafka.producer.Partitioner;import kafka.utils.VerifiableProperties;import org.apache.log4j.Logger;public class MyLogPartitioner implements Partitioner &#123; private static Logger logger = Logger.getLogger(MyLogPartitioner.class); public MyLogPartitioner(VerifiableProperties props) &#123; &#125; public int partition(Object obj, int numPartitions) &#123; return Integer.parseInt(obj.toString())%numPartitions;//hash取模之后，就可以确定消息发送到哪一个分区// return 1; &#125;&#125;","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"Kafka-生产者消费者API举例1(转)","date":"2017-04-16T04:47:25.112Z","path":"2017/04/16/bigdata/kafka/Kafka生产者消费者API举例1(转)/","text":"转自 接口 KafkaProperties.java12345678910111213public interface KafkaProperties &#123; final static String zkConnect = &quot;192.168.1.160:2181&quot;; final static String groupId = &quot;group1&quot;; final static String topic = &quot;topic1&quot;; // final static String kafkaServerURL = &quot;192.168.1.160&quot;; // final static int kafkaServerPort = 9092; // final static int kafkaProducerBufferSize = 64 * 1024; // final static int connectionTimeOut = 20000; // final static int reconnectInterval = 10000; // final static String topic2 = &quot;topic2&quot;; // final static String topic3 = &quot;topic3&quot;; // final static String clientId = &quot;SimpleConsumerDemoClient&quot;;&#125; 生产者 KafkaProducer.java 12345678910111213141516171819202122232425262728293031323334import java.util.Properties;import kafka.producer.KeyedMessage;import kafka.producer.ProducerConfig;public class KafkaProducer extends Thread &#123; private final kafka.javaapi.producer.Producer&lt;Integer, String&gt; producer; private final String topic; private final Properties props = new Properties(); public KafkaProducer(String topic) &#123; props.put(&quot;serializer.class&quot;, &quot;kafka.serializer.StringEncoder&quot;); props.put(&quot;metadata.broker.list&quot;, &quot;192.168.1.160:9092&quot;); // 配置kafka端口 producer = new kafka.javaapi.producer.Producer&lt;Integer, String&gt;(new ProducerConfig(props)); this.topic = topic; &#125; @Override public void run() &#123; int messageNo = 1; while (true) &#123; String messageStr = new String(&quot;This is a message, number: &quot; + messageNo); System.out.println(&quot;Send:&quot; + messageStr); producer.send(new KeyedMessage&lt;Integer, String&gt;(topic, messageStr)); messageNo++; try &#123; sleep(1000); &#125; catch (InterruptedException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;&#125; 消费者 KafkaConsumer.java 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.Properties;import kafka.consumer.ConsumerConfig;import kafka.consumer.ConsumerIterator;import kafka.consumer.KafkaStream;import kafka.javaapi.consumer.ConsumerConnector;public class KafkaConsumer extends Thread &#123; private final ConsumerConnector consumer; private final String topic; public KafkaConsumer(String topic) &#123; consumer = kafka.consumer.Consumer.createJavaConsumerConnector(createConsumerConfig()); this.topic = topic; &#125; private static ConsumerConfig createConsumerConfig() &#123; Properties props = new Properties(); props.put(&quot;zookeeper.connect&quot;, KafkaProperties.zkConnect); // zookeeper的地址 props.put(&quot;group.id&quot;, KafkaProperties.groupId); // 组ID //zk连接超时 props.put(&quot;zookeeper.session.timeout.ms&quot;, &quot;40000&quot;); props.put(&quot;zookeeper.sync.time.ms&quot;, &quot;200&quot;); props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;); return new ConsumerConfig(props); &#125; @Override public void run() &#123; Map&lt;String, Integer&gt; topicCountMap = new HashMap&lt;String, Integer&gt;(); topicCountMap.put(topic, new Integer(1)); Map&lt;String, List&lt;KafkaStream&lt;byte[], byte[]&gt;&gt;&gt; consumerMap = consumer.createMessageStreams(topicCountMap); KafkaStream&lt;byte[], byte[]&gt; stream = consumerMap.get(topic).get(0); ConsumerIterator&lt;byte[], byte[]&gt; it = stream.iterator(); while (it.hasNext()) &#123; System.out.println(&quot;receive：&quot; + new String(it.next().message())); try &#123; sleep(1000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 执行函数 KafkaConsumerProducerDemo.java 123456789public class KafkaConsumerProducerDemo &#123; public static void main(String[] args) &#123; KafkaProducer producerThread = new KafkaProducer(KafkaProperties.topic); producerThread.start(); KafkaConsumer consumerThread = new KafkaConsumer(KafkaProperties.topic); consumerThread.start(); &#125;&#125;","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"kafka文件存储机制","date":"2017-04-16T04:47:25.111Z","path":"2017/04/16/bigdata/kafka/kafka文件存储机制/","text":"Kafka文件存储基本结构 在Kafka文件存储中，同一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1。 1234567891011121314#日志文件在config/server.properties配置文件中，可以配置日志目录，如下：log.dirs=/export/servers/log/kafka #目录结构如下：[root@hdp-node-01 first-2]# tree /export/servers/log/kafka//export/servers/log/kafka/|-- cleaner-offset-checkpoint|-- first-2 #first是我创建的topic的名字，first-2是这个主题的第3个分区（从0开始），该分区下面有inde、log文件，启动log文件章存放的是消息队列的消息| |-- 00000000000000000000.index| |-- 00000000000000000000.log|-- meta.properties|-- recovery-point-offset-checkpoint|-- replication-offset-checkpoint 每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等（默认大小都是1G），这种特性方便old segment file快速被删除。默认保留7天的数据。 每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。（什么时候创建，什么时候删除） 数据有序的讨论？一个partition的数据是否是有序的？&emsp;&emsp;间隔性有序，不连续,针对一个topic里面的数据，只能做到partition内部有序，不能做到全局有序. 特别加入消费者的场景后，如何保证消费者消费的数据全局有序的？&emsp;&emsp;伪命题,如果要全局有序的，必须保证生产有序，存储有序，消费有序。由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。 Kafka Partition SegmentSegment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，后缀”.index”和“.log”分别表示为segment索引文件、数据文件。 Segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。 索引文件存储大量元数据，数据文件存储大量消息，索引文件中元数据指向对应数据文件中message的物理偏移地址。 在00000000000000368769.index索引文件中记录着的是稀疏索引,记录如下:00000000000000368769.log文件中第1条信息(“Message368700”),在磁盘的0的位置;00000000000000368769.log文件中第3条信息(“Message3687772”),在磁盘的497的位置; Kafka 查找message读取offset=368776的message，需要通过下面2个步骤查找。 查找segment file00000000000000000000.index表示最开始的文件，起始偏移量(offset)为000000000000000368769.index的消息量起始偏移量为368770 = 368769 + 100000000000000737337.index的起始偏移量为737338=737337 + 1其他后续文件依次类推。以起始偏移量命名并排序这些文件，只要根据offset 二分查找文件列表，就可以快速定位到具体文件。当offset=368776时定位到00000000000000368769.index和对应log文件。 通过segment file查找message当offset=368776时，依次定位到00000000000000368769.index的元数据物理位置和00000000000000368769.log的物理偏移地址然后再通过00000000000000368769.log顺序查找直到offset=368776为止。","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"Kafka整体结构图及相关说明","date":"2017-04-16T04:47:25.109Z","path":"2017/04/16/bigdata/kafka/Kafka整体结构图及相关说明/","text":"Kafka整体结构图 架构图中的组件解释 类JMS消息队列，结合JMS中的两种模式，可以有多个消费者主动拉取数据，在JMS中只有点对点模式才有消费者主动拉取数据，kafka是一个生产-消费模型。 Producer：生产者，只负责数据生产，生产者的代码可以集成到任务系统中。 数据的分发策略由producer决定，默认是由设定：partitioner.class=kafka.producer.DefaultPartitioner ，算法：Utils.abs(key.hashCode) % numPartitions Broker：当前服务器上的Kafka进程,俗称拉皮条。只管数据存储，不管是谁生产，不管是谁消费。在集群中每个broker都有一个唯一brokerid，不得重复 ,一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。 Topic:目标发送的目的地，这是一个逻辑上的概念，落到磁盘上是一个partition的目录。partition的目录中有多个segment组合(index,log)，一个Topic对应多个partition[0,1,2,3]，一个partition对应多个segment组合。一个segment有默认的大小是1G。每个partition可以设置多个副本(replication-factor 1),会从所有的副本中选取一个leader出来。所有读写操作都是通过leader来进行的。特别强调，和mysql中主从有区别，mysql做主从是为了读写分离，在kafka中读写操作都是leader。 ConsumerGroup：数据消费者组，ConsumerGroup可以有多个，每个ConsumerGroup消费的数据都是一样的。可以把多个consumer线程划分为一个组，组里面所有成员共同消费一个topic的数据，组员之间不能重复消费。 Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。 Kafka消息的分发 kafka集群中的任何一个broker都可以向producer提供metadata信息,这些metadata中包含”集群中存活的servers列表”/“partitions leader列表”等信息； 当producer获取到metadata信息之后, producer将会和Topic下所有partition leader保持socket连接； 消息由producer直接通过socket发送到broker，中间不会经过任何”路由层”，事实上，消息被路由到哪个partition上由producer客户端决定；比如可以采用”random””key-hash””轮询”等,如果一个topic中有多个partitions,那么在producer端实现”消息均衡分发”是必要的。 在producer端的配置文件中,开发者可以指定partition路由的方式。默认是defaultPartition Utils.abs(key.hashCode) % numPartitions上文中的key是producer在发送数据时传入的，produer.send(KeyedMessage(topic,myPartitionKey,messageContent)) kafka如何保证数据的完全生产123456设置一个参数：request.required.acks = 0 ##该参数是消息的确认模式（生产者发送消息到partition，partition是否确认（ack））## 0：不保证消息的到达确认，只管发送，低延迟但是会出现消息的丢失## 1：发送消息，并会等待leader 收到消息之后发送ack## -1：发送消息，当所有的follower都同步消息成功后发送ack（follower是partition的副本） broker如何保存数据在理论环境下，broker按照顺序读写的机制，可以每秒保存600M的数据。主要通过pagecache机制，尽可能的利用当前物理机器上的空闲内存来做缓存。 当前topic所属的broker，必定有一个该topic的partition，partition是一个磁盘目录。partition的目录中有多个segment组合(index,log) partition如何分布在不同的broker上1234567int i = 0list&#123;kafka01,kafka02,kafka03&#125;//这是所有的broker服务器for(int i=0;i&lt;5;i++)&#123;//在创建topic的时候会指定分区数量,假如是5 brIndex = i%broker; //broker=3 hostName = list.get(brIndex)&#125; consumerGroup的组员和partition之间如何做负载均衡1234567891011最好是一一对应，一个partition对应一个consumer。如果consumer的数量过多，必然有空闲的consumer。算法： 假如topic1,具有如下partitions: P0,P1,P2,P3 加入group中,有如下consumer: C1,C2 首先根据partition索引号对partitions排序: P0,P1,P2,P3 根据consumer.id排序: C0,C1 计算倍数: M = [P0,P1,P2,P3].size / [C0,C1].size,本例值M=2(向上取整) 然后依次分配partitions: C0 = [P0,P1],C1=[P2,P3],即Ci = [P(i * M),P((i + 1) * M -1)] #就是consumer排序之后,每排好序的consumer每个分M个排好序的分区 Consumer与topic关系本质上kafka只支持Topic； 每个group中可以有多个consumer，每个consumer属于一个consumer group；通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高”故障容错”性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。 对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer；那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个”订阅”者。 在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)；一个Topic中的每个partions，只会被一个”订阅者”中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。 kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。 kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。","tags":[{"name":"kafka","slug":"kafka","permalink":"http://yoursite.com/tags/kafka/"}]},{"title":"级联求和之累计报表","date":"2017-04-16T04:47:25.106Z","path":"2017/04/16/bigdata/hive/级联求和之累计报表/","text":"需求有如下访客访问次数统计表 t_access_times 访客 月份 访问次数 A 2015-01 5 A 2015-01 15 B 2015-01 5 A 2015-01 8 B 2015-01 25 A 2015-01 5 A 2015-02 4 A 2015-02 6 B 2015-02 10 B 2015-02 5 …… …… …… 需要输出报表：t_access_times_accumulate 访客 月份 访问次数 A 2015-01 33 33 A 2015-02 10 43 … …. … … B 2015-01 30 30 B 2015-02 22 52 … … … … 解析1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556create table t_access_times(username string,month string,salary int)row format delimited fields terminated by &apos;,&apos;; load data local inpath &apos;/home/hadoop/t_access_times.dat&apos; into table t_access_times; A,2015-01,5A,2015-01,15B,2015-01,5A,2015-01,8B,2015-01,25A,2015-01,5A,2015-02,4A,2015-02,6B,2015-02,10B,2015-02,5 1、第一步，先求个用户的月总金额select username,month,sum(salary) as salary from t_access_times group by username,month +-----------+----------+---------+--+| username | month | salary |+-----------+----------+---------+--+| A | 2015-01 | 33 || A | 2015-02 | 10 || B | 2015-01 | 30 || B | 2015-02 | 15 |+-----------+----------+---------+--+ 2、第二步，将月总金额表 自己连接 自己连接(username相同)+-------------+----------+-----------+-------------+----------+-----------+--+| a.username | a.month | a.salary | b.username | b.month | b.salary |+-------------+----------+-----------+-------------+----------+-----------+--+| A | 2015-01 | 33 | A | 2015-01 | 33 || A | 2015-01 | 33 | A | 2015-02 | 10 || A | 2015-02 | 10 | A | 2015-01 | 33 || A | 2015-02 | 10 | A | 2015-02 | 10 || B | 2015-01 | 30 | B | 2015-01 | 30 || B | 2015-01 | 30 | B | 2015-02 | 15 || B | 2015-02 | 15 | B | 2015-01 | 30 || B | 2015-02 | 15 | B | 2015-02 | 15 |+-------------+----------+-----------+-------------+----------+-----------+--+ 3、第三步，从上一步的结果中进行分组查询，分组的字段是a.username a.month求月累计值： 将b.month &lt;= a.month的所有b.salary求和即可select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulatefrom(select username,month,sum(salary) as salary from t_access_times group by username,month) Ainner join(select username,month,sum(salary) as salary from t_access_times group by username,month) BonA.username=B.usernamewhere B.month &lt;= A.monthgroup by A.username,A.monthorder by A.username,A.month;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"row number over 函数(生成分组内记录的序列)","date":"2017-04-16T04:47:25.104Z","path":"2017/04/16/bigdata/hive/row number over 函数(组内排名)/","text":"ROW_NUMBER() 从1开始，按照顺序，生成分组内记录的序列比如，按照pv降序排列，生成分组内每天的pv名次,ROW_NUMBER() 的应用场景非常多，再比如，获取分组内排序第一的记录;获取一个session中的第一条refer等。 123456789101112131415161718192021222324SELECT cookieid, createtime, pv, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn FROM lxw1234; cookieid day pv rn(这是生成的排名列)------------------------------------------- cookie1 2015-04-12 7 1cookie1 2015-04-11 5 2cookie1 2015-04-15 4 3cookie1 2015-04-16 4 4cookie1 2015-04-13 3 5cookie1 2015-04-14 2 6cookie1 2015-04-10 1 7cookie2 2015-04-15 9 1cookie2 2015-04-16 7 2cookie2 2015-04-13 6 3cookie2 2015-04-12 5 4cookie2 2015-04-14 3 5cookie2 2015-04-11 3 6cookie2 2015-04-10 2 7","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"RANK 和 DENSE_RANK(组内排名函数)","date":"2017-04-16T04:47:25.103Z","path":"2017/04/16/bigdata/hive/RANK 和 DENSE_RANK(组内排名函数)/","text":"RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位 DENSE_RANK() 生成数据项在分组中的排名，排名相等会在名次中不会留下空位 1234567891011121314151617181920212223SELECT cookieid, createtime, pv, RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn1, DENSE_RANK() OVER(PARTITION BY cookieid ORDER BY pv desc) AS rn2, ROW_NUMBER() OVER(PARTITION BY cookieid ORDER BY pv DESC) AS rn3 FROM lxw1234 WHERE cookieid = &apos;cookie1&apos;; cookieid day pv rn1 rn2 rn3 -------------------------------------------------- cookie1 2015-04-12 7 1 1 1cookie1 2015-04-11 5 2 2 2cookie1 2015-04-15 4 3 3 3cookie1 2015-04-16 4 3 3 4cookie1 2015-04-13 3 5 4 5cookie1 2015-04-14 2 6 5 6cookie1 2015-04-10 1 7 6 7 rn1: 15号和16号并列第3, 13号排第5rn2: 15号和16号并列第3, 13号排第4rn3: 如果相等，则按记录值排序，生成唯一的次序，如果所有记录值都相等，或许会随机排吧。","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive自定义函数","date":"2017-04-16T04:47:25.102Z","path":"2017/04/16/bigdata/hive/hive自定义函数/","text":"当Hive提供的内置函数无法满足你的业务处理需要时，此时就可以考虑使用用户自定义函数（UDF：user-defined function）。 自定义函数类别 UDF 作用于单个数据行，产生一个数据行作为输出。（数学函数，字符串函数） UDAF（用户定义聚集函数）：接收多个输入数据行，并产生一个输出数据行。（count，max） UDF开发实例123456789101112131415161718192021222324&apos;1.先开发一个java类，继承UDF，并重载evaluate方法&apos;##############################################################package cn.itcast.bigdata.udfimport org.apache.hadoop.hive.ql.exec.UDF;import org.apache.hadoop.io.Text; public final class Lower extends UDF&#123;public Text evaluate(final Text s)&#123; if(s==null)&#123;return null;&#125; return new Text(s.toString().toLowerCase());&#125;&#125;##############################################################&apos;2.打成jar包上传到服务器&apos;&apos;3.将jar包添加到hive的classpath&apos;hive&gt;add JAR /home/hadoop/udf.jar;&apos;4.创建临时函数与开发好的java class关联&apos;Hive&gt;create temporary function tolowercase as &apos;cn.itcast.bigdata.udf.Lower&apos;; #tolowercase 是临时函数的名字，as后面指定的是临时函数对应的java类&apos;5.即可在hql中使用自定义的函数tolowercase &apos;hive&gt;select tolowercase(name) from student ; Json数据解析UDF开发12345678910111213141516public class JsonParser extends UDF &#123; public String evaluate(String jsonLine) &#123; ObjectMapper objectMapper = new ObjectMapper(); try &#123; MovieRateBean bean = objectMapper.readValue(jsonLine, MovieRateBean.class); return bean.toString(); &#125; catch (Exception e) &#123; &#125; return &quot;&quot;;&#125; &#125; 电话号码转换函数123456789101112131415161718public class ToProvince extends UDF&#123; static HashMap&lt;String, String&gt; provinceMap = new HashMap&lt;String, String&gt;();static&#123; provinceMap.put(&quot;138&quot;, &quot;beijing&quot;); provinceMap.put(&quot;139&quot;, &quot;shanghai&quot;); provinceMap.put(&quot;137&quot;, &quot;dongjing&quot;); provinceMap.put(&quot;156&quot;, &quot;huoxing&quot;);&#125; //我们需要重载这个方法，来适应我们的业务逻辑public String evaluate(String phonenbr)&#123; String res = provinceMap.get(phonenbr.substring(0, 3)); return res==null?&quot;wukong&quot;:res;&#125;&#125; Transform实现调用脚本&emsp;Hive的 TRANSFORM 关键字提供了在SQL中调用自写脚本的功能，适合实现Hive中没有的功能又不想写UDF的情况下面这句sql就是借用了weekday_mapper.py对数据进行了处理.12345678910111213141516CREATE TABLE u_data_new ( movieid INT, rating INT, weekday INT, userid INT)ROW FORMAT DELIMITEDFIELDS TERMINATED BY &apos;\\t&apos;; add FILE weekday_mapper.py; INSERT OVERWRITE TABLE u_data_newSELECT TRANSFORM (movieid , rate, timestring,uid) #传递给脚本的字段 USING &apos;python weekday_mapper.py&apos; #调用的脚本 AS (movieid, rating, weekday,userid) #select 显示的字段名称FROM t_rating; 其中weekday_mapper.py内容如下123456789#!/bin/pythonimport sysimport datetime for line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split(&apos;\\t&apos;) weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print &apos;\\t&apos;.join([movieid, rating, str(weekday),userid])","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive实战","date":"2017-04-16T04:47:25.101Z","path":"2017/04/16/bigdata/hive/hive实战/","text":"1.分桶表示例12345678910111213141516171819202122232425262728#创建分桶表drop table stu_buck;create table stu_buck(Sno int,Sname string,Sex string,Sage int,Sdept string)clustered by(Sno) #以Sno来分桶sorted by(Sno DESC) #每个桶内使用Sno排序into 4 buckets #分成4个桶row format delimitedfields terminated by &apos;,&apos;;#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数set hive.enforce.bucketing = true;set mapreduce.job.reduces=4;#插入数据#开会往创建的分通表插入数据(插入数据需要是已分桶, 且排序的)#可以使用distribute by(sno) sort by(sno asc) 或是排序和分桶的字段相同的时候使用Cluster by(字段)#注意使用cluster by 就等同于分桶+排序(sort)insert into table stu_buckselect Sno,Sname,Sex,Sage,Sdept from student distribute by(Sno) sort by(Sno asc); insert overwrite table stu_buckselect * from student distribute by(Sno) sort by(Sno asc); insert overwrite table stu_buckselect * from student cluster by(Sno); 2.保存select查询结果的几种方式123456789101112131415161718&apos;1、将查询结果保存到一张新的hive表中&apos;create table t_tmpasselect * from t_p;&apos;2、将查询结果保存到一张已经存在的hive表中&apos;insert into table t_tmpselect * from t_p;&apos;3、将查询结果保存到指定的文件目录（可以是本地，也可以是hdfs）&apos;#本地insert overwrite local directory &apos;/home/hadoop/test&apos;select * from t_p; #hdfsinsert overwrite directory &apos;/user/test&apos;select * from t_p; 3.关于hive中的各种join3.1.准备数据12345678910111213#a.txt1,a2,b3,c4,d7,y8,u #b.txt2,bb3,cc7,yy9,pp 3.2.建表12345create table a(id int,name string)row format delimited fields terminated by &apos;,&apos;; create table b(id int,name string)row format delimited fields terminated by &apos;,&apos;; 3.3.导入数据12load data local inpath &apos;/home/hadoop/a.txt&apos; into table a;load data local inpath &apos;/home/hadoop/b.txt&apos; into table b; 3.4.实验123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566#inner joinselect * from a inner join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 2 | b | 2 | bb || 3 | c | 3 | cc || 7 | y | 7 | yy |+-------+---------+-------+---------+--+#left joinselect * from a left join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL |+-------+---------+-------+---------+--+#right joinselect * from a right join b on a.id=b.id; # full outer join select * from a full outer join b on a.id=b.id;+-------+---------+-------+---------+--+| a.id | a.name | b.id | b.name |+-------+---------+-------+---------+--+| 1 | a | NULL | NULL || 2 | b | 2 | bb || 3 | c | 3 | cc || 4 | d | NULL | NULL || 7 | y | 7 | yy || 8 | u | NULL | NULL || NULL | NULL | 9 | pp |+-------+---------+-------+---------+--+#semi join 只是返回inner join 的前半select * from a left semi join b on a.id = b.id;+-------+---------+--+| a.id | a.name |+-------+---------+--+| 2 | b || 3 | c || 7 | y |+-------+---------+--+#重写以下子查询为LEFT SEMI JOIN SELECT a.key, a.value FROM a WHERE a.key exist in (SELECT b.key FROM B);#可以被重写为： SELECT a.key, a.val FROM a LEFT SEMI JOIN b on (a.key = b.key) 4.多重插入12345from studentinsert into table student_p partition(part=&apos;a&apos;)select * where Sno&lt;95011;insert into table student_p partition(part=&apos;a&apos;)select * where Sno&lt;95011; 5.导出数据到本地12insert overwrite local directory &apos;/home/hadoop/student.txt&apos;select * from student; 6.UDF（自定义函数）案例1234567891011121314151617########################文件：rating.json ###############################&#123;&quot;movie&quot;:&quot;1193&quot;,&quot;rate&quot;:&quot;5&quot;,&quot;timeStamp&quot;:&quot;978300760&quot;,&quot;uid&quot;:&quot;1&quot;&#125;&#123;&quot;movie&quot;:&quot;661&quot;,&quot;rate&quot;:&quot;3&quot;,&quot;timeStamp&quot;:&quot;978302109&quot;,&quot;uid&quot;:&quot;1&quot;&#125;&#123;&quot;movie&quot;:&quot;914&quot;,&quot;rate&quot;:&quot;3&quot;,&quot;timeStamp&quot;:&quot;978301968&quot;,&quot;uid&quot;:&quot;1&quot;&#125;&#123;&quot;movie&quot;:&quot;3408&quot;,&quot;rate&quot;:&quot;4&quot;,&quot;timeStamp&quot;:&quot;978300275&quot;,&quot;uid&quot;:&quot;1&quot;&#125;#######################################################create table rat_json(line string) row format delimited;load data local inpath &apos;/home/hadoop/rating.json&apos; into table rat_json; drop table if exists t_rating;create table t_rating(movieid string,rate int,timestring string,uid string)row format delimited fields terminated by &apos;\\t&apos;; insert overwrite table t_ratingselect split(parsejson(line),&apos;\\t&apos;)[0]as movieid,split(parsejson(line),&apos;\\t&apos;)[1] as rate,split(parsejson(line),&apos;\\t&apos;)[2] as timestring,split(parsejson(line),&apos;\\t&apos;)[3] as uid from rat_json limit 10; 7.内置jason函数1select get_json_object(line,&apos;$.movie&apos;) as moive,get_json_object(line,&apos;$.rate&apos;) as rate from rat_json limit 10; 8.transform案例123456789101112131415161718192021222324252627282930313233&apos;1、先加载rating.json文件到hive的一个原始表 rat_json&apos;create table rat_json(line string) row format delimited;load data local inpath &apos;/home/hadoop/rating.json&apos; into table rat_json; &apos;2、需要解析json数据成四个字段，插入一张新的表 t_rating&apos;insert overwrite table t_ratingselect get_json_object(line,&apos;$.movie&apos;) as moive,get_json_object(line,&apos;$.rate&apos;) as rate from rat_json; &apos;3、使用transform+python的方式去转换unixtime为weekday&apos;先编辑一个python脚本文件########python######代码vi weekday_mapper.py#!/bin/pythonimport sysimport datetime for line in sys.stdin: line = line.strip() movieid, rating, unixtime,userid = line.split(&apos;\\t&apos;) weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday() print &apos;\\t&apos;.join([movieid, rating, str(weekday),userid]) #保存文件#然后，将文件加入hive的classpath：hive&gt;add FILE /home/hadoop/weekday_mapper.py;hive&gt;create TABLE u_data_new asSELECT TRANSFORM (movieid, rate, timestring,uid) USING &apos;python weekday_mapper.py&apos; AS (movieid, rate, weekday,uid)FROM t_rating; select distinct(weekday) from u_data_new limit 10; 8.数据ETL8.1.需求 对web点击流日志基础数据表进行etl（按照仓库模型设计） 按各时间维度统计来源域名top10 已有数据表 “t_orgin_weblog” ：12345678910111213+------------------+------------+----------+--+| col_name | data_type | comment |+------------------+------------+----------+--+| valid | string | || remote_addr | string | || remote_user | string | || time_local | string | || request | string | || status | string | || body_bytes_sent | string | || http_referer | string | || http_user_agent | string | |+------------------+------------+----------+--+ 8.2.数据实例123| true|1.162.203.134| - | 18/Sep/2013:13:47:35| /images/my.jpg | 200| 19939 | &quot;http://www.angularjs.cn/A0d9&quot; | &quot;Mozilla/5.0 (Windows | | true|1.202.186.37 | - | 18/Sep/2013:15:39:11| /wp-content/uploads/2013/08/windjs.png| 200| 34613 | &quot;http://cnodejs.org/topic/521a30d4bee8d3cb1272ac0f&quot; | &quot;Mozilla/5.0 (Macintosh;| 8.3.实现步骤12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&apos;1、对原始数据进行抽取转换&apos;#--将来访url分离出host path query query iddrop table if exists t_etl_referurl;create table t_etl_referurl asSELECT a.*,b.*FROM t_orgin_weblog a LATERAL VIEW parse_url_tuple(regexp_replace(http_referer, &quot;\\&quot;&quot;, &quot;&quot;), &apos;HOST&apos;, &apos;PATH&apos;,&apos;QUERY&apos;, &apos;QUERY:id&apos;) b as host, path, query, query_id #使用了hive的内置函数 parse_url_tuple&apos;2.从前述步骤进一步分离出日期时间形成ETL明细表“t_etl_detail” day tm &apos;drop table if exists t_etl_detail;create table t_etl_detail asselect b.*,substring(time_local,0,11) as daystr,substring(time_local,13) as tmstr,substring(time_local,4,3) as month,substring(time_local,0,2) as day,substring(time_local,13,2) as hourfrom t_etl_referurl b;&apos;3.对etl数据进行分区(包含所有数据的结构化信息)&apos;drop table t_etl_detail_prt;create table t_etl_detail_prt(valid string,remote_addr string,remote_user string,time_local string,request string,status string,body_bytes_sent string,http_referer string,http_user_agent string,host string,path string,query string,query_id string,daystr string,tmstr string,month string,day string,hour string)partitioned by (mm string,dd string);&apos;4.导入数据&apos;insert into table t_etl_detail_prt partition(mm=&apos;Sep&apos;,dd=&apos;18&apos;)select * from t_etl_detail where daystr=&apos;18/Sep/2013&apos;; insert into table t_etl_detail_prt partition(mm=&apos;Sep&apos;,dd=&apos;19&apos;)select * from t_etl_detail where daystr=&apos;19/Sep/2013&apos;;&apos;5.分个时间维度统计各referer_host的访问次数并排序&apos;create table t_refer_host_visit_top_tmp asselect referer_host,count(*) as counts,mm,dd,hh from t_display_referer_counts group by hh,dd,mm,referer_host order by hh asc,dd asc,mm asc,counts desc;&apos;6.来源访问次数topn各时间维度URL, 取各时间维度的referer_host访问次数topn&apos;select * from (select referer_host,counts,concat(hh,dd),row_number() over (partition by concat(hh,dd) order by concat(hh,dd) asc) as od from t_refer_host_visit_top_tmp) t where od&lt;=3; 9.级联求和9.1.需求有如下访客访问次数统计表 t_access_times 访客 月份 访问次数 A 2015-01 5 A 2015-01 15 B 2015-01 5 A 2015-01 8 B 2015-01 25 A 2015-01 5 A 2015-02 4 A 2015-02 6 B 2015-02 10 B 2015-02 5 …… …… …… 需要输出报表：t_access_times_accumulate 访客 月份 访问次数 A 2015-01 33 33 A 2015-02 10 43 … …. … … B 2015-01 30 30 B 2015-02 22 52 … … … … 9.2.实现步骤详见《面试用神sql–套路–累计报表.md》12345678910select A.username,A.month,max(A.salary) as salary,sum(B.salary) as accumulatefrom(select username,month,sum(salary) as salary from t_access_times group by username,month) Ainner join(select username,month,sum(salary) as salary from t_access_times group by username,month) BonA.username=B.usernamewhere B.month &lt;= A.monthgroup by A.username,A.monthorder by A.username,A.month;","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive安装","date":"2017-04-16T04:47:25.100Z","path":"2017/04/16/bigdata/hive/hive安装/","text":"1.解压12345cd /home/hadoop/app/#解压tar -zxvf apache-hive-1.2.1-bin.tar.gz #创建软链接ln -s apache-hive-1.2.1-bin hive 2.安装mysql数据库 参见《mysql的yum安装.md》注意：删除匿名用户，允许用户远程连接 12345#如果出现没有权限的问题，在mysql授权(在安装mysql的机器上执行)mysql -uroot -p #(执行下面的语句 *.*:所有库下的所有表 %：任何IP地址或主机都可以连接)GRANT ALL PRIVILEGES ON *.* TO &apos;root&apos;@&apos;%&apos; IDENTIFIED BY &apos;root&apos; WITH GRANT OPTION;FLUSH PRIVILEGES; 3.配置hive hive-env.sh1234#改名mv ./conf/hive-env.sh.template ./conf/hive-env.sh#指定：配置其中的$hadoop_home HADOOP_HOME=/home/hadoop/app/hadoop-2.6.4 hive-site.xml源文件太多，我们自己新建一个,主要是配置连接mysql的连接信息(连接的url,连接驱动,用户名,密码)12345678910111213141516171819202122232425262728293031323334353637383940 vi hive-site.xml#添加如下内容：&lt;configuration&gt;&lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;&lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;username to use against metastore database&lt;/description&gt;&lt;/property&gt; &lt;property&gt;&lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;&lt;value&gt;root&lt;/value&gt;&lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;/configuration&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/user/hive/warehouse&lt;/value&gt; &lt;description&gt;这里配置的是hive在HDFS中存储数据的目录:hdfs://host_name:9000/user/hive/warehouse 就是这个目录 &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://hadoop-master:9083&lt;/value&gt; &lt;description&gt;如果配置了这里,就可以通过远程连接到hive的元数据,注意要在远端(hadoop-master中)的hive中启动:hive --service metastore &amp;&lt;/description&gt;&lt;/property&gt; 4.mysql的连接驱动 安装hive和mysq完成后，将mysql的连接驱动jar包拷贝到$HIVE_HOME/lib目录下 5.Jline包版本不一致的问题 需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的 /home/hadoop/app/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar1234567891011121314151617181920[root@hdp-node-01 hive]# ./bin/hive Logging initialized using configuration in jar:file:/home/hadoop/app/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties[ERROR] Terminal initialization failed; falling back to unsupported&apos;java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected&apos; #hadoop中的jar和hive中的jar包版本不一致冲突了 at jline.TerminalFactory.create(TerminalFactory.java:101) at jline.TerminalFactory.get(TerminalFactory.java:158) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:229) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:221) at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:209) at org.apache.hadoop.hive.cli.CliDriver.setupConsoleReader(CliDriver.java:787) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:721) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136) 6.启动hive123bin/hive#hive启动之后，可以像MySQL一样操作，写SQL查询语句了 7.测试1234567891011#在./conf/hive-site.xml 中配置了连接MySQL时的&lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;&lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt; #在hive启动的时候，会在MySQL中生成hive数据库#在MySQL中存放的是元数据，其中有下面几张表需要注意：DBS：存放的是创建的数据库TBLS：是表信息COLUMNS_V2：是所有的字段信息 所创建的数据库 所有表，如下，我们创建了student表 所有的字段信息，如下是student表中存在的字段 8.使用方式8.1.Hive交互shell1bin/hive 8.2.Hive thrift服务 123456789101112131415161718#启动方式，（假如是在hadoop01上）：#启动为前台：bin/hiveserver2#启动为后台：nohup bin/hiveserver2 1&gt;/var/log/hiveserver.log 2&gt;/var/log/hiveserver.err &amp;#连接#方式（1）hive/bin/beeline #回车，进入beeline的命令界面，输入命令连接hiveserver2beeline&gt; !connect jdbc:hive2//mini1:10000 #（hadoop01是hiveserver2所启动的那台主机名，端口默认是10000）#方式（2）：启动就连接：bin/beeline -u jdbc:hive2://mini1:10000 -n hadoop&apos;接下来就可以做正常sql查询了&apos; 8.3.Hive命令1234567891011121314151617181920212223hive -e &lt;quoted-query-string&gt; #Sql from command line#Examplehive -e &quot;select * from mytable limit 3&quot;OKname1 10name2 20name3 30Time taken:4.99t secondeshive -f &lt;filename&gt; #Sql from files；执行指定文件中的一个或多个查询语句，按照惯例，一般将这些文件保存为具有.q或者.hql后缀名文件hive -f /path/to/file/withqueries.hqlhive -S,--slient #Slient mode in interactive shell ，将会去掉“OK”和后面的查询时间#Examplehive -S -e &quot;select * from mytable limit 3&quot; &gt;/tmp/myquery #将执行的结果输出到指定的文件中cat /tmp/myquery #这样文件中内容就只有查询到的结果name1 10name2 20name3 30","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"hive基本概念","date":"2017-04-16T04:47:25.098Z","path":"2017/04/16/bigdata/hive/hive基本概念/","text":"1.架构图 Jobtracker是hadoop1.x中的组件，它的功能相当于： Resourcemanager+AppMaster，TaskTracker 相当于： Nodemanager + yarnchild 2.基本组成 用户接口：包括 CLI、JDBC/ODBC、WebGUI。 元数据存储：通常是存储在关系数据库如 mysql , derby中。 解释器、编译器、优化器、执行器。 3.各组件的基本功能 用户接口主要由三个：CLI、JDBC/ODBC和WebGUI。其中，CLI为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。 元数据存储：Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。 解释器、编译器、优化器完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后有 MapReduce 调用执行。 4.Hive与Hadoop的关系&emsp;Hive利用HDFS存储数据，利用MapReduce查询数据 4.Hive与传统数据库对比 hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析 5.Hive的数据存储 Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，RCFILE等） 只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。 Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。 db：在hdfs中表现为${hive.metastore.warehouse.dir}目录下一个文件夹 table：在hdfs中表现所属db目录下一个文件夹 external table：与table类似，不过其数据存放位置可以在任意指定路径 partition：在hdfs中表现为table目录下的子目录 bucket：在hdfs中表现为同一个表目录下根据hash散列之后的多个文件","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"Hive基本操作","date":"2017-04-16T04:47:25.097Z","path":"2017/04/16/bigdata/hive/Hive基本操作/","text":"下面是对hive的DML和DDL操作 1.DDL操作1.1.创建表1.1.1.建表语法123456789CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name [(col_name data_type [COMMENT col_comment], ...)] [COMMENT table_comment] [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS] [ROW FORMAT row_format] [STORED AS file_format] [LOCATION hdfs_path] CREATE TABLE 创建一个指定名字的表。如果相同名字的表已经存在，则抛出异常；用户可以用 IF NOT EXISTS 选项来忽略这个异常。 LIKE 允许用户复制现有的表结构，但是不复制数据。123456 ROW FORMAT DELIMITED [FIELDS TERMINATED BY char] [COLLECTION ITEMS TERMINATED BY char] [MAP KEYS TERMINATED BY char][LINES TERMINATED BY char] | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)] 用户在建表的时候可以自定义 SerDe 或者使用自带的 SerDe。如果没有指定 ROW FORMAT 或者 ROW FORMAT DELIMITED，将会使用自带的 SerDe。在建表的时候，用户还需要为表指定列，用户在指定表的列的同时也会指定自定义的 SerDe，Hive通过 SerDe 确定表的具体的列的数据。 STORED AS SEQUENCEFILE | TEXTFILE | RCFILE如果文件数据是纯文本，可以使用 STORED AS TEXTFILE。如果数据需要压缩，使用 STORED AS SEQUENCEFILE。 CLUSTERED BY对于每一个表（table）或者分区， Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。Hive也是 针对某一列进行桶的组织。Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。把表（或者分区）组织成桶（Bucket）有两个理由： 获得更高的查询处理效率。桶为表加上了额外的结构，Hive 在处理有些查询时能利用这个结构。具体而言，连接两个在（包含连接列的）相同列上划分了桶的表，可以使用 Map 端连接 （Map-side join）高效的实现。比如JOIN操作。对于JOIN操作两个表有一个相同的列，如果对这两个表都进行了桶操作。那么将保存相同列值的桶进行JOIN操作就可以，可以大大较少JOIN的数据量。 使取样（sampling）更高效。在处理大规模数据集时，在开发和修改查询的阶段，如果能在数据集的一小部分数据上试运行查询，会带来很多方便。 1.1.2.具体事例创建内部表mytable1234567hive&gt; create table if not exists mytable(sid int ,name string) &gt; row format delimited field &gt; row format delimited fields terminated by &apos;\\005&apos; &gt; stored as textfile ;OKTime taken: 1.016 secondshive&gt; 创建外部表pageview123456789hive&gt; create external table if not exists pageview( &gt; pageid int, &gt; page_url string comment &apos;the page url&apos; &gt; ) &gt; row format delimited fields terminated by &apos;,&apos; &gt; location &apos;hdfs://192.168.0.11:9000/user/hive/warehouse/&apos; ;OKTime taken: 0.102 secondshive&gt; 创建分区表invites1234create table student_p(Sno int,Sname string,Sex string,Sage int,Sdept string) partitioned by(part string) row format delimited fields terminated by &apos;,&apos;stored as textfile; 创建带桶的表student123456789101112131415hive&gt; create table student(id int ,age int, name string) &gt; partitioned by(stat_data string) &gt; clustered by(id) sorted by(age) into 2 buckets &gt; row format delimited fields terminated by &apos;,&apos; ;OKTime taken: 0.13 secondshive&gt; #加载本地数据到指定分区hive&gt; load data local inpath &apos;/home/buckets.txt&apos; overwrite into table student partition(stat_data=&apos;20131230&apos;);Loading data to table default.student partition (stat_data=20131230)Partition default.student&#123;stat_data=20131230&#125; stats: [numFiles=1, numRows=0, totalSize=91, rawDataSize=0]OKTime taken: 3.756 seconds 1.2.修改表1.2.1.增加/删除分区123456789&apos;语法结构&apos;#增加ALTER TABLE table_name ADD [IF NOT EXISTS] partition_spec [ LOCATION &apos;location1&apos; ] partition_spec [ LOCATION &apos;location2&apos; ] ...partition_spec:: PARTITION (partition_col = partition_col_value, partition_col = partiton_col_value, ...)#删除ALTER TABLE table_name DROP partition_spec, partition_spec,... 实例12345678910111213141516171819202122232425262728293031323334alter table student_p add partition(part=&apos;a&apos;) partition(part=&apos;b&apos;);#添加分区hive&gt; alter table student add partition(stat_data=&apos;20140101&apos;) location &apos;/user/hive/warehouse/student&apos; #添加的同时指定分区的指向 partition(stat_data=&apos;20140102&apos;);OKTime taken: 0.23 secondshive&gt; hive&gt; show partitions student;OKstat_data=20131230stat_data=20140101stat_data=20140102Time taken: 0.145 seconds, Fetched: 3 row(s)hive&gt; #删除分区hive&gt; alter table student drop partition(stat_data=&apos;20140101&apos;),partition(stat_data=&apos;20140102&apos;) ;Dropped the partition stat_data=20140101Dropped the partition stat_data=20140102OKTime taken: 0.852 secondshive&gt; show partitions student;OKstat_data=20131230Time taken: 0.145 seconds, Fetched: 1 row(s)hive&gt; 1.2.2.重命名表 语法结构1ALTER TABLE table_name RENAME TO new_table_name 具体实例123456789101112131415161718hive&gt; show tables;OKmytablepageviewstudentTime taken: 0.077 seconds, Fetched: 3 row(s)hive&gt; alter table mytable rename to mytable1 ; #rename to 重命名OKTime taken: 0.34 secondshive&gt; show tables;OKmytable1pageviewstudentTime taken: 0.049 seconds, Fetched: 3 row(s)hive&gt; 1.2.3.增加/更新列 语法结构123456ALTER TABLE table_name ADD|REPLACE COLUMNS (col_name data_type [COMMENT col_comment], ...) 注：ADD是代表新增一字段，字段位置在所有列后面(partition列前)，REPLACE则是表示替换表中所有字段。 ALTER TABLE table_name CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] 具体实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 hive&gt; desc student;OKid int age int name string stat_data string # Partition Information # col_name data_type comment stat_data string Time taken: 0.15 seconds, Fetched: 9 row(s)#添加字段hive&gt; alter table student add columns(gender string);OKTime taken: 0.22 secondshive&gt; desc student;OKid int age int name string gender string stat_data string # Partition Information # col_name data_type comment stat_data string Time taken: 0.18 seconds, Fetched: 10 row(s)hive&gt; alter #修改hive&gt; alter table student replace columns (id int ,age int ,name string);OKTime taken: 0.192 secondshive&gt; desc student;OKid int age int name string stat_data string # Partition Information # col_name data_type comment stat_data string Time taken: 0.153 seconds, Fetched: 9 row(s)hive&gt; 1.3显示命令123456show tablesshow databasesshow partitionsshow functionsdesc extended t_name;desc formatted table_name; 2.DML操作2.1.Load2.1.1.语法结构12345678910111213141516171819202122232425262728LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTOTABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]/*说明：1、Load 操作只是单纯的复制/移动操作，将数据文件移动到 Hive 表对应的位置。2、filepath：相对路径，例如：project/data1绝对路径，例如：/user/hive/project/data1包含模式的完整 URI，列如：hdfs://namenode:9000/user/hive/project/data13、LOCAL关键字如果指定了 LOCAL， load 命令会去查找本地文件系统中的 filepath。如果没有指定 LOCAL 关键字，则根据inpath中的uri[如果指定了 LOCAL，那么：load 命令会去查找本地文件系统中的 filepath。如果发现是相对路径，则路径会被解释为相对于当前用户的当前路径。load 命令会将 filepath中的文件复制到目标文件系统中。目标文件系统由表的位置属性决定。被复制的数据文件移动到表的数据对应的位置。 如果没有指定 LOCAL 关键字，如果 filepath 指向的是一个完整的 URI，hive 会直接使用这个 URI。 否则：如果没有指定 schema 或者 authority，Hive 会使用在 hadoop 配置文件中定义的 schema 和 authority，fs.default.name 指定了 Namenode 的 URI。如果路径不是绝对的，Hive 相对于/user/进行解释。Hive 会将 filepath 中指定的文件内容移动到 table （或者 partition）所指定的路径中。]查找文件 4、OVERWRITE 关键字如果使用了 OVERWRITE 关键字，则目标表（或者分区）中的内容会被删除，然后再将 filepath 指向的文件/目录中的内容添加到表/分区中。如果目标表（分区）已经有一个文件，并且文件名和 filepath 中的文件名冲突，那么现有的文件会被新文件所替代。*/ 2.1.2.具体实例load123456789101112131415161718192021#加载绝对路径数据hive&gt; load data local inpath &apos;/home/buckets.txt&apos; overwrite into table student partition(stat_data=&apos;20131230&apos;);#加载包含模式数据[root@hdp-node-01 home]# hdfs dfs -ls /Found 3 items-rw-r--r-- 3 root supergroup 91 2016-11-28 09:43 /buckets.txtdrwx-wx-wx - root supergroup 0 2016-11-27 23:31 /tmpdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /userhive&gt; load data inpath &apos;hdfs://hdp-node-01:9000/buckets.txt&apos; into table student partition(stat_data=&apos;20140202&apos;);[root@hdp-node-01 home]# hdfs dfs -ls / #会将文件移动Found 2 itemsdrwx-wx-wx - root supergroup 0 2016-11-27 23:31 /tmpdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /user[root@hdp-node-01 home]# #覆盖：overwrite load data inpath &apos;hdfs://hdp-node-01:9000/buckets.txt&apos; overwrite into table student partition(stat_data=&apos;20140202&apos;); 2.2.Insert2.2.1.语法结构1234567891011INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statementMultiple inserts:FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ...] select_statement2] ...Dynamic partition inserts:INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement 2.2.2.实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&apos;基本模式插入&apos;hive&gt; insert overwrite table student partition(stat_data=&apos;20150202&apos;) select id,age,name from student where stat_data=&apos;20150101&apos;;hive&gt; select * from student;OK1 21 zhangsan 201501012 22 lisi 201501013 33 wangwu 201501014 44 zhouliu 201501011 21 zhangsan 201502022 22 lisi 201502023 33 wangwu 201502024 44 zhouliu 20150202&apos;多插入模式&apos;hive&gt; from student &gt; insert overwrite table student partition(stat_data=&apos;20160101&apos;) &gt; select id,age,name where stat_data=&apos;20150101&apos; &gt; insert overwrite table student partition(stat_data=&apos;20170101&apos;) &gt; select id,age,name where stat_data=&apos;20150101&apos;;hive&gt; select *from student;OK1 21 zhangsan 201501012 22 lisi 201501013 33 wangwu 201501014 44 zhouliu 201501011 21 zhangsan 201502022 22 lisi 201502023 33 wangwu 201502024 44 zhouliu 201502021 21 zhangsan 201601012 22 lisi 201601013 33 wangwu 201601014 44 zhouliu 201601011 21 zhangsan 201701012 22 lisi 201701013 33 wangwu 201701014 44 zhouliu 20170101Time taken: 0.147 seconds, Fetched: 16 row(s)hive&gt; &apos;自动分区模式&apos;hive&gt; set hive.exec.dynamic.partition.mode=nonstrict;hive&gt; insert overwrite table student partition(stat_data) &gt; select id,age,name,stat_data from student where stat_data=&apos;20150101&apos;; 2.3.导出表数据2.3.1.语法结构1234567INSERT OVERWRITE [LOCAL] DIRECTORY directory1 SELECT ... FROM ... multiple inserts:FROM from_statementINSERT OVERWRITE [LOCAL] DIRECTORY directory1 select_statement1[INSERT OVERWRITE [LOCAL] DIRECTORY directory2 select_statement2] ... 2.3.2.实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&apos;导出文件到本地&apos;hive&gt; insert overwrite local directory &apos;/home/student&apos; select *from student;[root@hdp-node-01 home]# cd /home/student/[root@hdp-node-01 student]# lltotal 4-rw-r--r-- 1 root root 340 Nov 28 10:16 000000_0-rw-r--r-- 1 root root 0 Nov 28 10:16 000001_0[root@hdp-node-01 student]# cat 000000_01\u000121\u0001zhangsan\u0001201501012\u000122\u0001lisi\u0001201501013\u000133\u0001wangwu\u0001201501014\u000144\u0001zhouliu\u0001201501011\u000121\u0001zhangsan\u0001201502022\u000122\u0001lisi\u0001201502023\u000133\u0001wangwu\u0001201502024\u000144\u0001zhouliu\u0001201502021\u000121\u0001zhangsan\u0001201601012\u000122\u0001lisi\u0001201601013\u000133\u0001wangwu\u0001201601014\u000144\u0001zhouliu\u0001201601011\u000121\u0001zhangsan\u0001201701012\u000122\u0001lisi\u0001201701013\u000133\u0001wangwu\u0001201701014\u000144\u0001zhouliu\u000120170101[root@hdp-node-01 student]#&apos;导出数据到HDFS&apos;hive&gt; insert overwrite directory &apos;hdfs://hdp-node-01:9000/student&apos; select *from student;[root@hdp-node-01 student]# hdfs dfs -ls /studentFound 1 items-rwxr-xr-x 3 root supergroup 340 2016-11-28 10:20 /student/000000_0[root@hdp-node-01 student]# hdfs dfs -cat /student/000000_01\u000121\u0001zhangsan\u0001201501012\u000122\u0001lisi\u0001201501013\u000133\u0001wangwu\u0001201501014\u000144\u0001zhouliu\u0001201501011\u000121\u0001zhangsan\u0001201502022\u000122\u0001lisi\u0001201502023\u000133\u0001wangwu\u0001201502024\u000144\u0001zhouliu\u0001201502021\u000121\u0001zhangsan\u0001201601012\u000122\u0001lisi\u000120160101//.......... 2.4.SELECT2.4.1.语法结构12345678910111213141516171819202122232425SELECT [ALL | DISTINCT] select_expr, select_expr, ...FROM table_reference[WHERE where_condition][GROUP BY col_list [HAVING condition]][CLUSTER BY col_list | [DISTRIBUTE BY col_list] [SORT BY| ORDER BY col_list]][LIMIT number]/*注：1、order by 会对输入做全局排序，因此强行设置reduce为1，会导致当输入规模较大时，需要较长的计算时间。2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。3、distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。4、Cluster by(字段) 除了具有Distribute by的功能外，还会对该字段进行排序。 因此，如果分桶和sort字段是同一个时，此时，cluster by = distribute by + sort by 分桶表的作用：最大的作用是用来提高join操作的效率；（思考这个问题：select a.id,a.name,b.addr from a join b on a.id = b.id;如果a表和b表已经是分桶表，而且分桶的字段是id字段做这个join操作时，还需要全表做笛卡尔积吗？）*/ 2.4.2.实例1234567891011121314151617181920212223242526&apos;获取年龄大的3个学生&apos;hive&gt; select id,age,name from student where stat_data=&apos;20150101&apos; order by age desc limit 3;Total MapReduce CPU Time Spent: 7 seconds 280 msecOK4 44 zhouliu3 33 wangwu2 22 lisiTime taken: 75.821 seconds, Fetched: 3 row(s)&apos;查询学生信息按年龄，降序排序select id,age,name from student sort by age desc;select id,age,name from student order by age desc; select id,age,name from student distribute by age;&apos;按学生名称汇总学生年龄&apos; select id,age,name from student group by name; 2.5.Hive Join2.5.1.语法结构1234567join_table: table_reference JOIN table_factor [join_condition] | table_reference &#123;LEFT|RIGHT|FULL&#125; [OUTER] JOIN table_reference join_condition | table_reference LEFT SEMI JOIN table_reference join_condition/*Hive 支持等值连接（equality joins）、外连接（outer joins）和（left/right joins）。Hive 不支持非等值的连接，因为非等值连接非常难转化到 map/reduce 任务。另外，Hive 支持多于 2 个表的连接。*/ 2.5.2.举例1234567891011121314151617181920212223242526272829303132333435363738&apos;只支持等值join&apos;例如： SELECT a.* FROM a JOIN b ON (a.id = b.id) SELECT a.* FROM a JOIN b ON (a.id = b.id AND a.department = b.department) #是正确的 SELECT a.* FROM a JOIN b ON (a.id&gt;b.id) #是错误的。&apos;可以 join 多于 2 个表&apos; SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)#如果join中多个表的 join key 是同一个，则 join 会被转化为单个 map/reduce 任务，因为 join 中只使用了 b.key1 作为 join key，例如： SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)#而这一 join 被转化为 2 个 map/reduce 任务。因为 b.key1 用于第一次 join 条件，而 b.key2 用于第二次 joinSELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)&apos;join 时，每次 map/reduce 任务的逻辑&apos;#reducer 会缓存 join 序列中除了最后一个表的所有表的记录，再通过最后一个表将结果序列化到文件系统。这一实现有助于在 reduce 端减少内存的使用量。实践中，应该把最大的那个表写在最后（否则会因为缓存浪费大量内存）。例如：SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)#所有表都使用同一个 join key（使用 1 次 map/reduce 任务计算）。Reduce 端会缓存 a 表和 b 表的记录，然后每次取得一个 c 表的记录就计算一次 join 结果，类似的还有： SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key2)#这里用了 2 次 map/reduce 任务。第一次缓存 a 表，用 b 表序列化；第二次缓存第一次 map/reduce 任务的结果，然后用 c 表序列化。&apos;LEFT，RIGHT 和 FULL OUTER 关键字用于处理 join 中空记录的情况&apos;SELECT a.val, b.val FROMa LEFT OUTER JOIN b ON (a.key=b.key) #对应所有 a 表中的记录都有一条记录输出。输出的结果应该是 a.val, b.val，当 a.key=b.key 时，而当 b.key 中找不到等值的 a.key 记录时也会输出:a.val, NULL#所以 a 表中的所有记录都被保留了；“a RIGHT OUTER JOIN b”会保留所有 b 表的记录。&apos;LEFT SEMI JOIN是IN/EXISTS的高效实现&apos;select id,name,age from student a left semi join class b on(a.name=b.std_name); #相当于内连接，但是只显示左表 3.Hive Shell参数3.1.Hive命令行3.1.1.语法结构123456789101112hive [-hiveconf x=y]* [&lt;-i filename&gt;]* [&lt;-f filename&gt;|&lt;-e query-string&gt;] [-S]/*说明：1、-i 从文件初始化HQL。2、-e从命令行执行指定的HQL3、-f 执行HQL脚本4、-v 输出执行的HQL语句到控制台5、-p &lt;port&gt; connect to Hive Server on port number6、-hiveconf x=y Use this to set hive/hadoop configuration variables.7. -S 是取消OK和时间的显示*/ 3.1.2.实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&apos;运行一个查询&apos;[root@hdp-node-01 hive]# ./bin/hive -S -e &apos;select *from student;&apos;; #-S 是取消OK和时间的显示1 21 zhangsan 201501012 22 lisi 201501013 33 wangwu 201501014 44 zhouliu 201501011 21 zhangsan 201502022 22 lisi 201502023 33 wangwu 201502024 44 zhouliu 201502021 21 zhangsan 201601012 22 lisi 201601013 33 wangwu 201601014 44 zhouliu 201601011 21 zhangsan 201701012 22 lisi 201701013 33 wangwu 201701014 44 zhouliu 20170101[root@hdp-node-01 hive]#&apos;运行一个文件&apos;[root@hdp-node-01 hive]# cat /home/query.hqlselect *from student; [root@hdp-node-01 hive]# ./bin/hive -f /home/query.hql Logging initialized using configuration in jar:file:/home/hadoop/app/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.propertiesOK1 21 zhangsan 201501012 22 lisi 201501013 33 wangwu 201501014 44 zhouliu 201501011 21 zhangsan 201502022 22 lisi 201502023 33 wangwu 201502024 44 zhouliu 201502021 21 zhangsan 201601012 22 lisi 201601013 33 wangwu 201601014 44 zhouliu 201601011 21 zhangsan 201701012 22 lisi 201701013 33 wangwu 201701014 44 zhouliu 20170101Time taken: 2.514 seconds, Fetched: 16 row(s)[root@hdp-node-01 hive]#&apos;运行参数文件&apos;[root@hdp-node-01 hive]# cat /home/initHQL.confset mapred.reduce.tasks=4[root@hdp-node-01 hive]# hive -i /home/initHQL.conf 3.2.Hive参数配置方式Hive参数大全：https://cwiki.apache.org/confluence/display/Hive/Configuration+Properties 对于一般参数，有以下三种设定方式： 配置文件 命令行参数 参数声明 配置文件 用户自定义配置文件：$HIVE_CONF_DIR/hive-site.xml 默认配置文件：$HIVE_CONF_DIR/hive-default.xml用户自定义配置会覆盖默认配置。另外，Hive也会读入Hadoop的配置，因为Hive是作为Hadoop的客户端启动的，Hive的配置会覆盖Hadoop的配置。配置文件的设定对本机启动的所有Hive进程都有效 命令行参数 启动Hive（客户端或Server方式）时，可以在命令行添加-hiveconf param=value来设定参数，例如：1bin/hive -hiveconf hive.root.logger=INFO,console #这一设定对本次启动的Session（对于Server方式启动，则是所有请求的Sessions）有效 参数声明 可以在HQL中使用SET关键字设定参数，例如：1set mapred.reduce.tasks=100; 这一设定的作用域也是session级的。 上述三种设定方式的优先级依次递增。即参数声明覆盖命令行参数，命令行参数覆盖配置文件设定。注意某些系统级的参数，例如log4j相关的设定，必须用前两种方式设定，因为那些参数的读取在Session建立以前已经完成了。","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"Hive函数大全(转)","date":"2017-04-16T04:47:25.096Z","path":"2017/04/16/bigdata/hive/hive函数/","text":"转自:[一起学Hive]之二–Hive函数大全-完整版 介绍hive的函数分类,以及举例 一、关系运算： 等值比较: = 等值比较:&lt;=&gt; 不等值比较: &lt;&gt;和!= 小于比较: &lt; 小于等于比较: &lt;= 大于比较: &gt; 大于等于比较: &gt;= 区间比较 空值判断: IS NULL 非空判断: IS NOT NULL LIKE比较: LIKE JAVA的LIKE操作: RLIKE REGEXP操作: REGEXP 二、数学运算： 加法操作: + 减法操作: – 乘法操作: * 除法操作: / 取余操作: % 位与操作: &amp; 位或操作: | 位异或操作: ^ 位取反操作: ~ 三、逻辑运算： 逻辑与操作: AND 、&amp;&amp; 逻辑或操作: OR 、|| 逻辑非操作: NOT、! 四、复合类型构造函数 map结构 struct结构 named_struct结构 array结构 create_union 五、复合类型操作符 获取array中的元素 获取map中的元素 获取struct中的元素 六、数值计算函数 取整函数: round 指定精度取整函数: round 向下取整函数: floor 向上取整函数: ceil 向上取整函数: ceiling 取随机数函数: rand 自然指数函数: exp 以10为底对数函数: log10 以2为底对数函数: log2 对数函数: log 幂运算函数: pow 幂运算函数: power 开平方函数: sqrt 二进制函数: bin 十六进制函数: hex 反转十六进制函数: unhex 进制转换函数: conv 绝对值函数: abs 正取余函数: pmod 正弦函数: sin 反正弦函数: asin 余弦函数: cos 反余弦函数: acos positive函数: positive negative函数: negative 七、集合操作函数 map类型大小：size array类型大小：size 判断元素数组是否包含元素：array_contains 获取map中所有value集合 获取map中所有key集合 数组排序 八、类型转换函数 二进制转换：binary 基础类型之间强制转换：cast 九、日期函数 UNIX时间戳转日期函数: from_unixtime 获取当前UNIX时间戳函数: unix_timestamp 日期转UNIX时间戳函数: unix_timestamp 指定格式日期转UNIX时间戳函数: unix_timestamp 日期时间转日期函数: to_date 日期转年函数: year 日期转月函数: month 日期转天函数: day 日期转小时函数: hour 日期转分钟函数: minute 日期转秒函数: second 日期转周函数: weekofyear 日期比较函数: datediff 日期增加函数: date_add 日期减少函数: date_sub 十、条件函数 If函数: if 非空查找函数: COALESCE 条件判断函数：CASE 条件判断函数：CASE 十一、字符串函数 字符ascii码函数：ascii base64字符串 字符串连接函数：concat 带分隔符字符串连接函数：concat_ws 数组转换成字符串的函数：concat_ws 小数位格式化成字符串函数：format_number 字符串截取函数：substr,substring 字符串截取函数：substr,substring 字符串查找函数：instr 字符串长度函数：length 字符串查找函数：locate 字符串格式化函数：printf 字符串转换成map函数：str_to_map base64解码函数：unbase64(string str) 字符串转大写函数：upper,ucase 字符串转小写函数：lower,lcase 去空格函数：trim 左边去空格函数：ltrim 右边去空格函数：rtrim 正则表达式替换函数：regexp_replace 正则表达式解析函数：regexp_extract URL解析函数：parse_url json解析函数：get_json_object 空格字符串函数：space 重复字符串函数：repeat 左补足函数：lpad 右补足函数：rpad 分割字符串函数: split 集合查找函数: find_in_set 分词函数：sentences 分词后统计一起出现频次最高的TOP-K 分词后统计与指定单词一起出现频次最高的TOP-K 十二、混合函数 调用Java函数：java_method 调用Java函数：reflect 字符串的hash值：hash 十三、XPath解析XML函数 xpath xpath_string xpath_boolean xpath_short, xpath_int, xpath_long xpath_float, xpath_double, xpath_number 十四、汇总统计函数（UDAF） 个数统计函数: count 总和统计函数: sum 平均值统计函数: avg 最小值统计函数: min 最大值统计函数: max 非空集合总体变量函数: var_pop 非空集合样本变量函数: var_samp 总体标准偏离函数: stddev_pop 样本标准偏离函数: stddev_samp 中位数函数: percentile 中位数函数: percentile 近似中位数函数: percentile_approx 近似中位数函数: percentile_approx 直方图: histogram_numeric 集合去重数：collect_set 集合不去重函数：collect_list 十五、表格生成函数Table-Generating Functions (UDTF) 数组拆分成多行：explode Map拆分成多行：explode 举例一、关系运算： 等值比较: = 语法：A=B操作类型：所有基本类型描述: 如果表达式 A 与表达式 B 相等，则为 TRUE；否则为 FALSE；只要有任意比较项为NULL,均返回 FALSE;举例：hive&gt; select 1 from lxw1234 where 1=1;1hive&gt; select 1 from lxw1234 where NULL = NULL; OK 等值比较:&lt;=&gt; 语法：A &lt;=&gt; B操作类型：所有基本类型描述：如果 A 和 B 都是非 NULL 值，则返回结果和=一样，如果两者都为 NULL，返回 TRUE,如果有一个为 NULL，则返回 FALSE。 举例：hive&gt; select 1 from lxw1234 where NULL &lt;=&gt; NULL; OK1 不等值比较: &lt;&gt;和!= 语法: A &lt;&gt; B A != B操作类型: 所有基本类型描述: 如果表达式 A 为 NULL，或者表达式 B 为 NULL，返回 NULL；如果表达式 A 与表 达式 B 不相等，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where 1 &lt;&gt; 2;1 小于比较: &lt; 语法: A &lt; B 操作类型: 所有基本类型 描述: 如果表达式 A 为 NULL，或者表达式 B 为 NULL，返回 NULL；如果表达式 A 小于 表达式 B，则为 TRUE；否则为 FALSE 举例： hive&gt; select 1 from lxw1234 where 1 &lt; 2;1 小于等于比较: &lt;= 语法: A &lt;= B 操作类型: 所有基本类型 描述: 如果表达式 A 为 NULL，或者表达式 B 为 NULL，返回 NULL；如果表达式 A 小于 或者等于表达式 B，则为 TRUE；否则为 FALSE 举例： hive&gt; select 1 from lxw1234 where 1 &lt;= 1; 1 6. 大于比较: &gt; 语法: A &gt; B操作类型: 所有基本类型描述: 如果表达式 A 为 NULL，或者表达式 B 为 NULL，返回 NULL；如果表达式 A 大于 表达式 B，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where 2 &gt; 1;1 大于等于比较: &gt;= 语法: A &gt;= B操作类型: 所有基本类型描述: 如果表达式 A 为 NULL，或者表达式 B 为 NULL，返回 NULL；如果表达式 A 大于 或者等于表达式 B，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where 1 &gt;= 1;1 注意：String 的比较要注意(常用的时间比较可以先 to_date 之后再比较)hive&gt; select * from lxw1234; OK 2011111209 00:00:00 2011111209 hive&gt; select a,b,ab,a=b from lxw1234;2011111209 00:00:00 2011111209 false true false 区间比较 语法: A [NOT] BETWEEN B AND C操作类型: 所有类型描述: 如果 A、B、C 有任一个为 NULL,则返回 FALSE. 等价于 B &lt;= A &lt; C. 举例： hive&gt; select 1 from lxw1234 where 1 between 1 and 2; OK1 空值判断: IS NULL 语法: A IS NULL操作类型: 所有类型描述: 如果表达式 A 的值为 NULL，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where null is null;1版权所有：http://lxw1234.com 非空判断: IS NOT NULL 语法: A IS NOT NULL操作类型: 所有类型描述: 如果表达式 A 的值为 NULL，则为 FALSE；否则为 TRUE举例：hive&gt; select 1 from lxw1234 where 1 is not null;1 LIKE 比较: LIKE 语法: A LIKE B操作类型: strings 描述: 如果字符串 A 或者字符串 B 为 NULL，则返回 NULL；如果字符串 A 符合表达式 B 的正则语法，则为 TRUE；否则为 FALSE。B 中字符””表示任意单个字符，而字 符”%”表示任意数量的字符。举例：hive&gt; select 1 from lxw1234 where ‘football’ like ‘foot%’;1hive&gt; select 1 from lxw1234 where ‘football’ like ‘foot ’;1注意：否定比较时候用 NOT A LIKE Bhive&gt; select 1 from lxw1234 where NOT ‘football’ like ‘fff%’;1 JAVA 的 LIKE 操作: RLIKE 语法: A RLIKE B操作类型: strings描述: 如果字符串 A 或者字符串 B 为 NULL，则返回 NULL；如果字符串 A 符合 JAVA 正 则表达式 B 的正则语法，则为 TRUE；否则为 FALSE。举例：hive&gt; select 1 from lxw1234 where ‘footbar’ rlike ‘^f.*r$’;1注意：判断一个字符串是否全为数字：hive&gt;select 1 from lxw1234 where ‘123456’ rlike ‘^\\d+$’;1hive&gt; select 1 from lxw1234 where ‘123456aa’ rlike ‘^\\d+$’; REGEXP 操作: REGEXP 语法: A REGEXP B操作类型: strings描述: 功能与 RLIKE 相同 举例：hive&gt; select 1 from lxw1234 where ‘footbar’ REGEXP ‘^f.*r$’;1版权所有：http://lxw1234.com 二、数学运算： 加法操作: + 语法: A + B操作类型：所有数值类型说明：返回 A 与 B 相加的结果。结果的数值类型等于 A 的类型和 B 的类型的最小父类 型（详见数据类型的继承关系）。比如，int + int 一般结果为 int 类型，而 int + double 一般 结果为 double 类型举例：hive&gt; select 1 + 9 from lxw1234;10hive&gt; create table lxw1234 as select 1 + 1.2 from lxw1234;hive&gt; describe lxw1234;_c0 double 减法操作: – 语法: A – B操作类型：所有数值类型说明：返回 A 与 B 相减的结果。结果的数值类型等于 A 的类型和 B 的类型的最小父类 型（详见数据类型的继承关系）。比如，int – int 一般结果为 int 类型，而 int – double 一 般结果为 double 类型举例：hive&gt; select 10 – 5 from lxw1234;5hive&gt; create table lxw1234 as select 5.6 – 4 from lxw1234;hive&gt; describe lxw1234;_c0 double 乘法操作: * 语法: A B操作类型：所有数值类型说明：返回 A 与 B 相乘的结果。结果的数值类型等于 A 的类型和 B 的类型的最小父类 型（详见数据类型的继承关系）。注意，如果 A 乘以 B 的结果超过默认结果类型的数值范围， 则需要通过 cast 将结果转换成范围更大的数值类型举例：hive&gt; select 40 5 from lxw1234; 200版权所有：http://lxw1234.com 除法操作: / 语法: A / B操作类型：所有数值类型说明：返回 A 除以 B 的结果。结果的数值类型为 double举例：hive&gt; select 40 / 5 from lxw1234;8.0 注意：hive 中最高精度的数据类型是 double,只精确到小数点后 16 位，在做除法运算的 时候要特别注意hive&gt;select ceil(28.0/6.999999999999999999999) from lxw1234 limit 1;结果为 4hive&gt;select ceil(28.0/6.99999999999999) from lxw1234 limit 1;结果为 5 取余操作: % 语法: A % B操作类型：所有数值类型说明：返回 A 除以 B 的余数。结果的数值类型等于 A 的类型和 B 的类型的最小父类型（详见数据类型的继承关系）。 举例：hive&gt; select 41 % 5 from lxw1234;1hive&gt; select 8.4 % 4 from lxw1234;0.40000000000000036注意：精度在 hive 中是个很大的问题，类似这样的操作最好通过 round 指定精度hive&gt; select round(8.4 % 4 , 2) from lxw1234;0.4 位与操作: &amp; 语法: A &amp; B操作类型：所有数值类型说明：返回 A 和 B 按位进行与操作的结果。结果的数值类型等于 A 的类型和 B 的类型 的最小父类型（详见数据类型的继承关系）。 举例：hive&gt; select 4 &amp; 8 from lxw1234;0hive&gt; select 6 &amp; 4 from lxw1234;4 位或操作: | 语法: A | B操作类型：所有数值类型说明：返回 A 和 B 按位进行或操作的结果。结果的数值类型等于 A 的类型和 B 的类型 的最小父类型（详见数据类型的继承关系）。举例：hive&gt; select 4 | 8 from lxw1234;12hive&gt; select 6 | 8 from lxw1234;14 位异或操作: ^ 语法: A ^ B操作类型：所有数值类型说明：返回 A 和 B 按位进行异或操作的结果。结果的数值类型等于 A 的类型和 B 的类 型的最小父类型（详见数据类型的继承关系）。举例：hive&gt; select 4 ^ 8 from lxw1234;12hive&gt; select 6 ^ 4 from lxw1234;2 9．位取反操作: ~ 语法: ~A操作类型：所有数值类型说明：返回 A 按位取反操作的结果。结果的数值类型等于 A 的类型。 举例：hive&gt; select ~6 from lxw1234;-7 hive&gt; select ~4 from lxw1234;-5 三、逻辑运算： 逻辑与操作: AND 、&amp;&amp; 语法: A AND B操作类型：boolean说明：如果 A 和 B 均为 TRUE，则为 TRUE；否则为 FALSE。如果 A 为 NULL 或 B 为 NULL， 则为 NULL举例：hive&gt; select 1 from lxw1234 where 1=1 and 2=2;1 逻辑或操作: OR 、|| 语法: A OR B操作类型：boolean说明：如果 A 为 TRUE，或者 B 为 TRUE，或者 A 和 B 均为 TRUE，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where 1=2 or 2=2;1 逻辑非操作: NOT、! 语法: NOT A、!A操作类型：boolean说明：如果 A 为 FALSE，或者 A 为 NULL，则为 TRUE；否则为 FALSE举例：hive&gt; select 1 from lxw1234 where not 1=2;1hive&gt; select 1 from lxw1234 where !1=2; OK1 四、复合类型构造函数 map 结构 语法：map(k1,v1,k2,v2,…)操作类型：map说明：使用给定的 key-value 对，构造一个 map 数据结构 举例：hive&gt; select map(‘k1′,’v1′,’k2′,’v2′) from lxw1234; OK{“k2″:”v2″,”k1″:”v1”} struct 结构 语法：struct(val1,val2,val3,…) 操作类型：struct 说明：使用给定的表达式，构造一个 struct 数据结构 举例：hive&gt; select struct(1,’aaa’,FALSE) from lxw1234; OK{“col1″:1,”col2″:”aaa”,”col3″:false} named_struct 结构 语法：named_struct(name1,val1,name2,val2,name3,val3,…) 操作类型：struct 说明：使用给定的表达式，构造一个指定列名的 struct 数据结构 举例：hive&gt; select named_struct(‘a’,1,’b’,’aaa’,’c’,FALSE) from lxw1234; OK{“a”:1,”b”:”aaa”,”c”:false} array 结构 语法：array(val1,val2,val3,…) 操作类型：array 说明：使用给定的表达式，构造一个 array 数据结构 举例： hive&gt; select array(1,2,3) from lxw1234; OK[1,2,3] create_union 语法：create_union (tag, val1, val2, …)操作类型：uniontype说明：使用给定的 tag 和表达式，构造一个 uniontype 数据结构。tag 表示使用第 tag 个 表达式作为 uniontype 的 value举例：hive&gt; select create_union(0,’ss’,array(1,2,3)) from lxw1234; OK{0:”ss”}hive&gt; select create_union(1,’ss’,array(1,2,3)) from lxw1234; OK{1:[1,2,3]} 五、复合类型操作符 获取 array 中的元素 语法：A[n]操作类型：所有基础类型说明：返回数组 A 中第 n 个索引的元素值。 举例：hive&gt; select array(‘a’,’b’,’c’)[1] from lxw1234; OKb 获取 map 中的元素 语法：M[key]操作类型：所有基础类型说明：返回 map 结构 M 中 key 对应的 value。 举例：hive&gt; select map(‘k1′,’v1’)[‘k1’] from lxw1234; OK v1 获取 struct 中的元素 语法：S.x操作类型：所有类型说明：返回 struct 结构 S 中名为 x 的元素。 举例：hive&gt; select named_struct(‘a’,1,’b’,’aaa’,’c’,FALSE).c from lxw1234; OKfalse 六、数值计算函数 取整函数: round 语法: round(double a)返回值: BIGINT说明: 返回 double 类型的整数值部分 （遵循四舍五入） 举例：hive&gt; select round(3.1415926) from lxw1234;3hive&gt; select round(3.5) from lxw1234;4hive&gt; create table lxw1234 as select round(9542.158) from lxw1234;hive&gt; describe lxw1234;_c0 bigint 指定精度取整函数: round 语法: round(double a, int d)返回值: DOUBLE说明: 返回指定精度 d 的 double 类型 举例：hive&gt; select round(3.1415926,4) from lxw1234;3.1416 向下取整函数: floor 语法: floor(double a)返回值: BIGINT说明: 返回等于或者小于该 double 变量的最大的整数 举例：hive&gt; select floor(3.1415926) from lxw1234;3hive&gt; select floor(25) from lxw1234;25 向上取整函数: ceil 语法: ceil(double a)返回值: BIGINT说明: 返回等于或者大于该 double 变量的最小的整数 举例：hive&gt; select ceil(3.1415926) from lxw1234;4hive&gt; select ceil(46) from lxw1234;46 向上取整函数: ceiling 语法: ceiling(double a)返回值: BIGINT说明: 与 ceil 功能相同 举例：hive&gt; select ceiling(3.1415926) from lxw1234;4hive&gt; select ceiling(46) from lxw1234;46 取随机数函数: rand 语法: rand(),rand(int seed)返回值: double 说明: 返回一个 0 到 1 范围内的随机数。如果指定种子 seed，则会等到一个稳定的随机 数序列举例：hive&gt; select rand() from lxw1234;0.5577432776034763hive&gt; select rand() from lxw1234;0.6638336467363424hive&gt; select rand(100) from lxw1234;0.7220096548596434hive&gt; select rand(100) from lxw1234;0.7220096548596434 自然指数函数: exp 语法: exp(double a)返回值: double说明: 返回自然对数 e 的 a 次方 举例：hive&gt; select exp(2) from lxw1234;7.38905609893065 自然对数函数: ln 语法: ln(double a) 返回值: double说明: 返回 a 的自然对数 举例：hive&gt; select ln(7.38905609893065) from lxw1234;2.0 绝对值函数: abs 语法: abs(double a) abs(int a) 返回值: double int 说明: 返回数值 a 的绝对值 举例：hive&gt; select abs(-3.9) from lxw1234;3.9hive&gt; select abs(10.9) from lxw1234;10.9 positive 函数: positive 语法: positive(int a), positive(double a)返回值: int double 说明: 返回 a 举例：hive&gt; select positive(-10) from lxw1234;-10hive&gt; select positive(12) from lxw1234;12 negative 函数: negative 语法: negative(int a), negative(double a)返回值: int double 说明: 返回-a 举例：hive&gt; select negative(-5) from lxw1234;5hive&gt; select negative(8) from lxw1234;-8 七、集合操作函数 map 类型大小：size 语法: size(Map)返回值: int说明: 返回 map 类型的 size举例：hive&gt; select size(map(‘k1′,’v1′,’k2′,’v2’)) from lxw1234; OK2 array 类型大小：size 语法: size(Array) 返回值: int说明: 返回 array 类型的 size举例：hive&gt; select size(array(1,2,3,4,5)) from lxw1234; OK5 判断元素数组是否包含元素：array_contains 语法: array_contains(Array, value)返回值: boolean说明: 返回 Array中是否包含元素 value举例：hive&gt; select array_contains(array(1,2,3,4,5),3) from lxw1234; OKtrue 获取 map 中所有 value 集合 语法: map_values(Map)返回值: array说明: 返回 Map中所有 value 的集合 举例：hive&gt; select map_values(map(‘k1′,’v1′,’k2′,’v2’)) from lxw1234; OK[“v2″,”v1”] 获取 map 中所有 key 集合 语法: map_keys(Map)返回值: array说明: 返回 Map中所有 key 的集合 举例：hive&gt; select map_keys(map(‘k1′,’v1′,’k2′,’v2’)) from lxw1234; OK[“k2″,”k1”] 数组排序 语法: sort_array(Array)返回值: array 说明: 对 Array进行升序排序 举例：hive&gt; select sort_array(array(5,7,3,6,9)) from lxw1234; OK[3,5,6,7,9] 八、类型转换函数 二进制转换：binary 语法: binary(string|binary)返回值: binary说明: 将 string 类型转换为二进制 举例：hive&gt; select binary(‘lxw1234’) from lxw1234; OKlxw1234 基础类型之间强制转换：cast 语法: cast(expr as )返回值:说明: 将 expr 转换成举例：hive&gt; select cast(‘1′ as DOUBLE) from lxw1234; OK1.0 九、日期函数 UNIX 时间戳转日期函数: from_unixtime 语法: from_unixtime(bigint unixtime[, string format])返回值: string说明: 转化 UNIX 时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的秒数）到当前时区 的时间格式 举例：hive&gt; select from_unixtime(1323308943,’yyyyMMdd’) from lxw1234;20111208 获取当前 UNIX 时间戳函数: unix_timestamp 语法: unix_timestamp()返回值: bigint说明: 获得当前时区的 UNIX 时间戳 举例：hive&gt; select unix_timestamp() from lxw1234;1323309615 日期转 UNIX 时间戳函数: unix_timestamp 语法: unix_timestamp(string date)返回值: bigint说明: 转换格式为”yyyy-MM-dd HH:mm:ss”的日期到 UNIX 时间戳。如果转化失败，则返 回 0。举例：hive&gt; select unix_timestamp(‘2011-12-07 13:01:03’) from lxw1234;1323234063 指定格式日期转 UNIX 时间戳函数: unix_timestamp 语法: unix_timestamp(string date, string pattern)返回值: bigint说明: 转换 pattern 格式的日期到 UNIX 时间戳。如果转化失败，则返回 0。 举例：hive&gt; select unix_timestamp(‘20111207 13:01:03′,’yyyyMMdd HH:mm:ss’) from lxw1234;1323234063 日期时间转日期函数: to_date 语法: to_date(string timestamp)返回值: string说明: 返回日期时间字段中的日期部分。 举例： hive&gt; select to_date(‘2011-12-08 10:03:01’) from lxw1234;2011-12-08 日期转年函数: year 语法: year(string date)返回值: int说明: 返回日期中的年。 举例：hive&gt; select year(‘2011-12-08 10:03:01’) from lxw1234;2011hive&gt; select year(‘2012-12-08’) from lxw1234;2012 日期转月函数: month 语法: month (string date)返回值: int说明: 返回日期中的月份。 举例：hive&gt; select month(‘2011-12-08 10:03:01’) from lxw1234;12hive&gt; select month(‘2011-08-08’) from lxw1234;8 日期转天函数: day 语法: day (string date)返回值: int说明: 返回日期中的天。 举例：hive&gt; select day(‘2011-12-08 10:03:01’) from lxw1234;8hive&gt; select day(‘2011-12-24’) from lxw1234;24 日期转小时函数: hour 语法: hour (string date)返回值: int说明: 返回日期中的小时。 举例：hive&gt; select hour(‘2011-12-08 10:03:01’) from lxw1234;10 日期转分钟函数: minute 语法: minute (string date)返回值: int说明: 返回日期中的分钟。 举例：hive&gt; select minute(‘2011-12-08 10:03:01’) from lxw1234;3 日期转秒函数: second 语法: second (string date)返回值: int说明: 返回日期中的秒。 举例：hive&gt; select second(‘2011-12-08 10:03:01’) from lxw1234;1 日期转周函数: weekofyear 语法: weekofyear (string date)返回值: int说明: 返回日期在当前的周数。 举例：hive&gt; select weekofyear(‘2011-12-08 10:03:01’) from lxw1234;49 日期比较函数: datediff 语法: datediff(string enddate, string startdate)返回值: int说明: 返回结束日期减去开始日期的天数。 举例：hive&gt; select datediff(‘2012-12-08′,’2012-05-09’) from lxw1234;213 日期增加函数: date_add 语法: date_add(string startdate, int days)返回值: string说明: 返回开始日期 startdate 增加 days 天后的日期。 举例：hive&gt; select date_add(‘2012-12-08’,10) from lxw1234;2012-12-18 日期减少函数: date_sub 语法: date_sub (string startdate, int days)返回值: string说明: 返回开始日期 startdate 减少 days 天后的日期。 举例：hive&gt; select date_sub(‘2012-12-08′,10) from lxw1234;2012-11-28 十、条件函数 If 函数: if 语法: if(boolean testCondition, T valueTrue, T valueFalseOrNull)返回值: T 说明: 当条件 testCondition 为 TRUE 时，返回 valueTrue；否则返回 valueFalseOrNull举例：hive&gt; select if(1=2,100,200) from lxw1234;200hive&gt; select if(1=1,100,200) from lxw1234;100 非空查找函数: COALESCE 语法: COALESCE(T v1, T v2, …)返回值: T说明: 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回 NULL举例：hive&gt; select COALESCE(null,’100’,’50′) from lxw1234;100 条件判断函数：CASE 语法: CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END返回值: T说明：如果 a 等于 b，那么返回 c；如果 a 等于 d，那么返回 e；否则返回 f举例：hive&gt; Select case 100 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;maryhive&gt; Select case 200 when 50 then ‘tom’ when 100 then ‘mary’ else ‘tim’ end from lxw1234;tim 十一、字符串函数 字符 ascii 码函数：ascii 语法: ascii(string str)返回值: int说明：返回字符串 str 中第一个字符的 ascii 码 举例：hive&gt; select ascii(‘ba’) from lxw1234; OK98 base64 字符串 语法: base64(binary bin)返回值: string说明：返回二进制 bin 的 base 编码字符串 举例：hive&gt; select base64(binary(‘lxw1234’)) from lxw1234; OKbHh3MTIzNA== 字符串连接函数：concat 语法: concat(string A, string B…) 返回值: string 说明：返回输入字符串连接后的结果，支持任意个输入字符串 举例：hive&gt; select concat(‘abc’,’def’,’gh’) from lxw1234;abcdefgh 带分隔符字符串连接函数：concat_ws 语法: concat_ws(string SEP, string A, string B…)返回值: string说明：返回输入字符串连接后的结果，SEP 表示各个字符串间的分隔符 举例：hive&gt; select concat_ws(‘,’,’abc’,’def’,’gh’) from lxw1234;abc,def,gh 数组转换成字符串的函数：concat_ws 语法: concat_ws(string SEP, array)返回值: string说明：返回将数组链接成字符串后的结果，SEP 表示各个字符串间的分隔符 举例：hive&gt; select concat_ws(‘|’,array(‘a’,’b’,’c’)) from lxw1234; OKa|b|c 小数位格式化成字符串函数：format_number 语法: format_number(number x, int d)返回值: string说明：将数值 x 的小数位格式化成 d 位，四舍五入 举例：hive&gt; select format_number(5.23456,3) from lxw1234; OK5.235 字符串截取函数：substr,substring 语法: substr(string A, int start),substring(string A, int start)返回值: string说明：返回字符串 A 从 start 位置到结尾的字符串 举例：hive&gt; select substr(‘abcde’,3) from lxw1234;cdehive&gt; select substring(‘abcde’,3) from lxw1234;cdehive&gt; select substr(‘abcde’,-1) from lxw1234; （和 ORACLE 相同）e 字符串截取函数：substr,substring 语法: substr(string A, int start, int len),substring(string A, int start, int len)返回值: string说明：返回字符串 A 从 start 位置开始，长度为 len 的字符串 举例：hive&gt; select substr(‘abcde’,3,2) from lxw1234;cdhive&gt; select substring(‘abcde’,3,2) from lxw1234;cdhive&gt;select substring(‘abcde’,-2,2) from lxw1234;de 字符串查找函数：instr 语法: instr(string str, string substr)返回值: int说明：返回字符串 substr 在 str 中首次出现的位置 举例：hive&gt; select instr(‘abcdf’,’df’) from lxw1234; OK4 字符串长度函数：length 语法: length(string A) 返回值: int 说明：返回字符串的长度 举例：hive&gt; select length(‘abc’) from lxw1234; OK3 字符串查找函数：locate 语法: locate(string substr, string str[, int pos])返回值: int 说明：返回字符串 substr 在 str 中从 pos 后查找，首次出现的位置 举例：hive&gt; select locate(‘a’,’abcda’,1) from lxw1234; OK1hive&gt; select locate(‘a’,’abcda’,2) from lxw1234; OK5 字符串格式化函数：printf 语法: printf(String format, Obj… args)返回值: string说明：将指定对象用 format 格式进行格式化.举例：hive&gt; select printf(“%08X”,123) from lxw1234; OK0000007B 字符串转换成 map 函数：str_to_map 语法: str_to_map(text[, delimiter1, delimiter2]) 返回值: map 说明：将字符串按照给定的分隔符转换成 map 结构. 举例：hive&gt; select str_to_map(‘k1:v1,k2:v2’) from lxw1234; OK{“k2″:”v2″,”k1″:”v1”}hive&gt; select str_to_map(‘k1=v1,k2=v2′,’,’,’=’) from lxw1234; OK{“k2″:”v2″,”k1″:”v1″} base64 解码函数：unbase64(string str) 语法: unbase64(string str)返回值: binary说明：将给定的 base64 字符串解码成二进制. 举例：hive&gt; select unbase64(‘bHh3MTIzNA==’) from lxw1234; OKlxw1234 字符串转大写函数：upper,ucase 语法: upper(string A) ucase(string A)返回值: string说明：返回字符串 A 的大写格式 举例：hive&gt; select upper(‘abSEd’) from lxw1234; ABSEDhive&gt; select ucase(‘abSEd’) from lxw1234; ABSED 字符串转小写函数：lower,lcase 语法: lower(string A) lcase(string A)返回值: string说明：返回字符串 A 的小写格式 举例：hive&gt; select lower(‘abSEd’) from lxw1234;absedhive&gt; select lcase(‘abSEd’) from lxw1234;absed 去空格函数：trim 语法: trim(string A) 返回值: string 说明：去除字符串两边的空格 举例：hive&gt; select trim(‘ abc ‘) from lxw1234;abc 左边去空格函数：ltrim 语法: ltrim(string A) 返回值: string 说明：去除字符串左边的空格 举例：hive&gt; select ltrim(‘ abc ‘) from lxw1234;abc 右边去空格函数：rtrim 语法: rtrim(string A) 返回值: string 说明：去除字符串右边的空格 举例：hive&gt; select rtrim(‘ abc ‘) from lxw1234;abc 正则表达式替换函数：regexp_replace 语法: regexp_replace(string A, string B, string C)返回值: string说明：将字符串 A 中的符合 java 正则表达式 B 的部分替换为 C。注意，在有些情况下要 使用转义字符,类似 oracle 中的 regexp_replace 函数。举例：hive&gt; select regexp_replace(‘foobar’, ‘oo|ar’, ”) from lxw1234;fb 正则表达式解析函数：regexp_extract 语法: regexp_extract(string subject, string pattern, int index)返回值: string说明：将字符串 subject 按照 pattern 正则表达式的规则拆分，返回 index 指定的字符。 举例：hive&gt; select regexp_extract(‘foothebar’, ‘foo(.*?)(bar)’, 1) from lxw1234;the hive&gt; select regexp_extract(‘foothebar’, ‘foo(.?)(bar)’, 2) from lxw1234;barhive&gt; select regexp_extract(‘foothebar’, ‘foo(.?)(bar)’, 0) from lxw1234;foothebar注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是 java 正则表 达式的规则。select data_field, regexp_extract(data_field,’.?bgStart\\=([^&amp;]+)’,1) as aaa, regexp_extract(data_field,’.?contentLoaded_headStart\\=([^&amp;]+)’,1) as bbb, regexp_extract(data_field,’.*?AppLoad2Req\\=([^&amp;]+)’,1) as cccfrom pt_nginx_loginlog_st where pt = ‘2012-03-26’ limit 2; URL 解析函数：parse_url 语法: parse_url(string urlString, string partToExtract [, string keyToExtract])返回值: string说明：返回 URL 中指定的部分。partToExtract 的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO.举例：hive&gt; select parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1’, ‘HOST’) from lxw1234;facebook.comhive&gt; select parse_url(‘http://facebook.com/path1/p.php?k1=v1&amp;k2=v2#Ref1’, ‘QUERY’, ‘k1’)from lxw1234;v1 json 解析函数：get_json_object 语法: get_json_object(string json_string, string path)返回值: string说明：解析 json 的字符串 json_string,返回 path 指定的内容。如果输入的 json 字符串无 效，那么返回 NULL。举例：hive&gt; select get_json_object(‘{“store”: {“fruit”:[{“weight”:8,”type”:”apple”},{“weight”:9,”type”:”pear”}],“bicycle”:{“price”:19.95,”color”:”red”}},“email”:”amy@only_for_json_udf_test.net”, “owner”:”amy”}‘,’$.owner’) from lxw1234;amy 空格字符串函数：space 语法: space(int n) 返回值: string 说明：返回长度为 n 的字符串 举例：hive&gt; select space(10) from lxw1234;hive&gt; select length(space(10)) from lxw1234;10 重复字符串函数：repeat 语法: repeat(string str, int n)返回值: string说明：返回重复 n 次后的 str 字符串 举例：hive&gt; select repeat(‘abc’,5) from lxw1234;abcabcabcabcabc 左补足函数：lpad 语法: lpad(string str, int len, string pad)返回值: string说明：将 str 进行用 pad 进行左补足到 len 位 举例：hive&gt; select lpad(‘abc’,10,’td’) from lxw1234;tdtdtdtabc注意：与 GP，ORACLE 不同，pad 不能默认 右补足函数：rpad 语法: rpad(string str, int len, string pad)返回值: string说明：将 str 进行用 pad 进行右补足到 len 位 举例：hive&gt; select rpad(‘abc’,10,’td’) from lxw1234;abctdtdtdt 分割字符串函数: split 语法: split(string str, string pat)返回值: array说明: 按照 pat 字符串分割 str，会返回分割后的字符串数组 举例：hive&gt; select split(‘abtcdtef’,’t’) from lxw1234; [“ab”,”cd”,”ef”] 集合查找函数: find_in_set 语法: find_in_set(string str, string strList)返回值: int说明: 返回 str 在 strlist 第一次出现的位置，strlist 是用逗号分割的字符串。如果没有找 该 str 字符，则返回 0举例：hive&gt; select find_in_set(‘ab’,’ef,ab,de’) from lxw1234;2hive&gt; select find_in_set(‘at’,’ef,ab,de’) from lxw1234;0 30.分词函数：sentences 语法: sentences(string str, string lang, string locale) 返回值: array 说明：返回输入 str 分词后的单词数组 举例：hive&gt; select sentences(‘hello word!hello hive,hi hive,hello hive’) from lxw1234; OK[[“hello”,”word”],[“hello”,”hive”,”hi”,”hive”,”hello”,”hive”]] 分词后统计一起出现频次最高的 TOP-K 语法: ngrams(array, int N, int K, int pf)返回值: array","tags":[{"name":"hive","slug":"hive","permalink":"http://yoursite.com/tags/hive/"}]},{"title":"mapreduce操作hbase","date":"2017-04-16T04:47:25.092Z","path":"2017/04/16/bigdata/hbase/mapreduce操作hbase/","text":"过程描述 就是将hbase中的数据经过mapreduce之后，再存入hbase的过程 在map端，读取到的是hbase的一行的记录，以row key为map的key，以一行记录的结果作为map的value 在reduce端最终要写入到hbase中，所以写将相当于hbase中的put，所以reduce端写出到hbase中要指定row key和put对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177package cn.itcast_01_hbase;import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.io.ImmutableBytesWritable;import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;import org.apache.hadoop.hbase.mapreduce.TableMapper;import org.apache.hadoop.hbase.mapreduce.TableReducer;import org.apache.hadoop.hbase.util.Bytes;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;/** * mapreduce操作hbase * @author wilson * */public class HBaseMr &#123; /** * 创建hbase配置 */ static Configuration config = null; static &#123; config = HBaseConfiguration.create(); config.set(&quot;hbase.zookeeper.quorum&quot;, &quot;slave1,slave2,slave3&quot;); config.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;); &#125; /** * 表信息 */ public static final String tableName = &quot;word&quot;;//表名1 public static final String colf = &quot;content&quot;;//列族 public static final String col = &quot;info&quot;;//列 public static final String tableName2 = &quot;stat&quot;;//表名2 /** * 初始化表结构，及其数据 */ public static void initTB() &#123; HTable table=null; HBaseAdmin admin=null; try &#123; admin = new HBaseAdmin(config);//创建表管理 /*删除表*/ if (admin.tableExists(tableName)||admin.tableExists(tableName2)) &#123; System.out.println(&quot;table is already exists!&quot;); admin.disableTable(tableName); admin.deleteTable(tableName); admin.disableTable(tableName2); admin.deleteTable(tableName2); &#125; /*创建表*/ HTableDescriptor desc = new HTableDescriptor(tableName); HColumnDescriptor family = new HColumnDescriptor(colf); desc.addFamily(family); admin.createTable(desc); HTableDescriptor desc2 = new HTableDescriptor(tableName2); HColumnDescriptor family2 = new HColumnDescriptor(colf); desc2.addFamily(family2); admin.createTable(desc2); /*插入数据*/ table = new HTable(config,tableName); table.setAutoFlush(false); table.setWriteBufferSize(500); List&lt;Put&gt; lp = new ArrayList&lt;Put&gt;(); Put p1 = new Put(Bytes.toBytes(&quot;1&quot;)); p1.add(colf.getBytes(), col.getBytes(), (&quot;The Apache Hadoop software library is a framework&quot;).getBytes()); lp.add(p1); Put p2 = new Put(Bytes.toBytes(&quot;2&quot;)); p2.add(colf.getBytes(),col.getBytes(),(&quot;The common utilities that support the other Hadoop modules&quot;).getBytes()); lp.add(p2); Put p3 = new Put(Bytes.toBytes(&quot;3&quot;)); p3.add(colf.getBytes(), col.getBytes(),(&quot;Hadoop by reading the documentation&quot;).getBytes()); lp.add(p3); Put p4 = new Put(Bytes.toBytes(&quot;4&quot;)); p4.add(colf.getBytes(), col.getBytes(),(&quot;Hadoop from the release page&quot;).getBytes()); lp.add(p4); Put p5 = new Put(Bytes.toBytes(&quot;5&quot;)); p5.add(colf.getBytes(), col.getBytes(),(&quot;Hadoop on the mailing list&quot;).getBytes()); lp.add(p5); table.put(lp); table.flushCommits(); lp.clear(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; try &#123; if(table!=null)&#123; table.close(); &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * MyMapper 继承 TableMapper * TableMapper&lt;Text,IntWritable&gt; * Text:输出的key类型， * IntWritable：输出的value类型 */ public static class MyMapper extends TableMapper&lt;Text, IntWritable&gt; &#123; private static IntWritable one = new IntWritable(1); private static Text word = new Text(); @Override //输入的类型为：key：rowKey； value：一行数据的结果集Result protected void map(ImmutableBytesWritable key, Result value, Context context) throws IOException, InterruptedException &#123; //获取一行数据中的colf：col String words = Bytes.toString(value.getValue(Bytes.toBytes(colf), Bytes.toBytes(col)));// 表里面只有一个列族，所以我就直接获取每一行的值 //按空格分割 String itr[] = words.toString().split(&quot; &quot;); //循环输出word和1 for (int i = 0; i &lt; itr.length; i++) &#123; word.set(itr[i]); context.write(word, one); &#125; &#125; &#125; /** * MyReducer 继承 TableReducer * TableReducer&lt;Text,IntWritable&gt; * Text:输入的key类型， * IntWritable：输入的value类型， * ImmutableBytesWritable：输出类型，表示rowkey的类型 */ public static class MyReducer extends TableReducer&lt;Text, IntWritable, ImmutableBytesWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //对mapper的数据求和 int sum = 0; for (IntWritable val : values) &#123;//叠加 sum += val.get(); &#125; // 创建put，设置rowkey为单词 Put put = new Put(Bytes.toBytes(key.toString())); // 封装数据 put.add(Bytes.toBytes(colf), Bytes.toBytes(col),Bytes.toBytes(String.valueOf(sum))); //写到hbase,需要指定rowkey、put context.write(new ImmutableBytesWritable(Bytes.toBytes(key.toString())),put); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; config.set(&quot;df.default.name&quot;, &quot;hdfs://master:9000/&quot;);//设置hdfs的默认路径 config.set(&quot;hadoop.job.ugi&quot;, &quot;hadoop,hadoop&quot;);//用户名，组 config.set(&quot;mapred.job.tracker&quot;, &quot;master:9001&quot;);//设置jobtracker在哪 //初始化表 initTB();//初始化表 //创建job Job job = new Job(config, &quot;HBaseMr&quot;);//job job.setJarByClass(HBaseMr.class);//主类 //创建scan Scan scan = new Scan(); //可以指定查询某一列 scan.addColumn(Bytes.toBytes(colf), Bytes.toBytes(col)); //创建查询hbase的mapper，设置表名、scan、mapper类、mapper的输出key、mapper的输出value TableMapReduceUtil.initTableMapperJob(tableName, scan, MyMapper.class,Text.class, IntWritable.class, job); //创建写入hbase的reducer，指定表名、reducer类、job TableMapReduceUtil.initTableReducerJob(tableName2, MyReducer.class, job); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"hbase的shell操作","date":"2017-04-16T04:47:25.091Z","path":"2017/04/16/bigdata/hbase/hbase的shell操作/","text":"官网shellhttps://learnhbase.wordpress.com/2013/03/02/hbase-shell-commands/ 1.进入客户端1234567#打开CLI#$HBASE_HOME/bin/hbase shell#退出#$HBASE_HOME/bin/hbase shell……&gt;quit 2.帮助命令help1234567891011help &quot;COMMAND&quot;#如查看describe的帮助文档hbase(main):008:0&gt; help &apos;describe&apos;Describe the named table. For example: hbase&gt; describe &apos;t1&apos; hbase&gt; describe &apos;ns1:t1&apos; Alternatively, you can use the abbreviated &apos;desc&apos; for the same thing. hbase&gt; desc &apos;t1&apos; hbase&gt; desc &apos;ns1:t1&apos; 3.创建表create12345678910#create &apos;表名&apos;, &apos;列族名1&apos;,&apos;列族名2&apos;,&apos;列族名N&apos;hbase(main):002:0&gt; create &apos;user&apos;,&apos;info1&apos;,&apos;info2&apos;#查看hbase(main):003:0&gt; listTABLE user 1 row(s) in 0.0120 seconds =&gt; [&quot;user&quot;] 4.查看所有表list123456hbase(main):003:0&gt; listTABLE user 1 row(s) in 0.0120 seconds =&gt; [&quot;user&quot;] 5.描述表describe123456789hbase(main):004:0&gt; describe &apos;user&apos;Table user is ENABLED COLUMN FAMILIES DESCRIPTION &#123;NAME =&gt; &apos;info1&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSIONS =&gt; &apos;1&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;FOREVER&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, BLOCKSIZE =&gt;&apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125; &#123;NAME =&gt; &apos;info2&apos;, DATA_BLOCK_ENCODING =&gt; &apos;NONE&apos;, BLOOMFILTER =&gt; &apos;ROW&apos;, REPLICATION_SCOPE =&gt; &apos;0&apos;, VERSIONS =&gt; &apos;1&apos;, COMPRESSION =&gt; &apos;NONE&apos;, MIN_VERSIONS =&gt; &apos;0&apos;, TTL =&gt; &apos;FOREVER&apos;, KEEP_DELETED_CELLS =&gt; &apos;FALSE&apos;, BLOCKSIZE =&gt;&apos;65536&apos;, IN_MEMORY =&gt; &apos;false&apos;, BLOCKCACHE =&gt; &apos;true&apos;&#125; 6.判断表存在exists123#exists &apos;表名&apos;hbase(main):011:0&gt; exists &apos;user&apos;Table user does exist 7.判断是否禁用表is_enabled/enable&emsp;在删除表的时候要先禁用表123456789101112131415161718192021222324#is_enabled &apos;表名&apos; hbase(main):012:0&gt; is_enabled &apos;user&apos;true 0 row(s) in 0.0410 seconds#is_disabled &apos;表名&apos;hbase(main):013:0&gt; is_disabled &apos;user&apos;false 0 row(s) in 0.0160 seconds/*If you want to delete a table or change its settings, as well as in some other situations, you need to disable the table first, using the disable command. */hbase(main):008:0&gt; disable &apos;test&apos;0 row(s) in 1.1820 seconds hbase(main):009:0&gt; enable &apos;test&apos;0 row(s) in 0.1770 seconds#如在drop之前，需要disable 表hbase(main):011:0&gt; drop &apos;test&apos;0 row(s) in 0.1370 seconds 8.添加记录put12345678910#put ‘表名’, ‘rowKey’, ‘列族 : 列‘ , &apos;值&apos; hbase(main):003:0&gt; put &apos;user&apos;,&apos;1234&apos;,&apos;info1:name&apos;,&apos;zhangsan&apos;0 row(s) in 0.3500 seconds #查看rowkey对应的值hbase(main):004:0&gt; get &apos;user&apos;,&apos;1234&apos;COLUMN CELL info1:name timestamp=1480639410845, value=zhangsan 1 row(s) in 0.0610 seconds 9.获取数据get1234567891011121314151617181920212223242526272829#查看记录rowkey下的所有数据#get &apos;表名&apos; , &apos;rowKey&apos;hbase(main):007:0&gt; get &apos;user&apos;,&apos;1234&apos;COLUMN CELL info1:address timestamp=1480639492886, value=beijing info1:age timestamp=1480639472692, value=22 info1:name timestamp=1480639410845, value=zhangsan #获取某个列族#get &apos;表名&apos;,&apos;rowkey&apos;,&apos;列族&apos;hbase(main):011:0&gt; get &apos;user&apos;,&apos;1234&apos;,&apos;info1&apos;COLUMN CELL info1:address timestamp=1480639492886, value=beijing info1:age timestamp=1480639472692, value=22 info1:name timestamp=1480639410845, value=zhangsan#获取某个列族的某个列#get &apos;表名&apos;,&apos;rowkey&apos;,&apos;列族：列’hbase(main):013:0&gt; get &apos;user&apos;,&apos;1234&apos;,&apos;info1:name&apos;COLUMN CELL info1:name timestamp=1480639410845, value=zhangsan 或者hbase(main):012:0&gt; get &apos;user&apos;, &apos;1234&apos;, &#123;COLUMN=&gt;&apos;info1:name&apos;&#125;&apos;备注:COLUMN 和 COLUMNS 是不同的,scan 操作中的 COLUMNS 指定的是表的列族, get操作中的 COLUMN 指定的是特定的列,COLUMNS 的值实质上为“列族:列修饰符”。COLUMN 和 COLUMNS 必须为大写&apos; 10.查看表中的记录总数count123456789101112#查看所有记录hbase(main):015:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:address, timestamp=1480639492886, value=beijing 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan abcd column=info2:name, timestamp=1480639913995, value=anglebady #count &apos;表名&apos;hbase(main):014:0&gt; count &apos;user&apos;2 row(s) in 0.1410 seconds #因为所有记录中的行健只有2个，所以只有2条记录 11.删除delete123456789101112131415161718192021222324252627282930313233343536373839404142434445#删除列记录#delete ‘表名’ ,‘行键’ , ‘列族：列&apos;&apos;删除前&apos;hbase(main):015:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:address, timestamp=1480639492886, value=beijing 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan abcd column=info2:name, timestamp=1480639913995, value=anglebady 2 row(s) in 0.0920 seconds &apos;删除&apos;hbase(main):016:0&gt; deletedelete delete_all_snapshot delete_snapshot deleteallhbase(main):016:0&gt; delete &apos;user&apos;,&apos;1234&apos;,&apos;info1:address&apos; &apos;删除后&apos;hbase(main):017:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan abcd column=info2:name, timestamp=1480639913995, value=anglebady #删除整行#deleteall &apos;表名&apos;,&apos;rowkey&apos;&apos;删除前&apos;hbase(main):020:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan abcd column=info2:name, timestamp=1480639913995, value=anglebady 2 row(s) in 0.0310 seconds &apos;删除&apos;hbase(main):021:0&gt; deleteall &apos;user&apos;,&apos;abcd&apos;0 row(s) in 0.0280 seconds &apos;删除后&apos;hbase(main):022:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan 12.清空表truncate1234#truncate &apos;表名&apos;hbase(main):023:0&gt; help &apos;truncate&apos; Disables, drops and recreates the specified table. 13.查看所有记录scan1234567891011121314151617181920212223242526272829#查看所有记录#scan &quot;表名&quot; hbase(main):008:0&gt; scan &apos;user&apos;ROW COLUMN+CELL 1234 column=info1:address, timestamp=1480639492886, value=beijing 1234 column=info1:age, timestamp=1480639472692, value=22 1234 column=info1:name, timestamp=1480639410845, value=zhangsan #查看某个表某个列中所有数据#scan &quot;表名&quot; , &#123;COLUMNS=&gt;&apos;列族名:列名&apos;&#125; COLUMNS 必须为大写 hbase(main):028:0&gt; scan &apos;user&apos;,&#123;COLUMNS=&gt;&apos;info1:name&apos;&#125;ROW COLUMN+CELL 1234 column=info1:name, timestamp=1480639410845, value=zhangsan abcd column=info1:name, timestamp=1480640586515, value=angebady #其他 hbase&gt; scan &apos;hbase:meta&apos; #全表扫描 hbase&gt; scan &apos;hbase:meta&apos;, &#123;COLUMNS =&gt; &apos;info:regioninfo&apos;&#125; #全表的regioninfo列 hbase&gt; scan &apos;ns1:t1&apos;, &#123;COLUMNS =&gt; [&apos;c1&apos;, &apos;c2&apos;], LIMIT =&gt; 10, STARTROW =&gt; &apos;xyz&apos;&#125; hbase&gt; scan &apos;t1&apos;, &#123;COLUMNS =&gt; [&apos;c1&apos;, &apos;c2&apos;], LIMIT =&gt; 10, STARTROW =&gt; &apos;xyz&apos;&#125; hbase&gt; scan &apos;t1&apos;, &#123;COLUMNS =&gt; &apos;c1&apos;, TIMERANGE =&gt; [1303668804, 1303668904]&#125; hbase&gt; scan &apos;t1&apos;, &#123;REVERSED =&gt; true&#125; hbase&gt; scan &apos;t1&apos;, &#123;FILTER =&gt; &quot;(PrefixFilter (&apos;row2&apos;) AND (QualifierFilter (&gt;=, &apos;binary:xyz&apos;))) AND (TimestampsFilter ( 123, 456))&quot;&#125; hbase&gt; scan &apos;t1&apos;, &#123;FILTER =&gt;org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)&#125; hbase&gt; scan &apos;t1&apos;, &#123;CONSISTENCY =&gt; &apos;TIMELINE&apos;&#125; 14.更新记录1就是重写一遍put，进行覆盖，hbase没有修改，都是追加 15.注意 get是查询一条记录，所以要指定rowkey scan是全表扫描，查询多条记录可以查询某些行的列","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"HBase的API示例2","date":"2017-04-16T04:47:25.089Z","path":"2017/04/16/bigdata/hbase/HBase的API示例2/","text":"1.配置HBaseConfiguration包：org.apache.hadoop.hbase.HBaseConfiguration作用：通过此类可以对HBase进行配置用法实例：12Configuration config = HBaseConfiguration.create();#说明： HBaseConfiguration.create() 默认会从classpath 中查找 hbase-site.xml 中的配置信息，初始化Configuration。 使用方法:123456static Configuration config = null;static &#123; config = HBaseConfiguration.create(); config.set(&quot;hbase.zookeeper.quorum&quot;, &quot;slave1,slave2,slave3&quot;); config.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);&#125; 2.表管理类HBaseAdmin包：org.apache.hadoop.hbase.client.HBaseAdmin作用：提供接口关系HBase 数据库中的表信息 用法：1HBaseAdmin admin = new HBaseAdmin(config); 3.表描述类HTableDescriptor包：org.apache.hadoop.hbase.HTableDescriptor作用：HTableDescriptor 类包含了表的名字以及表的列族信息,表的schema（设计）用法：12HTableDescriptor htd =new HTableDescriptor(tablename);htd.addFamily(new HColumnDescriptor(“myFamily”)); 4.列族的描述类HColumnDescriptor包：org.apache.hadoop.hbase.HColumnDescriptor作用：HColumnDescriptor 维护列族的信息 用法：1htd.addFamily(new HColumnDescriptor(“myFamily”)); 5.创建表的操作123456789101112131415CreateTable（一般我们用shell创建表）static Configuration config = null;static &#123; config = HBaseConfiguration.create(); config.set(&quot;hbase.zookeeper.quorum&quot;, &quot;slave1,slave2,slave3&quot;); config.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);&#125; HBaseAdmin admin = new HBaseAdmin(config);HTableDescriptor desc = new HTableDescriptor(tableName);HColumnDescriptor family1 = new HColumnDescriptor(“f1”);HColumnDescriptor family2 = new HColumnDescriptor(“f2”);desc.addFamily(family1);desc.addFamily(family2);admin.createTable(desc); 6.删除表123HBaseAdmin admin = new HBaseAdmin(config);admin.disableTable(tableName);admin.deleteTable(tableName); 7.创建一个表的类HTable包：org.apache.hadoop.hbase.client.HTable作用：HTable 和 HBase 的表通信用法：12345// 普通获取表 HTable table = new HTable(config,Bytes.toBytes(tablename);// 通过连接池获取表Connection connection = ConnectionFactory.createConnection(config);HTableInterface table = connection.getTable(TableName.valueOf(&quot;user&quot;)); 8.单条插入数据Put包：org.apache.hadoop.hbase.client.Put作用：插入数据用法：Put put = new Put(row);p.add(family,qualifier,value);说明：向表 tablename 添加 “family,qualifier,value”指定的值。 示例代码： 12345Connection connection = ConnectionFactory.createConnection(config);HTableInterface table = connection.getTable(TableName.valueOf(&quot;user&quot;));Put put = new Put(Bytes.toBytes(rowKey));put.add(Bytes.toBytes(family), Bytes.toBytes(qualifier),Bytes.toBytes(value));table.put(put); 9.批量插入123456//批量插入List&lt;Put&gt; list = new ArrayList&lt;Put&gt;();Put put = new Put(Bytes.toBytes(rowKey));//获取put，用于插入put.add(Bytes.toBytes(family), Bytes.toBytes(qualifier),Bytes.toBytes(value));//封装信息list.add(put);table.put(list);//添加记录 10.删除数据Delete包：org.apache.hadoop.hbase.client.Delete作用：删除给定rowkey的数据用法：1234567Delete del= new Delete(Bytes.toBytes(rowKey));table.delete(del);代码实例Connection connection = ConnectionFactory.createConnection(config);HTableInterface table = connection.getTable(TableName.valueOf(&quot;user&quot;));Delete del= new Delete(Bytes.toBytes(rowKey));table.delete(del); 11.单条查询Get包：org.apache.hadoop.hbase.client.Get作用：获取单个行的数据用法：HTable table = new HTable(config,Bytes.toBytes(tablename));Get get = new Get(Bytes.toBytes(row));Result result = table.get(get);说明：获取 tablename 表中 row 行的对应数据 代码示例：1234567891011Connection connection = ConnectionFactory.createConnection(config);HTableInterface table = connection.getTable(TableName.valueOf(&quot;user&quot;));Get get = new Get(rowKey.getBytes());Result row = table.get(get);for (KeyValue kv : row.raw()) &#123;System.out.print(new String(kv.getRow()) + &quot; &quot;);System.out.print(new String(kv.getFamily()) + &quot;:&quot;);System.out.print(new String(kv.getQualifier()) + &quot; = &quot;);System.out.print(new String(kv.getValue()));System.out.print(&quot; timestamp = &quot; + kv.getTimestamp() + &quot;\\n&quot;);&#125; 12.批量查询ResultScanner包：org.apache.hadoop.hbase.client.ResultScanner作用：获取值的接口用法：ResultScanner scanner = table.getScanner(scan);For(Result rowResult : scanner){ Bytes[] str = rowResult.getValue(family,column);}说明：循环获取行中列值。 代码示例：12345678910111213141516Connection connection = ConnectionFactory.createConnection(config);HTableInterface table = connection.getTable(TableName.valueOf(&quot;user&quot;));Scan scan = new Scan();scan.setStartRow(&quot;a1&quot;.getBytes());scan.setStopRow(&quot;a20&quot;.getBytes());ResultScanner scanner = table.getScanner(scan);for (Result row : scanner) &#123;System.out.println(&quot;\\nRowkey: &quot; + new String(row.getRow())); for (KeyValue kv : row.raw()) &#123; System.out.print(new String(kv.getRow()) + &quot; &quot;); System.out.print(new String(kv.getFamily()) + &quot;:&quot;); System.out.print(new String(kv.getQualifier()) + &quot; = &quot;); System.out.print(new String(kv.getValue())); System.out.print(&quot; timestamp = &quot; + kv.getTimestamp() + &quot;\\n&quot;); &#125;&#125; 13.hbase过滤器13.1.FilterListFilterList 代表一个过滤器列表，可以添加多个过滤器进行查询，多个过滤器之间的关系有：与关系（符合所有）：FilterList.Operator.MUST_PASS_ALL或关系（符合任一）：FilterList.Operator.MUST_PASS_ONE 使用方法：12345678FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ONE); Scan s1 = new Scan(); filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”), Bytes.toBytes(“c1”), CompareOp.EQUAL,Bytes.toBytes(“v1”) ) ); filterList.addFilter(new SingleColumnValueFilter(Bytes.toBytes(“f1”), Bytes.toBytes(“c2”), CompareOp.EQUAL,Bytes.toBytes(“v2”) ) ); // 添加下面这一行后，则只返回指定的cell，同一行中的其他cell不返回 s1.addColumn(Bytes.toBytes(“f1”), Bytes.toBytes(“c1”)); s1.setFilter(filterList); //设置filter ResultScanner ResultScannerFilterList = table.getScanner(s1); //返回结果列表 13.2.过滤器的种类过滤器的种类： 列植过滤器—SingleColumnValueFilter过滤列植的相等、不等、范围等 列名前缀过滤器—ColumnPrefixFilter过滤指定前缀的列名 多个列名前缀过滤器—MultipleColumnPrefixFilter过滤多个指定前缀的列名 rowKey过滤器—RowFilter通过正则，过滤rowKey值。 13.3.列植过滤器—SingleColumnValueFilterSingleColumnValueFilter 列值判断相等 (CompareOp.EQUAL ),不等(CompareOp.NOT_EQUAL),范围 (e.g., CompareOp.GREATER)…………下面示例检查列值和字符串’values’ 相等…SingleColumnValueFilter f = new SingleColumnValueFilter(Bytes.toBytes(“cFamily”), Bytes.toBytes(“column”), CompareFilter.CompareOp.EQUAL, Bytes.toBytes(“values”));s1.setFilter(f);注意：如果过滤器过滤的列在数据表中有的行中不存在，那么这个过滤器对此行无法过滤。 13.4.列名前缀过滤器—ColumnPrefixFilter过滤器—ColumnPrefixFilterColumnPrefixFilter 用于指定列名前缀值相等ColumnPrefixFilter f = new ColumnPrefixFilter(Bytes.toBytes(“values”));s1.setFilter(f); 13.5.多个列值前缀过滤器—MultipleColumnPrefixFilterMultipleColumnPrefixFilter 和 ColumnPrefixFilter 行为差不多，但可以指定多个前缀byte[][] prefixes = new byte[][] {Bytes.toBytes(“value1”),Bytes.toBytes(“value2”)};Filter f = new MultipleColumnPrefixFilter(prefixes);s1.setFilter(f); 13.6.rowKey过滤器—RowFilterRowFilter 是rowkey过滤器通常根据rowkey来指定范围时，使用scan扫描器的StartRow和StopRow方法比较好。Filter f = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(“^1234”)); //匹配以1234开头的rowkeys1.setFilter(f);","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"HBase的API示例","date":"2017-04-16T04:47:25.087Z","path":"2017/04/16/bigdata/hbase/HBase的API示例/","text":"1.常见的操作 获取表连接 创建的table 向table中put数据 修改数据 删除数据 单条查询get 全表扫描scan 列值过滤器 rowkey过滤器 匹配列名前缀 过滤器集合FilterList 2.API-示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358package cn.itcast_01_hbase;import java.util.ArrayList;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hbase.Cell;import org.apache.hadoop.hbase.CellUtil;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.MasterNotRunningException;import org.apache.hadoop.hbase.TableName;import org.apache.hadoop.hbase.ZooKeeperConnectionException;import org.apache.hadoop.hbase.client.Connection;import org.apache.hadoop.hbase.client.ConnectionFactory;import org.apache.hadoop.hbase.client.Delete;import org.apache.hadoop.hbase.client.Get;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HConnection;import org.apache.hadoop.hbase.client.HConnectionManager;import org.apache.hadoop.hbase.client.Put;import org.apache.hadoop.hbase.client.Result;import org.apache.hadoop.hbase.client.ResultScanner;import org.apache.hadoop.hbase.client.Scan;import org.apache.hadoop.hbase.client.Table;import org.apache.hadoop.hbase.filter.ColumnPrefixFilter;import org.apache.hadoop.hbase.filter.CompareFilter;import org.apache.hadoop.hbase.filter.FilterList;import org.apache.hadoop.hbase.filter.FilterList.Operator;import org.apache.hadoop.hbase.filter.RegexStringComparator;import org.apache.hadoop.hbase.filter.RowFilter;import org.apache.hadoop.hbase.filter.SingleColumnValueFilter;import org.apache.hadoop.hbase.util.Bytes;import org.junit.After;import org.junit.Before;import org.junit.Test;public class HbaseTest &#123; /** * 配置ss */ static Configuration config = null; private Connection connection = null; private Table table = null; @Before public void init() throws Exception &#123; config = HBaseConfiguration.create();// 配置 config.set(&quot;hbase.zookeeper.quorum&quot;, &quot;master,work1,work2&quot;);// zookeeper地址 config.set(&quot;hbase.zookeeper.property.clientPort&quot;, &quot;2181&quot;);// zookeeper端口 connection = ConnectionFactory.createConnection(config); table = connection.getTable(TableName.valueOf(&quot;user&quot;)); &#125; /** * 创建一个表 * * @throws Exception */ @Test public void createTable() throws Exception &#123; // 创建表管理类 HBaseAdmin admin = new HBaseAdmin(config); // hbase表管理 // 创建表描述类 TableName tableName = TableName.valueOf(&quot;test3&quot;); // 表名称 HTableDescriptor desc = new HTableDescriptor(tableName); // 创建列族的描述类 HColumnDescriptor family = new HColumnDescriptor(&quot;info&quot;); // 列族 // 将列族添加到表中 desc.addFamily(family); HColumnDescriptor family2 = new HColumnDescriptor(&quot;info2&quot;); // 列族 // 将列族添加到表中 desc.addFamily(family2); // 创建表 admin.createTable(desc); // 创建表 &#125; @Test @SuppressWarnings(&quot;deprecation&quot;) public void deleteTable() throws MasterNotRunningException, ZooKeeperConnectionException, Exception &#123; HBaseAdmin admin = new HBaseAdmin(config); //删除表之前先禁用表 admin.disableTable(&quot;test3&quot;); admin.deleteTable(&quot;test3&quot;); admin.close(); &#125; /** * 向hbase中增加数据 * * @throws Exception */ @SuppressWarnings(&#123; &quot;deprecation&quot;, &quot;resource&quot; &#125;) @Test public void insertData() throws Exception &#123; //关闭自动刷新 table.setAutoFlushTo(false); table.setWriteBufferSize(534534534); ArrayList&lt;Put&gt; arrayList = new ArrayList&lt;Put&gt;(); for (int i = 21; i &lt; 50; i++) &#123; Put put = new Put(Bytes.toBytes(&quot;1234&quot;+i));//设置行健 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), Bytes.toBytes(&quot;wangwu&quot;+i)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;), Bytes.toBytes(1234+i)); arrayList.add(put); &#125; //插入数据 table.put(arrayList); //提交 table.flushCommits(); &#125; /** * 修改数据 * * @throws Exception */ @Test public void uodateData() throws Exception &#123; Put put = new Put(Bytes.toBytes(&quot;1234&quot;));//设置行健 put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;namessss&quot;), Bytes.toBytes(&quot;lisi1234&quot;)); put.add(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;), Bytes.toBytes(1234)); //插入数据 table.put(put); //提交 table.flushCommits(); &#125; /** * 删除数据 * * @throws Exception */ @Test public void deleteDate() throws Exception &#123; Delete delete = new Delete(Bytes.toBytes(&quot;1234&quot;));//设置行健 table.delete(delete); table.flushCommits(); &#125; /** * 单条查询 * * @throws Exception */ @Test public void queryData() throws Exception &#123; Get get = new Get(Bytes.toBytes(&quot;1234&quot;)); Result result = table.get(get); System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;)))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;namessss&quot;)))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;sex&quot;)))); &#125; /** * 全表扫描 * * @throws Exception */ @Test public void scanData() throws Exception &#123; Scan scan = new Scan(); //scan.addFamily(Bytes.toBytes(&quot;info&quot;)); //scan.addColumn(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;)); //设置起始row_key scan.setStartRow(Bytes.toBytes(&quot;wangsf_0&quot;)); scan.setStopRow(Bytes.toBytes(&quot;wangwu&quot;)); ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;)))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;)))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;password&quot;)))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)))); &#125; &#125; /** * 全表扫描的过滤器 * 列值过滤器 * * @throws Exception */ @Test public void scanDataByFilter1() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //过滤器：列值过滤器 SingleColumnValueFilter filter = new SingleColumnValueFilter(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL,//列名 Bytes.toBytes(&quot;zhangsan2&quot;));//列值 // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;)))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;)))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;password&quot;)))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)))); &#125; &#125; /** * rowkey过滤器 * @throws Exception */ @Test public void scanDataByFilter2() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //匹配rowkey以12341开头的 RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;^12341&quot;)); // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;password&quot;)))); System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;)))); //System.out.println(Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;password&quot;)))); //System.out.println(Bytes.toString(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)))); &#125; &#125; /** * 匹配列名前缀 * @throws Exception */ @Test public void scanDataByFilter3() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //匹配rowkey以wangsenfeng开头的 ColumnPrefixFilter filter = new ColumnPrefixFilter(Bytes.toBytes(&quot;na&quot;)); // 设置过滤器 scan.setFilter(filter); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(&quot;rowkey：&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;info:name：&quot; + Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;)))); // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;)) != null) &#123; System.out.println(&quot;info:age：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;sex&quot;)) != null) &#123; System.out.println(&quot;infi:sex：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;sex&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)) != null) &#123; System.out .println(&quot;info2:name：&quot; + Bytes.toString(result.getValue( Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;age&quot;)) != null) &#123; System.out.println(&quot;info2:age：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;age&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;sex&quot;)) != null) &#123; System.out.println(&quot;info2:sex：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;sex&quot;)))); &#125; &#125; &#125; /** * 过滤器集合 * @throws Exception */ @Test public void scanDataByFilter4() throws Exception &#123; // 创建全表扫描的scan Scan scan = new Scan(); //过滤器集合：MUST_PASS_ALL（and）,MUST_PASS_ONE(or) FilterList filterList = new FilterList(Operator.MUST_PASS_ONE);//至少一个条件满足 //匹配rowkey以wangsenfeng开头的 RowFilter filter = new RowFilter(CompareFilter.CompareOp.EQUAL, new RegexStringComparator(&quot;^wangsenfeng&quot;)); //匹配name的值等于zhangsan SingleColumnValueFilter filter2 = new SingleColumnValueFilter(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;), CompareFilter.CompareOp.EQUAL, Bytes.toBytes(&quot;zhangsan&quot;)); filterList.addFilter(filter); filterList.addFilter(filter2); // 设置过滤器 scan.setFilter(filterList); // 打印结果集 ResultScanner scanner = table.getScanner(scan); for (Result result : scanner) &#123; System.out.println(&quot;rowkey：&quot; + Bytes.toString(result.getRow())); System.out.println(&quot;info:name：&quot; + Bytes.toString(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;name&quot;)))); // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;)) != null) &#123; System.out.println(&quot;info:age：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;age&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;sex&quot;)) != null) &#123; System.out.println(&quot;infi:sex：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info&quot;), Bytes.toBytes(&quot;sex&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)) != null) &#123; System.out .println(&quot;info2:name：&quot; + Bytes.toString(result.getValue( Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;name&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;age&quot;)) != null) &#123; System.out.println(&quot;info2:age：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;age&quot;)))); &#125; // 判断取出来的值是否为空 if (result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;sex&quot;)) != null) &#123; System.out.println(&quot;info2:sex：&quot; + Bytes.toInt(result.getValue(Bytes.toBytes(&quot;info2&quot;), Bytes.toBytes(&quot;sex&quot;)))); &#125; &#125; &#125; @After public void close() throws Exception &#123; table.close(); connection.close(); &#125;&#125;","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"HBase存储结构2","date":"2017-04-16T04:47:25.086Z","path":"2017/04/16/bigdata/hbase/HBase存储结构2/","text":"1.数据模型 1.1.行健（row key） 决定一行数据 按照字典顺序排序 row key只能存储64k的字节数据 1.2.Column Family列族 &amp; qualifier列 HBase表中的每个列都归属于某个列族，列族必须作为表模式(schema) 定义的一部分预先给出。如 create ‘test’, ‘course’； 列名以列族作为前缀，每个“列族”都可以有多个列成员(column)；如：course:math, course:english, 新的列族成员（列）可以随后按需、动态加入； 权限控制、存储以及调优都是在列族层面进行的； HBase把同一列族里面的数据存储在同一目录下，由几个文件保存。 1.3. Timestamp时间戳 在HBase每个cell存储单元对同一份数据有多个版本，根据唯一的时间戳来区分每个版本之间的差异，不同版本的数据按照时间倒序排序，最新的数据版本排在最前面。 时间戳的类型是 64位整型 时间戳可以由HBase(在数据写入时自动)赋值，此时时间戳是精确到毫秒的当前系统时间。 时间戳也可以由客户显式赋值，如果应用程序要避免数据版本冲突，就必须自己生成具有唯一性的时间戳。 1.4.Cell单元格 由行和列的坐标交叉决定 单元格是有版本的 单元格的内容是未解析的字节数组由{row key， column( = +)， version} 唯一确定的单元。 cell中的数据是没有类型的，全部是字节码形式存贮 1.5.HLog(WAL log) HLog文件就是一个普通的Hadoop Sequence File，Sequence File 的Key是HLogKey对象，HLogKey中记录了写入数据的归属信息，除了table和region名字外，同时还包括 sequence number和timestamp，timestamp是”写入时间”，sequence number的起始值为0，或者是最近一次存入文件系统中sequence number。 HLog SequeceFile的Value是HBase的KeyValue对象，即对应HFile中的KeyValue。 2. HBase体系架构 2.1.Client 包含访问HBase的接口并维护cache来加快对HBase的访问2.2.Zookeeper 保证任何时候，集群中只有一个master 存贮所有Region的寻址入口。 实时监控Region server的上线和下线信息。 并实时通知Master 存储HBase的schema和table元数据 2.3.Master 为Region server分配region 负责Region server的负载均衡 发现失效的Region server并重新分配其上的region 管理用户对table的增删改操作 2.4.RegionServer Region server维护region，处理对这些region的IO请求 Region server负责切分在运行过程中变得过大的region 2.5.Region HBase自动把表水平划分成多个区域(region)，每个region会保存一个表里面某段连续的数据；每个表一开始只有一个region，随着数据不断插入表，region不断增大，当增大到一个阀值的时候，region就会等分会两个新的region（裂变）； 当table中的行不断增多，就会有越来越多的region。 这样一张完整的表被保存在多个Regionserver 上。 2.6.Memstore 与 storefile 一个region由多个store组成，一个store对应一个CF（列族） store包括位于内存中的memstore和位于磁盘的storefile写操作先写入memstore，当memstore中的数据达到某个阈值，hregionserver会启动flashcache进程写入storefile，每次写入形成单独的一个storefile 当storefile文件的数量增长到一定阈值后，系统会进行合并（minor、major compaction），在合并过程中会进行版本合并和删除工作（majar），形成更大的storefile 当一个region所有storefile的大小和超过一定阈值后，会把当前的region分割为两个，并由hmaster分配到相应的regionserver服务器，实现负载均衡 客户端检索数据，先在memstore找，找不到再找storefile HRegion是HBase中分布式存储和负载均衡的最小单元。 最小单元就表示不同的HRegion可以分布在不同的 HRegion server上。 HRegion由一个或者多个Store组成，每个store保存一个columns family。 一个Store对应的是一个列簇，列簇中有StoreFile（在Hbase中是StoreFile，而在HDFS中叫HFile） 每个Strore又由一个memStore和0至多个StoreFile组成。 如图：StoreFile以HFile格式保存在HDFS上。","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"HBase存储结构(转)","date":"2017-04-16T04:47:25.084Z","path":"2017/04/16/bigdata/hbase/HBase存储结构1/","text":"转自:HBase笔记：存储结构 从HBase的架构图上可以看出，HBase中的存储包括HMaster、HRegionServer、HRegion、Store、MemStore、StoreFile、HFile、HLog等，本篇文章统一介绍他们的作用即存储结构。 以下是网络上流传的HBase存储架构图: HBase中的每张表都通过行键按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过256M就要被分割成两个，这个过程由HRegionServer管理，而HRegion的分配由HMaster管理。 HMaster的作用： 为Region server分配region 负责Region server的负载均衡 发现失效的Region server并重新分配其上的region HDFS上的垃圾文件回收 处理schema更新请求 HRegionServer作用： 维护master分配给他的region，处理对这些region的io请求 负责切分正在运行过程中变的过大的region 可以看到，client访问hbase上的数据并不需要master参与（寻址访问zookeeper和region server，数据读写访问region server），master仅仅维护table和region的元数据信息（table的元数据信息保存在zookeeper上），负载很低。 HRegionServer存取一个子表时，会创建一个HRegion对象，然后对表的每个列族创建一个Store实例，每个Store都会有一个MemStore和0个或多个StoreFile与之对应，每个StoreFile都会对应一个HFile， HFile就是实际的存储文件。因此，一个HRegion有多少个列族就有多少个Store。 一个HRegionServer会有多个HRegion和一个HLog。 HRegiontable在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。 Region按大小分隔，每个表一行是只有一个region。随着数据不断插入表，region不断增大，当region的某个列族达到一个阈值（默认256M）时就会分成两个新的region。 每个region由以下信息标识： &lt;表名,startRowkey,创建时间&gt; 由目录表(-ROOT-和.META.)可知该region的endRowkey HRegion定位：Region被分配给哪个Region Server是完全动态的，所以需要机制来定位Region具体在哪个region server。HBase使用三层结构来定位region： 1、 通过zk里的文件/hbase/rs得到-ROOT-表的位置。-ROOT-表只有一个region。 2、通过-ROOT-表查找.META.表的第一个表中相应的region的位置。其实-ROOT-表是.META.表的第一个region；.META.表中的每一个region在-ROOT-表中都是一行记录。 3、通过.META.表找到所要的用户表region的位置。用户表中的每个region在.META.表中都是一行记录。 -ROOT-表永远不会被分隔为多个region，保证了最多需要三次跳转，就能定位到任意的region。client会讲查询的位置信息保存缓存起来，缓存不会主动失效，因此如果client上的缓存全部失效，则需要进行6次网络来回，才能定位到正确的region，其中3次用来发现缓存失效，另外三次用来获取位置信息。 Store每一个region有一个或多个store组成，至少是一个store，hbase会把一起访问的数据放在一个store里面，即为每个ColumnFamily建一个store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者多个StoreFile组成。 HBase以store的大小来判断是否需要切分region。 MemStorememStore 是放在内存里的。保存修改的数据即keyValues。当memStore的大小达到一个阀值（默认64MB）时，memStore会被flush到文件，即生成一个快照。目前hbase 会有一个线程来负责memStore的flush操作。 StoreFilememStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile(其实在hdfs叫hfile,在hbase叫storefile)的格式保存。 HFileHBase中KeyValue数据的存储格式，是hadoop的二进制格式文件。 首先HFile文件是不定长的，长度固定的只有其中的两块：Trailer和FileInfo。Trailer中又指针指向其他数据块的起始点，FileInfo记录了文件的一些meta信息。 Data Block是hbase io的基本单元，为了提高效率，HRegionServer中又基于LRU的block cache机制。每个Data块的大小可以在创建一个Table的时候通过参数指定（默认块大小64KB），大号的Block有利于顺序Scan，小号的Block利于随机查询。每个Data块除了开头的Magic以外就是一个个KeyValue对拼接而成，Magic内容就是一些随机数字，目的是防止数据损坏，结构如下。 HFile结构图如下： Data Block段用来保存表中的数据，这部分可以被压缩。 Meta Block段（可选的）用来保存用户自定义的kv段，可以被压缩。 FileInfo段用来保存HFile的元信息，本能被压缩，用户也可以在这一部分添加自己的元信息。 Data Block Index段（可选的）用来保存Meta Blcok的索引。 Trailer这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会首先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，然后，DataBlock Index会被读取到内存中，这样，当检索某个key时，不需要扫描整个HFile，而只需从内存中找到key所在的block，通过一次磁盘io将整个 block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰。 HFile的Data Block，Meta Block通常采用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是需要花费cpu进行压缩和解压缩。目标HFile的压缩支持两种方式：gzip、lzo。 另外，针对目前针对现有HFile的两个主要缺陷： a) 暂用过多内存b) 启动加载时间缓慢 提出了HFile Version2设计：https://issues.apache.org/jira/secure/attachment/12478329/hfile_format_v2_design_draft_0.1.pdf HLog其实HLog文件就是一个普通的Hadoop Sequence File， Sequence File的value是key时HLogKey对象，其中记录了写入数据的归属信息，除了table和region名字外，还同时包括sequence number和timestamp，timestamp是写入时间，equence number的起始值为0，或者是最近一次存入文件系统中的equence number。 Sequence File的value是HBase的KeyValue对象，即对应HFile中的KeyValue。 HLog(WAL log)：WAL意为write ahead log，用来做灾难恢复使用，HLog记录数据的所有变更，一旦region server 宕机，就可以从log中进行恢复。 LogFlusher 前面提到，数据以KeyValue形式到达HRegionServer，将写入WAL，之后，写入一个SequenceFile。看过去没问题，但是因为数据流在写入文件系统时，经常会缓存以提高性能。这样，有些本以为在日志文件中的数据实际在内存中。这里，我们提供了一个LogFlusher的类。它调用HLog.optionalSync(),后者根据 hbase.regionserver.optionallogflushinterval (默认是10秒)，定期调用Hlog.sync()。另外，HLog.doWrite()也会根据 hbase.regionserver.flushlogentries (默认100秒)定期调用Hlog.sync()。Sync() 本身调用HLog.Writer.sync()，它由SequenceFileLogWriter实现。 LogRoller Log的大小通过$HBASE_HOME/conf/hbase-site.xml 的 hbase.regionserver.logroll.period 限制，默认是一个小时。所以每60分钟，会打开一个新的log文件。久而久之，会有一大堆的文件需要维护。首先，LogRoller调用HLog.rollWriter()，定时滚动日志，之后，利用HLog.cleanOldLogs()可以清除旧的日志。它首先取得存储文件中的最大的sequence number，之后检查是否存在一个log所有的条目的“sequence number”均低于这个值，如果存在，将删除这个log。 每个region server维护一个HLog，而不是每一个region一个，这样不同region（来自不同的table）的日志会混在一起，这样做的目的是不断追加单个文件相对于同时写多个文件而言，可以减少磁盘寻址次数，因此可以提高table的写性能。带来麻烦的时，如果一个region server下线，为了恢复其上的region，需要讲region server上的log进行拆分，然后分发到其他region server上进行恢复。","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"hbase分布式安装","date":"2017-04-16T04:47:25.083Z","path":"2017/04/16/bigdata/hbase/hbase分布式安装/","text":"1.安装前准备 安装Hadoop和zookeeper 启动Hadoop的hdfs和yarn 启动zookeeper的zkServer 2.机器规划表 hdp-node-01 NameNode dataNode resourceManger secondaryNameNode zookeeper Hmaster - hdp-node-02 - dataNode - - zookeeper - regionserver hdp-node-03 - dataNode - - zookeeper - regionserver 3.安装hbase123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#解压，并创建软链接tar -zxvf hbase-0.99.2-bin.tar.gz ln -s hbase-0.99.2/ hbase#删除软件包，因为占用空间rm -rf hbase-0.99.2-bin.tar.gz#添加环境变量vim /etc/profile #HBASE_HOMEexport HBASE_HOME=/home/hadoop/app/hbaseexport PATH=$PATH:$HBASE_HOME/bin#使生效source /etc/profile# vim hbase-env.shexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_80export HBASE_MANAGES_ZK=false //表示使用外部的zookeeper，而不是使用hbase自带的zookeeper# vim hbase-site.xml#####################################################################&lt;configuration&gt;&lt;property&gt;&lt;name&gt;hbase.master&lt;/name&gt; #hbasemaster的主机和端口&lt;value&gt;hdp-node-01:60000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.master.maxclockskew&lt;/name&gt; #时间同步允许的时间差&lt;value&gt;180000&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.rootdir&lt;/name&gt;&lt;value&gt;hdfs://hdp-node-01:9000/hbase&lt;/value&gt; #hdfs目录路径&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.cluster.distributed&lt;/name&gt; #是否分布式运行&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;&lt;value&gt;hdp-node-01,hdp-node-02,hdp-node-03&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; #zookeeper地址&lt;value&gt;/home/hadoop/hbase/tmp/zookeeper&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;###################################################################### vim ./conf/regionservershdp-node-01hdp-node-02hdp-node-03#需要将hbase的目录拷贝到另外两台机器上scp -r /home/hadoop/app/hbase hdp-node-02:/home/hadoop/app/scp -r /home/hadoop/app/hbase hdp-node-03:/home/hadoop/app/#启动hbase（在一个机器启动即可）start-hbase.sh stop-hbase.sh #停止 4.测试进入命令行 1hbase shell 页面监控 http://master:60010/","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"HBase之数据写入流程解析(转)","date":"2017-04-16T04:47:25.081Z","path":"2017/04/16/bigdata/hbase/HBase之数据写入流程解析(转)/","text":"转自:HBase － 数据写入流程解析 众所周知，HBase默认适用于写多读少的应用，正是依赖于它相当出色的写入性能：一个100台RS的集群可以轻松地支撑每天10T的写入量。当然，为了支持更高吞吐量的写入，HBase还在不断地进行优化和修正，这篇文章结合0.98版本的源码全面地分析HBase的写入流程，全文分为三个部分，第一部分介绍客户端的写入流程，第二部分介绍服务器端的写入流程，最后再重点分析WAL的工作原理。 客户端流程解析（1）用户提交put请求后，HBase客户端会将put请求添加到本地buffer中，符合一定条件就会通过AsyncProcess异步批量提交。HBase默认设置autoflush=true，表示put请求直接会提交给服务器进行处理；用户可以设置autoflush=false，这样的话put请求会首先放到本地buffer，等到本地buffer大小超过一定阈值（默认为2M，可以通过配置文件配置）之后才会提交。很显然，后者采用group commit机制提交请求，可以极大地提升写入性能，但是因为没有保护机制，如果客户端崩溃的话会导致提交的请求丢失。 （2）在提交之前，HBase会在元数据表.meta.中根据rowkey找到它们归属的region server，这个定位的过程是通过HConnection的locateRegion方法获得的。如果是批量请求的话还会把这些rowkey按照HRegionLocation分组，每个分组可以对应一次RPC请求。 （3）HBase会为每个HRegionLocation构造一个远程RPC请求MultiServerCallable，然后通过rpcCallerFactory. newCaller()执行调用，忽略掉失败重新提交和错误处理，客户端的提交操作到此结束。 服务器端流程解析服务器端RegionServer接收到客户端的写入请求后，首先会反序列化为Put对象，然后执行各种检查操作，比如检查region是否是只读、memstore大小是否超过blockingMemstoreSize等。检查完成之后，就会执行如下核心操作： （1）获取行锁、Region更新共享锁： HBase中使用行锁保证对同一行数据的更新都是互斥操作，用以保证更新的原子性，要么更新成功，要么失败。 （2）开始写事务：获取write number，用于实现MVCC，实现数据的非锁定读，在保证读写一致性的前提下提高读取性能。 （3）写缓存memstore：HBase中每列族都会对应一个store，用来存储该列数据。每个store都会有个写缓存memstore，用于缓存写入数据。HBase并不会直接将数据落盘，而是先写入缓存，等缓存满足一定大小之后再一起落盘。 （4）Append HLog：HBase使用WAL机制保证数据可靠性，即首先写日志再写缓存，即使发生宕机，也可以通过恢复HLog还原出原始数据。该步骤就是将数据构造为WALEdit对象，然后顺序写入HLog中，此时不需要执行sync操作。0.98版本采用了新的写线程模式实现HLog日志的写入，可以使得整个数据更新性能得到极大提升，具体原理见下一个章节。 （5）释放行锁以及共享锁 （6）Sync HLog：HLog真正sync到HDFS，在释放行锁之后执行sync操作是为了尽量减少持锁时间，提升写性能。如果Sync失败，执行回滚操作将memstore中已经写入的数据移除。 （7）结束写事务：此时该线程的更新操作才会对其他读请求可见，更新才实际生效。具体分析见上一篇文章《HBase – 并发控制深度解析》 （8）flush memstore：当写缓存满64M之后，会启动flush线程将数据刷新到硬盘。刷新操作涉及到HFile相关结构，后面会详细对此进行介绍。 WAL机制解析WAL(Write-Ahead Logging)是一种高效的日志算法，几乎是所有非内存数据库提升写性能的不二法门，基本原理是在数据写入之前首先顺序写入日志，然后再写入缓存，等到缓存写满之后统一落盘。之所以能够提升写性能，是因为WAL将一次随机写转化为了一次顺序写加一次内存写。提升写性能的同时，WAL可以保证数据的可靠性，即在任何情况下数据不丢失。假如一次写入完成之后发生了宕机，即使所有缓存中的数据丢失，也可以通过恢复日志还原出丢失的数据。 WAL持久化等级 HBase中可以通过设置WAL的持久化等级决定是否开启WAL机制、以及HLog的落盘方式。WAL的持久化等级分为如下四个等级： SKIP_WAL：只写缓存，不写HLog日志。这种方式因为只写内存，因此可以极大的提升写入性能，但是数据有丢失的风险。在实际应用过程中并不建议设置此等级，除非确认不要求数据的可靠性。 ASYNC_WAL：异步将数据写入HLog日志中。 SYNC_WAL：同步将数据写入日志文件中，需要注意的是数据只是被写入文件系统中，并没有真正落盘。 FSYNC_WAL：同步将数据写入日志文件并强制落盘。最严格的日志写入等级，可以保证数据不会丢失，但是性能相对比较差。 USER_DEFAULT：默认如果用户没有指定持久化等级，HBase使用SYNC_WAL等级持久化数据。 用户可以通过客户端设置WAL持久化等级，代码：put.setDurability(Durability. SYNC_WAL ); HLog数据结构 HBase中，WAL的实现类为HLog，每个Region Server拥有一个HLog日志，所有region的写入都是写到同一个HLog。下图表示同一个Region Server中的3个 region 共享一个HLog。当数据写入时，是将数据对按照顺序追加到HLog中，以获取最好的写入性能。 上图中HLogKey主要存储了log sequence number，更新时间 write time，region name，表名table name以及cluster ids。其中log sequncece number作为HFile中一个重要的元数据，和HLog的生命周期息息相关，后续章节会详细介绍；region name和table name分别表征该段日志属于哪个region以及哪张表；cluster ids用于将日志复制到集群中其他机器上。 WALEdit用来表示一个事务中的更新集合，在之前的版本，如果一个事务中对一行row R中三列c1，c2，c3分别做了修改，那么hlog中会有3个对应的日志片段如下所示：12345&lt;logseq1-for-edit1&gt;:&lt;keyvalue-for-edit-c1&gt;&lt;logseq2-for-edit2&gt;:&lt;keyvalue-for-edit-c2&gt;&lt;logseq3-for-edit3&gt;:&lt;keyvalue-for-edit-c3&gt; 然而，这种日志结构无法保证行级事务的原子性，假如刚好更新到c2之后发生宕机，那么就会产生只有部分日志写入成功的现象。为此，hbase将所有对同一行的更新操作都表示为一个记录，如下：1&lt;logseq#-for-entire-txn&gt;:&lt;WALEdit-for-entire-txn&gt; 其中WALEdit会被序列化为格式&lt;-1, # of edits, &lt;KeyValue&gt;, &lt;KeyValue&gt;, &lt;KeyValue&gt;&gt;，比如&lt;-1, 3, &lt;keyvalue-for-edit-c1&gt;, &lt;keyvalue-for-edit-c2&gt;, &lt;keyvalue-for-edit-c3&gt;&gt;，其中-1作为标示符表征这种新的日志结构。 WAL写入模型 了解了HLog的结构之后，我们就开始研究HLog的写入模型。HLog的写入可以分为三个阶段，首先将数据对写入本地缓存，然后再将本地缓存写入文件系统，最后执行sync操作同步到磁盘。在以前老的写入模型中，上述三步都由工作线程独自完成，如下图所示： 上图中，本地缓存写入文件系统那个步骤工作线程需要持有updateLock执行，不同工作线程之间必然会恶性竞争；不仅如此，在Sync HDFS这步中，工作线程之间需要抢占flushLock，因为Sync操作是一个耗时操作，抢占这个锁会导致写入性能大幅降低。 所幸的是，来自中国（准确的来说，是来自小米，鼓掌）的3位工程师意识到了这个问题，进而提出了一种新的写入模型并被官方采纳。根据官方测试，新写入模型的吞吐量比之前提升3倍多，单台RS写入吞吐量介于12150～31520，5台RS组成的集群写入吞吐量介于22000～70000（见HBASE-8755）。下图是小米官方给出来的对比测试结果： 在新写入模型中，本地缓存写入文件系统以及Sync HDFS都交给了新的独立线程完成，并引入一个Notify线程通知工作线程是否已经Sync成功，采用这种机制消除上述锁竞争，具体如下图所示： 上文中提到工作线程在写入WALEdit之后并没有进行Sync，而是等到释放行锁阻塞在syncedTillHere变量上，等待AsyncNotifier线程唤醒。 工作线程将WALEdit写入本地Buffer之后，会生成一个自增变量txid，携带此txid唤醒AsyncWriter线程 AsyncWriter线程会取出本地Buffer中的所有WALEdit，写入HDFS。注意该线程会比较传入的txid和已经写入的最大txid（writtenTxid），如果传入的txid小于writteTxid，表示该txid对应的WALEdit已经写入，直接跳过 AsyncWriter线程将所有WALEdit写入HDFS之后携带maxTxid唤醒AsyncFlusher线程 AsyncFlusher线程将所有写入文件系统的WALEdit统一Sync刷新到磁盘 数据全部落盘之后调用setFlushedTxid方法唤醒AyncNotifier线程 AyncNotifier线程会唤醒所有阻塞在变量syncedTillHere的工作线程，工作线程被唤醒之后表示WAL写入完成，后面再执行MVCC结束写事务，推进全局读取点，本次更新才会对用户可见 通过上述过程的梳理可以知道，新写入模型采取了多线程模式独立完成写文件系统、sync磁盘操作，避免了之前多工作线程恶性抢占锁的问题。同时，工作线程在将WALEdit写入本地Buffer之后并没有马上阻塞，而是释放行锁之后阻塞等待WALEdit落盘，这样可以尽可能地避免行锁竞争，提高写入性能。 总结本文首先介绍了HBase的写入流程，之后重点分析了WAL的写入模型以及相关优化。希望借此能够对HBase写入的高性能特性能够理解。后面一篇文章会接着介绍写入到memstore的数据如何真正的落盘，敬请期","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"hbase1.x之后的架构之META和ROOT","date":"2017-04-16T04:47:25.069Z","path":"2017/04/16/bigdata/hbase/hbase1.x之后的架构之META和ROOT/","text":"原来是通过-ROOT-table找到.META. ，然后通过.META.表找到Register 现在：直接向存放register的信息存放在hbase:meta （以前叫.META.） 而hbase:meta 表在zookeeper中存放着The hbase:meta table (previously called .META.) keeps a list of all regions ，now stored in ZooKeeper. The hbase:meta table structure is as follows: Key Region key of the format ([table],[region start key],[region id]) Values info:regioninfo (serialized HRegionInfo instance for this region) info:server (server:port of the RegionServer containing this region) info:serverstartcode (start-time of the RegionServer process containing this region) client 通过找hbase:meta table 找到RegionServers After locating the required region(s), the client contacts the RegionServer serving that region, rather than going through the master 找到的信心将被缓存在客户端，以便后续请求不需要经过查找过程。 如果RegionServers 重新分配，或者是宕机，那么才需要重新查找","tags":[{"name":"hbase","slug":"hbase","permalink":"http://yoursite.com/tags/hbase/"}]},{"title":"运行一个mapreduce例子程序","date":"2017-04-16T04:47:25.060Z","path":"2017/04/16/bigdata/hadoop/运行一个mapreduce例子程序/","text":"hadoop-mapreduce-examples-2.6.4.jar是一个hadoop中自带的一个词频统计程序 1234567891011121314#例子程序的路径share/hadoop/mapreduce/#需要启动hdfs和yarn./sbin/start-dfs.sh./sbin/start-yarn.sh#执行程序hadoop jar hadoop-mapreduce-examples-2.6.4.jar wordcount /wordcount/input/ /wordcount/outputwordcount 是运行的主类(会找wordcount中的main) /wordcount/input/ 输入目录 /wordcount/output 输出目录","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"运营商日志增强之自定义outputFormat","date":"2017-04-16T04:47:25.059Z","path":"2017/04/16/bigdata/hadoop/运营商日志增强之自定义outputFormat/","text":"1.需求现有一些原始日志需要做增强解析处理，流程：1、从原始日志文件中读取数据2、根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志3、如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录 如图所示： 2.分析程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 3.实现实现要点：1、在mapreduce中访问外部资源2、自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write() 代码实现如下：数据库获取数据的工具 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package cn.itcast.bigdata.mr.logenhance;import java.sql.Connection;import java.sql.DriverManager;import java.sql.ResultSet;import java.sql.Statement;import java.util.HashMap;import java.util.Map;public class DBLoader &#123; public static void dbLoader(Map&lt;String, String&gt; ruleMap) throws Exception &#123; Connection conn = null; Statement st = null; ResultSet res = null; try &#123; //注册驱动 Class.forName(&quot;com.mysql.jdbc.Driver&quot;); conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/urldb&quot;, &quot;root&quot;, &quot;root&quot;); st = conn.createStatement(); res = st.executeQuery(&quot;select url,content from url_rule&quot;); while (res.next()) &#123; ruleMap.put(res.getString(1), res.getString(2)); &#125; &#125; finally &#123; try&#123; if(res!=null)&#123; res.close(); &#125; if(st!=null)&#123; st.close(); &#125; if(conn!=null)&#123; conn.close(); &#125; &#125;catch(Exception e)&#123; e.printStackTrace(); &#125; &#125; &#125;&#125; 自定义一个outputformat 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.itcast.bigdata.mr.logenhance;import java.io.IOException;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.RecordWriter;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * maptask或者reducetask在最终输出时，先调用OutputFormat的getRecordWriter方法拿到一个RecordWriter * 然后再调用RecordWriter的write(k,v)方法将数据写出 */public class LogEnhanceOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt; &#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter(TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSystem fs = FileSystem.get(context.getConfiguration()); //指定要写的path Path enhancePath = new Path(&quot;D:/temp/en/log.dat&quot;); Path tocrawlPath = new Path(&quot;D:/temp/crw/url.dat&quot;); //指定要写的hdfs流 FSDataOutputStream enhancedOs = fs.create(enhancePath); FSDataOutputStream tocrawlOs = fs.create(tocrawlPath); return new EnhanceRecordWriter(enhancedOs, tocrawlOs); &#125; /** * 构造一个自己的recordwriter */ static class EnhanceRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream enhancedOs = null; FSDataOutputStream tocrawlOs = null; public EnhanceRecordWriter(FSDataOutputStream enhancedOs, FSDataOutputStream tocrawlOs) &#123; super(); this.enhancedOs = enhancedOs; this.tocrawlOs = tocrawlOs; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; String result = key.toString(); // 如果要写出的数据是待爬的url，则写入待爬清单文件 /logenhance/tocrawl/url.dat if (result.contains(&quot;tocrawl&quot;)) &#123; //写流数据 tocrawlOs.write(result.getBytes()); &#125; else &#123; // 如果要写出的数据是增强日志，则写入增强日志文件 /logenhance/enhancedlog/log.dat enhancedOs.write(result.getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; if (tocrawlOs != null) &#123; tocrawlOs.close(); &#125; if (enhancedOs != null) &#123; enhancedOs.close(); &#125; &#125; &#125;&#125; 开发mapreduce处理流程123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package cn.itcast.bigdata.mr.logenhance;import java.io.IOException;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Counter;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 这个程序是对每个小时不断产生的用户上网记录日志进行增强(将日志中的url所指向的网页内容分析结果信息追加到每一行原始日志后面) */public class LogEnhance &#123; static class LogEnhanceMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; Map&lt;String, String&gt; ruleMap = new HashMap&lt;String, String&gt;(); Text k = new Text(); NullWritable v = NullWritable.get(); /** * maptask在初始化时会先调用setup方法一次 利用这个机制，将外部的知识库加载到maptask执行的机器内存中 * 从数据库中加载规则信息倒ruleMap中 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; try &#123; DBLoader.dbLoader(ruleMap); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 获取一个计数器用来记录不合法的日志行数, 组名, 计数器名称 Counter counter = context.getCounter(&quot;malformed&quot;, &quot;malformedline&quot;); String line = value.toString(); String[] fields = StringUtils.split(line, &quot;\\t&quot;); try &#123; String url = fields[26]; //对这一行日志中的url去知识库中查找内容分析信息 String content_tag = ruleMap.get(url); // 判断内容标签是否为空，如果为空，则只输出url到待爬清单；如果有值，则输出到增强日志 if (content_tag == null) &#123; k.set(url + &quot;\\t&quot; + &quot;tocrawl&quot; + &quot;\\n&quot;);// 输往待爬清单的内容 context.write(k, v); &#125; else &#123;// 输往增强日志的内容 k.set(line + &quot;\\t&quot; + content_tag + &quot;\\n&quot;); context.write(k, v); &#125; &#125; catch (Exception exception) &#123; counter.increment(1); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(LogEnhance.class); job.setMapperClass(LogEnhanceMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要控制不同的内容写往不同的目标路径，可以采用自定义outputformat的方法 job.setOutputFormatClass(LogEnhanceOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(&quot;D:/srcdata/webloginput/&quot;)); // 尽管我们用的是自定义outputformat，但是它是继承制fileoutputformat // 在fileoutputformat中，必须输出一个_success文件，所以在此还需要设置输出path FileOutputFormat.setOutputPath(job, new Path(&quot;D:/temp/output/&quot;)); // 不需要reducer job.setNumReduceTasks(0); job.waitForCompletion(true); System.exit(0); &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"计数器应用","date":"2017-04-16T04:47:25.057Z","path":"2017/04/16/bigdata/hadoop/计数器应用/","text":"在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现示例代码如下： 12345678910111213141516171819202122public class MultiOutputs &#123;//通过枚举形式定义自定义计数器enum MyCounter&#123;MALFORORMED,NORMAL&#125; static class CommaMapper extends Mapper&lt;LongWritable, Text, Text, LongWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split(&quot;,&quot;); for (String word : words) &#123; context.write(new Text(word), new LongWritable(1)); &#125; //对枚举定义的自定义计数器加1 context.getCounter(MyCounter.MALFORORMED).increment(1); //通过动态设置自定义计数器加1 context.getCounter(&quot;counterGroupa&quot;, &quot;countera&quot;).increment(1); //groupName, counterName &#125; &#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"自定义可序列化的Bean","date":"2017-04-16T04:47:25.056Z","path":"2017/04/16/bigdata/hadoop/自定义可序列化的Bean/","text":"1.应用场景统计每一个用户（手机号）所耗费的总上行流量、下行流量，总流量1234567891363157985066 13726230503(手机号) 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481（上行流量） 24681（上行流量） 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 2001363154400022 13926251106 5C-0E-8B-8B-B1-50:CMCC 120.197.40.4 4 0 240 0 2001363157993044 18211575961 94-71-AC-CD-E6-18:CMCC-EASY 120.196.100.99 iface.qiyi.com 视频网站 15 12 1527 2106 200 2.mapreduce1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package cn.itcast.bigdata.mr.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCount &#123; static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将一行内容转成string String line = value.toString(); //切分字段 String[] fields = line.split(&quot;\\t&quot;); //取出手机号 String phoneNbr = fields[1]; //取出上行流量下行流量 long upFlow = Long.parseLong(fields[fields.length-3]); long dFlow = Long.parseLong(fields[fields.length-2]); context.write(new Text(phoneNbr), new FlowBean(upFlow, dFlow)); &#125; &#125; static class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123; //&lt;183323,bean1&gt;&lt;183323,bean2&gt;&lt;183323,bean3&gt;&lt;183323,bean4&gt;....... @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_dFlow = 0; //遍历所有bean，将其中的上行流量，下行流量分别累加 for(FlowBean bean: values)&#123; sum_upFlow += bean.getUpFlow(); sum_dFlow += bean.getdFlow(); &#125; FlowBean resultBean = new FlowBean(sum_upFlow, sum_dFlow); context.write(key, resultBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resoucemanager.hostname&quot;, &quot;mini1&quot;);*/ Job job = Job.getInstance(conf); /*job.setJar(&quot;/home/hadoop/wc.jar&quot;);*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCount.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125; &#125; 3.自定义可序列化的bean12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package cn.itcast.bigdata.mr.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class FlowBean implements Writable&#123; //实现了Writable private long upFlow; private long dFlow; private long sumFlow; //反序列化时，需要反射调用空参构造函数，所以要显示定义一个 public FlowBean()&#123;&#125; public FlowBean(long upFlow, long dFlow) &#123; this.upFlow = upFlow; this.dFlow = dFlow; this.sumFlow = upFlow + dFlow; &#125; //.........get/set方法 /** * 序列化方法 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(dFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 * 注意：反序列化的顺序跟序列化的顺序完全一致 */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); dFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + dFlow + &quot;\\t&quot; + sumFlow; &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"自定义InputFormat之大量小文件的问题","date":"2017-04-16T04:47:25.055Z","path":"2017/04/16/bigdata/hadoop/自定义InputFormat之大量小文件的问题/","text":"默认情况下，TextInputFormat对任务的切片机制是按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个maptask，这样，如果有大量小文件，就会产生大量的maptask，处理效率极其低下 分析小文件的优化无非以下几种方式：1、在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS2、在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并3、在mapreduce处理时，可采用combineInputFormat提高效率 采用combineInputFormat123456/*如果不设置InputFormat，它默认用的是TextInputformat.class * 只是将多个小文件放在一个切片中，即一个maptask中 */ job.setInputFormatClass(CombineTextInputFormat.class); CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); CombineTextInputFormat.setMinInputSplitSize(job, 2097152); 自定义InputFormat实现方式:1.自定义一个InputFormat2.改写RecordReader，实现一次读取一个完整文件封装为KV3.在输出时使用SequenceFileOutPutFormat输出合并文件 代码如下：自定义InputFromat 123456789101112131415161718192021222324252627package cn.itcast.bigdata.combinefile; import java.io.IOException; import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.JobContext;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; public class WholeFileInputFormat extends FileInputFormat&lt;NullWritable, BytesWritable&gt;&#123; @Overrideprotected boolean isSplitable(JobContext context, Path file) &#123; return false;&#125; @Overridepublic RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException,InterruptedException &#123; WholeFileRecordReader reader = new WholeFileRecordReader(); reader.initialize(split, context); return reader;&#125;&#125; 自定义RecordReader 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485package cn.itcast.bigdata.combinefile;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.RecordReader;import org.apache.hadoop.mapreduce.TaskAttemptContext;import org.apache.hadoop.mapreduce.lib.input.FileSplit;/** * * RecordReader的核心工作逻辑： * 通过nextKeyValue()方法去读取数据构造将返回的key value * 通过getCurrentKey 和 getCurrentValue来返回上面构造好的key和value */class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt; &#123; private FileSplit fileSplit; private Configuration conf; private BytesWritable value = new BytesWritable(); private boolean processed = false; @Override public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123; this.fileSplit = (FileSplit) split; this.conf = context.getConfiguration(); &#125; @Override public boolean nextKeyValue() throws IOException, InterruptedException &#123; if (!processed) &#123; byte[] contents = new byte[(int) fileSplit.getLength()]; Path file = fileSplit.getPath(); FileSystem fs = file.getFileSystem(conf); FSDataInputStream in = null; try &#123; in = fs.open(file); IOUtils.readFully(in, contents, 0, contents.length); value.set(contents, 0, contents.length); &#125; finally &#123; IOUtils.closeStream(in); &#125; processed = true;//设置为true，那么将只会读取一次，也就是只是返回一个keyValue，一个文件读取完毕 return true; &#125; return false; &#125; @Override public NullWritable getCurrentKey() throws IOException, InterruptedException &#123; return NullWritable.get(); &#125; @Override public BytesWritable getCurrentValue() throws IOException, InterruptedException &#123; return value; &#125; /** * 返回当前进度 */ @Override public float getProgress() throws IOException &#123; //框架会调用该方法，返回读取的进度，因为只是读取一次（读完整个文件），所以是要么读完了，要么没有读完 return processed ? 1.0f : 0.0f; &#125; @Override public void close() throws IOException &#123; // do nothing &#125;&#125; 定义mapreduce处理流程 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package cn.itcast.bigdata.combinefile;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.conf.Configured;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.BytesWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.InputSplit;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;import org.apache.hadoop.util.Tool;import org.apache.hadoop.util.ToolRunner;public class SmallFilesToSequenceFileConverter extends Configured implements Tool &#123; static class SequenceFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt; &#123; private Text filenameKey; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; InputSplit split = context.getInputSplit(); Path path = ((FileSplit) split).getPath(); //拿到文件的路径，作为key filenameKey = new Text(path.toString()); &#125; @Override protected void map(NullWritable key, BytesWritable value, Context context) throws IOException, InterruptedException &#123; //以文件路径作为key，以RecordReader.nextKeyValue()读取的一行（其实是整个文件）作为value context.write(filenameKey, value); &#125; &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*System.setProperty(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);*/ String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length != 2) &#123; System.err.println(&quot;Usage: combinefiles &lt;in&gt; &lt;out&gt;&quot;); System.exit(2); &#125; Job job = Job.getInstance(conf,&quot;combine small files to sequencefile&quot;); job.setJarByClass(SmallFilesToSequenceFileConverter.class); //默认是 TextInputFormat job.setInputFormatClass(WholeFileInputFormat.class); /** * 这里没有指定reduce，所以会调用默认的reduce，但是如果输出的格式仍然是文本的话，那么value.toString()就会 * 是一个对象的hash地址，所以这里指定输出value的格式为字节序列：SequenceFileOutputFormat * 这样输出的文件中的格式：文件名（key) 字节序列（value） * 问题： * 1.将所以的小文件最后都以 &quot; 文件名（key) 字节序列（value） &quot;的方式输出到了一个文件中，那么最后这个文件将非常的大 * 2.最后输出的文件是字节文件，那么我们再不能使用默认的InputFormat的方式（TextInputformat）来读取文件了 */ job.setOutputFormatClass(SequenceFileOutputFormat.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(BytesWritable.class); job.setMapperClass(SequenceFileMapper.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); return job.waitForCompletion(true) ? 0 : 1; &#125; public static void main(String[] args) throws Exception &#123; args=new String[]&#123;&quot;c:/wordcount/smallinput&quot;,&quot;c:/wordcount/smallout&quot;&#125;; int exitCode = ToolRunner.run(new SmallFilesToSequenceFileConverter(), args); System.exit(exitCode); &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"自定义GroupingComparator","date":"2017-04-16T04:47:25.054Z","path":"2017/04/16/bigdata/hadoop/自定义GroupingComparator/","text":"1.GroupingComparator的工作原理首先取第一个元素，然后将其的key作为一个基础，然后通过迭代器去取第一个后面的元素，取对应元素的key和第一个元素的key比较，如果相同则将元素的value拿过来，接着去迭代后面的元素，直到取到的元素的key和第一个元素不同，那么此时就将","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"求共同的粉丝问题","date":"2017-04-16T04:47:25.052Z","path":"2017/04/16/bigdata/hadoop/求共同的粉丝问题/","text":"1.问题描述123456789101112131415&apos;以下是微博的粉丝列表数据，冒号前是一个用户，冒号后是该用户的所有粉丝（数据中的好友关系是单向的）,求任意两个人的共同粉丝&apos;A:B,C,D,F,E,O //说明B,C,D,F,E,O 都关注了A，但是A不一定都关注了B,C,D,F,E,OB:A,C,E,KC:F,A,D,ID:A,E,F,LE:B,C,D,M,LF:A,B,C,D,E,O,MG:A,C,D,E,FH:A,C,D,E,OI:A,OJ:B,OK:A,C,DL:D,E,FM:E,F,GO:A,H,I,J 2.实现步骤1234567891011121314151617181920&apos;实现步骤&apos;/*1.每一个人都关注了哪些人a--&gt;b c eb--&gt;c d e2.那么，对于：a--&gt;b c e 将关注的人两两组合b--c：a #b和c就有共同的粉丝ab--e：ac--e：a对于：b--&gt;c d ec--d：bc--e：bd--e：b3.下面的步骤：以（d--e）作为key，然后将所有的value拼接在一起，就是（d--e）的共同粉丝*/ 3.代码实现步骤11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package cn.itcast.bigdata.mr.fensi;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class SharedFriendsStepOne &#123; static class SharedFriendsStepOneMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // A:B,C,D,F,E,O String line = value.toString(); String[] person_friends = line.split(&quot;:&quot;); String person = person_friends[0]; String friends = person_friends[1]; for (String friend : friends.split(&quot;,&quot;)) &#123; // 输出&lt;好友，人&gt; context.write(new Text(friend), new Text(person)); &#125; &#125; &#125; static class SharedFriendsStepOneReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; @Override protected void reduce(Text friend, Iterable&lt;Text&gt; persons, Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text person : persons) &#123; sb.append(person).append(&quot;,&quot;); &#125; context.write(friend, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SharedFriendsStepOne.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setMapperClass(SharedFriendsStepOneMapper.class); job.setReducerClass(SharedFriendsStepOneReducer.class); FileInputFormat.setInputPaths(job, new Path(&quot;D:/srcdata/friends&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;D:/temp/out&quot;)); job.waitForCompletion(true); &#125;&#125; 步骤2,3 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package cn.itcast.bigdata.mr.fensi;import java.io.IOException;import java.util.Arrays;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class SharedFriendsStepTwo &#123; static class SharedFriendsStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; // 拿到的数据是上一个步骤的输出结果 // A I,K,C,B,G,F,H,O,D, // 友 人，人，人 @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] friend_persons = line.split(&quot;\\t&quot;); String friend = friend_persons[0]; String[] persons = friend_persons[1].split(&quot;,&quot;); //求b-c的好友:防止b-c和c-d当做不同的key,所以需要排序 Arrays.sort(persons); for (int i = 0; i &lt; persons.length - 1; i++) &#123; for (int j = i + 1; j &lt; persons.length; j++) &#123; // 发出 &lt;人-人，好友&gt; ，这样，相同的“人-人”对的所有好友就会到同1个reduce中去 context.write(new Text(persons[i] + &quot;-&quot; + persons[j]), new Text(friend)); &#125; &#125; &#125; &#125; static class SharedFriendsStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123; @Override protected void reduce(Text person_person, Iterable&lt;Text&gt; friends, Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); //将共同好友拼接 for (Text friend : friends) &#123; sb.append(friend).append(&quot; &quot;); &#125; context.write(person_person, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(SharedFriendsStepTwo.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); job.setMapperClass(SharedFriendsStepTwoMapper.class); job.setReducerClass(SharedFriendsStepTwoReducer.class); FileInputFormat.setInputPaths(job, new Path(&quot;D:/temp/out/part-r-00000&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;D:/temp/out2&quot;)); job.waitForCompletion(true); &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"模拟datanode实现大文件分割的原理","date":"2017-04-16T04:47:25.051Z","path":"2017/04/16/bigdata/hadoop/模拟datanode实现大文件分割的原理/","text":"将一个200M的文件分成两段(0-128,129-200) 开启两个socket去并发的读取文件,然后写入不同的机器","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"安全模式","date":"2017-04-16T04:47:25.050Z","path":"2017/04/16/bigdata/hadoop/安全模式/","text":"1.namenode的safemode(安全模式) namenode在刚启动的时候，内存中只有文件和文件的块ID及副本数量，不知道块所在的datanode namenode需要等待所有datanode向他汇报自身持有的块信息，namenode才能在元数据中补全块信息中的位置信息 只有当namenode找到99.8%的块位置信息，才会退出安全模式，正常对外提供服务 2.图解","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"多job串联","date":"2017-04-16T04:47:25.048Z","path":"2017/04/16/bigdata/hadoop/多job串联/","text":"一个稍复杂点的处理逻辑往往需要多个mapreduce程序串联处理，多job的串联可以借助mapreduce框架的JobControl实现 1234567891011121314151617181920212223242526272829//.........所有的job设置完毕，然后进行下面的操作 ControlledJob cJob1 = new ControlledJob(job1.getConfiguration()); ControlledJob cJob2 = new ControlledJob(job2.getConfiguration()); ControlledJob cJob3 = new ControlledJob(job3.getConfiguration()); cJob1.setJob(job1); cJob2.setJob(job2); cJob3.setJob(job3); // 设置作业依赖关系 cJob2.addDependingJob(cJob1); cJob3.addDependingJob(cJob2); JobControl jobControl = new JobControl(&quot;RecommendationJob&quot;); jobControl.addJob(cJob1); jobControl.addJob(cJob2); jobControl.addJob(cJob3); // 新建一个线程来运行已加入JobControl中的作业，开始进程并等待结束 Thread jobControlThread = new Thread(jobControl); jobControlThread.start(); while (!jobControl.allFinished()) &#123; Thread.sleep(500); &#125; jobControl.stop(); return 0; 在实际应用中，并不建议这样做，因为在代码中将3个job的关系写死了，那么想改就很难了,建议的做法是：将3个job封装成为单独的jar文件，然后使用shell去启动job，shell可以控制每个job的启动顺序，来实现job之间的关联性，这样更加的灵活。","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"倒排索引的建立","date":"2017-04-16T04:47:25.046Z","path":"2017/04/16/bigdata/hadoop/倒排索引的建立/","text":"1.需求描述有下面的文件： 123456789101112131415//a.txthello tomhello jerryhello tom //b.txthello jerryhello jerrytom jerry //c.txthello jerryhello tom 实现分词索引123hello a.txt--&gt;3 b.txt--&gt;2 c.txt--&gt;2jerry a.txt--&gt;1 b.txt--&gt;3 c.txt--&gt;1tom a.txt--&gt;2 b.txt--&gt;1 c.txt--&gt;1 2.实现步骤对所有的文件，以分词+文件名作为key统计结果1234567891011hell0--a.txt 3hell0--b.txt 2hello--c.txt 2jerry--a.txt 1jerry--b.txt 3jerry--c.txt 1tom--a.txt 2tom--b.txt 1tom--c.txt 1 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class InverIndexStepOne &#123; static class InverIndexStepOneMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; Text k = new Text(); IntWritable v = new IntWritable(1); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] words = line.split(&quot; &quot;); FileSplit inputSplit = (FileSplit) context.getInputSplit(); String fileName = inputSplit.getPath().getName(); for (String word : words) &#123; //以分词+文件名作为key输出 k.set(word + &quot;--&quot; + fileName); context.write(k, v); &#125; &#125; &#125; static class InverIndexStepOneReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(InverIndexStepOne.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputPaths(job, new Path(&quot;D:/srcdata/inverindexinput&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;D:/temp/out&quot;)); // FileInputFormat.setInputPaths(job, new Path(args[0])); // FileOutputFormat.setOutputPath(job, new Path(args[1])); job.setMapperClass(InverIndexStepOneMapper.class); job.setReducerClass(InverIndexStepOneReducer.class); job.waitForCompletion(true); &#125;&#125; 上一步中的结果，将分词作为key统计结果123hello a.txt--&gt;3 b.txt--&gt;2 c.txt--&gt;2jerry a.txt--&gt;1 b.txt--&gt;3 c.txt--&gt;1tom a.txt--&gt;2 b.txt--&gt;1 c.txt--&gt;1 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package cn.itcast.bigdata.mr.inverindex;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class IndexStepTwo &#123; public static class IndexStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] files = line.split(&quot;--&quot;); context.write(new Text(files[0]), new Text(files[1])); &#125; &#125; public static class IndexStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text text : values) &#123; sb.append(text.toString().replace(&quot;\\t&quot;, &quot;--&gt;&quot;) + &quot;\\t&quot;); &#125; context.write(key, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws Exception &#123; if (args.length &lt; 1 || args == null) &#123; args = new String[]&#123;&quot;D:/temp/out/part-r-00000&quot;, &quot;D:/temp/out2&quot;&#125;; &#125; Configuration config = new Configuration(); Job job = Job.getInstance(config); job.setMapperClass(IndexStepTwoMapper.class); job.setReducerClass(IndexStepTwoReducer.class);// job.setMapOutputKeyClass(Text.class);// job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 1:0); &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"wordcount代码实现","date":"2017-04-16T04:47:25.044Z","path":"2017/04/16/bigdata/hadoop/wordcount代码实现及运行过程解析/","text":"1.wordcount程序实现1.1.map1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.itcast.bigdata.mr.wcdemo;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * KEYIN: 默认情况下，是mr框架所读到的一行文本的起始偏移量，Long, * 但是在hadoop中有自己的更精简的序列化接口，所以不直接用Long，而用LongWritable * * VALUEIN:默认情况下，是mr框架所读到的一行文本的内容，String，同上，用Text （序列化需要） * * KEYOUT：是用户自定义逻辑处理完成之后输出数据中的key，在此处是单词，String，同上，用Text（序列化需要） * VALUEOUT：是用户自定义逻辑处理完成之后输出数据中的value，在此处是单词次数，Integer，同上，用IntWritable（序列化需要） * * @author * */public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; /** * map阶段的业务逻辑就写在自定义的map()方法中 * maptask会对每一行输入数据调用一次我们自定义的map()方法 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将maptask传给我们的文本内容先转换成String String line = value.toString(); //根据空格将这一行切分成单词 String[] words = line.split(&quot; &quot;); //将单词输出为&lt;单词，1&gt; for(String word:words)&#123; //将单词作为key，将次数1作为value，以便于后续的数据分发，可以根据单词分发，以便于相同单词会到相同的reduce task context.write(new Text(word), new IntWritable(1)); &#125; &#125; &#125; 1.2.reduce1234567891011121314151617181920212223242526272829303132333435363738394041424344package cn.itcast.bigdata.mr.wcdemo;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * KEYIN, VALUEIN 对应 mapper输出的KEYOUT,VALUEOUT类型对应 * * KEYOUT, VALUEOUT 是自定义reduce逻辑处理结果的输出数据类型 * KEYOUT是单词 * VLAUEOUT是总次数 * @author * */public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; /** * &lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt; * &lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt; * &lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt; * 入参key，是一组相同单词kv对的key */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; /*Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext())&#123; count += iterator.next().get(); &#125;*/ for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; 1.3.启动程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667package cn.itcast.bigdata.mr.wcdemo;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于一个yarn集群的客户端:分配硬件资源，启动程序 * 需要在此封装我们的mr程序的相关运行参数，指定jar包 * 最后提交给yarn * @author * */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; if (args == null || args.length == 0) &#123; args = new String[2]; args[0] = &quot;hdfs://master:9000/wordcount/input/wordcount.txt&quot;; args[1] = &quot;hdfs://master:9000/wordcount/output8&quot;; &#125; Configuration conf = new Configuration(); //设置的没有用! ??????// conf.set(&quot;HADOOP_USER_NAME&quot;, &quot;hadoop&quot;);// conf.set(&quot;dfs.permissions.enabled&quot;, &quot;false&quot;); /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resoucemanager.hostname&quot;, &quot;mini1&quot;);*/ Job job = Job.getInstance(conf); /*job.setJar(&quot;/home/hadoop/wc.jar&quot;);*/ //指定本程序的jar包所在的本地路径（因为最后是要交给yarn来跑jar的，所以这里是指定jar的路径） job.setJarByClass(WordcountDriver.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125; &#125; 2.在linux上运行一个jar123456789101112#启动java -cp wordcount.jar cn.itcast.bigdata.mr.wcdemo.wordcountDriver /wordcount/input /wordcount/output/*-cp 和 -classpath 一样，是指定类运行所依赖其他类的路径，通常是类库，jar包之类，需要全路径到jar包，window上分号“;”分隔，linux上是分号“:”分隔。不支持通配符，需要列出所有jar包，用一点“.”代表当前路径。 wordcount.jar：是启动的jar文件cn.itcast.bigdata.mr.wcdemo.wordcountDriver ：是启动的main函数所在的类/wordcount/input /wordcount/output：参数*/ 使用java -cp 去启动一个jar，需要指定所有的依赖的jar文件 ,而使用hadoop命令则会帮我们加上依赖的jar和配置文件到classpath hadoop jar wordcount.jar cn.itcast.bigdata.mr.wcdemo.WordcountDriver /wordcount/input /wordcount/output 在MyEclipse中打jar包，如果是打成的是runnable的jar，那么会将依赖打到jar包中去, 如果是打包成普通的jar，那么运行时需要指定运行程序所依赖的其他jar文件","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"shell脚本定时采集日志数据到hdfs","date":"2017-04-16T04:47:25.043Z","path":"2017/04/16/bigdata/hadoop/shell脚本定时采集日志数据到hdfs/","text":"1.需求说明点击流日志每天都10T，在业务应用服务器上，需要准实时上传至数据仓库（Hadoop HDFS）上 2.技术分析上传至hdfs 12hadoop fs –put xxxx.tar /data 定时调度 123crontab -e*/5 * * * * $home/bin/command.sh //五分钟执行一次系统会自动执行脚本，每5分钟一次，执行时判断文件是否符合上传规则，符合则上传 3.图解 4.shell脚本实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970#!/bin/bash#set java envexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_51export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH#set hadoop envexport HADOOP_HOME=/home/hadoop/app/hadoop-2.6.4export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH#版本1的问题：#虽然上传到Hadoop集群上了，但是原始文件还在。如何处理？#日志文件的名称都是xxxx.log1,再次上传文件时，因为hdfs上已经存在了，会报错。如何处理？#如何解决版本1的问题# 1、先将需要上传的文件移动到待上传目录# 2、在讲文件移动到待上传目录时，将文件按照一定的格式重名名# /export/software/hadoop.log1 /export/data/click_log/xxxxx_click_log_&#123;date&#125;#日志文件存放的目录log_src_dir=/home/hadoop/logs/log/#待上传文件存放的目录log_toupload_dir=/home/hadoop/logs/toupload/#日志文件上传到hdfs的根路径hdfs_root_dir=/data/clickLog/20151226/#打印环境变量信息echo &quot;envs: hadoop_home: $HADOOP_HOME&quot;#读取日志文件的目录，判断是否有需要上传的文件echo &quot;log_src_dir:&quot;$log_src_dirls $log_src_dir | while read fileNamedo if [[ &quot;$fileName&quot; == access.log.* ]]; then # if [ &quot;access.log&quot; = &quot;$fileName&quot; ];then date=`date +%Y_%m_%d_%H_%M_%S` #将文件移动到待上传目录并重命名 #打印信息 echo &quot;moving $log_src_dir$fileName to $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date&quot; mv $log_src_dir$fileName $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date #将待上传的文件path写入一个列表文件willDoing，有日期的（如：$log_toupload_dir/willDoing.20161129, $log_toupload_dir/willDoing.20161221, ... ） echo $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date &gt;&gt; $log_toupload_dir&quot;willDoing.&quot;$date fi done#找到列表文件willDoing($log_toupload_dir/willDoing.20161129, $log_toupload_dir/willDoing.20161221, )ls $log_toupload_dir | grep will |grep -v &quot;_COPY_&quot; | grep -v &quot;_DONE_&quot; | while read line # line = $log_toupload_dir/willDoing.20161129do #打印信息 echo &quot;toupload is in file:&quot;$line #将待上传文件列表willDoing改名为willDoing_COPY_ mv $log_toupload_dir$line $log_toupload_dir$line&quot;_COPY_&quot; #读列表文件willDoing_COPY_的内容（一个一个的待上传文件名） ,此处的line 就是列表中的一个待上传文件的path cat $log_toupload_dir$line&quot;_COPY_&quot; |while read line do #打印信息 echo &quot;puting...$line to hdfs path.....$hdfs_root_dir&quot; hadoop fs -put $line $hdfs_root_dir done mv $log_toupload_dir$line&quot;_COPY_&quot; $log_toupload_dir$line&quot;_DONE_&quot;done","tags":[{"name":"shell","slug":"shell","permalink":"http://yoursite.com/tags/shell/"},{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"reduce端join算法实现","date":"2017-04-16T04:47:25.041Z","path":"2017/04/16/bigdata/hadoop/reduce端join算法实现/","text":"场景假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL查询运算 原理阐述通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发往同一个reduce task，在reduce中进行数据的串联 案例实现两张表的left join 查询 订单数据表t_order： id date pid amount 1001 20150710 P0001 2 1002 20150710 P0001 3 1002 20150710 P0002 3 商品信息表t_product： id pname category_id price P0001 小米5 1000 2 P0002 锤子T1 1000 3 SQL语句：1select a.id,a.date,b.name,b.category_id,b.price from t_order a left join t_product b on a.pid = b.id 实现原理:将两张表的以相同的pid(商品id)作为key发送到reduce,这样在reduce,订单表里所有pid相同的在一个reduce中(这样reduce中还有一个bean是商品信息表),因为在map发送前将bean用flag区分(flag=1为商品表,flag=0为订单表),所以在reduce端将所有的flag=0的bean放入一个list中,将flag=1的封装成一个bean,然后进行遍历list拼接商品表和订单表 代码实现mapreduce123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143package cn.itcast.bigdata.mr.rjoin;import java.io.IOException;import java.util.ArrayList;import org.apache.commons.beanutils.BeanUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 订单表和商品表合到一起order.txt(订单id, 日期, 商品编号, 数量) 1001 20150710 P0001 2 1002 20150710 P0001 3 1002 20150710 P0002 3 1003 20150710 P0003 3product.txt(商品编号, 商品名字, 分类id, 价格) P0001 小米5 1001 2 P0002 锤子T1 1000 3 P0003 锤子 1002 4 */public class RJoin &#123; static class RJoinMapper extends Mapper&lt;LongWritable, Text, Text, InfoBean&gt; &#123; InfoBean bean = new InfoBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); //获取切片，可以从切片中获取文件信息 FileSplit inputSplit = (FileSplit) context.getInputSplit(); String name = inputSplit.getPath().getName(); // 通过文件名判断是哪种数据 String pid = &quot;&quot;; if (name.startsWith(&quot;order&quot;)) &#123; String[] fields = line.split(&quot;\\t&quot;); // id date pid amount pid = fields[2]; bean.set(Integer.parseInt(fields[0]), fields[1], pid, Integer.parseInt(fields[3]), &quot;&quot;, 0, 0, &quot;0&quot;); &#125; else &#123; String[] fields = line.split(&quot;\\t&quot;); // id pname category_id price pid = fields[0]; bean.set(0, &quot;&quot;, pid, 0, fields[1], Integer.parseInt(fields[2]), Float.parseFloat(fields[3]), &quot;1&quot;); &#125; k.set(pid); context.write(k, bean); &#125; &#125; static class RJoinReducer extends Reducer&lt;Text, InfoBean, InfoBean, NullWritable&gt; &#123; @Override protected void reduce(Text pid, Iterable&lt;InfoBean&gt; beans, Context context) throws IOException, InterruptedException &#123; //商品信息 InfoBean pdBean = new InfoBean(); //订单信息列表 ArrayList&lt;InfoBean&gt; orderBeans = new ArrayList&lt;InfoBean&gt;(); for (InfoBean bean : beans) &#123; if (&quot;1&quot;.equals(bean.getFlag())) &#123; //产品的 try &#123; BeanUtils.copyProperties(pdBean, bean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; else &#123; InfoBean odbean = new InfoBean(); try &#123; BeanUtils.copyProperties(odbean, bean); orderBeans.add(odbean); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; // 拼接两类数据形成最终结果 for (InfoBean bean : orderBeans) &#123; bean.setPname(pdBean.getPname()); bean.setCategory_id(pdBean.getCategory_id()); bean.setPrice(pdBean.getPrice()); context.write(bean, NullWritable.get()); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.set(&quot;mapred.textoutputformat.separator&quot;, &quot;\\t&quot;); Job job = Job.getInstance(conf); // 指定本程序的jar包所在的本地路径 // job.setJarByClass(RJoin.class);// job.setJar(&quot;c:/join.jar&quot;); job.setJarByClass(RJoin.class); // 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(RJoinMapper.class); job.setReducerClass(RJoinReducer.class); // 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(InfoBean.class); // 指定最终输出的数据的kv类型 job.setOutputKeyClass(InfoBean.class); job.setOutputValueClass(NullWritable.class); // 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); // 指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); // 将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /* job.submit(); */ boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125; 可序列化Bean12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package cn.itcast.bigdata.mr.rjoin;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class InfoBean implements Writable &#123; private int order_id; private String dateString; private String p_id; private int amount; private String pname; private int category_id; private float price; // flag=0表示这个对象是封装订单表记录 // flag=1表示这个对象是封装产品信息记录 private String flag; public InfoBean() &#123; &#125; public void set(int order_id, String dateString, String p_id, int amount, String pname, int category_id, float price, String flag) &#123; this.order_id = order_id; this.dateString = dateString; this.p_id = p_id; this.amount = amount; this.pname = pname; this.category_id = category_id; this.price = price; this.flag = flag; &#125; //get/set方法 /** * private int order_id; private String dateString; private int p_id; * private int amount; private String pname; private int category_id; * private float price; */ @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(order_id); out.writeUTF(dateString); out.writeUTF(p_id); out.writeInt(amount); out.writeUTF(pname); out.writeInt(category_id); out.writeFloat(price); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.order_id = in.readInt(); this.dateString = in.readUTF(); this.p_id = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.category_id = in.readInt(); this.price = in.readFloat(); this.flag = in.readUTF(); &#125; @Override public String toString() &#123; return &quot;order_id=&quot; + order_id + &quot;, dateString=&quot; + dateString + &quot;, p_id=&quot; + p_id + &quot;, amount=&quot; + amount + &quot;, pname=&quot; + pname + &quot;, category_id=&quot; + category_id + &quot;, price=&quot; + price + &quot;, flag=&quot; + flag; &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"namenode目录故障时解决方案","date":"2017-04-16T04:47:25.040Z","path":"2017/04/16/bigdata/hadoop/namenode目录故障时解决方案/","text":"如果我们人为的将namenode的目录删除,那么该如何处理呢? 正常情况下,mk一个目录123456789[root@hdp-node-01 dfs]# hadoop fs -mkdir /test[root@hdp-node-01 dfs]# hadoop fs -ls /Found 6 itemsdrwxr-xr-x - root supergroup 0 2016-12-02 08:26 /hbasedrwxr-xr-x - root supergroup 0 2016-11-28 10:20 /studentdrwxr-xr-x - root supergroup 0 2017-02-25 23:52 /testdrwx-wx-wx - root supergroup 0 2016-11-28 10:01 /tmpdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /userdrwxr-xr-x - root supergroup 0 2016-12-17 21:13 /wordcount namenode和SecondaryNameNode的目录确定12345678910111213141516171819202122232425262728#namenode的目录# cat /home/hadoop/app/hadoop-2.6.4/etc/hadoop/hdfs-site.xml &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name&lt;/value&gt;&lt;/property&gt;#SecondaryNameNode&lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; #如果没有指定，默认是：file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary &lt;value&gt;/home/hadoop/data/namesecondary&lt;/value&gt;&lt;/property&gt;#如果没有配置,就去看core-site.xml#cat ./etc/hadoop/core-site.xml&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; #存放hdfs的数据目录，这是一个基础的目录，如果会和namenode和datanode以及secondarynamenode的目录相关 &lt;value&gt;/home/hadoop/app/hadoop-2.6.4/hadoopdata&lt;/value&gt;&lt;/property&gt;#此文中我们没有指定SecondaryNameNode的目录,那么就是:$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary[root@hdp-node-01 namesecondary]# pwd/home/hadoop/app/hadoop-2.6.4/hadoopdata/dfs/namesecondary 只是将namenode的工作目录删除12345678910111213[root@hdp-node-01 data]# pwd/home/hadoop/data[root@hdp-node-01 data]# lltotal 12drwxr-xr-x 2 root root 4096 Dec 23 23:55 click-datadrwx------ 3 root root 4096 Feb 25 23:44 datadrwxr-xr-x 3 root root 4096 Feb 25 23:44 name[root@hdp-node-01 data]# rm -rf ./name/[root@hdp-node-01 data]# lltotal 8drwxr-xr-x 2 root root 4096 Dec 23 23:55 click-datadrwx------ 3 root root 4096 Feb 25 23:44 data 还能正常工作，如下图：因为读取的是内存的数据 12345678910[root@hdp-node-01 data]# hadoop fs -ls /Found 6 itemsdrwxr-xr-x - root supergroup 0 2016-12-02 08:26 /hbasedrwxr-xr-x - root supergroup 0 2016-11-28 10:20 /studentdrwxr-xr-x - root supergroup 0 2017-02-25 23:52 /testdrwx-wx-wx - root supergroup 0 2016-11-28 10:01 /tmpdrwxr-xr-x - root supergroup 0 2016-11-27 23:34 /userdrwxr-xr-x - root supergroup 0 2016-12-17 21:13 /wordcount[root@hdp-node-01 data]# kill掉namenode进程后,重新启动它,发现起不来了1234567891011121314151617181920[root@hdp-node-01 data]# jps1117 NameNode1220 DataNode1350 SecondaryNameNode1774 Jps[root@hdp-node-01 data]# kill -9 1117[root@hdp-node-01 data]# jps1220 DataNode1350 SecondaryNameNode1786 Jps[root@hdp-node-01 data]# hadoop-daemon.sh start namenodestarting namenode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.out[root@hdp-node-01 data]# jps1220 DataNode1350 SecondaryNameNode1861 Jps#执行命令[root@hdp-node-01 data]# hadoop fs -ls /ls: Call From hdp-node-01/192.168.0.11 to hdp-node-01:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see: http://wiki.apache.org/hadoop/ConnectionRefused cp拷贝恢复：123456789[root@hdp-node-01 dfs]# pwd/home/hadoop/app/hadoop-2.6.4/hadoopdata/dfs[root@hdp-node-01 dfs]# cp -r ./namesecondary/ /home/hadoop/data/name[root@hdp-node-01 dfs]# ll /home/hadoop/data/name/total 8drwxr-xr-x 2 root root 4096 Feb 26 00:13 current-rw-r--r-- 1 root root 16 Feb 26 00:13 in_use.lock[root@hdp-node-01 dfs]# 12345678[root@hdp-node-01 dfs]# hadoop-daemon.sh start namenodestarting namenode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.out[root@hdp-node-01 dfs]# jps1927 NameNode1220 DataNode1995 Jps1350 SecondaryNameNode[root@hdp-node-01 dfs]# 查看数据是否完整12345678#执行命令[root@hdp-node-01 dfs]# hadoop fs -ls /Found 3 items-rw-r--r-- 3 root supergroup 258 2016-11-18 19:11 /hosts-rw-r--r-- 3 root supergroup 2025 2016-11-19 23:00 /profiledrwxr-xr-x - root supergroup 0 2016-11-19 23:01 /test#其实有数据的丢失,通过比较上面的ls /的目录就知道 这里的恢复只能是恢复到secondarynamenode 中存在的镜像位置，如果对于没有从edits中合并到secondarynamenode 的部分是无法恢复的 为了避免namenode数据损坏，将namenode的工作目录放在多个磁盘上，如下配置 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name1,/home/hadoop/data/name2&lt;/value&gt;&lt;/property&gt;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"namenode工作机制","date":"2017-04-16T04:47:25.039Z","path":"2017/04/16/bigdata/hadoop/namenode工作机制/","text":"1.namenode 的职责 负责客户端请求的响应 元数据的管理（查询，修改） 2.namenode对数据的管理形式namenode对数据的管理采用了三种存储形式:1.内存元数据(NameSystem)2.磁盘元数据镜像文件(Image)3.数据操作日志文件（可通过日志运算出元数据）(edits) 3.元数据存储机制A、内存中有一份完整的元数据(内存meta data)B、磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)C、用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中 4.元数据手动查看可以通过hdfs的一个工具来查看edits中的信息12bin/hdfs oev -i edits -o edits.xmlbin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml 5.元数据的checkpoint每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地(fsimage只会下载一次)，并加载到内存进行merge（这个过程称为checkpoint） 5.1.checkpoint的详细过程 5.2.checkpoint操作的触发条件配置参数1234567dfs.namenode.checkpoint.check.period=60 #检查触发条件是否满足的频率，60秒dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125; dfs.namenode.checkpoint.max-retries=3 #最大重试次数dfs.namenode.checkpoint.period=3600 #两次checkpoint之间的时间间隔3600秒dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录 5.3.checkpoint的附带作用（元数据恢复）&emsp;namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据 参见: namenode目录故障时解决方案.md 6.元数据目录说明6.1.总体结构在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘：1$HADOOP_HOME/bin/hdfs namenode -format 格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构123456current/|-- VERSION|-- edits_*|-- fsimage_0000000000008547077|-- fsimage_0000000000008547077.md5`-- seen_txid 其中的dfs.namenode.dir是在hdfs-site.xml文件中配置的，默认值如下：1234567891011&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file://$&#123;hadoop.tmp.dir&#125;/dfs/name&lt;/value&gt;&lt;/property&gt; hadoop.tmp.dir是在core-site.xml中配置的，默认值如下&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/tmp/hadoop-$&#123;user.name&#125;&lt;/value&gt; &lt;description&gt;A base for other temporary directories.&lt;/description&gt;&lt;/property&gt; dfs.namenode.name.dir属性可以配置多个目录:12345/*如/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,....各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。*/ 6.2.VERSION文件VERSION文件是Java属性文件，内容大致如下： 1234567#Fri Nov 15 19:47:46 CST 2013namespaceID=934548976 #namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的；clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196cTime=0 #cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳；storageType=NAME_NODE #storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）；blockpoolID=BP-893790215-192.168.24.72-1383809616115layoutVersion=-47 #layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用； clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明 使用如下命令格式化一个Namenode：$HADOOP_HOME/bin/hdfs namenode -format [-clusterId ]&emsp;选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。 使用如下命令格式化其他Namenode：$HADOOP_HOME/bin/hdfs namenode -format -clusterId 升级集群至最新版本。在升级过程中需要提供一个ClusterID，例如：\\$HADOOP_PREFIX_HOME/bin/hdfs start namenode –config $HADOOP_CONF_DIR -upgrade -clusterId ,如果没有提供ClusterID，则会自动生成一个ClusterID。 blockpoolID 是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。 6.3.seen_txid文件&emsp;非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯&emsp;文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"map端join算法实现","date":"2017-04-16T04:47:25.037Z","path":"2017/04/16/bigdata/hadoop/map端join算法实现/","text":"场景实现两个“表”的join操作，其中一个表数据量小，一个表很大，这种场景在实际中非常常见，比如“订单日志” join “产品信息” 原理阐述 先在mapper类中预先定义好小表，进行join 并用distributedcache机制将小表的数据分发到每一个maptask执行节点，从而每一个maptask节点可以从本地加载到小表的数据，进而在本地即可实现join 案例实现两张表的left join 查询 订单数据表t_order： id date pid amount 1001 20150710 P0001 2 1002 20150710 P0001 3 1002 20150710 P0002 3 商品信息表t_product： id pname category_id price P0001 小米5 1000 2 P0002 锤子T1 1000 3 SQL语句：1select a.id,a.date,b.name,b.category_id,b.price from t_order a left join t_product b on a.pid = b.id 解决方法:将商品表缓存在map端本地，然后在map端直接拼接1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package cn.itcast.bigdata.mr.mapsidejoin;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class MapSideJoin &#123; public static class MapSideJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; // 用一个hashmap来加载保存产品信息表 Map&lt;String, String&gt; pdInfoMap = new HashMap&lt;String, String&gt;(); Text k = new Text(); /** * 通过阅读父类Mapper的源码，发现 setup方法是在maptask处理数据之前调用一次 可以用来做一些初始化工作 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; //因为已经将普通文件加入(addCacheFile)了当前的工作目录，所以可以直接获取到该文件 BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pdts.txt&quot;))); String line; while (StringUtils.isNotEmpty(line = br.readLine())) &#123; String[] fields = line.split(&quot;,&quot;); pdInfoMap.put(fields[0], fields[1]); &#125; br.close(); &#125; // 由于已经持有完整的产品信息表，所以在map方法中就能实现join逻辑了（直接拼接） @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String orderLine = value.toString(); String[] fields = orderLine.split(&quot;\\t&quot;); String pdName = pdInfoMap.get(fields[1]); k.set(orderLine + &quot;\\t&quot; + pdName); context.write(k, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(MapSideJoin.class); job.setMapperClass(MapSideJoinMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(&quot;D:/srcdata/mapjoininput&quot;)); FileOutputFormat.setOutputPath(job, new Path(&quot;D:/temp/output&quot;)); // 指定需要缓存一个文件到所有的maptask运行节点工作目录 /* job.addArchiveToClassPath(archive); */// 缓存jar包到task运行节点的classpath中 /* job.addFileToClassPath(file); */// 缓存普通文件到task运行节点的classpath中 /* job.addCacheArchive(uri); */// 缓存压缩包文件到task运行节点的工作目录（当前类文件所在的目录） /* job.addCacheFile(uri) */// 缓存普通文件到task运行节点的工作目录 // 将产品表文件缓存到task工作节点的工作目录中去 // job.addCacheFile(new URI(&quot;hdfs://hpd-node-01:9000/srcdata/mapjoincache/pdts.txt&quot;)); job.addCacheFile(new URI(&quot;file:/D:/srcdata/mapjoincache/pdts.txt&quot;)); //默认是有一个reduce的，map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); boolean res = job.waitForCompletion(true); System.exit(res ? 0 : 1); &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"MapTask和ReduceTask并行度决定机制","date":"2017-04-16T04:47:25.035Z","path":"2017/04/16/bigdata/hadoop/MapTask和ReduceTask并行度决定机制/","text":"maptask的并行度决定map阶段的任务处理并发度，进而影响到整个job的处理速度,那么，mapTask并行实例是否越多越好呢？其并行度又是如何决定呢？ 1.mapTask并行度的决定机制一个job的map阶段并行度由客户端在提交job时决定,而客户端对map阶段并行度的规划的基本逻辑为：将待处理数据执行逻辑切片（即按照一个特定切片大小，将待处理数据划分成逻辑上的多个split），然后每一个split分配一个mapTask并行实例处理 这段逻辑及形成的切片规划描述文件，由FileInputFormat实现类的getSplits()方法完成，其过程如下图： 2.FileInputFormat切片机制2.1.切片定义在InputFormat类中的getSplit()方法2.2.FileInputFormat中默认的切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于block大小 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片比如待处理数据有两个文件： 12file1.txt 320M file2.txt 10M 经过FileInputFormat的切片机制运算后，形成的切片信息如下： 1234file1.txt.split1-- 0~128file1.txt.split2-- 128~256file1.txt.split3-- 256~320file2.txt.split1-- 0~10M 2.3.FileInputFormat中切片的大小的参数配置通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize,blockSize)); 切片主要由这几个值来运算决定12345minsize：默认值：1 #配置参数： mapreduce.input.fileinputformat.split.minsize maxsize：默认值：Long.MAXValue #配置参数：mapreduce.input.fileinputformat.split.maxsizeblocksize #块大小 因此，默认情况下，切片大小=blocksizemaxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值minsize （切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大 选择并发数的影响因素： 运算节点的硬件配置 运算任务的类型：CPU密集型还是IO密集型 运算任务的数据量 3.map并行度的经验之谈如果硬件配置为2*12core + 64G，恰当的map并行度是大约每个节点20-100个map，最好每个map的执行时间至少一分钟。 如果job的每个map或者 reduce task的运行时间都只有30-40秒钟，那么就减少该job的map或者reduce数，每一个task(map|reduce)的setup和加入到调度器中进行调度，这个中间的过程可能都要花费几秒钟，所以如果每个task都非常快就跑完了，就会在task的开始和结束的时候浪费太多的时间。配置task的JVM重用[JVM重用技术不是指同一Job的两个或两个以上的task可以同时运行于同一JVM上，而是排队按顺序执行。]可以改善该问题：（mapred.job.reuse.jvm.num.tasks，默认是1，表示一个JVM上最多可以顺序执行的task数目（属于同一个Job）是1。也就是说一个task启一个JVM） 如果input的文件非常的大，比如1TB，可以考虑将hdfs上的每个block size设大，比如设成256MB或者512MB 4.ReduceTask并行度的决定reducetask的并行度同样影响整个job的执行并发度和执行效率，但与maptask的并发数由切片数决定不同，Reducetask数量的决定是可以直接手动设置：12//默认值是1，手动设置为4job.setNumReduceTasks(4); 如果数据分布不均匀，就有可能在reduce阶段产生数据倾斜注意： reducetask数量并不是任意设置，还要考虑业务逻辑需求，有些情况下，需要计算全局汇总结果，就只能有1个reducetask 尽量不要运行太多的reduce task。对大多数job来说，最好reduce的个数最多和集群中的map持平，或者比集群的 reduce slots小。这个对于小集群而言，尤其重要。","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce组件之combiner","date":"2017-04-16T04:47:25.033Z","path":"2017/04/16/bigdata/hadoop/mapreduce组件之combiner/","text":"其实combiner是一个特殊的reducer，如果他们的业务逻辑是一样的，那么可以用reducer来替代combiner map1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package cn.itcast.bigdata.mr.wcdemo;import java.io.IOException;import java.util.HashMap;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;/** * KEYIN: 默认情况下，是mr框架所读到的一行文本的起始偏移量，Long, * 但是在hadoop中有自己的更精简的序列化接口，所以不直接用Long，而用LongWritable * * VALUEIN:默认情况下，是mr框架所读到的一行文本的内容，String，同上，用Text * * KEYOUT：是用户自定义逻辑处理完成之后输出数据中的key，在此处是单词，String，同上，用Text * VALUEOUT：是用户自定义逻辑处理完成之后输出数据中的value，在此处是单词次数，Integer，同上，用IntWritable * * @author * */public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123; /** * map阶段的业务逻辑就写在自定义的map()方法中 * maptask会对每一行输入数据调用一次我们自定义的map()方法 */ @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //将maptask传给我们的文本内容先转换成String String line = value.toString(); //根据空格将这一行切分成单词 String[] words = line.split(&quot; &quot;); //将单词输出为&lt;单词，1&gt; for(String word:words)&#123; //将单词作为key，将次数1作为value，以便于后续的数据分发，可以根据单词分发，以便于相同单词会到相同的reduce task context.write(new Text(word), new IntWritable(1)); &#125; &#125; &#125; reduce123456789101112131415161718192021222324252627282930313233343536373839404142434445package cn.itcast.bigdata.mr.wcdemo;import java.io.IOException;import java.util.Iterator;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * KEYIN, VALUEIN 对应 mapper输出的KEYOUT,VALUEOUT类型对应 * * KEYOUT, VALUEOUT 是自定义reduce逻辑处理结果的输出数据类型 * KEYOUT是单词 * VLAUEOUT是总次数 * @author * */public class WordcountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; /** * &lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt;&lt;angelababy,1&gt; * &lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt;&lt;hello,1&gt; * &lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt;&lt;banana,1&gt; * 入参key，是一组相同单词kv对的key */ @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; /*Iterator&lt;IntWritable&gt; iterator = values.iterator(); while(iterator.hasNext())&#123; count += iterator.next().get(); &#125;*/ for(IntWritable value:values)&#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125; &#125; 启动mapreduce12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package cn.itcast.bigdata.mr.wcdemo;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于一个yarn集群的客户端 * 需要在此封装我们的mr程序的相关运行参数，指定jar包 * 最后提交给yarn * @author * */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); //是否运行为本地模式，就是看这个参数值是否为local，默认就是local /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;);*/ //本地模式运行mr程序时，输入输出的数据可以在本地，也可以在hdfs上 //到底在哪里，就看以下两行配置你用哪行，默认就是file:/// /*conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://mini1:9000/&quot;);*/ /*conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);*/ //运行集群模式，就是把程序提交到yarn中去运行 //要想运行为集群模式，以下3个参数要指定为集群上的值 /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;mini1&quot;); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://mini1:9000/&quot;);*/ Job job = Job.getInstance(conf); job.setJar(&quot;c:/wc.jar&quot;); //指定本程序的jar包所在的本地路径 /*job.setJarByClass(WordcountDriver.class);*/ //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); &apos;设置combiner&apos; /*指定需要使用combiner，以及用哪个类作为combiner的逻辑（WordcountCombiner.class中的逻辑和WordcountReducer中的逻辑是一样的， 所以使用WordcountReducer来替代WordcountCombiner.class） */ /*job.setCombinerClass(WordcountCombiner.class);*/ job.setCombinerClass(WordcountReducer.class); //如果不设置InputFormat，它默认用的是TextInputformat.class job.setInputFormatClass(CombineTextInputFormat.class); CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); CombineTextInputFormat.setMinInputSplitSize(job, 2097152); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125; &#125; conbiner123456789101112131415161718192021222324252627package cn.itcast.bigdata.mr.wcdemo;import java.io.IOException;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;/** * 输如为map的输出 * @author: 张政 * @date: 2016年4月11日 下午7:08:18 * @package_name: day07.sample */public class WordcountCombiner extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count=0; for(IntWritable v: values)&#123; count += v.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 使用reduce代替combiner如果combiner和reducer的业务逻辑是一样的，那么在启动程序中，将combiner的设置类改成reducer123/*job.setCombinerClass(WordcountCombiner.class);*/ job.setCombinerClass(WordcountReducer.class);*/ combiner是MR程序中Mapper和Reducer之外的一种组件 combiner组件的父类就是Reducer combiner和reducer的区别在于运行的位置： Combiner是在每一个maptask所在的节点运行 Reducer是接收全局所有Mapper的输出结果； combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量 具体实现步骤：1、自定义一个combiner继承Reducer，重写reduce方法2、在job中设置： job.setCombinerClass(CustomCombiner.class) combiner能够应用的前提是不能影响最终的业务逻辑,而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来 Combiner的使用要非常谨慎,因为combiner在mapreduce过程中可能调用也肯能不调用，可能调一次也可能调多次,所以：combiner使用的原则是：有或没有都不能影响业务逻辑","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce程序运行模式","date":"2017-04-16T04:47:25.032Z","path":"2017/04/16/bigdata/hadoop/mapreduce程序运行模式/","text":"1.本地运行模式（建议）如果实在Windows下跑，需要修改Hadoop的bin目录和lib目录，因为Windows和linux中目录分隔符的表示不一样在 F:\\weizhi_data\\Data\\1327401579@qq.com\\My Notes\\大数据\\Hadoop 中 &lt;无jar版windows平台hadoop-2.6.1&gt; 有Windows的Hadoop 本地运行模式 （1）mapreduce程序是被提交给LocalJobRunner在本地以单进程的形式运行,其实是启动了多个线程（2）而处理的数据及输出结果可以在本地文件系统，也可以在hdfs上（3）怎样实现本地运行？写一个程序，不要带集群的配置文件（本质是你的mr程序的conf中是否有mapreduce.framework.name=local以及yarn.resourcemanager.hostname参数）（4）本地模式非常便于进行业务逻辑的debug，只要在eclipse中打断点即可 1234567//是否运行为本地模式，就是看这个参数值是否为local，默认就是local/*conf.set(&quot;mapreduce.framework.name&quot;, &quot;local&quot;);*///本地模式运行mr程序时，输入输出的数据可以在本地，也可以在hdfs上//到底在哪里，就看以下两行配置你用哪行，默认就是file:////*conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://mini1:9000/&quot;);*//*conf.set(&quot;fs.defaultFS&quot;, &quot;file:///&quot;);*/ 如果在windows下想运行本地模式来测试程序逻辑，需要在windows中配置环境变量：12％HADOOP_HOME％ = d:/hadoop-2.6.1%PATH% = ％HADOOP_HOME％\\bin 并且要将d:/hadoop-2.6.1的lib和bin目录替换成windows平台编译的版本 2.集群运行模式12345//运行集群模式，就是把程序提交到yarn中去运行 //要想运行为集群模式，以下3个参数要指定为集群上的值 /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;mini1&quot;); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://mini1:9000/&quot;);*/","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce的演化过程图示","date":"2017-04-16T04:47:25.030Z","path":"2017/04/16/bigdata/hadoop/mapreduce的演化过程图示/","text":"1.单机词频统计 maptask.jar是一个已经写好的程序jar文件，可以读取词频文件，然后计算统计结果Driver程序是一个启动maptask.jar的启动程序，如：java maptask.jar /home/words.txt -Xmx=200m -Xms=200m 2.分布式（多机）词频统计如果词频文件很大，那么读取文件并进行计算的时间将会很长，于是有了下面的：将词频文件分成多个小文件，分发到多个机器上，同时执行maptask.jar，最后将多个机器上的执行结果合并 2.1.将客户端的统计程序（maptask.jar）分发到2台服务器上 在真实的业务场景中，词频文件（日志文件）是通过负载均衡直接分发到不同的服务器上，所以词频文件就直接存在于各个服务器上 2.2.客户端向各个服务器发送启动maptask.jar程序的命令客户端向各个服务器发送命令【java -cp cn.it.bigdata.Task maptask.jar /home/words.txt -Xms=2000m -Xmx=2000m】 2.3.合并统计将各个机器计算的结果放到另一个机器上（服务器3）合并，并将最后合并结果保存 3.总结 node manager是负责管理启动maptask.jar程序的，其要为程序准备硬件资源（如CPU、内存等） resource manager是管理node manager的 node manager和resource manager组成了yarn（管理一台机器上的硬件资源，负责资源的分配） maptask.jar 和 reduc.jar共同组成了mapreduce的map阶段和reduce阶段 而词频文件是存放在一个指定的文件系统上的（可以是localFS，HDFS，GFS等）","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce的shuffle机制","date":"2017-04-16T04:47:25.029Z","path":"2017/04/16/bigdata/hadoop/mapreduce的shuffle机制/","text":"概述 mapreduce中，map阶段处理的数据如何传递给reduce阶段，是mapreduce框架中最关键的一个流程，这个流程就叫shuffle shuffle: 洗牌、发牌——（核心机制：数据分区，排序，缓存） 具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序 整体过程图解 简单说明一下shuffle的过程: Map Task的整体流程：1）Read：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。2）Map：该阶段主要将解析出的key/value交给用户编写的map()函数处理，并产生一系列的key/value。3）Collect：在用户编写的map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输入结果。在该函数内部，它会将生成的 key/value分片（通过Partitioner），并写入一个环形内存缓冲区中。4）Spill：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并，压缩等操作。5）Combine：当所有数据处理完成后，Map Task对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 Reduce的整体流程：1）Shuffle：也称Copy阶段。Reduce Task从各个Map Task上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阀值，则写到磁盘上，否则直接放到内存中。2）Merge：在远程拷贝的同时，Reduce Task启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或者磁盘上文件过多。3）Sort：按照MapReduce语义，用户编写的reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一 起，Hadoop采用了基于排序的策略。由于各个Map Task已经实现了对自己的处理结果进行了局部排序，因此，Reduce Task只需对所有数据进行一次归并排序即可。4）Reduce：在该阶段中，Reduce Task将每组数据依次交给用户编写的reduce()函数处理。5）Write：reduce()函数将计算结果写到HDFS。 这里涉及到”环形缓冲区”的知识,可以参见mapreduce环形缓冲区的结构说明(转)说明 当环形缓冲区写到80%(默认)时,将会有溢出(spill),首先将80%的要溢出的buffer锁定(另外的20%空间还是可以继续写入的),然后对环形缓冲区中的元数据进行排序(按照分区进行,因为元数据中存放了partition数据),之后进行溢出操作,溢出是将内存(环形缓冲区80%)的数据写入到一个临时文件中(所以可以想象一下,这样的临时文件将会在每次环形缓冲区发生溢出的时候都会生成一个,所以临时文件有可能有多个),这样溢出到临时文件中的数据是分区,且在分区中是有排序的,如果在溢出的过程中我们指定了combiner,则溢出的文件将是合并之后的结果,如上图所示 溢出的多个临时文件会经过合并(merge),将多个文件中相同分区的放在一起,分区内进行key排序,同样如果我们指定了combiner,则会在分区内进行相同的key的合并,这样在每个map task上就形成了一个最终的文件,如上图 Shuffle：也称Copy阶段。Reduce Task从各个Map Task上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阀值，则写到磁盘上，否则直接放到内存中","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce的Mapper类结构","date":"2017-04-16T04:47:25.027Z","path":"2017/04/16/bigdata/hadoop/mapreduce的Mapper类结构/","text":"1234567891011121314151617181920212223242526272829public class Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; public abstract class Context implements MapContext&lt;KEYIN,VALUEIN,KEYOUT,VALUEOUT&gt; &#123; &#125; protected void setup(Context context) throws IOException, InterruptedException &#123; &#125; protected void map(KEYIN key, VALUEIN value, Context context) throws IOException, InterruptedException &#123; context.write((KEYOUT) key, (VALUEOUT) value);//默认是有一个map方法的,所以如果我们不写,还是有map的存在的 &#125; protected void cleanup(Context context) throws IOException, InterruptedException &#123; &#125; /* 会启动一个线程去执行 */ public void run(Context context) throws IOException, InterruptedException &#123; setup(context);//map中最开始会执行一次 try &#123; while (context.nextKeyValue()) &#123;//context中对应的keyValue，就去调用map方法 map(context.getCurrentKey(), context.getCurrentValue(), context); &#125; &#125; finally &#123; cleanup(context);//最终会执行一次 &#125; &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce环形缓冲区的结构说明(转)","date":"2017-04-16T04:47:25.026Z","path":"2017/04/16/bigdata/hadoop/mapreduce环形缓冲区的结构说明/","text":"MapOutPutBuffer就是map任务暂存记录的内存缓冲区。不过这个缓冲区是有限的，当写入的数据超过缓冲区设定的阈值时，需要将缓冲区的数据写入到磁盘，这个过程叫spill。在溢出数据到磁盘的时候，会按照key进行排序，保证刷新到磁盘的记录时排好序的。该缓冲区的设计非常有意思，它做到了将数据的meta data(索引)和raw data(原始数据)一起存放到了该缓冲区中，并且，在spill的过程中，仍然能够往该缓冲区中写入数据，我们在下面会详细分析该缓冲区是怎么实现这些功能的。 缓冲区分析MapoutPutBuffer是一个环形缓冲区，每个输入的key-&gt;value键值对以及其索引信息都会写入到该缓冲区，当缓冲区块满的时候，有一个后台的守护线程会负责对数据排序，将其写入到磁盘。 核心成员变量1、 kvbuffer :字节数组，数据和数据的索引都会存在该数组中2、 kvmeta：只是kvbuffer中索引存储部分的一个视角，为什么这么说？因为索引往往是按整型存储（4个字节），所以使用kvmeta来重新组织该部分的字节（kvmeta中的一个单元相当于4个字节，但是kvmeta并没有重新开辟内存，其指向的还是kvbuffer）3、 equator:缓冲区的分割线，用来分割数据和数据的索引信息。4、 kvindex:下次要插入的索引的位置5、 kvstart:溢出时索引的起始位置6、 kvend:溢出时索引的结束位置7、 bufindex:下次要写入的raw数据的位置8、 bufstart:溢出时raw数据的起始位置9、 bufend:溢出时raw数据的结束位置10、spiller:当数据占用超过这个比例时，就溢出11、sortmb:kvbuffer总的内存量，默认值是100m，可以配置12、indexCacheMemoryLimit:存放溢出文件信息的缓存大小，默认1m，可以配置13、bufferremaining:buffer剩余空间，字节为单位14、softLimit:溢出阈值，超出后就溢出。Sortmb*spiller 初始状态 初始时，equator=0，在写入数据时，raw data往数组下标增大的方向延伸，而meta data（索引信息）往从数组后面往下标减小的方向延伸。从上图来看，raw data就是按照顺时针来写入数据，而meta data按照逆时针写入数据。我们再看一下各个变量的初始化情况，raw data部分的变量，bufstart、bufend、bufindex都初始化为0。Meta data部分的变量，kvstart 、kvend、kvindex都是按逆时针偏移了16个字节（metasize=16个字节），因为一个meta data占用16个字节（4个整数，分别存储keystart,valuestart,partion,valuelen），所以需要逆时针偏移16个字节来标记第一个存储的metadata的起始位置。还有一个重要的变量，bufferremaining = softlimit（默认是sortmb*80%）。 写入第一个的状态 我们看一下写入第一个的情况，首先放入key，在bufferindex的基础上累加key的字节数，然后放入value，继续累加bufferindex的字节数。接下来放入metadata，meta data一共包括4个整数，第一个int放valuestart，第二个int放keystart，第三个int放partion，第四个int放value的长度。为什么只有value的长度，没有key的长度？个人理解key的长度可以有valuestart – keystart得出，不需要额外的空间来存储key的长度。需要注意的是，bufindex和kvindex发生了变化，分别指向了下一个数据需要插入的地方。但是bufstart，endstart,kvstart,kvend都没有变化，bufferremaining相应地减少了meta data 和raw data占据的空间。 溢出文件 随着kvindex和bufindex的不断偏移，剩余的空间越来越小，当剩余空间不足时，就会触发spill操作。如上图，kvindex和bufindex之间的空间已经很小了。 溢出文件开始前，需要先更新kvend、bufend,kvend 需要更新成kvindex + 4 int，因为kvindex始终指向下一个需要写入的meta data的位置，必须往后回退4 int 才是meta data真正结束的位置，如上图，kvend加了4int往顺时针方向偏移了。Kvstart指向最后一个meta data写入的位置。Bufstart标识着最后一个key 开始的位置，bufend 标识最第一个value的结束位置。Kvstart和kvend之间（黄色部分）是需要溢出的meta data。Bufstart和bufend之间（浅绿色）是需要溢出的raw data。溢出的时候，其他的缓存空间（深绿色）仍然可以写入数据，不会被溢出操作阻塞住。默认的Spiller是80%，也就是还有20%的空间可以在溢出的时候使用。溢出开始前，需要确定新的equator，新的equator一定在kvend和bufend之间。新的equator一定要做到合适的划分，保证能写入更多的metadata和raw data。确定了equator后，我们需要更新bufindex和kvindex的位置，更新bufferremaining的大小，bufferremaining要选择equator到bufindex、kvindex较小的那个。任何一个用完了，都代表不能写入数据，这也说明了equator划分均匀的重要性。 在溢出完成后，空间都已经释放出来，溢出完成后的缓存状态就变成了上图：meta data从新的equator开始逆时针写入数据，raw data从新的equator开始顺时针写入数据。当剩余的空间又到了溢出的阈值时，再次划分equator，再次溢出文件。 转自:MapReduce之mapOutputBuffer解析","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce参数优化","date":"2017-04-16T04:47:25.024Z","path":"2017/04/16/bigdata/hadoop/mapreduce参数优化/","text":"1.资源相关参数1234567891011121314151617181920//以下参数是在用户自己的mr应用程序中配置就可以生效(1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。(2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。(3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “”(4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “”(5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1(6) mapreduce.reduce.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1 //应该在yarn启动之前就配置在服务器的配置文件中才能生效(7) yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存(8) yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存(9) yarn.scheduler.minimum-allocation-vcores 1(10)yarn.scheduler.maximum-allocation-vcores 32(11)yarn.nodemanager.resource.memory-mb 8192 //shuffle性能优化的关键参数，应在yarn启动之前就配置好(12)mapreduce.task.io.sort.mb 100 //shuffle的环形缓冲区大小，默认100mmapreduce.map.sort.spill.percent 0.8 //环形缓冲区溢出的阈值，默认80% 2.容错相关参数123456 (1) mapreduce.map.maxattempts: #每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(2) mapreduce.reduce.maxattempts: #每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(3) mapreduce.map.failures.maxpercent: #当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。(4) mapreduce.reduce.failures.maxpercent: #当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0.(5) mapreduce.task.timeout: #Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 3.本地运行mapreduce 作业1234设置以下几个参数:mapreduce.framework.name=localmapreduce.jobtracker.address=localfs.defaultFS=local 4.效率和稳定性相关参数1234567(1) mapreduce.map.speculative: #是否为Map Task打开推测执行机制，默认为false(2) mapreduce.reduce.speculative: #是否为Reduce Task打开推测执行机制，默认为false(3) mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence： #当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。(4) mapreduce.input.fileinputformat.split.minsize: #FileInputFormat做切片时的最小切片大小，(5)mapreduce.input.fileinputformat.split.maxsize: #FileInputFormat做切片时的最大切片大小(切片的默认大小就等于blocksize，即 134217728)","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"mapreduce中的排序","date":"2017-04-16T04:47:25.022Z","path":"2017/04/16/bigdata/hadoop/mapreduce中的排序/","text":"1.应用场景统计每一个用户（手机号）所耗费的总流量倒序排序123451363157985066 13726230503(手机号) 00-FD-07-A4-72-B8:CMCC 120.196.100.82 i02.c.aliimg.com 24 27 2481（上行流量） 24681（上行流量） 2001363157995052 13826544101 5C-0E-8B-C7-F1-E0:CMCC 120.197.40.4 4 0 264 0 2001363157991076 13926435656 20-10-7A-28-CC-0A:CMCC 120.196.100.99 2 4 132 1512 200 思路分析 基本思路：实现自定义的bean来封装流量信息，并将bean作为map输出的key来传输 MR程序在处理数据的过程中会对数据排序(map输出的kv对传输到reduce之前，会排序)，排序的依据是map输出的key,所以，我们如果要实现自己需要的排序规则，则可以考虑将排序因素放到key中，让key实现接口：WritableComparable,然后重写key的compareTo方法 2.实现步骤2.1.求出单个手机号对应的总流量1234137xxx 66 66 132127xx 88 11 99157xx 88 22 100.... 2.2.对步骤1中的结果再进行mapreduce2.2.1.mapredce123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package cn.itcast.bigdata.mr.flowsum;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import cn.itcast.bigdata.mr.flowsum.FlowCount.FlowCountMapper;import cn.itcast.bigdata.mr.flowsum.FlowCount.FlowCountReducer;public class FlowCountSort &#123; static class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; //将new 的工作放在map的外面，这样不用每次map函数中new 对象 FlowBean bean = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 拿到的是上一个统计程序的输出结果，已经是各手机号的总流量信息 String line = value.toString(); String[] fields = line.split(&quot;\\t&quot;); String phoneNbr = fields[0]; long upFlow = Long.parseLong(fields[1]); long dFlow = Long.parseLong(fields[2]); bean.set(upFlow, dFlow); v.set(phoneNbr); //以bean作为key，那么内部会以bean来排序，所以bean要重写比较器方法 context.write(bean, v); /*context.write是将bean此时的数据序列化了，所以对于所有的map调用，尽管都是使用的是同一个bean，但是因为 写一次就序列化一次，那么最终经过map之后的写出去的值是不一样的，所以不用担心bean的引用问题 */ &#125; &#125; /** * 根据key来掉, 传过来的是对象, 每个对象都是不一样的, 所以每个对象都调用一次reduce方法 */ static class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; // &lt;bean(),phonenbr&gt; @Override protected void reduce(FlowBean bean, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; //values中只有一个值了 context.write(values.iterator().next(), bean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resoucemanager.hostname&quot;, &quot;mini1&quot;);*/ Job job = Job.getInstance(conf); /*job.setJar(&quot;/home/hadoop/wc.jar&quot;);*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCountSort.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 Path outPath = new Path(args[1]); /*FileSystem fs = FileSystem.get(conf); if(fs.exists(outPath))&#123; fs.delete(outPath, true); &#125;*/ FileOutputFormat.setOutputPath(job, outPath); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125;&#125; 2.2.2.可序列化，可排序的bean123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package cn.itcast.bigdata.mr.flowsum;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.WritableComparable;//实现了比较器和Writer接口public class FlowBean implements WritableComparable&lt;FlowBean&gt;&#123; private long upFlow; private long dFlow; private long sumFlow; //反序列化时，需要反射调用空参构造函数，所以要显示定义一个 public FlowBean()&#123;&#125; public FlowBean(long upFlow, long dFlow) &#123; this.upFlow = upFlow; this.dFlow = dFlow; this.sumFlow = upFlow + dFlow; &#125; public void set(long upFlow, long dFlow) &#123; this.upFlow = upFlow; this.dFlow = dFlow; this.sumFlow = upFlow + dFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getdFlow() &#123; return dFlow; &#125; public void setdFlow(long dFlow) &#123; this.dFlow = dFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; /** * 序列化方法 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(dFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 * 注意：反序列化的顺序跟序列化的顺序完全一致 */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); dFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + dFlow + &quot;\\t&quot; + sumFlow; &#125; //实现比较方法 @Override public int compareTo(FlowBean o) &#123; return this.sumFlow&gt;o.getSumFlow()?-1:1; //从大到小, 当前对象和要比较的对象比, 如果当前对象大, 返回-1, 交换他们的位置(自己的理解) &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"Mapreduce中的分区Partitioner","date":"2017-04-16T04:47:25.021Z","path":"2017/04/16/bigdata/hadoop/Mapreduce中的分区Partitioner/","text":"需求根据归属地输出流量统计数据结果到不同文件，以便于在查询统计结果时可以定位到省级范围进行 分析Mapreduce中会将map输出的kv对，按照相同key分组，然后分发给不同的reducetask,默认的分发规则为：根据key的hashcode%reducetask数来分发,所以：如果要按照我们自己的需求进行分组，则需要改写数据分发（分组）组件Partitioner,自定义一个CustomPartitioner继承抽象类：Partitioner,然后在job对象中，设置自定义partitioner： job.setPartitionerClass(CustomPartitioner.class) 代码mapreduce 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package cn.itcast.bigdata.mr.provinceflow;import java.io.IOException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class FlowCount &#123; static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); //将一行内容转成string String[] fields = line.split(&quot;\\t&quot;); //切分字段 String phoneNbr = fields[1]; //取出手机号 long upFlow = Long.parseLong(fields[fields.length-3]); //取出上行流量下行流量 long dFlow = Long.parseLong(fields[fields.length-2]); context.write(new Text(phoneNbr), new FlowBean(upFlow, dFlow)); &#125; &#125; static class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123; //&lt;183323,bean1&gt;&lt;183323,bean2&gt;&lt;183323,bean3&gt;&lt;183323,bean4&gt;....... @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sum_upFlow = 0; long sum_dFlow = 0; //遍历所有bean，将其中的上行流量，下行流量分别累加 for(FlowBean bean: values)&#123; sum_upFlow += bean.getUpFlow(); sum_dFlow += bean.getdFlow(); &#125; FlowBean resultBean = new FlowBean(sum_upFlow, sum_dFlow); context.write(key, resultBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); /*conf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;); conf.set(&quot;yarn.resoucemanager.hostname&quot;, &quot;mini1&quot;);*/ Job job = Job.getInstance(conf); /*job.setJar(&quot;/home/hadoop/wc.jar&quot;);*/ //指定本程序的jar包所在的本地路径 job.setJarByClass(FlowCount.class); //指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); &apos;指定我们自定义的数据分区器&apos; job.setPartitionerClass(ProvincePartitioner.class); &apos;同时指定相应“分区”数量的reducetask&apos; job.setNumReduceTasks(5); //指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); //指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); //指定job的输出结果所在目录 FileOutputFormat.setOutputPath(job, new Path(args[1])); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean res = job.waitForCompletion(true); System.exit(res?0:1); &#125; &#125; 自定义可序列化的bean 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package cn.itcast.bigdata.mr.provinceflow;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;import org.apache.hadoop.io.Writable;public class FlowBean implements Writable&#123; private long upFlow; private long dFlow; private long sumFlow; //反序列化时，需要反射调用空参构造函数，所以要显示定义一个 public FlowBean()&#123;&#125; public FlowBean(long upFlow, long dFlow) &#123; this.upFlow = upFlow; this.dFlow = dFlow; this.sumFlow = upFlow + dFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getdFlow() &#123; return dFlow; &#125; public void setdFlow(long dFlow) &#123; this.dFlow = dFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; /** * 序列化方法 */ @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(dFlow); out.writeLong(sumFlow); &#125; /** * 反序列化方法 * 注意：反序列化的顺序跟序列化的顺序完全一致 */ @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); dFlow = in.readLong(); sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + &quot;\\t&quot; + dFlow + &quot;\\t&quot; + sumFlow; &#125;&#125; 自定义的partition 12345678910111213141516171819202122232425262728293031package cn.itcast.bigdata.mr.provinceflow;import java.util.HashMap;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Partitioner;/** * K2 V2 对应的是map输出kv的类型 * @author * */public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt;&#123; public static HashMap&lt;String, Integer&gt; proviceDict = new HashMap&lt;String, Integer&gt;(); static&#123; proviceDict.put(&quot;136&quot;, 0); proviceDict.put(&quot;137&quot;, 1); proviceDict.put(&quot;138&quot;, 2); proviceDict.put(&quot;139&quot;, 3); &#125; @Override public int getPartition(Text key, FlowBean value, int numPartitions) &#123; String prefix = key.toString().substring(0, 3); Integer provinceId = proviceDict.get(prefix); return provinceId==null?4:provinceId; //返回分区号 &#125;&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"MapReduce与YARN的工作机制及提交一个job的流程","date":"2017-04-16T04:47:25.019Z","path":"2017/04/16/bigdata/hadoop/MapReduce与YARN的工作机制及提交一个job的流程/","text":"1 YARN概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而mapreduce等运算程序则相当于运行于操作系统之上的应用程序 2.YARN的重要概念 yarn并不清楚用户提交的程序的运行机制 yarn只提供运算资源的调度（用户程序向yarn申请资源，yarn就负责分配资源） yarn中的主管角色叫ResourceManager yarn中具体提供运算资源的角色叫NodeManager 这样一来，yarn其实就与运行的用户程序完全解耦，就意味着yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序，tez …… 所以，spark、storm等运算框架都可以整合在yarn上运行，只要他们各自的框架中有符合yarn规范的资源请求机制即可 Yarn就成为一个通用的资源调度平台，从此，企业中以前存在的各种运算集群都可以整合在一个物理集群上，提高资源利用率，方便数据共享 3.提交一个job的流程示意图 提交job所需的资源文件说明job.split #任务的切片信息job.xml #里面有各种配置组件信息wordcount.jar #待运行的程序jar包","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"},{"name":"mapreduce","slug":"mapreduce","permalink":"http://yoursite.com/tags/mapreduce/"}]},{"title":"HDFS的工作机制(读写数据)","date":"2017-04-16T04:47:25.016Z","path":"2017/04/16/bigdata/hadoop/HDFS的工作机制(读写数据)/","text":"1.概述 HDFS集群分为两大角色：NameNode、DataNode (Secondary Namenode) NameNode负责管理整个文件系统的元数据 DataNode 负责管理用户的文件数据块 文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上 每一个文件块可以有多个副本，并存放在不同的datanode上 Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量 HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行 2.HDFS写数据流程客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本 3.HDFS读数据流程 1、跟namenode通信查询元数据，找到文件块所在的datanode服务器2、挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流3、datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）4、客户端以packet为单位接收，现在本地缓存，然后写入目标文件","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"HDFS的API使用","date":"2017-04-16T04:47:25.012Z","path":"2017/04/16/bigdata/hadoop/HDFS的API使用/","text":"1.获取api中的客户端对象在java中操作hdfs，首先要获得一个客户端实例12Configuration conf = new Configuration()FileSystem fs = FileSystem.get(conf) &emsp;而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例；get方法是从何处判断具体实例化那种客户端类呢？从conf中的一个参数 fs.defaultFS的配置值判断；如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象 2.HDFS客户端增删改查123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157package cn.itcast.bigdata.hdfs;import java.io.FileNotFoundException;import java.io.IOException;import java.net.URI;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.BlockLocation;import org.apache.hadoop.fs.FileStatus;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.LocatedFileStatus;import org.apache.hadoop.fs.Path;import org.apache.hadoop.fs.RemoteIterator;import org.apache.hadoop.hdfs.DistributedFileSystem;import org.apache.hadoop.hdfs.protocol.DatanodeInfo;import org.junit.Before;import org.junit.Test;public class HdfsClient &#123; FileSystem fs = null; @Before public void init() throws Exception &#123; // 构造一个配置参数对象，设置一个参数：我们要访问的hdfs的URI // 从而FileSystem.get()方法就知道应该是去构造一个访问hdfs文件系统的客户端，以及hdfs的访问地址 // new Configuration();的时候，它就会去加载jar包中的hdfs-default.xml // 然后再加载classpath下的hdfs-site.xml Configuration conf = new Configuration(); conf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hdp-node-01:9000&quot;); /** * 参数优先级： 1、客户端代码中设置的值 2、classpath下的用户自定义配置文件 3、然后是服务器的默认配置 */ conf.set(&quot;dfs.replication&quot;, &quot;2&quot;); conf.set(&quot;dfs.block.size&quot;,&quot;64m&quot;); // 获取一个hdfs的访问客户端，根据参数，这个实例应该是DistributedFileSystem的实例// fs = FileSystem.get(conf); // 如果这样去获取，那conf里面就可以不要配&quot;fs.defaultFS&quot;参数，而且，这个客户端的身份标识已经是root用户 fs = FileSystem.get(new URI(&quot;hdfs://hdp-node-01:9000&quot;), conf, &quot;root&quot;); // 获取文件系统相关信息 DatanodeInfo[] dataNodeStats = ((DistributedFileSystem) fs).getDataNodeStats(); for(DatanodeInfo dinfo: dataNodeStats)&#123; System.out.println(dinfo.getHostName()); &#125; &#125; /** * 往hdfs上传文件 * * @throws Exception */ @Test public void testAddFileToHdfs() throws Exception &#123; // 要上传的文件所在的本地路径 Path src = new Path(&quot;g:/apache-flume-1.6.0-bin.tar.gz&quot;); // 要上传到hdfs的目标路径 Path dst = new Path(&quot;/&quot;); fs.copyFromLocalFile(src, dst); fs.close(); &#125; /** * 从hdfs中复制文件到本地文件系统 */ @Test public void testDownloadFileToLocal() throws IllegalArgumentException, IOException &#123;// fs.copyToLocalFile(new Path(&quot;/apache-flume-1.6.0-bin.tar.gz&quot;), new Path(&quot;d:/&quot;)); fs.copyToLocalFile(false,new Path(&quot;/apache-flume-1.6.0-bin.tar.gz&quot;), new Path(&quot;d:/&quot;),true); fs.close(); &#125; /** * 目录操作 */ @Test public void testMkdirAndDeleteAndRename() throws IllegalArgumentException, IOException &#123; // 创建目录 fs.mkdirs(new Path(&quot;/a1/b1/c1&quot;)); // 删除文件夹 ，如果是非空文件夹，参数2必须给值true fs.delete(new Path(&quot;/aaa&quot;), true); // 重命名文件或文件夹 fs.rename(new Path(&quot;/a1&quot;), new Path(&quot;/a2&quot;)); &#125; /** * 显示所有的文件 */ @Test public void testListFiles() throws FileNotFoundException, IllegalArgumentException, IOException &#123; // 思考：为什么返回迭代器，而不是List之类的容器， 如果文件特大， 那不就崩啦！ 迭代器是每迭代一次都向服务器取一次 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(new Path(&quot;/&quot;), true);//true表示递归调用 while (listFiles.hasNext()) &#123; LocatedFileStatus fileStatus = listFiles.next(); System.out.println(fileStatus.getPath().getName());//文件名 System.out.println(fileStatus.getBlockSize());//block块的大小 System.out.println(fileStatus.getPermission());//文件的权限 System.out.println(fileStatus.getLen());//字节数 BlockLocation[] blockLocations = fileStatus.getBlockLocations();//获取block块 for (BlockLocation bl : blockLocations) &#123; System.out.println(&quot;block-length:&quot; + bl.getLength() + &quot;--&quot; + &quot;block-offset:&quot; + bl.getOffset()); String[] hosts = bl.getHosts(); //主机名 for (String host : hosts) &#123; System.out.println(host); &#125; &#125; System.out.println(&quot;--------------为angelababy打印的分割线--------------&quot;); &#125; &#125; /** * 查看一个目录下的文件及文件夹信息（只是一级） * * @throws IOException * @throws IllegalArgumentException * @throws FileNotFoundException */ @Test public void testListAll() throws FileNotFoundException, IllegalArgumentException, IOException &#123; FileStatus[] listStatus = fs.listStatus(new Path(&quot;/&quot;)); String flag = &quot;d-- &quot;; for (FileStatus fstatus : listStatus) &#123; if (fstatus.isFile())&#123; flag = &quot;f-- &quot;; &#125;else&#123; flag = &quot;d-- &quot;; &#125; System.out.println(flag + fstatus.getPath().getName()); System.out.println(fstatus.getPermission()); &#125; &#125;&#125; 3.通过流的方式访问hdfs123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package cn.itcast.bigdata.hdfs;import java.io.FileInputStream;import java.io.FileOutputStream;import java.io.IOException;import java.net.URI;import org.apache.commons.io.IOUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FSDataInputStream;import org.apache.hadoop.fs.FSDataOutputStream;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.junit.Before;import org.junit.Test;/** * 用流的方式来操作hdfs上的文件 * 可以实现读取指定偏移量范围的数据 * @author * */public class HdfsStreamAccess &#123; FileSystem fs = null; Configuration conf = null; @Before public void init() throws Exception&#123; conf = new Configuration(); //拿到一个文件系统操作的客户端实例对象// fs = FileSystem.get(conf); //可以直接传入 uri和用户身份 fs = FileSystem.get(new URI(&quot;hdfs://mini1:9000&quot;),conf,&quot;hadoop&quot;); &#125; /** * 通过流的方式上传文件到hdfs * @throws Exception */ @Test public void testUpload() throws Exception &#123; FSDataOutputStream outputStream = fs.create(new Path(&quot;/angelababy.love&quot;), true); FileInputStream inputStream = new FileInputStream(&quot;c:/angelababy.love&quot;); IOUtils.copy(inputStream, outputStream); &#125; /** * 通过流的方式获取hdfs上数据 * @throws Exception */ @Test public void testDownLoad() throws Exception &#123; //先获取一个文件的输入流----针对hdfs上的 FSDataInputStream inputStream = fs.open(new Path(&quot;/angelababy.love&quot;)); //再构造一个文件的输出流----针对本地的 FileOutputStream outputStream = new FileOutputStream(&quot;d:/angelababy.love&quot;); //再将输入流中数据传输到输出流 IOUtils.copy(inputStream, outputStream); &#125; /** * 获取指定偏移处的数据 * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度 *用于上层分布式运算框架并发处理数据 */ @Test public void testRandomAccess() throws Exception&#123; FSDataInputStream inputStream = fs.open(new Path(&quot;/angelababy.love&quot;)); inputStream.seek(12);//指定偏移 FileOutputStream outputStream = new FileOutputStream(&quot;d:/angelababy.love.part2&quot;); IOUtils.copy(inputStream, outputStream); &#125; /** * 显示hdfs上文件的内容 */ @Test public void testCat() throws IllegalArgumentException, IOException&#123; FSDataInputStream in = fs.open(new Path(&quot;/angelababy.love&quot;)); IOUtils.copy(in, System.out);//打印到控制台 // IOUtils.copyBytes(in, System.out, 1024); &#125; &#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Hadoop集群搭建","date":"2017-04-16T04:47:25.008Z","path":"2017/04/16/bigdata/hadoop/Hadoop集群搭建/","text":"1.hdfs和yarn的作用 HDFS就是负责管理文件的，其中data node 就是数据节点存储数据的，而name node就是管理data node的 yarn是负责跑mapreduce，调度硬件资源给执行的程序用的，node manager 负责调用mapreduce程序，而resource manager是负责管理node manager的 2.主机规划 节点 说明 hdp-node-01 NameNode 、ResourceManager hdp-node-02 DataNode 、NodeManager hdp-node-03 DataNode 、NodeManager hdp-node-04 DataNode 、NodeManager 3.准备工作3.1.添加HADOOP用户，设置密码12useradd hadooppasswd hadoop 3.2.为HADOOP用户分配sudoer权限12345#vim /etc/sudoersroot ALL=(ALL) ALL//添加hadoop ALL=(ALL) ALL 3.3.时间同步12345#d.time sync/usr/sbin/ntpdate time.nist.govecho &apos;#time sync by oldboy at 2010-2-1&apos; &gt;&gt;/var/spool/cron/rootecho &apos;*/5 * * * * /usr/sbin/ntpdate time.nist.gov &gt;/dev/null 2&gt;&amp;1&apos; &gt;&gt;/var/spool/cron/rootcrontab -l 3.4.设置主机名123456#vim /etc/sysconfig/network NETWORKING=yesHOSTNAME=hdp-node-01&apos;对应的机器设置为hdp-node-01、hdp-node-02、hdp-node-03、hdp-node-04&apos; 3.5.配置内网域名映射（hosts）123456#vim /etc/hosts192.168.0.11 hdp-node-01192.168.0.22 hdp-node-02192.168.0.33 hdp-node-03192.168.0.44 hdp-node-04 3.6.配置防火墙12/etc/init.d/iptables stop/etc/init.d/iptables status #查看状态 3.7.配置ssh免密登陆12ssh-keygen -t rsa #一直回车ssh-copy-id -i .ssh/id_rsa.pub root@hdp-node-04 #另外的机器上也要拷贝：hdp-node-01（本机）、hdp-node-02、hdp-node-03、hdp-node-04 3.8.安装JDK12345678910111213141516171819202122#1.解压jdkmkdir /home/hadoop/app -p #jdk解压目录tar -zxvf jdk-7u80-linux-x64.tar.gz -C /home/hadoop/app/ #解压到指定文件#2.修改配置/etc/profile，添加JAVA_HOME=/home/hadoop/app/jdk1.7.0_80CLASSPATH=.:$JAVA_HOME/lib/tools.jarPATH=$JAVA_HOME/bin:$PATHexport JAVA_HOME CLASSPATH PATH#3.使配置生效source /etc/profile#4.检查[root@hdp-node-01 app]# java -versionjava version &quot;1.7.0_80&quot;Java(TM) SE Runtime Environment (build 1.7.0_80-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)[root@hdp-node-01 app]# 4.Hadoop安装4.1.解压123上传：cenos-6.5-hadoop-2.6.4.tar.gz #这是一个经过编译的文件tar -zxvf cenos-6.5-hadoop-2.6.4.tar.gz -C /home/hadoop/app/ #解压到指定的目录 4.2.修改配置文件所有的配置文件在官网可以看到： Configuration core-default.xml hdfs-default.xml mapred-default.xml yarn-default.xml Deprecated Properties cd /home/hadoop/app/hadoop-2.6.4/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172#vim ./etc/hadoop/hadoop-env.shexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_80#vim ./etc/hadoop/core-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; #指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 &lt;value&gt;hdfs://hdp-node-01:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; #存放hdfs的数据目录，这是一个基础的目录，如果会和namenode和datanode以及secondarynamenode的目录相关 &lt;value&gt;/home/hadoop/app/hadoop-2.6.4/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#vim ./etc/hadoop/hdfs-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; #name node 的镜像文件存储目录，如果没有指定，默认是：file://$&#123;hadoop.tmp.dir&#125;/dfs/name &lt;value&gt;/home/hadoop/data/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; #如果没有指定，默认是： file://$&#123;hadoop.tmp.dir&#125;/dfs/data &lt;value&gt;/home/hadoop/data/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.checkpoint.dir&lt;/name&gt; #如果没有指定，默认是：file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary &lt;value&gt;/home/hadoop/data/namesecondary&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; #副本数量 &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#mv ./etc/hadoop/mapred-site.xml.template ./etc/hadoop/mapred-site.xmlvim ./etc/hadoop/mapred-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; #程序是要放在一个指定的资源平台上去跑，这里指定yarn，默认是local &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;#vim ./etc/hadoop/yarn-site.xml&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; #配置yarn的resourcemanager存放的主机 &lt;value&gt;hdp-node-01&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&apos;vim ./etc/hadoop/slaves &apos;hdp-node-01hdp-node-02hdp-node-03hdp-node-04 5.格式化namenode这里只是初始化了namenode的工作目录（/home/hadoop/data/name），而datanode的工作目录是在datanode启动后自动初始化的注意：namenode的初始化，只需要一次就够了12345&apos;格式化namenode（是对namenode进行初始化，相当于生成账本）&apos; hdfs namenode -format # (过时的一个：hadoop namenode -format)#出现下面的语句表示成功Storage directory /home/hadoop/data/name has been successfully formatted. 6.先启动HDFS12345678910111213141516171819202122232425262728293031#sbin/start-dfs.sh[root@hdp-node-01 hadoop-2.6.4]# ./sbin/start-dfs.shStarting namenodes on [hdp-node-01]hdp-node-01: starting namenode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.outhdp-node-01: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-01.outhdp-node-03: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-03.outhdp-node-02: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-02.outhdp-node-04: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-04.out&apos;结果：&apos;/*[root@hdp-node-01 hadoop-2.6.4]# jps1561 DataNode1901 Jps1474 NameNode1682 SecondaryNameNode[root@hdp-node-02 ~]# jps1293 Jps1263 DataNode[root@hdp-node-03 ~]# jps1237 Jps1207 DataNode[root@hdp-node-04 ~]# jps1223 Jps1188 DataNode*/ 7.再启动YARN1234567891011121314151617181920212223242526272829303132333435363738#sbin/start-yarn.sh[root@hdp-node-01 hadoop-2.6.4]# sbin/start-yarn.shstarting yarn daemonsstarting resourcemanager, logging to /home/hadoop/app/hadoop-2.6.4/logs/yarn-root-resourcemanager-hdp-node-01.outhdp-node-01: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.4/logs/yarn-root-nodemanager-hdp-node-01.outhdp-node-03: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.4/logs/yarn-root-nodemanager-hdp-node-03.outhdp-node-04: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.4/logs/yarn-root-nodemanager-hdp-node-04.outhdp-node-02: starting nodemanager, logging to /home/hadoop/app/hadoop-2.6.4/logs/yarn-root-nodemanager-hdp-node-02.out&apos;结果：&apos;/*[root@hdp-node-01 hadoop-2.6.4]# jps1561 DataNode1474 NameNode1682 SecondaryNameNode27803 ResourceManager2039 NodeManager2158 Jps[root@hdp-node-02 ~]# jps1473 Jps1263 DataNode1376 NodeManager[root@hdp-node-03 ~]# jps1320 NodeManager1207 DataNode1416 Jps [root@hdp-node-04 ~]# jps1188 DataNode1397 Jps1301 NodeManager*/ 8.测试1234567&apos;向hdfs中put一个文件&apos;hadoop fs -put /etc/profile / #查看[root@hdp-node-02 ~]# hadoop fs -ls /Found 1 items-rw-r--r-- 3 root supergroup 2025 2016-11-18 02:59 /profile http://192.168.0.11:50070 9.停止1234567891011121314151617181920212223[root@hdp-node-01 hadoop-2.6.4]# ./sbin/stop-yarn.shstopping yarn daemonsno resourcemanager to stophdp-node-01: no nodemanager to stophdp-node-03: no nodemanager to stophdp-node-04: no nodemanager to stophdp-node-02: no nodemanager to stopno proxyserver to stop[root@hdp-node-01 hadoop-2.6.4]# ./sbin/stop-dfs.shStopping namenodes on [hdp-node-01]hdp-node-01: stopping namenodehdp-node-01: stopping datanodehdp-node-02: stopping datanodehdp-node-03: stopping datanodehdp-node-04: stopping datanodeStopping secondary namenodes [0.0.0.0]0.0.0.0: stopping secondarynamenode[root@hdp-node-01 hadoop-2.6.4]# jps2927 Jps","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"Hadoop集群增加新节点","date":"2017-04-16T04:47:25.005Z","path":"2017/04/16/bigdata/hadoop/Hadoop集群增加新节点/","text":"首先需要在新机器上配置slave,参见hadoop的集群安装中的slave部分 在master节点的slaves文件中增加新节点的主机名 在新节点手动启动hadoop-daemon.sh start datanodeyarn-daemon.sh start nodemanager 在主节点hdfs dfsadmin -refreshNodesstart-balancer.sh 通过web ui查看新节点是否增加查看nameNode的web UI:namenode_host:50070查看yarn的webUI:resouceManager_host:8088","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"hadoop的高可用机制搭建","date":"2017-04-16T04:47:25.002Z","path":"2017/04/16/bigdata/hadoop/hadoop的高可用机制搭建/","text":"1.前期准备 修改Linux主机名 修改IP 修改主机名和IP的映射关系 /etc/hosts （注意：如果你们公司是租用的服务器或是使用的云主机（如华为用主机、阿里云主机等））/etc/hosts里面要配置的是内网IP地址和主机名的映射关系 关闭防火墙 ssh免登陆 安装JDK，配置环境变量等 2.集群规划2.1.分布示意图 2.2.规划表 主机名 IP 安装的软件 运行的进程 mini1 192.168.1.200 jdk、hadoop NameNode、DFSZKFailoverController(zkfc) mini2 192.168.1.201 jdk、hadoop NameNode、DFSZKFailoverController(zkfc) mini3 192.168.1.202 jdk、hadoop ResourceManager mini4 192.168.1.203 jdk、hadoop ResourceManager mini5 192.168.1.205 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain（zk） mini6 192.168.1.206 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain mini7 192.168.1.207 jdk、hadoop、zookeeper DataNode、NodeManager、JournalNode、QuorumPeerMain 说明： 在hadoop2.0中通常由两个NameNode组成，一个处于active状态，另一个处于standby状态。Active NameNode对外提供服务，而Standby NameNode则不对外提供服务，仅同步active namenode的状态，以便能够在它失败时快速进行切换。hadoop2.0官方提供了两种HDFS HA的解决方案，一种是NFS，另一种是QJM。这里我们使用简单的QJM。在该方案中，主备NameNode之间通过一组JournalNode同步元数据信息，一条数据只要成功写入多数JournalNode即认为写入成功。通常配置奇数个JournalNode 这里还配置了一个zookeeper集群，用于ZKFC（DFSZKFailoverController）故障转移，当Active NameNode挂掉了，会自动切换Standby NameNode为standby状态 hadoop-2.2.0中依然存在一个问题，就是ResourceManager只有一个，存在单点故障，hadoop-2.6.4解决了这个问题，有两个ResourceManager，一个是Active，一个是Standby，状态由zookeeper进行协调 3.安装步骤3.1.安装配置zooekeeper集群（在mini5上） 1.1解压1tar -zxvf zookeeper-3.4.5.tar.gz -C /home/hadoop/app/ 1.2修改配置123456789cd /home/hadoop/app/zookeeper-3.4.5/conf/cp zoo_sample.cfg zoo.cfgvim zoo.cfg#修改：dataDir=/home/hadoop/app/zookeeper-3.4.5/tmp#在最后添加：server.1=hadoop05:2888:3888server.2=hadoop06:2888:3888server.3=hadoop07:2888:3888#保存退出 1.3.创建一个tmp文件夹，创建myid文件12mkdir /home/hadoop/app/zookeeper-3.4.5/tmpecho 1 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid 1.4.将配置好的zookeeper拷贝到其他节点(首先分别在mini6、mini7根目录下创建一个hadoop目录：mkdir /hadoop)12scp -r /home/hadoop/app/zookeeper-3.4.5/ mini6:/home/hadoop/app/scp -r /home/hadoop/app/zookeeper-3.4.5/ mini7:/home/hadoop/app/ 1.5.修改mini6、mini7对应/hadoop/zookeeper-3.4.5/tmp/myid内容1234#mini6： echo 2 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid#mini7： echo 3 &gt; /home/hadoop/app/zookeeper-3.4.5/tmp/myid 3.2.安装配置hadoop集群（在mini1上操作）3.2.1解压1tar -zxvf hadoop-2.6.4.tar.gz -C /home/hadoop/app/ 3.2.2配置HDFS&emsp;hadoop2.0所有的配置文件都在$HADOOP_HOME/etc/hadoop目录下 cd /home/hadoop/app/hadoop-2.6.4/etc/hadoop12345#将hadoop添加到环境变量中vim /etc/profileexport JAVA_HOME=/usr/java/jdk1.7.0_55export HADOOP_HOME=/hadoop/hadoop-2.6.4export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin 3.2.3.修改hadoo-env.sh1export JAVA_HOME=/home/hadoop/app/jdk1.7.0_55 3.2.4.修改core-site.xml123456789101112131415161718192021###############################################################################&lt;configuration&gt;&lt;!-- 指定hdfs的nameservice为ns1 --&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://bi/&lt;/value&gt; #这是两个namenode的逻辑名称&lt;/property&gt;&lt;!-- 指定hadoop临时目录 --&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/app/hdpdata/&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定zookeeper主机所在地址 --&gt;&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; ############################################################################### 3.2.5.修改hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485############################################################################### &lt;configuration&gt;&lt;!--指定hdfs的nameservice为bi，需要和core-site.xml中的保持一致 --&gt;&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;bi&lt;/value&gt; #配置的namenode的名称空间，可以有多个，多个就是联邦机制&lt;/property&gt;&lt;!-- bi下面有两个NameNode，分别是nn1，nn2 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.bi&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt; #指定只是指定名称，下面会指定物理地址 &lt;/property&gt; &lt;!-- nn1的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.bi.nn1&lt;/name&gt; #物理地址&lt;value&gt;mini1:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn1的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.bi.nn1&lt;/name&gt;&lt;value&gt;mini1:50070&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的RPC通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.bi.nn2&lt;/name&gt;&lt;value&gt;mini2:9000&lt;/value&gt;&lt;/property&gt;&lt;!-- nn2的http通信地址 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.bi.nn2&lt;/name&gt;&lt;value&gt;mini2:50070&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定NameNode的edits元数据在JournalNode上的存放位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://mini5:8485;mini6:8485;mini7:8485/bi&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定JournalNode在本地磁盘存放数据的位置 --&gt;&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/home/hadoop/journaldata&lt;/value&gt;&lt;/property&gt; &lt;!-- 开启NameNode失败自动切换 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 配置失败自动切换实现方式 --&gt;&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.bi&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt; &lt;!-- 配置隔离机制方法：一个是通过ssh发送kill命令，另一个是调用用户自己的shell脚本，多个机制用换行分割，即每个机制暂用一行--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfenceshell(/bin/true)&lt;/value&gt;&lt;/property&gt; &lt;!-- 使用sshfence隔离机制时需要ssh免登陆 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt; &lt;!-- 配置sshfence隔离机制超时时间 --&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;&lt;value&gt;30000&lt;/value&gt;&lt;/property&gt; &lt;/configuration&gt; ############################################################################### 3.2.6.修改mapred-site.xml1234567891011############################################################################### &lt;configuration&gt;&lt;!-- 指定mr框架为yarn方式 --&gt;&lt;property&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; ############################################################################### 3.2.7.修改yarn-site.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344############################################################################### &lt;configuration&gt;&lt;!-- 开启RM高可用 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定RM的cluster id --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;&lt;value&gt;yrc&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定RM的名字 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;&lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt; &lt;!-- 分别指定RM的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;&lt;value&gt;mini3&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;&lt;value&gt;mini4&lt;/value&gt;&lt;/property&gt; &lt;!-- 指定zk集群地址（yarn的高可用，也是交给zk） --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;&lt;value&gt;mini5:2181,mini6:2181,mini7:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; ############################################################################### 3.2.8.修改slavesslaves是指定子节点的位置，因为要在mini1上启动HDFS、在mini3启动yarn，所以mini1上的slaves文件指定的是datanode的位置，mini3上的slaves文件指定的是nodemanager的位置)mini5mini6mini7 3.2.9.配置免密码登陆12345678910111213141516171819202122232425#首先要配置hadoop00到hadoop01、hadoop02、hadoop03、hadoop04、hadoop05、hadoop06、hadoop07的免密码登陆#在hadoop01上生产一对钥匙ssh-keygen -t rsa#将公钥拷贝到其他节点，包括自己ssh-coyp-id hadoop00ssh-coyp-id hadoop01ssh-coyp-id hadoop02ssh-coyp-id hadoop03ssh-coyp-id hadoop04ssh-coyp-id hadoop05ssh-coyp-id hadoop06ssh-coyp-id hadoop07#配置hadoop02到hadoop04、hadoop05、hadoop06、hadoop07的免密码登陆#在hadoop02上生产一对钥匙ssh-keygen -t rsa#将公钥拷贝到其他节点ssh-coyp-id hadoop03 ssh-coyp-id hadoop04ssh-coyp-id hadoop05ssh-coyp-id hadoop06ssh-coyp-id hadoop07#注意：两个namenode之间要配置ssh免密码登陆，别忘了配置hadoop01到hadoop00的免登陆在hadoop01上生产一对钥匙ssh-keygen -t rsassh-coyp-id -i hadoop00 3.2.10.将配置好的hadoop拷贝到其他节点123456scp -r /hadoop/ hadoop02:/ scp -r /hadoop/ hadoop03:/ scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop04:/hadoop/ scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop05:/hadoop/ scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop06:/hadoop/ scp -r /hadoop/hadoop-2.6.4/ hadoop@hadoop07:/hadoop/ 4.启动4.1.启动zookeeper集群 分别在mini5、mini6、mini7上启动zk1234cd /hadoop/zookeeper-3.4.5/bin/./zkServer.sh start#查看状态：一个leader，两个follower./zkServer.sh status 4.2.启动journalnode 分别在在mini5、mini6、mini7上执行123cd /hadoop/hadoop-2.6.4sbin/hadoop-daemon.sh start journalnode#运行jps命令检验，hadoop05、hadoop06、hadoop07上多了JournalNode进程 4.3.格式化HDFS12345#在mini1上执行命令:hdfs namenode -format#格式化后会在根据core-site.xml中的hadoop.tmp.dir配置生成个文件，这里我配置的是/hadoop/hadoop-2.6.4/tmp，然后将/hadoop/hadoop-2.6.4/tmp拷贝到hadoop02的/hadoop/hadoop-2.6.4/下。因为两个namenode要完全一致scp -r tmp/ hadoop02:/home/hadoop/app/hadoop-2.6.4/##也可以这样，建议hdfs namenode -bootstrapStandby 4.4.格式化ZKFC 在mini1上执行一次即可1hdfs zkfc -formatZK 4.5.启动HDFS 在mini1上执行1sbin/start-dfs.sh 4.6.启动YARN123456###注意#####：是在mini2上执行start-yarn.sh，把namenode和resourcemanager分开是因为性能问题，因为他们都要占用大量资源，所以把他们分开了，他们分开了就要分别在不同的机器上启动) sbin/start-yarn.sh #启动yarn的机器到其他的机器也是要配置免密登录#在另外一台resource manager的机器上启动yarn yarn-daemon.sh start resourcemanager 5.测试123456789101112131415161718192021222324252627282930313233 #统计浏览器访问: http://mini1:50070 NameNode &apos;mini1:9000&apos; (active) http://mini2:50070 NameNode &apos;mini2:9000&apos; (standby) #验证HDFS HA#1.首先向hdfs上传一个文件 hadoop fs -put /etc/profile /profile hadoop fs -ls / #2.然后再kill掉active的NameNode kill -9 &lt;pid of NN&gt; #3.通过浏览器访问：http://192.168.1.202:50070 NameNode &apos;hadoop02:9000&apos; (active) #4.这个时候hadoop02上的NameNode变成了active， 在执行命令： hadoop fs -ls / -rw-r--r-- 3 root supergroup 1926 2014-02-06 15:36 /profile#5. 刚才上传的文件依然存在！！！ 手动启动那个挂掉的NameNode sbin/hadoop-daemon.sh start namenode#6. 通过浏览器访问：http://192.168.1.201:50070 NameNode &apos;hadoop01:9000&apos; (standby) #7.验证YARN： ##运行一下hadoop提供的demo中的WordCount程序： hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar wordcount /profile /out OK，大功告成！！！ 6.测试集群工作状态的一些指令123456789bin/hdfs dfsadmin -report #查看hdfs的各节点状态信息 bin/hdfs haadmin -getServiceState nn1 #获取一个namenode节点的HA状态 sbin/hadoop-daemon.sh start namenode #单独启动一个namenode进程 ./hadoop-daemon.sh start zkfc #单独启动一个zkfc进程","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"hadoop的各种启动命令","date":"2017-04-16T04:47:24.998Z","path":"2017/04/16/bigdata/hadoop/hadoop的各种启动命令/","text":"1.格式化1.1.启动的命令1hadoop namenode -format 1.2.生成的目录1234567891011121314151617181920[root@hdp-node-01 hadoop-2.6.4]# tree /home/hadoop/data/name//home/hadoop/data/name/|-- current| |-- VERSION| |-- edits_0000000000000000001-0000000000000000002| |-- edits_0000000000000000003-0000000000000000010| |-- edits_0000000000000000011-0000000000000000012| |-- edits_0000000000000000013-0000000000000000014| |-- edits_0000000000000000015-0000000000000000016| |-- edits_0000000000000000017-0000000000000000018| |-- edits_0000000000000000019-0000000000000000020| |-- edits_inprogress_0000000000000000021| |-- fsimage_0000000000000000018| |-- fsimage_0000000000000000018.md5| |-- fsimage_0000000000000000020| |-- fsimage_0000000000000000020.md5| `-- seen_txid`-- in_use.lock 1 directory, 15 files 2.手动启动namenode2.1.启动的命令1hadoop-daemons.sh start namenode 2.2.生成的目录namenode的文件目录是格式化的时候生成的，在：./etc/hadoop/hdfs-site.xml配置123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.手动启动datanode3.1.启动的命令1hadoop-daemons.sh start datanode 3.2.生成的目录在：./etc/hadoop/hdfs-site.xml配置123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动datanode的时候会生成datanode文件目录12345678910111213141516171819[root@hdp-node-01 hadoop-2.6.4]# tree /home/hadoop/data/data//home/hadoop/data/data/|-- current| |-- BP-1263609123-192.168.0.11-1479467214259| | |-- current| | | |-- VERSION| | | |-- finalized| | | | `-- subdir0| | | | `-- subdir0| | | | |-- blk_1073741825| | | | `-- blk_1073741825_1001.meta| | | `-- rbw| | |-- dncp_block_verification.log.curr| | |-- dncp_block_verification.log.prev| | `-- tmp| `-- VERSION`-- in_use.lock 8 directories, 7 files 4.启动yarn4.1.启动的命令1start-yarn.sh 5.集群启动12345678910start-dfs.sh #启动namenode和datanodestart-yarn.sh #启动yarn#集群关闭stop-yarn.shstop-dfs.sh","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"hadoop的shell命令操作","date":"2017-04-16T04:47:24.994Z","path":"2017/04/16/bigdata/hadoop/hadoop的shell命令操作/","text":"-ls功能:显示目录信息示例: hadoop fs -ls hdfs://hadoop-server01:9000/备注：这些参数中，所有的hdfs路径都可以简写–&gt;hadoop fs -ls / 等同于上一条命令的效果 -mkdir功能：在hdfs上创建目录，-p 递归创建示例：hadoop fs -mkdir -p /aaa/bbb/cc/dd -moveFromLocal功能：从本地剪切粘贴到hdfs示例：hadoop fs - moveFromLocal /home/hadoop/a.txt /aaa/bbb/cc/dd -moveToLocal功能：从hdfs剪切粘贴到本地示例：hadoop fs - moveToLocal /aaa/bbb/cc/dd /home/hadoop/a.txt -appendToFile功能：追加一个文件到已经存在的文件末尾示例：hadoop fs -appendToFile ./hello.txt hdfs://hadoop-server01:9000/hello.txt可以简写为：Hadoop fs -appendToFile ./hello.txt /hello.txt -cat功能：显示文件内容示例：hadoop fs -cat /hello.txt -tail功能：显示一个文件的末尾示例：hadoop fs -tail /weblog/access_log.1 -text功能：以字符形式打印一个文件的内容示例：hadoop fs -text /weblog/access_log.1 -chgrp-chmod-chown功能：linux文件系统中的用法一样，对文件所属权限示例：hadoop fs -chmod 666 /hello.txthadoop fs -chown someuser:somegrp /hello.txt -copyFromLocal功能：从本地文件系统中拷贝文件到hdfs路径去示例：hadoop fs -copyFromLocal ./jdk.tar.gz /aaa/ -copyToLocal功能：从hdfs拷贝到本地示例：hadoop fs -copyToLocal /aaa/jdk.tar.gz -cp功能：从hdfs的一个路径拷贝hdfs的另一个路径示例： hadoop fs -cp /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -mv功能：在hdfs目录中移动文件示例： hadoop fs -mv /aaa/jdk.tar.gz / -get功能：等同于copyToLocal，就是从hdfs下载文件到本地示例：hadoop fs -get /aaa/jdk.tar.gz -getmerge功能：合并下载多个文件示例：比如hdfs的目录 /aaa/下有多个文件:log.1, log.2,log.3,…hadoop fs -getmerge /aaa/log.* ./log.sum -put功能：等同于copyFromLocal示例：hadoop fs -put /aaa/jdk.tar.gz /bbb/jdk.tar.gz.2 -rm功能：删除文件或文件夹示例：hadoop fs -rm -r /aaa/bbb/-r表示目录 -rmdir功能：删除空目录示例：hadoop fs -rmdir /aaa/bbb/ccc -df功能：统计文件系统的可用空间信息示例：hadoop fs -df -h / -du功能：统计文件夹的大小信息示例：hadoop fs -du -s -h hadoop://mini:9000/hadoop fs -du -s -h /aaa/ -count功能：统计一个指定目录下的文件节点数量示例：hadoop fs -count /aaa/ -setrep功能：设置hdfs中文件的副本数量示例：hadoop fs -setrep 3 /aaa/jdk.tar.gz&lt;这里设置的副本数只是记录在namenode的元数据中，是否真的会有这么多副本，还得看datanode的数量,如果datanode的数量为3，那么真实的副本数量只有3份，但是如果添加一个机器，那么机器数量就变成了4，此时副本数变成了4&gt; 查看节点的详细信息（相当于web页面展示的信息）hdfs dfsadmin -report [root@hdp-node-01 hadoop-2.6.4]# hdfs dfsadmin -reportConfigured Capacity: 7718977536 (7.19 GB)Present Capacity: 4682670080 (4.36 GB)DFS Remaining: 4682625024 (4.36 GB)DFS Used: 45056 (44 KB)DFS Used%: 0.00%Under replicated blocks: 1Blocks with corrupt replicas: 0Missing blocks: 0 Live datanodes (1): Name: 192.168.0.11:50010 (hdp-node-01)Hostname: hdp-node-01Decommission Status : NormalConfigured Capacity: 7718977536 (7.19 GB)DFS Used: 45056 (44 KB)Non DFS Used: 3036356608 (2.83 GB)DFS Remaining: 4682575872 (4.36 GB)DFS Used%: 0.00%DFS Remaining%: 60.66%Configured Cache Capacity: 0 (0 B)Cache Used: 0 (0 B)Cache Remaining: 0 (0 B)Cache Used%: 100.00%Cache Remaining%: 0.00%Xceivers: 1Last contact: Fri Nov 18 19:27:56 CST 2016","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"hadoop文件目录说明","date":"2017-04-16T04:47:24.990Z","path":"2017/04/16/bigdata/hadoop/hadoop文件目录说明/","text":"1.主文件列表123456789101112131415161718[root@hdp-node-01 hadoop]# cd app/hadoop-2.6.4/[root@hdp-node-01 hadoop-2.6.4]# lltotal 68drwxrwxr-x 2 hadoop hadoop 4096 Mar 8 2016 bindrwxrwxr-x 3 hadoop hadoop 4096 Mar 8 2016 etcdrwxrwxr-x 2 hadoop hadoop 4096 Mar 8 2016 includedrwxrwxr-x 3 hadoop hadoop 4096 Mar 8 2016 libdrwxrwxr-x 2 hadoop hadoop 4096 Mar 8 2016 libexec-rw-r--r-- 1 hadoop hadoop 15429 Mar 8 2016 LICENSE.txtdrwxr-xr-x 3 root root 4096 Nov 18 12:09 logs-rw-r--r-- 1 hadoop hadoop 101 Mar 8 2016 NOTICE.txt-rw-r--r-- 1 hadoop hadoop 1366 Mar 8 2016 README.txt-rw-r--r-- 1 root root 2025 Nov 18 02:55 root@hdp-node-02-rw-r--r-- 1 root root 2025 Nov 18 02:55 root@hdp-node-03-rw-r--r-- 1 root root 2025 Nov 18 02:55 root@hdp-node-04drwxrwxr-x 2 hadoop hadoop 4096 Mar 8 2016 sbindrwxrwxr-x 4 hadoop hadoop 4096 Mar 8 2016 share[root@hdp-node-01 hadoop-2.6.4]# 2.include目录是c语言本地库的头文件 3.lib是本地库 4.share是存放的jar包（common、hdfs、yarn等jar），和doc文档 5.logs是日志文件，如果出现错误，看日志，很重要123456789101112#启动hdfs的时候，会有下面的打印信息[root@hdp-node-01 current]# start-dfs.shStarting namenodes on [hdp-node-01]hdp-node-01: starting namenode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.out#hdp-node-01: starting namenode 在hdp-node-01上启动了namenodehdp-node-01: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-01.outhdp-node-03: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-03.outhdp-node-02: starting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-02.out#到hdp-node-01机器下，执行：可以看打印的日志，一般的报错信息会显示在下面的日志文件中tail /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.log 6.sbin目录123456789101112131415161718192021222324252627282930313233343536373839404142&apos;是一些启动停止脚本&apos;-rwxr-xr-x 1 hadoop hadoop 6452 Mar 8 2016 hadoop-daemon.sh-rwxr-xr-x 1 hadoop hadoop 1360 Mar 8 2016 hadoop-daemons.sh-rwxr-xr-x 1 hadoop hadoop 1471 Mar 8 2016 start-all.sh-rwxr-xr-x 1 hadoop hadoop 1462 Mar 8 2016 stop-all.sh-rwxr-xr-x 1 hadoop hadoop 3206 Mar 8 2016 stop-dfs.sh-rwxr-xr-x 1 hadoop hadoop 3705 Mar 8 2016 start-dfs.sh-rwxr-xr-x 1 hadoop hadoop 1347 Mar 8 2016 start-yarn.sh-rwxr-xr-x 1 hadoop hadoop 1340 Mar 8 2016 stop-yarn.sh-rwxr-xr-x 1 hadoop hadoop 4295 Mar 8 2016 yarn-daemon.sh-rwxr-xr-x 1 hadoop hadoop 1353 Mar 8 2016 yarn-daemons.sh#可以使用hadoop-daemon来启动[root@hdp-node-01 sbin]# hadoop-daemon.shUsage: hadoop-daemon.sh [--config &lt;conf-dir&gt;] [--hosts hostlistfile] [--script script] (start|stop) &lt;hadoop-command&gt; &lt;args...&gt;[root@hdp-node-01 sbin]# hadoop-daemon.sh start datanodestarting datanode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-datanode-hdp-node-01.out[root@hdp-node-01 sbin]# jps1082 Jps1052 DataNode[root@hdp-node-01 sbin]# hadoop-daemon.sh start namenodestarting namenode, logging to /home/hadoop/app/hadoop-2.6.4/logs/hadoop-root-namenode-hdp-node-01.out[root@hdp-node-01 sbin]# jps1052 DataNode1205 Jps1137 NameNode[root@hdp-node-01 sbin]#/*hadoop-daemon.sh start datanode #在本地手动启动datanodehadoop-daemon.sh start namenode #在本地手动启动namenode动态扩容：可以使用上面的方式手动启动增加的机器之后，再添加slave，这样就可以运行了，然后下一次启动的时候，会读slave，也是可以自动启动添加的机器start-all.sh是将hdfs启动之后，然后启动yarn的，我们并不建议这样做因为我们先启动hdfs看是否报错，然后启动yarn看，看是否报错，这样不容易出错*/ 7.配置文件12345678910111213141516171819202122232425262728[root@hdp-node-01 hadoop-2.6.4]# tree /home/hadoop/app/hadoop-2.6.4/etc/ /home/hadoop/app/hadoop-2.6.4/etc/-- hadoop |-- capacity-scheduler.xml |-- configuration.xsl |-- container-executor.cfg |-- &apos;core-site.xml&apos; |-- &apos;hadoop-env.sh&apos; |-- hadoop-policy.xml |-- &apos;hdfs-site.xml&apos; |-- httpfs-env.sh |-- httpfs-log4j.properties |-- httpfs-signature.secret |-- httpfs-site.xml |-- kms-log4j.properties |-- kms-site.xml |-- log4j.properties |-- mapred-env.cmd |-- mapred-env.sh |-- mapred-queues.xml.template |-- mapred-site.xml |--&apos;slaves&apos; #这里配置就是为了自动化启动脚本(start-dfs.sh、start-yarn.sh)要用，如果不是自动化启动，我们也可以手动启动，那么就不必要使用该配置文件 |-- ssl-client.xml.example |-- ssl-server.xml.example |-- yarn-env.sh -- &apos;yarn-site.xml&apos; 1 directory, 29 files 8.基础目录在core-site.xml配置文件中配置如下：1234567891011121314151617&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.4/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;/*namenode、datanode、secondary namenode的目录默认是在基础目录的前提下配置的，如：dfs.namenode.name.dir file://$&#123;hadoop.tmp.dir&#125;/dfs/namedfs.datanode.data.dir file://$&#123;hadoop.tmp.dir&#125;/dfs/data dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary #以上两个参数做checkpoint操作时，secondary namenode的本地工作目录dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;*/ 9.namenode、datanode的目录 在hdfs-site.xml配置文件中配置如下： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/data/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 查看 12345678910111213141516171819202122232425262728293031323334353637[root@hdp-node-01 hadoop]# tree /home/hadoop/data//home/hadoop/data/|-- data # &apos;datanode对应的目录&apos;| |-- current | | |-- BP-1263609123-192.168.0.11-1479467214259| | | |-- current| | | | |-- VERSION| | | | |-- finalized| | | | | `-- subdir0| | | | | `-- subdir0| | | | | |-- blk_1073741825| | | | | `-- blk_1073741825_1001.meta| | | | `-- rbw| | | |-- dncp_block_verification.log.curr| | | |-- dncp_block_verification.log.prev| | | `-- tmp| | `-- VERSION| `-- in_use.lock`-- name # &apos;namenode对应的目录&apos; |-- current | |-- VERSION | |-- edits_0000000000000000001-0000000000000000002 | |-- edits_0000000000000000003-0000000000000000010 | |-- edits_0000000000000000011-0000000000000000012 | |-- edits_0000000000000000013-0000000000000000014 | |-- edits_0000000000000000015-0000000000000000016 | |-- edits_0000000000000000017-0000000000000000018 | |-- edits_0000000000000000019-0000000000000000020 | |-- edits_inprogress_0000000000000000021 | |-- fsimage_0000000000000000018 | |-- fsimage_0000000000000000018.md5 | |-- fsimage_0000000000000000020 | |-- fsimage_0000000000000000020.md5 | `-- seen_txid `-- in_use.lock #上面是既有namenode又有datanode的机器，例如如果是只有datanode的机器，那么将只会有data目录 11.secondary namenode的目录在hdfs-site.xml配置文件中配置如下： 12#如果没有配置，默认如下：dfs.namenode.checkpoint.dir=file://$&#123;hadoop.tmp.dir&#125;/dfs/namesecondary 在core-site.xml中${hadoop.tmp.dir}配置如下：123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/app/hadoop-2.6.4/hadoopdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 所以namesecondary的目录结构为：123456789101112131415161718192021222324[root@hdp-node-01 hadoop-2.6.4]# tree /home/hadoop/app/hadoop-2.6.4/hadoopdata//home/hadoop/app/hadoop-2.6.4/hadoopdata/|-- dfs| `-- namesecondary| |-- current| | |-- VERSION| | |-- edits_0000000000000000001-0000000000000000002| | |-- edits_0000000000000000003-0000000000000000010| | |-- edits_0000000000000000011-0000000000000000012| | |-- edits_0000000000000000013-0000000000000000014| | |-- edits_0000000000000000015-0000000000000000016| | |-- edits_0000000000000000017-0000000000000000018| | |-- edits_0000000000000000019-0000000000000000020| | |-- fsimage_0000000000000000018| | |-- fsimage_0000000000000000018.md5| | |-- fsimage_0000000000000000020| | `-- fsimage_0000000000000000020.md5| `-- in_use.lock`-- nm-local-dir |-- filecache |-- nmPrivate `-- usercache 7 directories, 13 files 12.edits（日志文件）的目录在hdfs-site.xml配置文件中配置如下：12#如果没有配置，默认如下：dfs.namenode.checkpoint.edits.dir=$&#123;dfs.namenode.checkpoint.dir&#125;","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"hadoop安装过程中的问题集锦","date":"2017-04-16T04:47:24.988Z","path":"2017/04/16/bigdata/hadoop/hadoop安装过程中的问题集锦/","text":"yarn没有启动 1216/11/18 13:06:22 INFO client.RMProxy: Connecting to ResourceManager at hdp-node-01/192.168.0.11:8032 #ResourceManager (没有连接上ResourceManager 的话就说明yarn没有启动16/11/18 13:06:23 INFO ipc.Client: Retrying connect to server: hdp-node-01/192.168.0.11:8032. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS) datanode不被namenode识别的问题namenode在format初始化的时候会形成两个标识：blockPoolId、clusterId新的datanode加入的时候，会获取这两个标识作为自己的工作目录中你的标识一旦namenode重新format后，namenode的身份标识已变，而datanode持有原来的id，就不会被namenode识别 修改了name node 的存储目录&emsp;如果修改了name node的存储目录，那么需要重新格式化name node ，此时和原来关联的data node的工作目录中的数据将不能使用了，因为原来data node中的数据version是和name node 关联的，所以此时需要将data node中的数据删除，因为name node是记录data node中的数据的，所以如果name node 重新初始化了，那么data node留着就没有意义了 副本数量的问题副本数由客户端的参数dfs.replication决定（优先级： conf.setProperty&gt; 自定义配置文件 &gt; jar包中的hdfs-default.xml）","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"datanode的工作机制","date":"2017-04-16T04:47:24.945Z","path":"2017/04/16/bigdata/hadoop/datanode的工作机制/","text":"1.问题场景 集群容量不够，怎么扩容？ 如果有一些datanode宕机，该怎么办？ datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？ 2.Datanode工作职责 存储管理用户的文件块数据 定期向namenode汇报自身所持有的block信息（通过心跳信息上报）（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题） 12345&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; 3.Datanode掉线判断时限参数datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：1timeout = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval 而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒,需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒12345678&lt;property&gt; &lt;name&gt;heartbeat.recheck.interval&lt;/name&gt; &lt;value&gt;2000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt; HDFS冗余数据块的自动删除 在日常维护hadoop集群的过程中发现这样一种情况：某个节点由于网络故障或者DataNode进程死亡，被NameNode判定为死亡，HDFS马上自动开始数据块的容错拷贝；当该节点重新添加到集群中时，由于该节点上的数据其实并没有损坏，所以造成了HDFS上某些block的备份数超过了设定的备份数。通过观察发现，这些多余的数据块经过很长的一段时间才会被完全删除掉，那么这个时间取决于什么呢？该时间的长短跟数据块报告的间隔时间有关。Datanode会定期将当前该结点上所有的BLOCK信息报告给Namenode，参数dfs.blockreport.intervalMsec就是控制这个报告间隔的参数。hdfs-site.xml文件中有一个参数：12345&lt;property&gt; &lt;name&gt;dfs.blockreport.intervalMsec&lt;/name&gt; &lt;value&gt;3600000&lt;/value&gt; &lt;description&gt;Determines block reporting interval in milliseconds.&lt;/description&gt;&lt;/property&gt; 其中3600000为默认设置，3600000毫秒，即1个小时，也就是说，块报告的时间间隔为1个小时，所以经过了很长时间这些多余的块才被删除掉。通过实际测试发现，当把该参数调整的稍小一点的时候（60秒），多余的数据块确实很快就被删除了。 4.datanode不被namenode识别的问题namenode在format初始化的时候会形成两个标识：blockPoolId、clusterId, 新的datanode加入的时候，会获取这两个标识作为自己的工作目录中你的标识，一旦namenode重新format后，namenode的身份标识已变，而datanode持有原来的id，就不会被namenode识别 修改了name node 的存储目录 &emsp;如果修改了name node的存储目录，那么需要重新格式化name node ，此时和原来关联的data node的工作目录中的数据将不能使用了，因为原来data node中的数据version是和name node 关联的，所以此时需要将data node中的数据删除，因为name node是记录data node中的数据的，所以如果name node 重新初始化了，那么data node留着就没有意义了","tags":[{"name":"hadoop","slug":"hadoop","permalink":"http://yoursite.com/tags/hadoop/"}]},{"title":"flume采集案例","date":"2017-04-16T04:47:24.942Z","path":"2017/04/16/bigdata/flume/flume采集案例/","text":"下面是flume的几种使用案例,更多的案例可以参见flume官网 1.采集文件到HDFS1.1.采集需求比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs 根据需求，首先定义以下3大要素 采集源，即source——监控文件内容更新 : exec ‘tail -F file’ 下沉目标，即sink——HDFS文件系统 : hdfs sink Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel &emsp;就是通过去执行（exec)一个命令(tail) 看文件的内容是否有更新，如果有就将更新的内容添加到hdfs中 1.2.配置文件用tail命令获取数据，下沉到hdfsvim ./conf/tail-hdfs.conf 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 #exec 指的是命令# Describe/configure the sourcea1.sources.r1.type = exec#F根据文件名追中, f根据文件的nodeid追中a1.sources.r1.command = tail -F /home/hadoop/log/test.loga1.sources.r1.channels = c1 # Describe the sink#下沉目标a1.sinks.k1.type = hdfsa1.sinks.k1.channel = c1#指定目录, flum帮做目的替换a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/ #采集到hdfs中, 文件中的目录不用自己建的#文件的命名, 前缀a1.sinks.k1.hdfs.filePrefix = events- #10 分钟就改目录a1.sinks.k1.hdfs.round = truea1.sinks.k1.hdfs.roundValue = 10a1.sinks.k1.hdfs.roundUnit = minute #文件滚动之前的等待时间(秒)a1.sinks.k1.hdfs.rollInterval = 3 #文件滚动的大小限制(bytes)a1.sinks.k1.hdfs.rollSize = 500 #写入多少个event数据后滚动文件(事件个数)a1.sinks.k1.hdfs.rollCount = 20 #5个事件就往里面写入a1.sinks.k1.hdfs.batchSize = 5 #用本地时间格式化目录a1.sinks.k1.hdfs.useLocalTimeStamp = true #下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 1.3.提供测试数据创建目录，循环向文件添加内容1234567mkdir /home/hadoop/log while truedoecho 111111 &gt;&gt; /home/hadoop/log/test.logsleep 0.5done 1.4.启动命令1bin/flume-ng agent -c conf -f conf/tail-hdfs.conf -n a1 1.5.前端页面查看在： master:50070, 文件目录: /flum/events/ 2.采集目录到HDFS2.1.采集需求&emsp;采集需求：某服务器的某特定目录下，会不断产生新的文件(而不是以存在的文件中的内容的变化)，每当有新文件出现，就需要把文件采集到HDFS中去，根据需求，首先定义以下3大要素： 采集源，即source——监控文件目录 : spooldir 下沉目标，即sink——HDFS文件系统 : hdfs sink source和sink之间的传递通道——channel，可用file channel 也可以用内存channel 2.2.配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#spooldir-hdfs.conf#定义三大组件的名称agent1.sources = source1agent1.sinks = sink1agent1.channels = channel1 # 配置source组件#采集源使用的协议类型agent1.sources.source1.type = spooldir #监控的目录agent1.sources.source1.spoolDir = /home/hadoop/logs/ agent1.sources.source1.fileHeader = false #配置拦截器#agent1.sources.source1.interceptors = i1#agent1.sources.source1.interceptors.i1.type = host#agent1.sources.source1.interceptors.i1.hostHeader = hostname # 配置sink组件#下沉到hdfsagent1.sinks.sink1.type = hdfs #指定目录, flum帮做目的替换agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M #文件的命名, 前缀 agent1.sinks.sink1.hdfs.filePrefix = access_log agent1.sinks.sink1.hdfs.maxOpenFiles = 5000 #100个事件就往里面写入agent1.sinks.sink1.hdfs.batchSize= 100 #下沉后, 生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本agent1.sinks.sink1.hdfs.fileType = DataStream agent1.sinks.sink1.hdfs.writeFormat =Text #文件滚动的大小限制(bytes)agent1.sinks.sink1.hdfs.rollSize = 102400 #写入多少个event数据后滚动文件(事件个数)agent1.sinks.sink1.hdfs.rollCount = 1000000 #文件滚动（生成新文件）之前的等待时间(秒)agent1.sinks.sink1.hdfs.rollInterval = 60 #10 分钟就改目录#agent1.sinks.sink1.hdfs.round = true#agent1.sinks.sink1.hdfs.roundValue = 10#agent1.sinks.sink1.hdfs.roundUnit = minute#用本地时间格式化目录agent1.sinks.sink1.hdfs.useLocalTimeStamp = true # Use a channel which buffers events in memory#events in memoryagent1.channels.channel1.type = memory #event添加到通道中或者移出的允许时间agent1.channels.channel1.keep-alive = 120 #默认该通道中最大的可以存储的event数量 agent1.channels.channel1.capacity = 500000 #每次最大可以从source中拿到或者送到sink中的event数量agent1.channels.channel1.transactionCapacity = 600 # Bind the source and sink to the channelagent1.sources.source1.channels = channel1agent1.sinks.sink1.channel = channel1 2.3.启动命令1bin/flume-ng agent -c conf -f conf/spooldir-hdfs.conf -n agent1 注意： 添加到监控目录中的文件，最后将会被改名（添加后缀：COMPLETED ） 前端页面查看在： master:50070, 文件目录: /weblog/flume-collection/ 3.目录到控制台3.1.配置文件源：目录文件的变化目标：console（控制台） &emsp;使用spooldir协议去监听指定目录（/home/hadoop/flumespool）是否有变化，如果有就将变化打印到console中12345678910111213141516171819202122#[root@hdp-node-01 flume]# cat ./conf/spooldir-hdfs.conf#Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = spooldira1.sources.r1.spoolDir = /home/hadoop/flumespoola1.sources.r1.fileHeader = true # Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 3.2.启动1bin/flume-ng agent -c ./conf -f ./conf/spooldir-hdfs.conf -n a1 -Dflume.root.logger=INFO,console 3.3测试1234567891011121314[root@hdp-node-01 hadoop]# cat test.txtaa_testbb_testcc_test[root@hdp-node-01 hadoop]# mv test.txt /home/hadoop/flumespool/&apos;console打印信息&apos;2016-11-24 19:23:33,941 (pool-3-thread-1) [INFO - org.apache.flume.client.avro.ReliableSpoolingFileEventReader.rollCurrentFile(ReliableSpoolingFileEventReader.java:348)] Preparing to move file /home/hadoop/flumespool/test.txt to /home/hadoop/flumespool/test.txt.COMPLETED #将mv 到目录的文件改名，以后缀：.COMPLETED 结尾#从下面可以看出，向目录中添加了一个文件，实际上是将文件中的每一行当做一个Event，下沉到console中2016-11-24 19:23:33,941 (SinkRunner-PollingRunner-&apos;DefaultSinkProcessor&apos;) [INFO - org.apache.flume.&apos;sink&apos;.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;file=/home/hadoop/flumespool/test.txt&#125; body: 61 61 5F 74 65 73 74 &apos;aa_test &apos;&#125; 2016-11-24 19:23:33,942 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;file=/home/hadoop/flumespool/test.txt&#125; body: 62 62 5F 74 65 73 74 &apos;bb_test &apos;&#125;2016-11-24 19:23:33,942 (SinkRunner-PollingRunner-DefaultSinkProcessor) [INFO - org.apache.flume.sink.LoggerSink.process(LoggerSink.java:94)] Event: &#123; headers:&#123;file=/home/hadoop/flumespool/test.txt&#125; body: 63 63 5F 74 65 73 74 &apos; cc_test&apos; &#125; 注意: 如果向监听目录中添加一个文件，那么会将文件中的内容以行的形式下沉到console中 添加到监控目录中的文件，最后将会被改名（添加后缀：COMPLETED ）","tags":[{"name":"flume","slug":"flume","permalink":"http://yoursite.com/tags/flume/"}]},{"title":"flume的简介及安装","date":"2017-04-16T04:47:24.941Z","path":"2017/04/16/bigdata/flume/flume的简介及安装/","text":"1.概述 Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。 Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中 一般的采集需求，通过对flume的简单配置即可实现 Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景 2.运行机制 Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成 通常会将一个文件中的一行单做一个Event事件 每一个agent相当于一个数据传递员(Source 到 Channel 到 Sink之间传递数据的形式是Event事件；Event事件是一个数据流单元)，内部有三个组件: Source：采集源，用于跟数据源对接，以获取数据 Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据 Channel：angent内部的数据传输通道，用于从source将数据传递到sink 3.Flume采集系统结构图3.1.简单结构 单个agent采集数据 3.2.复杂结构多级agent之间串联 4.Flume的安装部署4.1.解压1234567cd /home/hadoop/app/tar -zxvf apache-flume-1.6.0-bin.tar.gz ln -s apache-flume-1.6.0-bin flume #修改conf下的flume-env.sh，在里面配置JAVA_HOME# vim flume-env.shexport JAVA_HOME=/home/hadoop/app/jdk1.7.0_80 4.2.测试案例从网络端口接收数据，下沉到logger先在flume的conf目录下新建一个文件:vi netcat-logger.conf12345678910111213141516171819202122# 定义这个agent中各组件的名字a1.sources = r1 a1.sinks = k1a1.channels = c1 # 描述和配置source组件：r1（上面指定的逻辑名称）a1.sources.r1.type = netcat #协议a1.sources.r1.bind = localhost #绑定的地址和端口a1.sources.r1.port = 44444 # 描述和配置sink组件：k1（上面指定的逻辑名称）a1.sinks.k1.type = logger #下沉到logger中 # 描述和配置channel组件，此处使用是内存缓存的方式，下沉的时候是一批一批的, 下沉的时候是一个个eventChannela1.channels.c1.type = memorya1.channels.c1.capacity = 1000 #默认该通道中最大的可以存储的event数量a1.channels.c1.transactionCapacity = 100 #每次最大可以从source中拿到或者送到sink中的event数量 # 描述和配置source channel sink之间的连接关系a1.sources.r1.channels = c1a1.sinks.k1.channel = c1#注意：上面的a1.sources.r1.channels和a1.sinks.k1.channel 一个没有复数的情况，一个有，这是因为对于Channel来说，源可以有多个，但是对于sink来说，Channel只能有一个，因为一个sink从Channel中取走了数据之后，Channel之中就没有了该数据 2.启动agent去采集数据1234567891011bin/flume-ng agent --conf conf --conf-file conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console #或者下面的方式bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1 -Dflume.root.logger=INFO,console/*-c conf 指定flume自身的配置文件所在目录（这个目录应该包含： flume-env.sh log4j properties file）-f conf/netcat-logger.con 指定我们所描述的采集方案-n a1 指定我们这个agent的名字-Dflume 是给log4j传递的参数，即log4j日志级别-Dflume.root.logger=INFO,console 指定DEBUF模式在console输出INFO信息*/ 打印结果123456782016-11-24 20:48:30,934 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:145)] Starting Channel c12016-11-24 20:48:31,017 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.register(MonitoredCounterGroup.java:120)] Monitored counter group for type: CHANNEL, name: c1: Successfully registered new MBean.2016-11-24 20:48:31,017 (lifecycleSupervisor-1-0) [INFO - org.apache.flume.instrumentation.MonitoredCounterGroup.start(MonitoredCounterGroup.java:96)] Component type: CHANNEL, name: c1 started2016-11-24 20:48:31,021 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:173)] Starting Sink k12016-11-24 20:48:31,022 (conf-file-poller-0) [INFO - org.apache.flume.node.Application.startAllComponents(Application.java:184)] Starting Source r12016-11-24 20:48:31,025 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:150)] Source starting2016-11-24 20:48:31,113 (lifecycleSupervisor-1-4) [INFO - org.apache.flume.source.NetcatSource.start(NetcatSource.java:164)] Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/&apos;127.0.0.1:44444&apos;] 3.测试 先要往agent采集监听的端口上发送数据，让agent有数据可采，随便在一个能跟agent节点联网的机器上 12345678# telnet anget-hostname port （telnet localhost 44444） $ telnet localhost 44444Trying 127.0.0.1...Connected to localhost.localdomain (127.0.0.1).Escape character is &apos;^]&apos;.Hello world! &lt;ENTER&gt;OK","tags":[{"name":"flume","slug":"flume","permalink":"http://yoursite.com/tags/flume/"}]},{"title":"flume多agent连接","date":"2017-04-16T04:47:24.939Z","path":"2017/04/16/bigdata/flume/flume多agent连接/","text":"1.需求下面是多个agent的使用示例,从tail到avro ,再从avro到控制台 2.tail 到avrovim ./conf/tail-avro.conf12345678910111213141516171819202122232425262728# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the sourcea1.sources.r1.type = execa1.sources.r1.command = tail -F /home/hadoop/log/test.loga1.sources.r1.channels = c1 # Describe the sink#绑定的不是本机, 是另外一台机器的服务地址, sink端的avro是一个发送端, avro的客户端, 往hadoop01这个机器上发a1.sinks = k1a1.sinks.k1.type = avroa1.sinks.k1.hostname =hadoop01a1.sinks.k1.port = 4141a1.sinks.k1.batch-size = 2 # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动命令1bin/flume-ng agent -c conf -f conf/tail-avro.conf -n al 3.avro到logger(console)vim ./conf/avro-logger.conf12345678910111213141516171819202122# Name the components on this agenta1.sources = r1a1.sinks = k1a1.channels = c1 # Describe/configure the source#source中的avro组件是接收者服务, 绑定本机a1.sources.r1.type = avroa1.sources.r1.bind = 0.0.0.0a1.sources.r1.port = 4141 # Describe the sinka1.sinks.k1.type = logger # Use a channel which buffers events in memorya1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动命令1bin/flume-ng agent -c conf -f conf/avro-logger.conf -n al -Dflume.root.logger=INFO,console","tags":[{"name":"flume","slug":"flume","permalink":"http://yoursite.com/tags/flume/"}]},{"title":"工作流调度器azkaban的介绍及安装","date":"2017-04-16T04:47:24.937Z","path":"2017/04/16/bigdata/azkaban/工作流调度器azkaban的介绍及安装/","text":"1.为什么需要工作流调度系统 一个完整的数据分析系统通常都是由大量任务单元组成：shell脚本程序，java程序，mapreduce程序、hive脚本等 各任务单元之间存在时间先后及前后依赖关系 为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行； 例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示： 通过Hadoop先将原始数据同步到HDFS上； 借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中； 需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表； 将明细数据进行复杂的统计分析，得到结果报表信息； 需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。 2.Azkaban介绍&emsp;Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。它有如下功能特点： Web用户界面 方便上传工作流 方便设置任务之间的关系 调度工作流 认证/授权(权限的工作) 能够杀死并重新启动工作流 模块化和可插拔的插件机制 项目工作区 工作流和任务的日志记录和审计 3.Azkaban安装部署3.1.准备工作 Azkaban Web服务器azkaban-web-server-2.5.0.tar.gz Azkaban执行服务器azkaban-executor-server-2.5.0.tar.gz MySQL目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root. 3.2.安装&emsp;将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行，在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序 1234567891011121314151617181920mkdir /home/hadoop/azkabantool -pcd /home/hadoop/azkabantool/#azkaban web服务器安装tar -zxvf azkaban-web-server-2.5.0.tar.gz #解压mv azkaban-web-2.5.0/ webserver #改名为webserver#azkaban 执行服器安装tar -zxvf azkaban-executor-server-2.5.0.tar.gz #解压mv azkaban-executor-2.5.0/ executor #改名为executor#azkaban脚本导入tar -zxvf azkaban-sql-script-2.5.0.tar.gz #将解压后的mysql 脚本,导入到mysql中:#进入mysql：mysql&gt; create database azkaban;mysql&gt; use azkaban;Database changedmysql&gt; source /home/hadoop/azkabantool/azkaban-2.5.0/create-all-sql-2.5.0.sql 4.创建SSL配置参考地址: http://docs.codehaus.org/display/JETTY/How+to+configure+SSL123456789101112131415161718192021222324命令: # keytool -keystore keystore -alias jetty -genkey -keyalg RSA运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下: 输入keystore密码： 再次输入新密码:您的名字与姓氏是什么？ [Unknown]： 您的组织单位名称是什么？ [Unknown]： 您的组织名称是什么？ [Unknown]： 您所在的城市或区域名称是什么？ [Unknown]： 您所在的州或省份名称是什么？ [Unknown]： 该单位的两字母国家代码是什么 [Unknown]： CNCN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？ [否]： y 输入&lt;jetty&gt;的主密码 （如果和 keystore 密码相同，按回车）： 再次输入新密码:完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.如: # cp keystore azkaban/server 5.配置文件5.1.时区配置 注：先配置好服务器节点上的时区 先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可 1tzselect 拷贝该时区文件，覆盖系统本地时区配置 1cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime 5.2.azkaban web服务器配置 进入azkaban web服务器安装目录 conf目录，修改azkaban.properties文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# vi azkaban.properties#Azkaban Personalization Settingsazkaban.name=Test #服务器UI名称,用于服务器上方显示的名字azkaban.label=My Local Azkaban #描述azkaban.color=#FF3601 #UI颜色azkaban.default.servlet.path=/index #web.resource.dir=web/ #默认根web目录default.timezone.id=Asia/Shanghai #默认时区,已改为亚洲/上海 默认为美国 #Azkaban UserManager classuser.manager.class=azkaban.user.XmlUserManager #用户权限管理默认类user.manager.xml.file=conf/azkaban-users.xml #用户配置,具体配置参加下文 #Loader for projectsexecutor.global.properties=conf/global.properties # global配置文件所在位置azkaban.project.dir=projects # database.type=mysql #数据库类型mysql.port=3306 #端口号mysql.host=localhost #数据库连接IPmysql.database=azkaban #数据库实例名mysql.user=root #数据库用户名mysql.password=root #数据库密码mysql.numconnections=100 #最大连接数 # Velocity dev modevelocity.dev.mode=false# Jetty服务器属性.jetty.maxThreads=25 #最大线程数jetty.ssl.port=8443 #Jetty SSL端口jetty.port=8081 #Jetty端口jetty.keystore=keystore #SSL文件名jetty.password=123456 #SSL文件密码jetty.keypassword=123456 #Jetty主密码 与 keystore文件相同jetty.truststore=keystore #SSL文件名jetty.trustpassword=123456 # SSL文件密码 # 执行服务器属性executor.port=12321 #执行服务器端口 # 邮件设置mail.sender=xxxxxxxx@163.com #发送邮箱mail.host=smtp.163.com #发送邮箱smtp地址mail.user=xxxxxxxx #发送邮件时显示的名称mail.password=********** #邮箱密码job.failure.email=xxxxxxxx@163.com #任务失败时发送邮件的地址job.success.email=xxxxxxxx@163.com #任务成功时发送邮件的地址lockdown.create.projects=false #cache.directory=cache #缓存目录 5.3.azkaban 执行服务器executor配置 进入执行服务器安装目录conf,修改azkaban.properties 12345678910111213141516171819202122232425# vim azkaban.properties#Azkabandefault.timezone.id=Asia/Shanghai #时区 # Azkaban JobTypes 插件配置azkaban.jobtype.plugin.dir=plugins/jobtypes #jobtype 插件所在位置 #Loader for projectsexecutor.global.properties=conf/global.propertiesazkaban.project.dir=projects #数据库设置database.type=mysql #数据库类型(目前只支持mysql)mysql.port=3306 #数据库端口号mysql.host=192.168.20.200 #数据库IP地址mysql.database=azkaban #数据库实例名mysql.user=root #数据库用户名mysql.password=root #数据库密码mysql.numconnections=100 #最大连接数 # 执行服务器配置executor.maxThreads=50 #最大线程数executor.port=12321 #端口号(如修改,请与web服务中一致)executor.flow.threads=30 #线程数 5.4.用户配置 进入azkaban web服务器conf目录,修改azkaban-users.xml，vi azkaban-users.xml 增加 管理员用户1234567&lt;azkaban-users&gt; &lt;user username=&quot;azkaban&quot; password=&quot;azkaban&quot; roles=&quot;admin&quot; groups=&quot;azkaban&quot; /&gt; &lt;user username=&quot;metrics&quot; password=&quot;metrics&quot; roles=&quot;metrics&quot;/&gt; &lt;user username=&quot;admin&quot; password=&quot;admin&quot; roles=&quot;admin,metrics&quot; /&gt; &lt;role name=&quot;admin&quot; permissions=&quot;ADMIN&quot; /&gt; &lt;role name=&quot;metrics&quot; permissions=&quot;METRICS&quot;/&gt;&lt;/azkaban-users&gt; 6.启动6.1.web服务器123456789在azkaban web服务器目录下执行启动命令bin/azkaban-web-start.sh注:在web服务器根目录运行或者启动到后台nohup bin/azkaban-web-start.sh 1&gt;/tmp/azstd.out 2&gt;/tmp/azerr.out &amp;#关闭bin/azkaban-web-shutdown.sh 6.2.执行服务器123456在执行服务器目录下执行启动命令bin/azkaban-executor-start.sh注:只能要执行服务器根目录运行#关闭bin/azkaban-executor-shutdown.sh 7.登录测试&emsp;启动完成后,在浏览器(建议使用谷歌浏览器)中输入https://服务器IP地址:8443 ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login. 1https://192.168.0.11:8443/","tags":[{"name":"azkaban","slug":"azkaban","permalink":"http://yoursite.com/tags/azkaban/"}]},{"title":"azkaban实例","date":"2017-04-16T04:47:24.936Z","path":"2017/04/16/bigdata/azkaban/azkaban实例/","text":"Azkaba内置的任务类型支持command、java,下面是一些azkaban的应用实例 1.Command类型单一job示例 1.创建job描述文件vi command.job123#command.jobtype=command command=echo &apos;hello&apos; 2.将job资源文件打包成zip文件1zip command.job 3.通过azkaban的web管理平台创建project并上传job压缩包首先：Create Project 4.上传zip包(Upload) 5.启动执行该job 2.Command类型多job工作流flow 1.创建有依赖关系的多个job描述：第一个job：foo.job123# foo.jobtype=commandcommand=echo foo 第二个job：bar.job依赖foo.job 12345# bar.jobtype=command#依赖上一个foo.jobdependencies=foocommand=echo bar 2.将所有job资源文件打到一个zip包中 3.在azkaban的web管理界面创建工程并上传zip包 4.启动工作流flow 3.HDFS操作任务 1.创建job描述文件123# fs.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz 2.将job资源文件打包成zip文件 3、通过azkaban的web管理平台创建project并上传job压缩包4、启动执行该job 4.MAPREDUCE任务Mr任务依然可以使用command的job类型来执行 1.创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar）123# mrwc.jobtype=commandcommand=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout 2.将所有job资源文件打到一个zip包中 3、在azkaban的web管理界面创建工程并上传zip包4、启动job 5.HIVE脚本任务创建job描述文件和hive脚本1.Hive脚本： test.sql123456use default;drop table aztest;create table aztest(id int,name string) row format delimited fields terminated by &apos;,&apos;;load data inpath &apos;/aztest/hiveinput&apos; into table aztest;create table azres as select * from aztest;insert overwrite directory &apos;/aztest/hiveoutput&apos; select count(1) from aztest; 2.Job描述文件：hivef.job1234# hivef.jobtype=command#-f 执行的是文件，-e执行的命令command=/home/hadoop/apps/hive/bin/hive -f &apos;test.sql&apos; 3、将所有job资源文件打到一个zip包中4、在azkaban的web管理界面创建工程并上传zip包5、启动job","tags":[{"name":"azkaban","slug":"azkaban","permalink":"http://yoursite.com/tags/azkaban/"}]},{"title":"第五章 函数式对象","date":"2017-01-23T16:00:00.000Z","path":"2017/01/24/functionalProgrammingInScala/第五章 函数式对象/","text":"1.类ration的规格说明书1有理数是一种可以表达式比率 n/d 的数字,这里n和d是数字,其中d不能为零,n被称为分子,d被称为分母 2.创建rational12345678910111213141516class Rational(n:Int, d:Int)/*这行代码里首先应当注意到的是如果类没有主体,就不需要指定一对空的花括号(当然,如果你想指定也是可以的),在类名Rational之后的括号里的n和d,被称为类参数,scala编译器会收集这两个参数并创造出同样的两个参数的主构造器Java类具有可以带参数的构造器,而scala类可以直接带参数,scala的写法更简洁,类参数可以直接在类的主体中使用没有必要定义字段然后写赋值函数把构造器的参数复制到字段里,这里无形省略了很多固定写法,尤其是对小类scala编译器将把类内部的任何既不是字段也不是方法定义的代码编译至主机构造器中,例如:*/class Rational(n:Int, d:Int) &#123; println(&quot;Create &quot;+n+&quot;/&quot;+d) //主构造器中的内容&#125;/*scala编译器把这段代码的println调用放进Rational的主构造器,因此,println调用将在每次创建新的Rational实例时打印这条信息*/ 不可变对象的权衡12345678910/*不可变对象提供了若干可变对象的优点和一个潜在的缺点1.不可变对象常常比可变对象更易理清头绪,因为他们没有回随着时间变化的复杂的状态空间2.其次,你可以很自由的传递不可变对象,但是对于可变对象而言,传递给其他代码之前,需要先建造一个以防万一的副本3.一旦不可变对象完成构造之后,就不会有线程因为并发访问而破坏对象内部状态,因为根本没有线程可以改变不可变对象的状态4.不可变对象让哈希表键值更安全,比方说,如果可变对昂在进入HashSet之后被改变,那么你下一次查找这个HashSet时就找不到这个对象了#缺点有时需要复制很大的对象表而可变对象的更新可以在原址发生,有些情况下这会变成难以快速完成而可能产生性能瓶颈*/ 3.重新实现toString方法123class Rational(n:Int, d:Int) &#123; override def toString = n + &quot;/&quot; + d //override修饰符说明这是对原有方法定义的重载&#125; 4.检查先决条件123456789//确保分母不能为零,两种做法://方式一class Rational(n:Int, d:Int) &#123; require(d != 0) //require方法定义在对象Predef中 override def toString = n + &quot;/&quot; + d &#125;/*require方法带有一个布尔型参数,如果传入的值为真,require将正常返回,反之,require将抛出llegalArgumentException阻止对象被构造*/ 5.添加字段123456789101112131415161718192021222324252627282930313233343536373839class Rational(n:Int, d:Int) &#123; require(d != 0) override def toString = n + &quot;/&quot; + d def add(that: Rational): Rational =&#123; new Rational(n*that.d + that.n*d, d*that.d) //n和d是构造器中的n和d,add方法中是拿不到该属性的,但是对象可以拿到 &#125;&#125;/*尽管类参数n和d都在add代码可引用的范围内,但是add方法仅能访问调用对象自身的值,并不能访问that对象的d和n,因为that并不是调用add的Rational对象,所以上述代码回报下面的异常:*/Error:(5, 25) value d is not a member of Rational new Rational(n*that.d + that.n*d, d*that.d)Error:(5, 46) value d is not a member of Rational new Rational(n*that.d + that.n*d, d*that.d)/*如果想要访问that的d和n,需要把他们放在字段中,如下*/class Rational(n:Int, d:Int) &#123; require(d != 0) val number = n val denom = d override def toString = number + &quot;/&quot; + denom def add(that: Rational): Rational =&#123; new Rational(n*that.denom + that.number*d, d*that.denom) &#125;&#125;/*尽管n和d在类范围内有效,但因为他们只是构造器的一部分,所以scala编译器不会为他们自动构造字段,所以我们要手动添加字段(即number和denom)*//*我们之前不能在对象外部直接访问有理数的分子和分母,现在可以了,只要访问公共的number,denom字段即可*/val r = new Rational(1, 2)r.numberr.denom 6.自指向123456789101112/*关键字this指向当前执行方法被调用的对象实例*///下面的方法测试有理数是否小于传入的参数def lessThan(that:Rational) = &#123; this.number * that.denom &lt; that.number * this.denom&#125;//下面的方法返回有理数和参数中的较大者def max(that: Rational) = &#123; if (this.lessThan(that)) that else this //如果返回的是this,表示返回的是当前对象&#125; 7.辅助构造器123456789101112131415161718192021222324252627/*有时候一个类里需要多个构造器,scala里主构造器之外的构造器被称为辅助构造器,比如:分母为1的有理数只写分子的话就更为简洁,写成这样Rational(5), 这就需要给Rational添加只传分子的辅助构造器并预设分母为1,如下:*/class Rational(n:Int, d:Int) &#123; require(d != 0) val number = n val denom = d def this(n:Int) = this(n,1) //辅助构造器 override def toString = number + &quot;/&quot; + denom def add(that: Rational): Rational =&#123; new Rational(n*that.denom + that.number*d, d*that.denom) &#125; def lessThan(that:Rational) = &#123; this.number * that.denom &lt; that.number * this.denom &#125; def max(that: Rational) = &#123; if (this.lessThan(that)) that else this &#125;&#125;/*scala里的每个辅助构造器的第一个动作都是调用同类的别的构造器,换句话说,每个scala类的每个辅助构造器都是以 this(...) 形式开头的,被调用的构造器就可以是主构造器,也可以是源文件中早于调用构造器定义的其他辅助构造器,规则的根本结果就是每个scala的构造器调用最终结束于对主构造器的调用,因此主构造器是类的唯一入口点scala的类里面只有主构造器可以调用超类的构造器*/ 8.私有字段和方法12345678910111213class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125;&#125; 9.定义操作符12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849/*有理数(分数)的加法写成如下的形式:x + y而不用写成:x.add(y)*/class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125; def +(that: Rational): Rational = &#123; new Rational( number * that.denom + that.number * denom, denom * that.denom ) &#125; def *(that: Rational): Rational = &#123; new Rational(number*that.number,denom * that.denom) &#125; override def toString = number + &quot;/&quot; + denom&#125;/*测试*/object Rational&#123; def main(args: Array[String]): Unit = &#123; val x = new Rational(3,4) val y = new Rational(1,2) val z = x + y //也可以写成 x.+(y)//不过这样写可读性不佳 //按照scala的优先级规则,Rational的*方法比+方法优先级更高 x + x * y //等同于 x + (x * y) &#125;&#125; 10.scala的标识符和命名规范1234567891011121314151617181920#字母数字标识符:以字母或下划线开始,之后可以跟字母,数字,或下划线/*scala遵循Java的驼峰式标识符习惯,例如:toString和HashSet,,尽管下划线在标识符内是合法的,但是scala程序里并不常用,部分原因是为了保持与Java一致,同样也由于下划线在scala代码里有许多其他非标识符用法,因此,最好避免使用像to_string, _init_ 这样的标识符字段/方法参数/本地变量/还有函数的驼峰式名称,应该以小写字母开始,如:length, flatMap类和特质的驼峰式名称应该以大写字母开始,如:BigInt, List scala与Java的习惯不一致的地方在于常量名,scala里constant这个词并不等同于val,尽管val被初始化之后的确保持不变,但他仍然是变量在Java里,习惯上常量名称全都是大写的,用下划线分割单词,如:MAX_VALUE或PI,在scala里,习惯只是第一个字母必须大写,因此Java风格的常量名在scala里也可以用,但是scala的习惯是常数也用驼峰式风格,如:XOffset操作符标识符由一个或多个操作符字符组成,操作符字符是一些+, :, ?, ~, # 混合标识符:由字母数字组成,后面跟着下划线和一个操作符标识,如: unary_+ 被用作定义一元的&quot;+&quot; 操作符的方法名,或 myvar_= 被用作定义赋值操作符的方法名,字面量标识符,使用反引号 `....` 包括的任意字符串,如:`x` `&lt;clinit&gt;` `yield`在Java的Thread类中访问静态的yield方法是他的典型用例,你不能写Thread.yield() ,因为yield是scala的保留字,然而可以在反引号里引用方法的名称,例如:Thread.`yield`()*/ 11.方法 重载12345678910111213141516171819202122232425262728293031323334353637/*上述 * 的操作符数只能是有理数,所以对于有理数r不能写r * 2 ,只能写成r* new Rational(2) ,这样写很不美观,为了让Rational用起来方便,可以在类上增加能够执行有理数和整数之间的加法和乘法的新方法*/class Rational(n:Int, d:Int) &#123; require(d != 0) private val g = gcd(n.abs, d.abs) val number = n / g val denom = d / g def this(n:Int) = this(n,1) //计算传入的两个Int的最大公约数 private def gcd(a: Int, b: Int):Int = &#123; if (b==0) a else gcd(b, a%b) &#125; def +(that: Rational): Rational = &#123; new Rational( number * that.denom + that.number * denom, denom * that.denom ) &#125; def + (i: Int): Rational = &#123; new Rational(number + i*denom, denom) &#125; def *(that: Rational): Rational = &#123; new Rational(number*that.number,denom * that.denom) &#125; def * (i: Int):Rational = &#123; new Rational(number * i, denom) &#125; override def toString = number + &quot;/&quot; + denom&#125; 12.隐式转换12345678910/*上面的做法可以对 r*2 进行计算了,但是 2 * r 还是不能进行,因为2*r等同于2.*(r),因此这是在整数2上的方法调用,但Int类没有带Rational参数的乘法不过scala有另外的方法解决这个问题,可以创建在需要的时候自动把整数转换为有理数的隐式转换,如下:*/implicit def intToTational(x: Int) = new Rational(x)/*上述代码定义了从Int到Rational的转换方法,方法前面的implicit修饰符告诉编译器可以在一些情况下自动调用*/val x = new Rational(3,4)val z = 2 * x","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]},{"title":"scala初探","date":"2017-01-05T00:55:29.000Z","path":"2017/01/05/functionalProgrammingInScala/scala/","text":"1.变量定义 先写变量后写类型12345678scala&gt; val msg: java.lang.String = &quot;hello world!&quot;msg: String = hello world!//或者scala&gt; val msg: String = &quot;hello world!&quot; #先写变量名称,再写变量类型msg: String = hello world! #定义了一个名称为msg的变量,类型为String, 值为 &quot;hello world!&quot;scala&gt; 自动类型推断12345#也可以省略类型的定义,scala会自动推断出变量的类型scala&gt; val msg = &quot;hello world!&quot; #因为值的类型是String,所以可以推断出msg的类型是String,所以在定义变量的时候可以省略对msg类型的推断msg: String = hello world! scala&gt; val类型变量不能修改,var可以123456789101112131415161718#val类型的变量不能再次对其进行赋值操作scala&gt; msg = &quot;goodby cruel world!&quot;&lt;console&gt;:8: error: reassignment to val msg = &quot;goodby cruel world!&quot; ^#var可以重复赋值scala&gt; var msg_var: String = &quot;hello world!&quot;msg_var: String = hello world!#对变量重新赋值scala&gt; msg_var = &quot;hello 2222 world!&quot;msg_var: String = hello 2222 world!scala&gt; 2.函数定义 函数的基本结构1234567def max(x: Int, y:Int): Int = &#123; if(x &gt; y) x else y&#125;#scala的条件表达式可以像Java的三元操作符那样生成结果值if(x &gt; y) x else y #scala中的if/else不仅控制语句的执行流程,同时有返回值,所以他不等同于Java中的if/else#等同于Java中的:(x &gt; y)? x: y 函数的返回值类型123#函数的返回值类型可以不用写,因为可以使用函数体推断出来,但是如果函数是递归的,那么必须明确的说明返回值的类型#尽管如此,显示的说明函数结果类型也经常是一个好主意,这种类型标注可以使代码便于阅读,因为读者不用研究函数体之后再去猜测结果类型 函数体12如果函数仅包含一个语句,那么连花括号都可以选择不谢,这样max函数就可以写成:def max2(x: Int, y:Int): Int = if(x &gt; y) x else y 没有参数和返回值的函数123456 scala&gt; def max2(x: Int, y:Int): Int = if(x &gt; y) x else ymax2: (x: Int, y: Int)Int scala&gt; def greet() = println(&quot;Hello world!&quot;)greet: ()Unit #greet是函数名, ()说明函数不带参数, Unit是greet的结果类型,指的是函数没有有效的返回值 3.编写scala脚本1234567891011121314151617#在hello.scala文件中,有如下的代码:println(&quot;hello world , from a script! &quot;)#执行上述脚本文件$ scala hello.scala#系统输出hello world , from a script! #传参,scala脚本的命令行参数保存在名为args的scala数组中println(&quot;hello world , from &quot; + args(0) + &quot; a script! &quot;) //注意scala中的数组是以()去取元素的,而不是像Java中是以[]去取元素#再次执行脚本$ scala hello.scala xxxxhello world , from xxxx a script! 4.用while循环,用if做判断123456789#在printargs.scala文件里,输入以下代码测试whilevar i = 0while (i &lt; args.length) &#123; println(args(i)) i += 1&#125;#上述代码并不是scala推荐的代码风格,在这里只是有助于解释while循环#在scala中并没有++/--,必须写成+= / -=这样的操作 5.用foreach和for做枚举 foreach1234567891011121314151617181920#上面所写的while循环的编码风格被称之为指令式编程(即:逐条执行指令,并经常改变不同函数之间的共享状态,在Java/C++/C这些语言中常见),在scala中更偏向的是函数式编程,如下:#在pa.scala文件中,如下代码:args.foreach(arg=&gt;println(arg))#执行$ scala pa.scala xx1 xx2 xx3#打印xx1xx2xx3#在上述例子中,scala解释器可以推断arg的类型为String,因为String是调用foreach的那个数组的元素类型,当然也可以更明确的给args加上类型名:args.foreach( (arg: String) =&gt;p rintln(arg))#更加简洁的写法args.foreach(p rintln(arg)) for123456#scala也是提供了像指令式的forfor (arg &lt;- args)&#123; println(arg)&#125;#&lt;- 右侧是已经在前面见过的args数组,&lt;-左侧的arg是val的名称(不是var,这里一定是val),尽管arg可能感觉像var,因为每次枚举都会得到新的值,但这的确是val,因为他不能在for表达式的函数体中被重新赋值,所以,对于args数组的每个元素,枚举的时候都会创建并初始化新的arg值,然后调用执行for的函数体","tags":[{"name":"scala","slug":"scala","permalink":"http://yoursite.com/tags/scala/"}]}]